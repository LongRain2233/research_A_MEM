{
    "is_related_to_agent_memory": true,
    "has_github_link": true,
    "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
    "problem_and_motivation": "LLM-based web agents在复杂、多轮的知识密集型搜索任务中，受限于有限的上下文窗口（如32k tokens）。流行的ReAct范式会不断将完整的交互历史（Thought, Action, Observation）追加到上下文中，导致在问题解决前就耗尽上下文预算，迫使长轨迹任务提前终止。现有方法如简单截断历史会破坏推理的连续性。本文的核心切入点是：通过**周期性的上下文摘要**，将冗长的交互历史压缩为紧凑的、可重启的推理状态，从而绕过上下文限制，实现无限探索。核心假设是：摘要可以保留关键证据并指导后续探索。",
    "core_method": "**ReSum范式核心数据流**：\n1.  **初始化**：轨迹以用户查询q开始，历史H0 = (q)。\n2.  **迭代探索**：在每一轮t，智能体根据当前历史H_{t-1}生成推理τ_t和工具调用a_t，执行工具获得观察o_t，并更新历史：H_t = H_{t-1} ◦ (τ_t, a_t, o_t)。\n3.  **上下文摘要**：当**压缩触发器**（如接近上下文限制）激活时，调用摘要工具π_sum，将累积历史H_t总结为一个结构化摘要s。随后，将压缩状态q' = (q, s)设置为新的历史起点，重置工作历史：H_t ← (q')。\n4.  **轨迹终止**：智能体继续从摘要状态探索，直到生成最终答案或达到资源限制。\n\n**关键创新模块**：\n- **摘要工具专业化**：开发了ReSumTool-30B，通过使用从强大开源模型收集的⟨Conversation, Summary⟩对，对Qwen3-30B-A3B-Thinking进行监督微调，使其专门擅长从冗长、嘈杂的交互中提取可验证证据和识别信息缺口。\n- **ReSum-GRPO训练算法**：\n  - **轨迹分割**：ReSum在摘要发生时自然地将长轨迹分割为K+1个片段。每个片段作为一个独立的训练episode。\n  - **优势广播**：从完整轨迹的最终答案计算轨迹级奖励R_g ∈ {0, 1}，在组内归一化得到优势Â_g，然后将该优势**广播**给该轨迹内的所有片段（Â_g^(i) = Â_g）。\n  - **目标函数**：采用GRPO的目标函数，但使用广播后的优势进行计算，鼓励智能体有效利用摘要并从压缩状态成功推理。",
    "key_experiments_and_results": "**核心数据集**：GAIA、BrowseComp-en、BrowseComp-zh。\n**最强对比基线**：\n1.  **ReAct**：标准的追加全部历史范式。\n2.  **Recent History**：简单截断，仅保留最近22k tokens。\n3.  **标准GRPO训练**：在ReAct范式下进行强化学习训练。\n\n**关键定量提升**：\n- **训练无关设置**：在WebSailor-30B上，使用ReSumTool-30B，ReSum在BrowseComp-en上的Pass@1达到16.0%，相比ReAct（12.8%）绝对提升3.2个百分点（相对提升25%）。在GAIA上，Pass@1从45.0%提升至47.3%。\n- **训练相关设置**：经过ReSum-GRPO训练（仅1K样本）的WebSailor-30B（WebResummer-30B），在BrowseComp-zh上Pass@1达到33.3%，相比其ReAct基线（23.9%）绝对提升9.4个百分点（相对提升39.3%）；在BrowseComp-en上达到18.3%，相比基线（12.8%）绝对提升5.5个百分点（相对提升43.0%）。\n- **消融实验核心结论**：ReSumTool-30B作为摘要工具，其性能优于其基础模型Qwen3-30B，且与更大的模型（如Qwen3-235B、DeepSeek-R1-671B）表现相当甚至更优，证明了专业化训练的有效性。",
    "limitations_and_critique": "**方法边界与未解决问题**：\n1.  **依赖外部摘要工具**：ReSum需要调用一个独立的、经过专门训练的摘要模型（ReSumTool-30B），这增加了系统复杂性和部署成本。\n2.  **规则驱动的摘要触发机制**：摘要的触发基于预设规则（如接近上下文限制），而非由智能体自主、智能地决定何时进行摘要。这在动态、不确定的搜索环境中可能不是最优的。\n3.  **摘要质量瓶颈**：整个范式的有效性高度依赖于摘要工具的质量。如果摘要丢失了关键信息或引入了错误，后续探索将基于有缺陷的上下文进行，可能导致任务失败。\n4.  **极端场景下的崩溃风险**：对于信息极度分散、需要极长探索路径（远超预设工具调用次数上限）的任务，即使有摘要机制，也可能因资源耗尽而失败。此外，如果初始探索方向完全错误，摘要可能固化错误路径，导致无法纠正。",
    "ai_inspiration_and_opportunities": "**可迁移的组件与思想**：\n1.  **“压缩-重启”的通用记忆管理范式**：ReSum的核心思想——将冗长的工作记忆（交互历史）周期性地压缩为语义密集的长期记忆（摘要），并以此为基础重启推理——可以迁移到任何需要长序列交互的智能体场景，如**长期对话系统**（压缩多轮对话历史）、**复杂任务规划**（压缩子任务执行历史）、**代码生成与调试**（压缩多次尝试与错误信息）。\n2.  **轨迹分割与优势广播的RL训练技巧**：ReSum-GRPO中，将因外部操作（如摘要）自然分割的长轨迹视为多个训练episode，并将轨迹级奖励信号（优势）广播给所有片段的方法，为解决**稀疏奖励、长视界RL任务**提供了新思路。这适用于任何包含“检查点”或“状态保存”机制的任务。\n\n**低算力验证的新idea**：\n1.  **轻量级自主摘要触发学习**：在资源受限下，可以研究让小型策略模型学习一个简单的二分类器，基于当前历史长度、信息熵或探索不确定性，预测是否需要进行摘要，替代固定的规则触发。这只需收集少量（摘要时机， 轨迹成功/失败）标签对进行微调。\n2.  **分层摘要与检索**：借鉴ReSum思想，可以设计一个**两级记忆系统**：第一级是固定长度的滑动窗口工作记忆；第二级是外部向量数据库，存储历史摘要的嵌入。当需要时，智能体可以不仅基于最新摘要，还能检索相关的历史摘要片段。这可以用开源向量数据库（如FAISS）低成本实现，以增强长期信息保持能力。",
    "source_file": "ReSum Unlocking Long-Horizon Search Intelligence via Context Summarization.md"
}