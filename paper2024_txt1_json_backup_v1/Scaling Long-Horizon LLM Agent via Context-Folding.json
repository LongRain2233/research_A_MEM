{
    "is_related_to_agent_memory": true,
    "has_github_link": false,
    "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
    "problem_and_motivation": "现有LLM智能体在处理长视野任务（如深度研究、软件工程）时，面临线性累积的交互历史导致上下文无限膨胀的根本性约束。这引发两个关键缺陷：1. **性能下降**：LLM难以在超长上下文中有效利用相关信息；2. **效率低下**：注意力机制的二次方缩放和KV-cache管理开销巨大。现有方法如基于总结的方法会突然中断智能体的工作流，而多智能体系统则依赖难以泛化的手工流程。本文的核心切入点是让智能体**主动管理其工作上下文**，通过分支和折叠机制，将耗时的子任务卸载到独立分支中，完成后仅保留结果摘要，从而维持主上下文的紧凑性。",
    "core_method": "#### **核心数据流与机制**\n智能体通过两个特殊工具主动管理上下文：\n1.  **`branch(description, prompt)`**：从主线程分支，使用独立的32K令牌工作上下文处理子任务`q'`。\n2.  **`return(message)`**：折叠该分支内的所有中间步骤（动作-观察对），仅将结果`message`附加到主线程后返回。\n\n#### **上下文管理器 \\(\\mathcal{F}\\)**\n在每次生成时，策略 \\(\\pi_\\theta(a_i \\mid q, \\mathcal{F}(\\tau_{<i}))\\) 的上下文是经过折叠的历史。管理器 \\(\\mathcal{F}\\) 会识别`branch`和`return`调用之间的所有交互，并将其从主上下文中移除，仅保留`return`的消息。例如，历史 \\((a_1, o_1, a_2, [o_2, a_3, o_3, a_4]_{\\text{branch1}}, o_4, ...)\\) 被折叠为 \\((a_1, o_1, a_2, o_4, ...)\\)，其中`branch1`内的步骤被`o_4`（即`return`的消息）替代。\n\n#### **FoldGRPO强化学习算法**\n采用**分组相对策略优化（GRPO）**，并引入两个关键创新：\n1.  **动态折叠的LLM上下文**：在优化策略时，对历史 \\(\\tau_{i,<t}\\) 应用相同的 \\(\\mathcal{F}\\) 操作，保持训练与推理的一致性。\n2.  **密集的令牌级过程奖励**：\n    *   **未折叠令牌惩罚（Unfolded Token Penalty）**：当主线程上下文长度超过工作限制（32K）的50%时，对主线程中除创建分支外的所有令牌施加 \\(Q_{i,t} = -1\\)，迫使耗令牌操作进入分支。\n    *   **超范围惩罚（Out-of-Scope Penalty）**：使用GPT-5-nano判断分支内行为是否超出子任务范围，若是则对该分支所有令牌施加 \\(Q_{i,t} = -0.2\\)。\n    *   **失败惩罚（Failure Penalty）**：对失败的工具调用回合的所有令牌施加 \\(Q_{i,t} = -1\\)。",
    "key_experiments_and_results": "#### **核心数据集与基线**\n在**BrowseComp-Plus (BC-Plus)**（深度研究）和**SWE-Bench Verified (SWEB-V)**（软件工程）两个长视野基准上评估。最强对比基线包括：\n1.  **ReAct Agent**：保持全部上下文的基线，使用327K（长上下文）、131K/65K（中上下文）、32K（短上下文）三种配置。\n2.  **Summary Agent**：上下文满时触发总结的基线，最大上下文32K，允许10次总结会话。\n3.  **100B+参数大模型**（如GPT-5、DeepSeek-V3.1）的ReAct Agent作为性能上限参考。\n\n#### **关键定量结果**\n使用Seed-OSS-36B基础模型，在最大32K活动上下文和最多10个分支的设置下：\n*   **BrowseComp-Plus (Pass@1)**：经过FoldGRPO训练的Folding Agent达到 **62.0%**，相比使用327K上下文的ReAct基线（Seed-OSS-36B，47.8%）绝对提升 **14.2个百分点**（相对提升29.7%）。相比同规模（32K）的Summary Agent+RL（52.7%）绝对提升 **9.3个百分点**。\n*   **SWE-Bench Verified (Pass@1)**：FoldGRPO训练的Folding Agent达到 **58.0%**，相比327K ReAct基线（55.2%）绝对提升 **2.8个百分点**（相对提升5.1%）。相比同规模Summary Agent+RL（55.0%）绝对提升 **3.0个百分点**。\n*   **效率**：Folding Agent的**峰值活动上下文长度仅为32K**，而达到相当性能的ReAct基线需要327K上下文，实现了**10倍的上下文压缩**。\n\n#### **消融实验核心结论**\n1.  **FoldGRPO vs. 标准GRPO**：在BC-Plus上，使用FoldGRPO（62.0%）相比使用标准GRPO（54.3%）绝对提升 **7.7个百分点**，证明过程奖励和动态折叠上下文对学习有效分支行为至关重要。\n2.  **RL的必要性**：未经RL的Folding Agent在BC-Plus上仅42.0%，RL训练带来**20.0个百分点的绝对提升**。\n3.  **行为统计**：FoldGRPO训练后，主线程长度（Main Len）从约22K压缩至约8K，分支内任务专注度（Scope）从0.762提升至0.895，任务完成率（Finish）从0.738提升至0.935。",
    "limitations_and_critique": "#### **方法边界与理论漏洞**\n1.  **任务结构依赖**：该方法基于**计划-执行框架**，假设任务可被清晰地分解为独立的子任务。对于高度交织、难以模块化的任务（如需要持续交叉引用的创造性写作或复杂数学证明），分支的边界可能模糊，导致折叠时丢失关键信息或产生无效摘要。\n2.  **摘要质量瓶颈**：分支结果的摘要完全由智能体的`return`调用生成，缺乏质量保证机制。低质量的摘要（如遗漏关键细节或引入错误）会污染主上下文，导致后续推理失败，且错误会沿任务链传播。\n3.  **并行与嵌套限制**：当前实现**禁止在分支内创建新的嵌套分支**，并默认采用顺序执行。论文探索了并行分支，但在深度优先任务（如BC-Plus）上未显示优势。这限制了该方法在需要广度优先探索或递归子任务分解场景下的适用性。\n\n#### **极端崩溃场景**\n*   如果智能体错误地将一个**本应保持在线状态的核心推理步骤**（例如，一个需要持续参考和更新的假设）放入分支并折叠，将导致后续步骤失去关键前提，整个任务链可能崩溃。\n*   当面对**对抗性或模棱两可的子任务描述**时，Out-of-Scope惩罚依赖的外部模型（GPT-5-nano）可能误判，导致智能体被不当惩罚，从而学会过度保守，不敢进行必要的探索性工具调用。",
    "ai_inspiration_and_opportunities": "#### **可迁移的组件与思想**\n1.  **主动上下文管理范式**：将上下文管理从被动的、启发式的总结转变为智能体**可学习的、基于任务语义的主动技能**。这一思想可迁移到任何需要长期状态维护的序列决策AI中，例如对话机器人（主动总结对话段落）、游戏AI（总结探索过的区域特征）、机器人任务规划（折叠已完成的动作序列）。\n2.  **过程奖励设计框架**：`Unfolded Token Penalty`和`Out-of-Scope Penalty`提供了一种**低算力可验证**的模板，用于塑造智能体的结构化行为。其他AI可以借鉴此框架，设计针对特定领域低效行为（如冗余API调用、无关信息检索）的轻量级惩罚信号，无需依赖昂贵的最终结果奖励。\n\n#### **低算力/零算力下的改进方向**\n1.  **基于规则的分支触发启发式**：在无法进行RL训练的情况下，可以预先定义**分支触发规则**。例如，当检测到工具调用输出长度超过阈值（如500 tokens）、或连续进行同一类型工具调用（如连续3次搜索）时，系统自动建议或强制智能体开启新分支。这可以部分实现论文中的上下文压缩效益。\n2.  **摘要验证与修正回路**：为缓解摘要质量瓶颈，可以引入一个**轻量级的验证步骤**。在分支`return`后，用一个极小的模型（如TinyLlama）或规则检查摘要是否包含原始分支中的关键实体、数字和结论。若缺失，则触发一个简短的提示让智能体补充。这能以极低成本提升折叠的可靠性。\n3.  **分层折叠策略探索**：论文提到未来可探索多层折叠。一个低算力研究契机是：**仅对“工具执行轨迹”进行折叠，而保持“核心推理链”完全展开**。这可以通过在过程奖励中区分“工具调用令牌”和“推理令牌”来实现，优先折叠前者，从而在压缩上下文的同时最大程度保留逻辑连贯性。",
    "source_file": "Scaling Long-Horizon LLM Agent via Context-Folding.md"
}