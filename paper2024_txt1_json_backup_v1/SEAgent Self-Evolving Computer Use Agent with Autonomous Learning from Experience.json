{
    "is_related_to_agent_memory": true,
    "has_github_link": true,
    "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience",
    "problem_and_motivation": "当前计算机使用智能体（CUA）严重依赖昂贵的人工标注数据，导致其在面对缺乏标注的、新颖或专业软件时表现不佳。本文旨在解决CUA在无人类监督下，自主探索并掌握陌生软件的核心挑战。现有方法的关键缺陷在于：1) 无法在陌生软件环境中自动生成可执行的探索任务；2) 缺乏对任务完成情况的精确、分步评估。本文的切入点是构建一个完全自主的、基于经验学习的智能体自进化框架，其核心假设是：通过课程生成、分步状态评估和强化学习，智能体可以像人类一样通过试错在陌生软件中自我进化。",
    "core_method": "#### **核心数据流**\n1.  **输入**：陌生软件的初始GUI截图。\n2.  **处理**：\n    *   **课程生成器** (Curriculum Generator)：基于**软件指南记忆** (software guidebook memory `U`) 和智能体当前能力，生成由易到难的任务指令集 `T_p`。该记忆 `U` 根据世界状态模型的评估结果 (`I_p`, `C_p`) 迭代更新：`U_{p+1}, T_{p+1} = M_task(U_p, I_p, C_p)`。\n    *   **执行器模型** (Actor Model `π`)：执行任务，产生状态-动作轨迹 `H`。\n    *   **世界状态模型** (World State Model `M_state`)：接收完整轨迹 `H`，进行分步评估，将每个动作分类为正确 (`a_T`) 或失败 (`a_F`)，并提供GUI状态变化描述 `C`。\n3.  **输出/优化**：基于 `M_state` 提供的分步奖励信号，通过**经验强化学习**更新执行器策略 `π`。\n#### **关键技术创新**\n*   **分步奖励信号**：`M_state` 模型（基于Qwen2.5-VL-7B微调）以完整轨迹为输入，提供高精度的分步成功/失败判断，替代了传统稀疏的最终结果奖励。\n*   **混合策略优化损失**：结合对成功和失败动作的学习。\n    *   **对抗模仿 (AI)**：惩罚失败动作，损失函数为 `L_AI(π_θ) = E_ν[-log(π_θ(a|s,I) / π_ref(a_F|s,I))]`，促使策略偏离失败行为。\n    *   **分组相对策略优化 (GRPO)**：鼓励正确动作，基于可验证奖励 `r(a^(i), a_T)` 计算相对优势 `A^(i)`。奖励函数 `r` 包含动作类型匹配指示函数和基于距离的奖励项 `r_dist`（如坐标L1距离、框IoU、文本BLEU）。\n*   **专家到通才策略**：先为每个软件训练专家智能体，再将其成功轨迹通过监督微调（SFT）蒸馏到一个通才模型，最后用SEAgent在多软件上进一步强化学习，性能超越专家集合。",
    "key_experiments_and_results": "#### **实验设计**\n*   **评估基准**：OS-World 中的五个专业软件（VScode, GIMP, Impress, VLC, Writer）。\n*   **基线模型**：开源CUA基线 **UI-TARS-7B-DPO** (成功率11.3%)，以及两种强化学习方法 **DigiRL** 和 **WebRL**。\n*   **核心指标**：任务成功率 (Success Rate, SR)。\n#### **主要结果**\n1.  **整体性能**：SEAgent采用专家到通才策略，在OS-World上的**整体成功率从基线UI-TARS的11.3%提升至34.5%**，绝对提升23.2个百分点，相对提升205.3%。\n2.  **组件有效性**：\n    *   **世界状态模型**：在AgentRewardBench上，其判断精度 (Precision) 达到71.6%，比其基础模型Qwen2.5-VL-7B (25.4%) **提升了46.2个百分点**，接近GPT-4o (72.1%)。\n    *   **训练策略对比**：SEAgent专家强化学习 (32.2%) 优于通用强化学习 (30.6%) 和直接SFT (27.9%)。专家到通才策略 (34.5%) 进一步超越专家集合 (32.2%)。\n3.  **消融实验**：在VScode上，仅使用GRPO和世界状态模型获得34.8% SR，加入对抗模仿 (AI) 后提升至37.7%。使用基础Qwen2.5-VL-72B作为奖励模型时，GRPO+AI仅获得11.6% SR，凸显了高质量奖励模型的关键作用。",
    "limitations_and_critique": "#### **原文局限性**\n1.  **奖励信号依赖仿真**：系统的自进化依赖于世界状态模型提供的奖励信号，而非来自真实环境的反馈。在复杂环境中学习稀疏奖励信号仍然是一个挑战。\n2.  **任务复杂度有限**：实验中的任务相对简单，人类专家可在20步内完成。系统尚未验证能否处理真实人类专家使用的、长达数小时的复杂工作流。\n3.  **泛化边界未知**：世界状态模型仅在Chrome数据上微调，虽表现出向其他软件的泛化能力，但其在更广泛、差异更大的软件生态（如专业IDE、3D建模软件）中的泛化性能未经验证。\n#### **潜在致命缺陷**\n*   **奖励模型幻觉风险**：若世界状态模型对复杂、多模态的GUI状态变化产生错误判断（幻觉），将提供错误的奖励信号，导致策略在错误方向上优化，甚至崩溃。\n*   **课程生成器瓶颈**：课程生成器的能力上限（基于LLM）可能限制其生成真正具有挑战性、能推动智能体能力边界任务的能力，导致进化停滞。\n*   **计算成本高昂**：框架涉及多个大模型（执行器、世界状态模型、课程生成器）的迭代交互与训练，对算力要求极高，限制了其在资源受限场景下的应用。",
    "ai_inspiration_and_opportunities": "#### **可迁移组件与思想**\n1.  **软件指南记忆 (Guidebook Memory)**：这种结构化的、可迭代更新的外部记忆机制，可用于任何需要长期探索和知识积累的智能体场景，如**游戏智能体**（记录地图、敌人行为模式）或**机器人操作**（记录物体属性和操作效果）。其“探索-更新-生成新任务”的循环是通用的课程学习范式。\n2.  **分步轨迹评估模型**：将完整交互历史作为输入进行细粒度评估的思路，可迁移至**具身智能**（评估机器人动作序列的有效性）或**对话智能体**（评估多轮对话的连贯性与目标达成度），提供比最终结果更丰富的训练信号。\n3.  **混合策略优化 (GRPO + AI)**：结合可验证奖励的GRPO和对失败行为的显式规避（AI），为训练**安全、可靠的序列决策智能体**提供了新范式，尤其适用于行动空间离散、错误代价高的场景。\n#### **低算力改进方向**\n1.  **轻量级奖励模型蒸馏**：研究如何将强大的世界状态模型（或GPT-4o的评估能力）蒸馏到更小的模型中（如3B参数），甚至设计**规则与模型混合的奖励函数**，在保证评估精度的同时大幅降低推理成本。\n2.  **课程生成的任务池复用与进化**：构建一个跨软件、跨智能体的**开源课程任务池**。新智能体无需从零开始探索，可从任务池中采样并贡献新任务，实现社区驱动的协同进化，极大降低个体训练成本。\n3.  **失败动作的元学习**：分析对抗模仿学习中学到的“避免失败”的模式，将其抽象为**可解释的规则或提示**，注入到智能体的初始策略或反思模块中，实现“零次”或“少次”的失败规避，加速训练收敛。",
    "source_file": "SEAgent Self-Evolving Computer Use Agent with Autonomous Learning from Experience.md"
}