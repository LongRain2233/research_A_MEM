{
    "is_related_to_agent_memory": true,
    "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
    "problem_and_motivation": "#### 核心问题\n长视野（long-horizon）LLM智能体（如深度研究、软件工程）的性能受限于其工作上下文（context）的长度。传统ReAct式智能体线性累积整个交互历史，导致上下文无限膨胀，引发**性能下降**（LLM难以从超长上下文中提取相关信息）和**效率低下**（注意力机制二次方复杂度及KV-cache管理开销）。\n\n#### 现有方法缺陷\n1.  **基于总结的方法**：当工作上下文满时触发事后总结。这会**突然中断**智能体的工作上下文和推理流，可能导致次优结果。\n2.  **多智能体系统**：依赖**手工定制**的、针对特定问题的工作流，难以泛化且抵抗端到端优化。\n\n#### 本文切入点\n提出**Context-Folding（上下文折叠）**机制，使智能体能够**主动管理**其工作上下文，通过程序性地分支和折叠子轨迹来处理子任务，从而在保持短期上下文完整的同时，自动管理长期上下文。",
    "core_method": "#### 核心机制：上下文折叠\n智能体通过两个特殊工具主动管理上下文：\n1.  **`branch(description, prompt)`**：从主线分支，使用独立的上下文处理子任务 `q'`。`description`是子任务摘要，`prompt`是详细指令。\n2.  **`return(message)`**：折叠该分支内生成的上下文并返回主线。`message`描述分支结果。调用后，智能体上下文切换回主线，并附加分支的模板化结果消息。\n\n#### 形式化建模\n上下文折叠智能体的概率模型为：\n\\[ p _ {\\theta} ^ {\\text {ContextFold}} (\\tau \\mid q) := \\prod_ {i \\in [ T ]} \\pi_ {\\theta} \\left(a _ {i} \\mid q, \\mathcal {F} \\left(\\tau_ {< i}\\right)\\right) \\]\n其中 \\(\\mathcal {F}\\) 是上下文管理器，它根据 `branch` 和 `return` 调用**折叠**分支内的交互历史。\n\n#### 训练算法：FoldGRPO\n为优化折叠行为，提出**Folded-context Group Relative Policy Optimization (FoldGRPO)**，关键创新包括：\n1.  **动态折叠上下文**：在策略优化时，对历史 \\(\\tau_{i, < t}\\) 应用上下文管理器 \\(\\mathcal{F}(\\cdot)\\)，而非附加全部历史。\n2.  **密集过程奖励**：在优势估计 \\(\\widehat{A}_{i,t}\\) 中加入**词元级过程奖励** \\(Q_{i,t}\\)，具体包括：\n    *   **未折叠词元惩罚**：当主线上下文长度超过工作限制的50%时，对主线中除创建分支外的所有词元施加 \\(Q_{i,t} = -1\\)，鼓励将词元密集型操作放入分支。\n    *   **超范围惩罚**：使用GPT-5-nano判断分支行为是否超出指定子任务，若是则对该分支所有词元施加 \\(Q_{i,t} = -0.2\\)。\n    *   **失败惩罚**：对失败的工具调用回合的所有词元施加 \\(Q_{i,t} = -1\\)。",
    "key_experiments_and_results": "#### 核心数据集与基线\n*   **数据集**：深度研究任务 **BrowseComp-Plus (BC-Plus)**，软件工程任务 **SWE-Bench Verified (SWEB-V)**。\n*   **最强对比基线**：\n    1.  **ReAct Agent**：保持全部上下文，使用**327K**最大上下文窗口的版本（Seed-OSS-36Bψ）作为主要对比。\n    2.  **Summary Agent**：上下文满时触发总结，最大上下文长度32K，允许10个总结会话。\n\n#### 主要定量结果\n*   在**BrowseComp-Plus**上，经过FoldGRPO训练的**Folding Agent**（使用32K活动上下文，最多10个分支）达到 **Pass@1 = 62.0%**。相比327K上下文的ReAct基线（Seed-OSS-36Bψ, Pass@1 = 47.8%），**绝对提升14.2个百分点（相对提升29.7%）**。\n*   在**SWE-Bench Verified**上，Folding Agent达到 **Pass@1 = 58.0%**。相比同基线（Pass@1 = 55.2%），**绝对提升2.8个百分点（相对提升5.1%）**。\n*   **强化学习的关键作用**：相比未经RL的Folding Agent，FoldGRPO在BrowseComp-Plus上带来**+20.0%的绝对提升**（从42.0%到62.0%），在SWEB-V上带来**+8.8%的绝对提升**（从49.2%到58.0%）。\n\n#### 消融实验核心结论\n*   **FoldGRPO vs. 标准GRPO**：在BrowseComp-Plus上，FoldGRPO比GRPO带来**+7.7%的绝对提升**（54.3% vs. 62.0%）。\n*   **行为分析**：FoldGRPO显著改善了上下文管理行为，将主线轨迹平均长度从约22K词元压缩至约8K词元，**上下文压缩率超过90%**，同时将子任务保持专注的准确率（Scope）从76.2%提升至89.5%。",
    "limitations_and_critique": "#### 方法边界与未解决问题\n1.  **任务结构依赖**：实验表明，该方法的优势在**深度优先（depth-first）** 的任务（如深度研究、代码调试）中更明显。对于**广度优先（breadwidth-first）** 的任务（如WideSearch），并行分支的潜力未被充分挖掘，初步实验显示并行分支版本并未带来分数提升。\n2.  **训练复杂度与成本**：方法依赖**端到端的强化学习**（FoldGRPO），这需要构建复杂的训练流程（如异步rollout、过程奖励计算），并消耗大量计算资源进行策略优化，提高了复现门槛。\n3.  **过程奖励的外部依赖**：**超范围惩罚**依赖于一个外部轻量级模型（GPT-5-nano）进行判断，这引入了额外的模型调用和潜在的判断偏差，并非完全自洽的优化。\n4.  **理论泛化性存疑**：智能体在训练时最多使用10个分支，但在推理时面对更复杂任务（如50个问题组合）时，需要自适应地使用平均32.6个分支。这种**长度泛化能力**的边界和理论保障尚未明确。\n\n#### 潜在崩溃场景\n在**高度动态、子任务间强耦合**的环境中，如果智能体错误地将一个本应在主线中持续跟踪状态的关键操作放入分支并折叠，可能导致返回主线后**丢失关键中间状态信息**，从而无法完成后续任务。",
    "ai_inspiration_and_opportunities": "#### 可迁移的组件与思想\n1.  **主动上下文管理范式**：将上下文管理从被动的、启发式的总结，转变为智能体**可学习的、与任务分解对齐的主动技能**。这一思想可迁移至任何受长上下文困扰的序列决策场景，如**长文档对话、复杂游戏对局分析、多步骤科学实验规划**。\n2.  **过程奖励设计模式**：为解决稀疏最终奖励下难以学习复杂技能（如分支管理）的问题，本文提供了**针对特定失败模式设计密集过程奖励**的模板（如未折叠惩罚、超范围惩罚）。其他AI可借鉴此模式，为学习**工具使用顺序、信息检索策略、安全约束遵守**等内在技能设计奖励。\n\n#### 低算力验证与改进方向\n1.  **零算力启发：模仿学习的起点**：无需进行昂贵的RL训练，其他研究者可以**直接使用本文提出的`branch`/`return`工具接口和“计划-执行”状态框架**，通过**高质量的人类示范或合成轨迹**进行监督微调（SFT），作为学习上下文折叠行为的低成本起点。\n2.  **轻量级改进：基于规则的过程奖励替代**：为降低训练复杂度，可以探索用**基于规则的、无需调用外部模型的替代方案**来实现过程奖励。例如，通过检测工具调用模式或输出关键词来判断是否“超范围”，或通过简单的词元计数阈值来触发惩罚，从而在保持训练效果的同时大幅降低实现成本。\n3.  **新研究契机：分层上下文折叠**：本文结尾提出了“多层折叠”的未来方向，即**分支本身可以进一步被折叠**。这启发了对**层次化任务分解与记忆压缩**的研究。一个具体的研究问题是：如何设计奖励或架构，让智能体自动识别何时需要创建“子分支”，并学习生成不同抽象层次的摘要，实现更深度的上下文压缩。",
    "source_file": "Scaling Long-Horizon LLM Agent via Context-Folding.md"
}