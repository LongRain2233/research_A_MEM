{
    "is_related_to_agent_memory": true,
    "title": "Scaling Agents via Continual Pre-training",
    "problem_and_motivation": "当前基于通用基础模型（如 Qwen2.5-72B）进行监督微调（SFT）或强化学习微调（RL）来构建深度研究智能体（Deep Research Agent）的方法存在根本性缺陷。其核心问题在于：通用模型缺乏**Agentic Inductive Biases（智能体归纳偏置）**，导致在微调阶段需要同时学习复杂的智能体行为（如多步推理、工具调用）和与专家演示的对齐，从而产生**内在的优化冲突**。这导致现有开源智能体模型（如 WebSailor-12.0、GLM-4.5-26.4、DeepSeek-V3.1-30.0）在 BrowseComp 等挑战性基准上，与 OpenAI Deep Research（51.5）存在巨大性能差距。本文的核心切入点是：**在预训练与后训练之间引入一个新的训练阶段——Agentic Continual Pre-training (Agentic CPT)**，旨在构建一个**预对齐的智能体基础模型**，使其天然具备智能体行为，从而为下游微调提供更优的起点。",
    "core_method": "本文提出 **AgentFounder**，其核心是 **Agentic Continual Pre-training (Agentic CPT)** 两阶段训练流程，并设计了两种无监督的数据合成方法。\n\n#### **数据合成方法**\n1.  **一阶动作合成（FAS）**：无需外部工具调用，离线生成训练数据。\n    *   **知识到问题的转化**：将网络爬取数据、工具调用历史等构建为 **实体-知识记忆库**。随机采样实体及其知识陈述，合成涵盖事实检索、数值计算、多跳推理等**多风格问题**。\n    *   **规划动作合成**：给定问题 Q，使用 LLM 生成 K 个**不同的第一步推理和工具调用计划**（例如，`思考：需要搜索X... 行动：Search(X)`），通过 **LLM-as-Judge 拒绝采样** 过滤低质量数据。\n    *   **推理动作合成**：针对已有答案的问题，要求 LLM 进行**两步逻辑推理**：第一步基于内部知识生成初步答案 A₁；第二步结合问题 Q 和所需知识，修正 A₁ 得到最终答案 A₂，并进行对齐验证。\n\n2.  **高阶动作合成（HAS）**：重用后训练阶段被丢弃的次优轨迹。\n    *   **步骤级扩展**：对于轨迹中第 k 步的上下文 Cₖ（包含原始问题和前 k-1 步），使用 LLM 生成 N 个**替代的“思考-调用”候选动作** Aₖ = {Sₖ⁽¹⁾, ..., Sₖ⁽ᴺ⁾}，与原始步骤 Sₖ⁽⁰⁾ 混合并随机打乱，形成 N+1 个选项序列 Ãₖ。\n    *   **对比决策-动作合成**：将扩展后的轨迹转化为**渐进式决策文本**。在每一步，模型从 Ãₖ 中选择一个选项（例如，“我将选择选项 n”），然后附上该选项对应的真实环境反馈 Rₖ。最终，根据原始轨迹的成功/失败标签 J ∈ {0,1}，追加判断文本“我的决策是{正确/错误}”。\n\n#### **两阶段训练策略**\n*   **CPT Stage 1**：使用约 200B token 的 FAS 数据和短 HAS 数据，上下文长度 32K，进行初步的智能体行为学习。\n*   **CPT Stage 2**：使用约 100B token 的精选高质量 HAS 数据，上下文长度扩展至 128K，用于学习复杂的动作空间和长视野规划策略。\n损失函数为标准的下一个 token 预测交叉熵损失：\\(\\mathcal{L} = - \\sum_{t=1}^{T} \\log P(x_{t+1} \\mid x_{1}, x_{2}, \\dots, x_{t})\\)。",
    "key_experiments_and_results": "实验在 10 个基准上评估 **AgentFounder-30B**，并与通用 LLM、商业及开源深度研究智能体对比。\n\n#### **主要结果**\n*   **BrowseComp-en**：达到 **39.9%**，优于最佳开源模型 DeepSeek-V3.1（30.0%），**绝对提升 9.9 个百分点**，接近 OpenAI-o3（49.7%）和 OpenAI Deep Research（51.5%）。\n*   **BrowseComp-zh**：达到 **43.3%**，优于 GLM-4.5（37.5%），但低于 DeepSeek-V3.1（49.2%）。\n*   **GAIA**：达到 **72.8%**，**超越所有对比模型**，包括 OpenAI-o3（70.5%）。\n*   **HLE**：达到 **31.5%**，是首个超过 30% 的开源模型，**显著超越** Gemini Deep Research（26.9%）、Kimi-Researcher（26.9%）和 OpenAI Deep Research（26.6%）。\n*   **Frames**：达到 **89.6%**，**全面超越**所有开源和闭源模型（如 OpenAI-o3 84.0%）。\n\n#### **消融实验核心结论**\n1.  **Agentic CPT 的有效性**：在 Qwen3-30B-A3B-Base 上进行 Agentic CPT 得到 AgentFounder-30B-Base，再使用三种不同的 SFT 数据（SFT-A/B/C）微调。结果表明，基于 AgentFounder-Base 的模型在所有配置下均一致优于基于原始基座的模型，平均性能提升在 **5.75% 到 6.45%** 之间。\n2.  **两阶段训练策略**：与仅使用 Stage 1 相比，完整的 Stage 1 & 2 训练在 BrowseComp-en 的 Pass@1 上带来 **+4.1 个百分点** 的提升（从 31.4% 到 35.5%）。\n3.  **数据类型的贡献**：FAS 数据单独使用在 GAIA 上带来显著提升（从 67.0% 到 72.8%，+5.8 个百分点）。结合 HAS 数据后，在 BrowseComp-zh 上 Pass@1 进一步提升 **+3.1 个百分点**（从 37.0% 到 40.1%），但在 GAIA 上略有下降（-2.9 个百分点），表明不同数据对不同任务类型有侧重影响。",
    "limitations_and_critique": "#### **方法局限性**\n1.  **中文能力瓶颈**：AgentFounder-30B 在 BrowseComp-zh 上表现（43.3%）显著低于英文版（39.9% vs. 闭源 SOTA 51.5%），且不及 DeepSeek-V3.1（49.2%）。作者归因于**训练语料中中文数据比例相对有限**，以及底层搜索工具（Google Search）在中文语境下可能存在性能偏差或偏见。这表明该方法对**数据分布和工具生态的依赖性**较强。\n2.  **合成数据的真实性鸿沟**：FAS 和 HAS 的核心数据均通过 **LLM 合成**，且**禁止实际调用外部工具**。虽然通过拒绝采样过滤，但合成的“规划”和“推理”动作与真实工具调用环境反馈（如网络延迟、搜索结果噪声、API 错误）存在**分布差异**。模型在合成数据上学习的决策策略，在部署到真实、嘈杂、动态的互联网环境中时，**泛化鲁棒性存疑**。\n3.  **知识密集型任务的瓶颈**：实验表明，与信息检索任务（如 BrowseComp）相比，Agentic CPT 对知识密集型任务（如 HLE）的提升相对较小。作者指出，这类任务不仅需要成功检索信息，还需要**强大的知识理解和推理能力**来正确利用检索到的知识。这表明当前方法主要增强了**行动规划能力**，但对**深度语义理解和逻辑推理**的增强有限。\n\n#### **理论漏洞与崩溃场景**\n*   **对合成数据的过拟合风险**：如果合成数据的多样性或质量不足，模型可能学会**模仿特定的、模式化的推理和行动模板**，而非发展出真正的**情境感知和灵活决策能力**。在遇到训练数据分布外、需要创造性问题分解或非常规工具组合的极端复杂任务时，模型可能**无法有效探索新的解决路径**而失败。\n*   **长上下文决策的稳定性**：第二阶段使用 128K 长上下文训练 HAS 数据，旨在处理复杂规划。然而，模型在超长序列中维持连贯的多步决策和依赖关系的能力**未经严格压力测试**。在需要回溯和修正早期决策的**长程、多分支探索任务**中，模型可能因注意力分散或记忆混淆而崩溃。",
    "ai_inspiration_and_opportunities": "#### **可迁移的组件与思想**\n1.  **Agentic CPT 范式**：在预训练与指令微调/对齐之间插入一个**专门针对智能体能力进行持续预训练**的阶段，这一思想具有普适性。其他领域的 AI Agent（如代码生成、机器人控制）可以借鉴此范式，先通过大规模、低成本合成的**领域特定行为数据**进行 CPT，构建“预对齐”的基础模型，再针对具体任务进行高效微调。\n2.  **无监督行为数据合成框架**：**FAS（实体-知识记忆库 → 多风格问题 → 规划/推理合成）** 提供了一套**不依赖真实环境交互**即可大规模生成智能体训练数据的系统方法。这对于**仿真成本高昂**（如机器人物理交互）或**API调用受限**的领域（如金融交易、医疗诊断）具有极高参考价值。研究者可以替换“实体-知识”的来源（如代码库、医疗文献、物理仿真日志），快速生成本领域的训练语料。\n3.  **轨迹数据的高阶利用（HAS）**：将失败的或次优的轨迹，通过**步骤级选项扩展**转化为**对比决策文本**进行学习，极大提升了低质量轨迹数据的利用率。这一思想可直接应用于**强化学习中的经验回放**或**模仿学习中的次优演示**，通过构建“在某个状态下，有多种可能动作，但其中某个（或某些）更优”的对比样本来进行训练，是一种高效的**数据增强和策略正则化**技术。\n\n#### **低算力下的验证与改进方向**\n1.  **轻量级实体-记忆库构建**：对于算力有限的研究者，可以聚焦于构建**小型、高质量、领域聚焦的实体-知识记忆库**。例如，针对特定垂直领域（如法律条款、学术论文），手动或半自动构建实体及其关键陈述，然后使用轻量级模型（如 7B 参数）进行有限度的 FAS 数据合成。这可以验证**小规模、高质量合成数据**对智能体基础能力培养的有效性。\n2.  **探索“课程学习”式 CPT**：本文采用两阶段（32K → 128K）训练。一个低算力可验证的 idea 是：**从短轨迹、简单任务合成数据开始 CPT，逐步增加轨迹长度和任务复杂度**。可以设计一个渐进式课程，验证这种**由易到难**的数据调度是否能以更少的总体计算量，达到同等甚至更好的效果，特别是对于**长程规划能力**的培养。\n3.  **混合真实与合成反馈**：完全依赖合成反馈是主要局限。一个低成本改进方向是：在 HAS 的步骤级扩展中，对于**关键决策点**（如第一次搜索查询、遇到矛盾信息时的判断），引入**少量真实环境调用**获取真实反馈，与大量合成选项混合。这可以用极低的 API 成本（<5%），为模型提供关键的**真实性锚点**，可能显著提升在真实环境中的泛化能力。",
    "source_file": "Scaling Agents via Continual Pre-training.md"
}