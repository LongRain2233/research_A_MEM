{
    "is_related_to_agent_memory": true,
    "has_github_link": true,
    "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
    "problem_and_motivation": "本文旨在解决LLM驱动的Web智能体在**长视野任务**中面临的**上下文管理根本性权衡**问题。现有方法存在两大关键缺陷：1. **基于ReAct的智能体**（如WebThinker、WebSailor）采用仅追加历史的方式，导致上下文被原始、嘈杂的网页数据淹没（**上下文饱和**），损害推理能力。2. **固定式全历史摘要方法**（如MEM1、MemAgent）在每一步都对完整历史进行摘要，虽然保持了上下文简洁，但存在**关键细节过早且不可逆丢失**的风险。本文的核心切入点是受人类**回顾性巩固**认知过程启发，提出让智能体主动管理其上下文，将其视为一个动态的认知工作空间进行“雕琢”，而非被动填充的日志。核心假设是：通过赋予智能体主动、多尺度的“折叠”能力，可以超越保留噪声细节与风险信息丢失之间的残酷权衡。",
    "core_method": "**AgentFold的核心架构是一个`感知->推理->折叠->行动`的循环**。其上下文被明确划分为四个组件：不变的用户问题（Q）、可用工具列表（T）、**多尺度状态摘要（S）**（作为长期记忆）和**最新交互（I）**（作为工作记忆）。\n\n**核心创新是智能体在每一步生成一个包含`折叠指令`的结构化响应**。该指令是一个JSON对象`{\"range\": [k, t-1], \"summary\": σ_t}`，支持两种操作模式：\n1.  **细粒度压缩（Granular Condensation）**：当`k = t-1`时，仅将最新的完整交互`I_{t-1}`折叠成一个新的细粒度摘要块`s_{t-1, t-1}`，追加到`S`序列中。\n2.  **深度整合（Deep Consolidation）**：当`k < t-1`时，将最新交互`I_{t-1}`与`S`中从第`k`步开始的一系列先前摘要块融合，用一个新的、更粗粒度的摘要块`s_{k, t-1}`替换它们。\n\n**数据处理流程**：在步骤`t`，智能体接收上下文`C_t = (Q, T, S_{t-2}, I_{t-1})`，生成响应`R_t = (th_t, f_t, e_t, a_t)`，其中`f_t`是折叠指令。应用`f_t`将`S_{t-2}`更新为`S_{t-1}`。执行动作`a_t`获得观察`o_t`，与解释`e_t`和动作`a_t`共同构成新的最新交互`I_t = (e_t, a_t, o_t)`。最终，下一轮的上下文为`C_{t+1} = (Q, T, S_{t-1}, I_t)`。这种方法将上下文管理内化为智能体推理过程的一个可学习的核心组成部分。",
    "key_experiments_and_results": "**核心实验设计**：在Qwen3-30B-A3B-Instruct-2507模型上进行监督微调，得到AgentFold-30B-A3B。在四个基准上评估：BrowseComp、BrowseComp-ZH、WideSearch（Item-F1）和GAIA（文本子集）。最大工具调用次数设为100。\n\n**主要定量结果**：\n1.  **性能超越**：在BrowseComp上达到**36.2%**，显著超过参数量大22倍的DeepSeek-V3.1-671B-A37B（30.0%），绝对提升6.2个百分点（相对提升20.7%）。在BrowseComp-ZH上达到**47.3%**，优于OpenAI的专有智能体o4-mini（44.3%）。在WideSearch上达到**62.1%**，是所有开源和专有智能体中的最佳成绩（优于OpenAI-o3的60.0%和Claude-4-Sonnet的62.0%）。在GAIA上达到**67.0%**。\n2.  **上下文效率**：在BrowseComp的200条轨迹分析中，经过100轮交互，AgentFold的平均上下文长度仅从约3.5k tokens增长到**7k tokens**，呈亚线性增长。相比之下，ReAct基线在100轮后上下文长度超过91k tokens，AgentFold的上下文比ReAct**小了超过84k tokens（约92%）**，估计每次推理节省近7GB内存。\n3.  **扩展性**：将交互轮次上限扩展到256轮时，AgentFold-30B的性能持续提升，而采用仅追加上下文的GLM-4.5-355B智能体在64轮后性能饱和并失效。在扩展到500轮的实验中，AgentFold的上下文长度大部分保持在20k tokens以下，且不会单调增长，展示了从死胡同中恢复的能力。\n\n**消融实验核心结论**：通过分析上下文中的“块”数量（摘要块+最新交互），证明深度整合操作使块数量呈亚线性增长，与ReAct的线性增长形成鲜明对比，凸显了主动管理的复合效率优势。",
    "limitations_and_critique": "**方法边界与未解决的困难**：\n1.  **训练数据依赖与生成**：方法依赖于一个尚不存在的数据集，即展示情境化行动与战略性上下文管理交互的轨迹。本文通过**Fold-Generator**管道和拒绝采样机制生成数据，但这可能引入生成模型的偏见，且数据质量完全依赖于所用开源LLM的能力。\n2.  **折叠策略的次优性**：本文采用简单的监督微调（SFT）来学习折叠策略，作者承认这并非最优。智能体可能无法自主发现非显而易见的、最优的折叠策略，尤其是在面对训练数据未覆盖的复杂、新颖任务模式时。\n3.  **评估任务的局限性**：实验主要聚焦于信息寻求型Web任务（BrowseComp, WideSearch）和一个通用问答基准（GAIA）。方法在**高度动态、需要实时世界状态跟踪**的交互环境（如游戏、机器人控制）中的有效性尚未验证。\n4.  **潜在崩溃场景**：在**信息极度稀疏或高度对抗性**的搜索任务中，智能体可能频繁进入死胡同并进行深度整合，导致上下文被过度压缩，丢失可能在未来步骤中变得关键的细微线索。此外，如果折叠指令生成错误（例如，错误地总结了关键信息），错误将**不可逆地**污染长期记忆，且没有内置的纠错或回滚机制。\n5.  **计算开销转移**：虽然推理时上下文更短，但训练阶段需要运行复杂的数据生成管道（Fold-Generator）和拒绝采样，这带来了显著的**前期计算成本**。",
    "ai_inspiration_and_opportunities": "**对其他AI智能体的高价值洞察与可迁移组件**：\n1.  **可迁移的“工作空间”架构**：将智能体上下文明确划分为**锚定目标（Q）、长期记忆（S）、工作记忆（I）和可用技能（T）** 的范式，可以广泛应用于需要长期规划和多轮交互的任何AI智能体领域，如**对话机器人（维护用户画像和对话历史）、代码生成智能体（管理代码库上下文和开发历史）、游戏AI（记忆游戏状态和策略历史）**。\n2.  **“折叠”作为核心原语**：将**折叠**（主动的、多尺度记忆管理）提升为与**思考**、**行动**并列的智能体核心原语，这一思想具有普适性。其他领域的智能体可以定义适合其领域的折叠操作，例如：\n    *   **对话智能体**：将多轮关于同一主题的讨论折叠成一个“共识摘要”。\n    *   **编程智能体**：将一系列失败的调试尝试折叠成一个“已排除的假设列表”。\n3.  **低算力/零算力下的改进方向**：\n    *   **启发式折叠策略**：在资源受限无法进行RL训练时，可以基于规则设计折叠启发函数。例如，当连续N步未获得新的有效信息时，自动触发深度整合；或根据信息熵、与目标的相关性得分来决定折叠粒度。\n    *   **混合记忆索引**：将`S`中的摘要块不仅存储为文本，同时构建一个轻量级的**向量索引**或**关键词索引**。这样，在决定折叠范围`[k, t-1]`时，不仅可以基于当前推理，还可以快速检索与当前子任务最相关的历史块进行针对性整合，提升折叠的准确性。\n    *   **离线轨迹分析与策略提炼**：收集大量智能体（即使是基线模型）的任务轨迹，使用更强大的模型（作为“教师”）离线分析这些轨迹，标注出**理想的折叠时机和摘要内容**，从而创建一个高质量的“折叠决策”数据集，用于SFT训练小型模型，实现策略蒸馏。",
    "source_file": "AgentFold Long-Horizon Web Agents with Proactive Context Management.md"
}