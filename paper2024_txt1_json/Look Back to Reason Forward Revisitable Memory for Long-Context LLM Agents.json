{
    "is_related_to_agent_memory": true,
    "has_github_link": true,
    "title": "LOOK BACK TO REASON FORWARD: REVISITABLE MEMORY FOR LONG-CONTEXT LLM AGENTS",
    "problem_and_motivation": "本文旨在解决长上下文问答（QA）中，关键证据分散在数百万token中，现有“边读边记”范式智能体的核心缺陷。现有方法（如MemAgent）采用线性扫描更新固定长度的**记忆缓冲区**，存在三大关键缺陷：1. **潜在证据的过早剪枝**：仅基于当前记忆状态评估信息重要性，无法识别未来才显现关联的早期证据。2. **记忆覆盖导致的渐进性信息丢失**：固定长度缓冲区迫使信息被不断压缩和覆盖，导致远距离证据难以整合。3. **稀疏且延迟的监督**：仅依赖最终答案正确性的单一奖励信号，无法有效指导长序列的中间记忆更新步骤。本文的核心切入点是：将**显式记忆检索机制**引入“边读边记”范式，使智能体能够通过**回调查询**非线性地访问历史记忆，从而缓解信息丢失并支持复杂多跳推理。",
    "core_method": "本文提出ReMemR1，一个集成了**历史增强状态**和**多级奖励强化学习**的记忆增强智能体。\n\n#### **1. 历史增强状态机制**\n*   **状态定义**：将传统MDP状态 \\( s_t = m_t \\) 扩展为 \\( s_t = (m_t, q_t) \\)，其中 \\( q_t \\) 是用于检索历史记忆的**回调查询**。\n*   **数据流**：在每一步 \\( t \\)，智能体接收问题 \\( Q \\)、当前文档块 \\( c_t \\)、当前状态 \\( s_t \\)。它通过策略 \\( \\pi_{\theta} \\) 生成**下一个记忆** \\( m_{t+1} \\) 和**下一个查询** \\( q_{t+1} \\)。关键创新在于，状态更新会集成检索函数 \\( \\mathcal{E} \\) 返回的内容：\\( s_{t+1} = \\pi_{\theta}(Q, c_t, m_t, \\mathcal{E}(\\}m_i\\}_{i \\le t}, q_t)) \\)。\n*   **检索函数**：\\( \\mathcal{E}(X, b) = \\arg\\max_{x \\in X",
    "source_file": "Look Back to Reason Forward Revisitable Memory for Long-Context LLM Agents.md"
}