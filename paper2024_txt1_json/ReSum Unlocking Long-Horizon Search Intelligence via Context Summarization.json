{
    "is_related_to_agent_memory": true,
    "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
    "problem_and_motivation": "#### **核心问题**\n基于LLM的Web智能体（如ReAct范式）在处理涉及**多实体、复杂关系、高不确定性**的复杂查询时，需要进行**多轮长程探索**（如多次搜索、浏览、验证）。然而，ReAct范式会将每一步的思考、动作、观察结果**全部追加**到对话历史中，导致**上下文长度迅速耗尽**（例如32K限制），迫使探索**提前终止**，无法完成需要大量工具调用的任务。\n#### **现有方法缺陷**\n现有方法（如简单截断Recent History）会**破坏上下文连续性**，丢失早期关键证据，导致推理中断。\n#### **本文切入点**\n提出**ReSum范式**，核心假设是：通过**周期性调用摘要工具**将冗长的交互历史压缩为**紧凑的推理状态**，智能体可以从该状态**重启探索**，从而在**不突破上下文限制**的前提下实现**无限探索**。",
    "core_method": "#### **核心数据流**\n1.  **初始化**：轨迹始于用户查询q，初始历史H0 = (q)。\n2.  **ReAct循环**：在第t轮，智能体基于历史Ht-1生成思考τt和动作at，执行后获得观察ot，更新历史Ht = Ht-1 ∘ (τt, at, ot)。\n3.  **上下文摘要触发**：当**触发条件**（如接近上下文长度限制）满足时，调用摘要模型πsum，输入当前历史Ht，生成一个**目标导向的结构化摘要**s。\n4.  **状态压缩与重启**：将原始查询q与摘要s组合成压缩状态q' = (q, s)，并**重置工作历史**为Ht ← (q')。智能体基于此新状态继续探索。\n5.  **终止**：当收集到足够信息后生成最终答案，或超过预设资源预算（如工具调用次数）时终止。\n#### **关键创新模块**\n*   **ReSumTool-30B**：专为Web搜索任务微调的摘要模型。基于Qwen3-30B-A3B-Thinking，使用从强大开源模型收集的⟨Conversation, Summary⟩对进行监督微调。其**核心功能**是：从冗长、嘈杂的交互历史中提取**可验证的证据**、识别**信息缺口**、并**提出下一步行动方向**。\n*   **ReSum-GRPO训练算法**：\n    *   **轨迹分割**：ReSum在摘要发生时自然地将长轨迹分割为K+1个片段。每个片段作为一个独立的训练episode。\n    *   **优势广播**：从完整轨迹的最后一个片段提取最终答案aT，使用LLM-as-Judge计算轨迹级奖励Rg ∈ {0, 1}。在每组G个轨迹内归一化得到优势值Âg，并将其**广播**给该轨迹内的**所有片段**作为共享优势信号Âg(i) = Âg。\n    *   **目标函数**：采用GRPO框架，优化目标为最大化所有片段优势加权概率比的总和。\n#### **与ReAct的本质区别**\nReAct**线性累积**所有交互，导致上下文无限增长；ReSum则通过**周期性压缩-重启**，将历史**蒸馏**为可重启的紧凑状态，突破了上下文长度的硬性限制。",
    "key_experiments_and_results": "#### **核心数据集**\n在三个需要长程探索的挑战性基准上评估：**GAIA**（103样本文本验证子集）、**BrowseComp-en**、**BrowseComp-zh**。\n#### **主要对比基线**\n1.  **ReAct**：标准范式，作为主要对比基线。\n2.  **Recent History**：简单截断基线，仅保留最近22K token的历史。\n#### **关键定量提升**\n*   **训练无关范式（ReSum vs. ReAct）**：\n    *   在WebSailor-3B上，BrowseComp-zh的Pass@1从ReAct的**8.2%** 提升至使用ReSumTool-30B的**13.7%**（绝对提升**5.5个百分点**，相对提升**67.1%**）。\n    *   在WebSailor-30B上，BrowseComp-en的Pass@1从ReAct的**12.8%** 提升至使用ReSumTool-30B的**16.0%**（绝对提升**3.2个百分点**，相对提升**25.0%**）。\n    *   平均而言，ReSum在三个基准上比ReAct带来**平均4.5%** 的绝对提升。\n*   **训练后范式（ReSum-GRPO vs. GRPO）**：\n    *   在WebSailor-30B上，经过ReSum-GRPO训练后，BrowseComp-zh的Pass@1达到**33.3%**，远超使用标准GRPO训练ReAct范式的**23.3%**（绝对提升**10.0个百分点**）。\n    *   仅用**1K训练样本**训练的WebSailor-30B（WebResummer-30B），在BrowseComp-zh上达到**33.3%** Pass@1，超越了使用**10K+样本**训练的ASearcher-32B（15.6%）、MiroThinker-32B（17.0%）等强大基线。\n#### **消融实验核心结论**\n*   **摘要工具规模与质量**：专精微调的**ReSumTool-30B**在多数配置下性能**匹配或超越**参数量大得多的通用模型（如Qwen3-235B、DeepSeek-R1-671B），证明了**任务特定训练的有效性**。\n*   **上下文窗口大小的影响**：即使对于支持128K上下文的强大智能体（Tongyi-DeepResearch-30B-A3B），ReSum范式在64K和128K设置下仍能带来性能提升，证明其**有效性不依赖于小上下文**，而是**正交的优化**。",
    "limitations_and_critique": "#### **方法边界条件**\n1.  **依赖外部摘要工具**：ReSum范式**强依赖于一个高质量的、外部的摘要模型**（ReSumTool-30B）。如果该工具失效或质量不佳，整个系统的性能将严重下降。\n2.  **基于规则的摘要触发机制**：当前采用**系统性触发**（如达到token预算或轮次限制），这**不够灵活**。在上下文尚未饱和但信息冗余时，可能错过压缩机会；反之，在关键推理中途触发摘要可能**中断连贯性**。\n3.  **摘要的信息损失风险**：压缩过程**必然导致信息丢失**。虽然摘要旨在保留关键证据和缺口，但对于**高度依赖细微线索或长链逻辑**的任务，压缩可能丢失关键中间步骤，导致后续推理失败。\n#### **理论漏洞与崩溃场景**\n*   **摘要质量与探索方向的耦合**：摘要不仅总结过去，还指导未来探索。如果摘要**错误地识别了信息缺口或提出了误导性的下一步**，可能导致智能体陷入**无效或错误的搜索循环**，且难以从压缩状态中恢复。\n*   **“冷启动”问题**：在探索初期，历史信息较少，生成的摘要可能**信息量不足**，导致压缩后的状态无法提供有效的推理基础，反而**浪费了一次工具调用**。\n*   **资源预算的硬约束**：虽然理论上支持无限探索，但实践中仍受**工具调用次数上限**（如60次）限制。ReSum可能将预算**消耗在多次“压缩-重启”循环**上，而非实质性的信息收集，在预算严格受限的场景下效率可能反而降低。",
    "ai_inspiration_and_opportunities": "#### **可迁移组件与思想**\n1.  **周期性状态压缩范式**：ReSum的核心思想——**在状态空间膨胀时进行有损压缩，然后从压缩状态重启**——可广泛应用于任何受限于**固定长度上下文或内存**的**序列决策AI系统**。例如：\n    *   **长文档问答/摘要系统**：在处理超长文档时，可定期将已处理部分摘要为“当前理解状态”，然后基于该状态继续阅读剩余部分。\n    *   **代码生成/调试智能体**：在漫长的代码迭代过程中，可将当前的代码状态、错误日志、修改意图摘要为“开发上下文”，避免历史过长。\n2.  **优势广播的强化学习训练技巧**：ReSum-GRPO中**将长轨迹分割、并广播轨迹级优势到所有片段**的方法，为解决**长程信用分配**问题提供了新思路。这适用于任何**动作序列长、奖励稀疏**的强化学习任务，能更稳定地训练策略利用中间状态。\n#### **低算力/零算力下的新idea与改进方向**\n1.  **轻量级自适应触发机制**：无需训练大模型，可设计**基于启发式的触发规则**。例如，监控历史中**新信息的熵增率**，当新增token的信息密度低于阈值时触发摘要；或检测**搜索查询的重复性**，当出现循环时触发摘要以重新规划。\n2.  **分层摘要与记忆索引**：在资源受限时，不生成单一完整摘要，而是构建一个**分层记忆结构**：\n    *   **第一层**：保留最近N轮完整交互（如N=5）。\n    *   **第二层**：对更早的历史，仅提取**实体-关系对**或**已验证的事实列表**（一种极简摘要）。\n    *   推理时，智能体可**按需从不同层检索**，这比运行一个30B的摘要模型**计算成本低得多**。\n3.  **利用现有模型的“自我摘要”能力**：许多现有LLM已具备一定的总结能力。一个零算力改进是：在ReSum框架中，**让智能体自身（而非外部模型）生成摘要**，并将其作为一次特殊的“内部动作”输出。这消除了对外部工具的依赖，虽然摘要质量可能下降，但实现了完全自包含，且**无需额外部署成本**。",
    "source_file": "ReSum Unlocking Long-Horizon Search Intelligence via Context Summarization.md"
}