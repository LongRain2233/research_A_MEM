{
    "is_related_to_agent_memory": true,
    "has_github_link": true,
    "title": "MEMORY IN VISION-LANGUAGE-ACTION MODELS FOR ROBOTIC MANIPULATION",
    "problem_and_motivation": "**核心问题**：主流视觉-语言-动作（VLA）模型（如 OpenVLA, π₀）仅依赖当前观测，忽视了机器人操作任务固有的**非马尔可夫性**（即历史动作影响当前决策），因此在长视野、时序依赖任务（如“按按钮”任务前后视觉差异极小）上表现不佳。\n**现有方法缺陷**：1. 简单串联连续帧作为输入会因自注意力二次复杂度限制上下文长度，且与模型单帧预训练分布不匹配；2. 现有方法（如 RoboFlamingo, TraceVLA, UniVLA）要么丢弃细粒度感知细节，要么仅将历史信息作为提示，未能有效利用历史信息。\n**本文切入点**：受人类**双记忆系统**（工作记忆用于短期控制，海马体系统用于长期情景记忆）的认知科学启发，提出显式建模时序依赖的记忆机制。",
    "core_method": "**核心数据流**：当前RGB观测和语言指令 → **视觉-语言认知模块**（7B Prismatic VLM + DINOv2/SigLIP编码器）生成**感知令牌**（256个）和**认知令牌**（1个），构成**工作记忆** → **感知-认知记忆库**（PCMB）长期存储历史细节与语义 → **记忆检索**：工作记忆通过带时间步位置编码的交叉注意力查询PCMB，获取相关历史特征 \\( \\hat{H}^{x} = \\operatorname\n{softmax}(\\frac{q^{x}(K^{x})^{\top}}{\\sqrt{d_x}}) V^{x",
    "source_file": "MemoryVLA Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation.md"
}