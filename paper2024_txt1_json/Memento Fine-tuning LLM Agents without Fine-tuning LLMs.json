{
    "is_related_to_agent_memory": true,
    "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs",
    "problem_and_motivation": "现有LLM智能体存在两大缺陷：**1. 静态工作流**：依赖预定义、硬编码的反思流程，部署后无法在线学习或适应新情况，缺乏灵活性。**2. 参数调优成本高昂**：通过监督微调或强化学习更新LLM参数，计算成本高，不适合持续适应和在线学习。\n\n本文核心问题是：**如何在不微调底层LLM的高昂成本下，构建能够从变化环境中持续学习的LLM智能体？** 核心切入点是**模仿人类记忆机制**，提出基于记忆的在线强化学习框架。核心假设是：通过外部记忆存储过往轨迹（包括成功与失败），并从中检索相似经验来指导决策，可以实现低成本、持续的适应能力，而无需修改LLM参数。",
    "core_method": "#### **核心架构：基于记忆的马尔可夫决策过程 (M-MDP)**\n将智能体的序列决策过程形式化为一个元组 \\(\\langle s , \\mathcal { A } , \\mathcal { P } , \\mathcal { R } , \\gamma , \\mathcal { M } \\rangle\\)，其中 \\(\\mathcal { M } = \\left( \\mathcal { S } \\times \\mathcal { A } \\times \\mathbb { R } \\right) ^ { \\ast }\\) 是关键引入的**记忆空间**，存储过往经验（状态、动作、奖励）三元组。\n\n#### **核心数据流：CBR智能体决策循环**\n1.  **检索 (Retrieve)**：给定当前状态 \\(s_t\\) 和案例库 \\(M_t\\)，通过检索策略 \\(\\mu(c | s_t, M_t)\\) 采样一个过往案例 \\(c_t = (s_i, a_i, r_i)\\)。\n2.  **重用与修订 (Reuse & Revise)**：LLM基于当前状态 \\(s_t\\) 和检索到的案例 \\(c_t\\)，生成动作 \\(a_t \\sim p_{\\mathrm{LLM}}(\\cdot | s_t, c_t)\\)。\n3.  **执行与评估**：执行动作 \\(a_t\\)，获得奖励 \\(r_t\\) 和下一状态 \\(s_{t+1}\\)。\n4.  **保留 (Retain)**：将新经验 \\((s_t, a_t, r_t)\\) 加入案例库：\\(M_{t+1} = M_t \\cup \\{(s_t, a_t, r_t)\\}\\)。\n\n#### **关键技术创新：基于状态相似性的软Q学习**\n核心是学习最优的案例检索策略 \\(\\mu^*\\)。\n- **目标函数**：采用最大熵RL框架，优化目标为 \\(J (\\pi) = \\mathbb{E} _{\\tau \\sim p} \\left[ \\sum_{t = 0} ^ {T - 1} \\left[ \\mathcal{R} (s_{t}, a_{t}) + \\alpha \\mathcal{H} \\left(\\mu \\left(\\cdot | s_{t}, M_{t}\\right)\\right) \\right] \\right]\\)，其中 \\(\\alpha\\) 是熵权重超参数。\n- **最优策略形式**：推导出最优检索策略是Q值的softmax：\\(\\mu^{*} (c | s, M) = \\frac {\\exp \\left(Q ^{*} (s , M , c) / \\alpha\\right)}{\\sum_{c ^{\\prime} \\in M} \\exp \\left(Q ^{*} (s , M , c ^{\\prime}) / \\alpha\\right)}\\)。\n- **Q函数学习**：为避免直接学习自然语言状态和案例描述的复杂Q函数，提出基于核的估计：\\(Q_{\\mathrm{EC}}(s, M, c; \\theta) = \\sum_{(s ^{\\prime}, c ^{\\prime}, Q ^{\\prime}) \\in \\mathcal{D} _{c}} \\frac {k _{\\theta} \\left(s , s ^{\\prime}\\right) Q ^{\\prime}}{\\sum_{(\\hat {s} , \\hat {c} , \\hat {Q}) \\in \\mathcal{D} _{c}} k _{\\theta} (s , \\hat {s})}\\)，其中 \\(k_{\\theta}\\) 是参数化的核网络，\\(\\mathcal{D}_c\\) 是存储了相同检索案例c的过往交互的记忆。通过时序差分学习（公式10）优化核参数 \\(\\theta\\)。\n- **简化实现**：在深度研究场景中，规划简化为单步设置，Q学习损失简化为二元分类的交叉熵损失：\\(\\mathcal{L}(\\theta) = \\mathbb{E}_{(s, c, r)} \\left[ - r \\log Q(s, c; \\theta) - (1 - r) \\log \\left(1 - Q(s, c; \\theta)\\right) \\right]\\)，其中Q值代表给定状态s和案例库M下，检索案例c是好参考的概率 \\(p(r=1|s,c;\\theta)\\)。\n\n#### **与现有方法的本质区别**\n1.  **非参数化学习**：不更新LLM参数，而是通过外部**案例库（记忆）** 和可学习的**检索策略**实现持续适应。\n2.  **形式化框架**：将CBR智能体严格建模为M-MDP，并推导出基于最大熵RL的最优检索策略学习目标。\n3.  **混合记忆机制**：支持**非参数化**（基于相似性检索）和**参数化**（基于Q函数检索）两种记忆操作，后者通过在线更新Q函数实现自适应案例选择。",
    "key_experiments_and_results": "#### **核心数据集与基线**\n在**GAIA**（长视野工具使用）、**DeepResearcher**（实时网络研究）、**SimpleQA**（事实精确性）、**HLE**（长尾学术推理）四个基准上评估。主要对比两类基线：**1. 基于提示的方法**（如CoT、CoT+RAG、Search-o1）和**2. 基于训练的方法**（如Search-r1-base、DeepResearcher）。\n\n#### **关键定量结果**\n- **GAIA基准**：在验证集上达到 **87.88% Pass@3**，在测试集上达到 **79.40%**，在开源智能体框架中排名第一。具体地，在Level 1/2/3任务上分别达到96.23%、90.70%、61.54%（验证集）。\n- **DeepResearcher基准（7个数据集平均）**：Memento (GPT-4.1 + o4-mini) 达到 **F1 66.6%** 和 **PM 80.4%**。相比最强的训练基线 **DeepResearcher (Zheng et al., 2025)**（F1 51.8%， PM 60.5%），F1绝对提升 **14.8个点**（相对提升28.6%），PM绝对提升 **19.9个点**（相对提升32.9%）。\n- **SimpleQA**：达到 **95.0% PM**。\n- **分布外（OOD）任务泛化**：基于案例的记忆（CBR）为OOD任务带来了 **4.7% 到 9.6%** 的绝对性能提升。\n\n#### **消融实验核心结论**\n- **记忆设计的影响**：参数化记忆（通过Q函数学习）与非参数化记忆（基于相似性检索）在持续学习曲线中表现出不同特性，但两者都显著优于无记忆的基线。\n- **案例库的作用**：移除案例库（CBR）会导致性能显著下降，证明了从过往经验中检索和学习对于持续适应至关重要。",
    "limitations_and_critique": "#### **方法边界条件**\n1.  **任务结构依赖性**：该方法依赖于任务可以被分解为可重复的“状态-动作-奖励”三元组。对于高度创造性、无结构化或奖励信号极其稀疏的任务，案例检索和Q函数学习的有效性可能大幅降低。\n2.  **记忆增长与检索成本**：案例库在线增长，**检索复杂度随记忆大小线性增加**。虽然论文使用了Top-K检索，但在长期部署中可能面临经典的“淹没问题”，即检索成本超过效用。论文未提供大规模记忆下的效率衰减曲线。\n3.  **状态表示瓶颈**：核网络 \\(k_{\\theta}\\) 和Q函数的学习质量严重依赖于状态 \\(s_t\\)（任务指令）的文本编码质量。对于语义复杂、歧义或高度专业化的领域，固定的文本编码器（如SimCSE）可能无法捕获细微的相似性，导致检索不相关案例。\n\n#### **理论漏洞与未解决的困难**\n- **信用分配问题**：在长视野、多步任务中，**奖励是稀疏且延迟的**。论文将CBR规划器简化为单步设置（公式14-15），避免了多步TD学习的非平稳目标，但这本质上**忽略了多步决策中的信用分配**。对于需要复杂序列规划的任务，这种简化可能限制性能上限。\n- **案例质量与偏差**：记忆库中存储的成功和失败案例**质量不均**，且**分布可能随时间偏移**。系统缺乏对记忆进行**选择性遗忘或提炼**的机制，可能导致低质量或过时案例污染检索分布，特别是在动态变化的环境中。\n- **极端场景崩溃风险**：当遇到与记忆库中所有案例都**完全不相似**的全新任务时，检索机制可能失效，智能体将退化为仅依赖LLM先验知识的普通提示方法，失去持续学习的优势。",
    "ai_inspiration_and_opportunities": "#### **可迁移组件与思想**\n1.  **M-MDP形式化框架**：将**外部记忆作为MDP的一部分**进行形式化建模的思想，可以广泛应用于任何需要从历史交互中学习的序列决策AI系统，如游戏AI、机器人控制、对话系统。其核心是将“选择历史经验”也视为一个可优化的策略动作。\n2.  **基于核的Q值估计**：\\(Q_{\\mathrm{EC}}\\) 的核估计方法（公式9）为解决**高维、非结构化状态空间**下的值函数近似提供了一个低算力方案。该方法不依赖于大型神经网络，而是通过维护一个**情景记忆 \\(\\mathcal{D}\\)** 并基于状态相似性进行加权插值，适合资源受限的边缘设备或对延迟敏感的应用。\n3.  **参数化与非参数化记忆的混合架构**：Memento展示了两种记忆模式可以共存。**非参数化记忆（快速检索）** 适合冷启动和快速响应；**参数化记忆（学习型检索）** 适合长期优化和适应。这种混合设计可以迁移到任何需要平衡效率与自适应性的记忆增强系统中。\n\n#### **低/零算力下的可验证新想法**\n- **Idea 1: 基于记忆重放的课程学习**：无需训练LLM，可以设计一个**记忆重放调度器**。在智能体学习新任务时，不仅检索相似案例，还**主动重放（重新执行）过去成功但“被遗忘”的旧案例**，以防止灾难性遗忘。这可以通过在记忆库中为每个案例添加“最后访问时间”和“成功次数”元数据，并设计一个基于访问频率和成功率的抽样策略来实现，零额外模型训练成本。\n- **Idea 2: 跨任务记忆迁移的元检索策略**：当前检索策略 \\(\\mu\\) 是在单个任务流中学习的。可以探索一个**元检索策略**，它学习在面临新任务时，**应该从哪个旧任务的经验库中检索案例**。这可以通过构建一个任务描述符的嵌入空间，并学习一个简单的映射函数（如小型MLP）来实现，该函数将新任务描述映射到最有帮助的旧任务记忆库索引。这个小型网络的训练数据可以来自智能体跨多个任务的历史性能日志，计算成本远低于微调LLM。",
    "source_file": "Memento Fine-tuning LLM Agents without Fine-tuning LLMs.md"
}