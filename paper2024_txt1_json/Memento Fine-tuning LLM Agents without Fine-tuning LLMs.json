{
    "is_related_to_agent_memory": true,
    "has_github_link": true,
    "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs",
    "problem_and_motivation": "现有LLM智能体存在两大核心缺陷：1. **静态工作流**：依赖固定、手工设计的反思流程，部署后无法在线学习或适应新情况，缺乏灵活性。2. **高昂计算成本**：通过监督微调或强化学习更新LLM参数，虽能实现灵活行为，但计算成本高，不适用于持续适应和在线学习的开放场景。\n\n本文旨在解决核心挑战：**如何在不微调底层LLM的高昂成本下，构建能够从变化环境中持续学习的通用智能体？** 核心切入点是**受人类记忆机制启发**，提出一种基于记忆的在线强化学习范式，通过外部记忆存储过往经验（轨迹），并从中检索相似案例来指导决策，实现无需参数更新的低成本持续适应。",
    "core_method": "本文提出**Memento**框架，其核心是基于记忆的马尔可夫决策过程（M-MDP）。\n\n#### **核心数据流**\n1.  **记忆写入（Write）**：在每个时间步t，将三元组（状态 $s_t$，动作 $a_t$，奖励 $r_t$）作为案例（case）存储到不断增长的案例库（Case Bank）$M_t$ 中：$M_{t+1} = M_t \\cup \\{(s_t, a_t, r_t)\\}$。\n2.  **记忆读取（Read）**：给定当前状态 $s_t$，通过**案例检索策略 $\\mu$** 从案例库 $M_t$ 中检索案例 $c_t$。\n3.  **动作生成**：LLM基于当前状态 $s_t$ 和检索到的案例 $c_t$ 生成动作 $a_t \\sim p_{\\mathrm{LLM}}(\\cdot | s_t, c_t)$。\n\n#### **关键创新：可学习的案例检索策略**\n将案例检索策略 $\\mu$ 的优化建模为**最大熵强化学习**问题，目标函数为 $J(\\pi) = \\mathbb{E}_{\\tau \\sim p} \\left[ \\sum_{t} [\\mathcal{R}(s_t, a_t) + \\alpha \\mathcal{H}(\\mu(\\cdot | s_t, M_t))] \\right]$，其中 $\\alpha$ 为熵权重。最优策略 $\\mu^*$ 是Q值的softmax分布：\n$$\\mu^* (c | s, M) = \\frac {\\exp \\left(Q ^ {*} (s , M , c) / \\alpha\\right)}{\\sum_{c^{\\prime} \\in M} \\exp \\left(Q ^ {*} (s , M , c ^ {\\prime}) / \\alpha\\right)}.$$\n\n#### **Q函数学习**\n为避免直接学习自然语言状态描述的复杂Q函数，提出**基于状态相似性的核估计**方法。维护一个情节记忆 $\\mathcal{D} = \\{(s, c, Q)\\}$，通过参数为 $\\theta$ 的核网络 $k_\\theta(\\cdot, \\cdot)$ 近似Q值：\n$$Q_{\\mathrm{EC}}(s, M, c; \\theta) = \\sum_{(s^{\\prime}, c^{\\prime}, Q^{\\prime}) \\in \\mathcal{D}_c} \\frac {k_{\\theta}(s , s^{\\prime}) Q^{\\prime}}{\\sum_{(\\hat{s} , \\hat{c} , \\hat{Q}) \\in \\mathcal{D}_c} k_{\\theta}(s , \\hat{s})}.$$\n通过时序差分学习（TD learning）损失 $\\mathcal{L}(\\theta)$ 优化核参数 $\\theta$，实现检索策略的在线更新。\n\n#### **与现有方法的本质区别**\n1.  **非参数化 vs. 参数化记忆**：提供两种记忆变体：非参数化（仅基于相似性检索）和参数化（通过在线学习的Q函数驱动检索）。\n2.  **策略在记忆层面更新**：不更新LLM参数，而是通过强化学习优化外部记忆的检索策略，实现智能体的持续适应。",
    "key_experiments_and_results": "#### **核心数据集与基线**\n在**GAIA**（长视野工具使用）和**DeepResearcher**（实时网络研究）基准上进行评估。最强对比基线包括：\n- **DeepResearcher (Zheng et al., 2025)**：当前基于训练的SOTA方法。\n- **Alita**、**Skywork Super Agents**等开源智能体框架。\n\n#### **关键定量结果**\n1.  **GAIA基准**：Memento在验证集上达到 **87.88%** Pass@3（平均分），在测试集上达到 **79.40%**。在验证集上超越基线Alita（87.27%）0.61个点，在测试集上排名第三。\n2.  **DeepResearcher基准**：在七个开放域QA数据集上，Memento（GPT-4.1 + o4-mini）的加权平均F1达到 **66.6%**，PM达到 **80.4%**。相比基于训练的SOTA方法DeepResearcher（F1 51.8%， PM 60.5%），F1绝对提升 **14.8个点**（相对提升28.6%），PM绝对提升 **19.9个点**（相对提升32.9%）。\n3.  **分布外（OOD）泛化**：基于案例的记忆（CBR）在OOD任务上带来 **4.7% 到 9.6%** 的绝对分数提升。\n4.  **消融实验核心结论**：移除案例推理（CBR）组件（即Memento w/o CBR）会导致性能显著下降，在OOD数据集上准确率下降高达 **9.6个点**，证明了外部案例记忆对于持续学习和泛化的关键作用。",
    "limitations_and_critique": "#### **方法边界条件**\n1.  **任务结构依赖**：方法假设任务可分解为序列决策过程，并建模为MDP。对于高度非结构化、探索性极强的开放任务（如无明确目标的创造性写作），状态和奖励的定义可能失效。\n2.  **案例检索的语义瓶颈**：检索依赖于状态编码的语义相似性。对于需要**跨领域类比推理**的任务（如将物理原理应用于经济模型），仅基于表面语义相似性的检索可能失败，因为缺乏深层次的关系映射。\n\n#### **未解决的困难与理论漏洞**\n1.  **记忆爆炸与检索效率**：案例库随时间线性增长，缺乏主动的**记忆压缩、总结或遗忘机制**。长期运行后可能面临“淹没问题”（swamping problem），检索成本超过效用。论文仅提及简单的Top-K检索，未解决大规模记忆下的可扩展性。\n2.  **稀疏奖励下的信用分配**：在深度研究等长视野任务中，奖励通常是稀疏且二元的（最终成功/失败）。方法依赖于单步Q学习（损失函数简化为式(15)），但**在多步决策中，如何将最终奖励准确分配（credit assignment）给序列中早期的关键案例选择**，仍是一个未充分解决的挑战。\n\n#### **极端崩溃场景**\n1.  **对抗性状态描述**：如果用户查询经过精心设计，与失败案例在语义上高度相似，但与成功案例不相似，系统可能被“误导”检索到失败案例，导致性能下降。\n2.  **工具环境动态剧变**：当外部工具API发生重大变更或失效时，存储的旧案例（包含过时的工具调用）可能变得完全无效，而系统缺乏检测和淘汰过时经验的机制，可能导致连锁失败。",
    "ai_inspiration_and_opportunities": "#### **可迁移组件与思想**\n1.  **记忆增强的MDP形式化**：将**外部记忆作为MDP状态的一部分**进行形式化（M-MDP），为任何序列决策智能体提供了一个通用的、可学习的记忆接口框架。其他领域的智能体（如游戏AI、机器人控制）可以借鉴此框架，将历史观测-动作-奖励元组存储为“案例”，并通过学习策略进行检索。\n2.  **基于核的Q函数估计**：**利用状态相似性核 $k_\\theta$ 来泛化Q值**的思想，可以迁移到任何具有高维、结构化状态空间（如图像、文本）的强化学习任务中，以缓解Q函数学习的样本复杂性。\n\n#### **低算力/零算力下的可验证新方向**\n1.  **轻量级记忆质量评估器**：可以设计一个**极小的判别模型（如微调后的BERT-tiny）**，仅用于评估新经验（案例）的“存储价值”，替代复杂的Q函数学习。该评估器根据案例的稀缺性、信息增益或预期未来效用进行打分，实现**选择性记忆写入**，从源头控制记忆增长。这可以在单GPU上快速验证。\n2.  **基于聚类的记忆组织与抽象**：在检索前，对案例库进行**在线聚类**（如使用流式K-means）。每个聚类中心形成一个“抽象案例”或“原型”。检索时，先匹配到聚类，再从该聚类中检索具体案例。这既能**压缩记忆**，又能通过聚类发现任务中的潜在子模式（sub-pattern），提升泛化。实现仅需离线聚类算法，无需额外训练。\n3.  **跨任务记忆迁移的元协议**：探索为记忆案例标注**可迁移的元特征**（如涉及的工具类型、推理模式、领域标签）。当智能体面对新任务时，首先通过元特征进行快速过滤，找到结构相似而非表面语义相似的过往经验。这可以通过简单的基于规则的元特征提取器实现，计算成本极低。",
    "source_file": "Memento Fine-tuning LLM Agents without Fine-tuning LLMs.md"
}