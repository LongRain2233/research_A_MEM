# LIGHTMEM: LIGHTWEIGHT AND EFFICIENTMEMORY-AUGMENTED GENERATION

Jizhan Fang♠, Xinle Deng♠, Haoming $\mathbf { X } \mathbf { u } ^ { \pmb { \alpha } }$ , Ziyan Jiang♠, Yuqi Tang♠, Ziwen $\mathbf { X } \mathbf { u } ^ { \pmb { \alpha } }$ , Shumin Deng♢, Yunzhi $\mathbf { Y a o } ^ { \mathbf { A } }$ , Mengru Wang♠, Shuofei Qiao♠, Huajun Chen♠, Ningyu Zhang♠♣∗

♠Zhejiang University ♢National University of Singapore

♣State Key Lab. for Novel Software Technology, Nanjing University, P.R. China {fangjizhan, zhangningyu}@zju.edu.cn

# ABSTRACT

Despite their remarkable capabilities, Large Language Models (LLMs) struggle to effectively leverage historical interaction information in dynamic and complex environments. Memory systems enable LLMs to move beyond stateless interactions by introducing persistent information storage, retrieval, and utilization mechanisms. However, existing memory systems often introduce substantial time and computational overhead. To this end, we introduce a new memory system called LightMem, which strikes a balance between the performance and efficiency of memory systems. Inspired by the Atkinson–Shiffrin model of human memory, LightMem organizes memory into three complementary stages. First, cognition-inspired sensory memory rapidly filters irrelevant information through lightweight compression and groups information according to their topics. Next, topic-aware short-term memory consolidates these topic-based groups, organizing and summarizing content for more structured access. Finally, long-term memory with sleep-time update employs an offline procedure that decouples consolidation from online inference. On LONGMEMEVAL and LOCOMO, using GPT and Qwen backbones, LightMem consistently surpasses strong baselines, improving QA accuracy by up to $7 . 7 \%$ / $2 9 . 3 \%$ , reducing total token usage by up to $3 8 \times { } /$ $2 0 . 9 \times$ and API calls by up to $3 0 \times / 5 5 . 5 \times$ , while purely online test-time costs are even lower, achieving up to $1 0 6 \times / 1 1 7 \times$ token reduction and $1 5 9 \times / 3 1 0 \times$ fewer API calls. The code is available at https://github.com/zjunlp/LightMem.

# 1 INTRODUCTION

Memory is fundamental to intelligent agent, enabling the assimilation of prior experiences, contextual cues, and task-specific knowledge that underpin robust reasoning and decision-making (Wang et al., 2024; Behrouz et al., 2024; Du et al., 2025; Zhang et al., 2024). While Large Language Models (LLMs) (DeepSeek-AI et al., 2025; Achiam et al., 2023) demonstrate remarkable capabilities across a wide range of tasks, they exhibit significant limitations when engaged in long-context or multiturn interaction scenarios due to fixed context windows and the “lost in the middle” problem (Liu et al., 2024). Memory systems are pivotal for overcoming these limitations, as they allow LLMs to maintain a persistent state across extended interactions. Recent works (Li et al., 2025b; Yang et al., 2024; Chhikara et al., 2025; Kang et al., 2025) address this challenge by building explicit external memory through sequential summarization and long term storage, enabling models to retain and retrieve relevant information over long horizons.

Note that a typical LLM memory system processes raw interaction data into manageable chunks, such as turn- or session-level in dialogue scenarios (Xu et al., 2025; Li et al., 2025a), organizes them into long-term memory (e.g., databases or knowledge graphs) by indexing them into memory units, and continuously updates by adding new information and discarding outdated or conflicting content (Zhong et al., 2024). This enables retrieval of relevant memories, improving coherence, and personalization in long-context, multi-turn scenarios.

Challenges. Despite these advances, as shown in Figure 1, contemporary memory systems still suffer from significant inefficiencies and consistency issues. First, in long interactions (e.g., dialogue scenarios), both user inputs and model responses often contain substantial redundant information (Maharana et al., 2024; Wu et al., 2025). Such information is typically irrelevant to downstream tasks or subsequent memory construction, and in some cases, may even negatively affect the model’s in-context learning capability (Liu et al., 2023; Pan et al., 2025). However, current mainstream memory-related studies generally process the raw information directly without any filtering or refinement, leading to high overhead from noisy or irrelevant data. This inflates token consumption without proportional gains in reasoning quality or coherence. Second, memory construction typically treats each turn in isolation or relies on rigid context-window boundaries, failing to model semantic connections across different turns (Tan et al., 2025). As a result, during subsequent memory item construction, the backbone LLM may generate inaccurate or incomplete item representations due to overly entangled topics or semantics, leading to the loss of crucial contextual details. Third, memory updates and forgetting are usually performed directly during inference and task execution. This tight coupling introduces long test-time latency in long-horizon tasks and prevents deeper, reflective processing of past experiences.

In contrast, human memory efficiently processes information through a hierarchical system: sensory memory pre-filters stimuli, short-term memory actively integrates and reasons over relevant content, and long-term memory selectively consolidates salient information in sleep time.

Building Lightweight Memory. Inspired by the efficiency and structure of human memory, we introduce LightMem, a lightweight memory architecture designed to minimize redundancy while preserving performance. In particular, LightMem emulates human memory through three key components: (1) A precompression sensory memory module that filters redundant or low-value tokens from raw input and buffers the distilled content for downstream processing. This initial filtering step reduces noise before information enters the memory pipeline. (2) A topic-aware short-term memory that leverages semantic and topical similarity to dynamically group related utterances into coherent segments. By adaptively determining segment

![](images/17d96ce489300c94d32b060ca224c29788d79cbef0c4dbfe0779d548061c596f.jpg)  
Figure 1: Comparison of previous works and LightMem.

boundaries based on content instead of fixed window sizes, this module produces more concentrated and meaningful memory units. This not only reduces the frequency of memory construction but also enables more precise and efficient retrieval during inference. (3) A sleep-time update mechanism for long-term memory maintenance. New memory entries are initially stored with timestamps to support immediate (“soft”) updates for real-time responsiveness. Later, during designated offline periods (i.e., “sleep”), the system reorganizes, de-duplicates, and abstracts these entries, resolving inconsistencies and strengthening cross-knowledge connections. Crucially, this decouples expensive memory maintenance from real-time inference, enabling reflective, high-fidelity updates without introducing latency. By systematically filtering, organizing, and consolidating relevant information, LightMem substantially reduces computational overhead and API costs while sustaining accurate, coherent reasoning over extended interactions. We detail each component in $\ S$ .

Results and Evaluation. On LongMemEval (Wu et al., 2025), LightMem consistently outperforms the strongest baseline, improving accuracy by $2 . 0 9 \% - 6 . 4 0 \%$ with GPT and up to $7 . 6 7 \%$ with Qwen. In terms of overall efficiency (online $^ +$ offline), LightMem reduces total token usage by up to $3 8 \times$ for GPT and $2 1 . 8 \times$ for Qwen, lowers API calls by up to $3 0 \times$ and $1 7 . 1 \times$ , and accelerates runtime by up to $1 2 . 4 \times$ and $6 . 3 \times$ , respectively. If considering only online test-time costs, the gains become even larger: LightMem cuts token usage by up to $1 0 5 . 9 \times$ (GPT) and $1 1 7 . 1 \times$ (Qwen), and reduces API

calls by up to $1 5 9 . 4 \times$ and $3 0 9 . 9 \times$ . On the LoCoMo benchmark (Maharana et al., 2024), LightMem maintains strong advantages, achieving $6 . 1 0 \% - 2 9 . 2 9 \%$ higher accuracy and substantial efficiency improvements—boosting token efficiency by up to $2 0 . 9 2 \times$ , reducing API calls by up to $5 5 . 4 8 \times$ , and speeding up runtime by up to $8 . 2 1 \times$ across GPT and Qwen backbones. Furthermore, case studies in $\ S$ show that the offline “sleep-time” consolidation enhances long-term memory reliability, mitigating information loss.

# 2 PRELIMINARY

# 2.1 CONVENTIONAL MEMORY SYSTEMS FOR LLMS

We describe mainstream memory architectures pipeline in terms of two major stages. (I) Memory Bank Construction. This stage can be further decomposed into three sub-stages: (a) Raw data $D$ are first processed at a chosen level of granularity, $D ^ { ( \bar { g } ) } = f _ { \mathrm { s e g } } ( D ; g ) , g \in \{ \mathrm { t u r n } .$ , session, topic} in dialog scenario; (b) The segmented data $D ^ { ( g ) }$ are then summarized or extracted to generate memory entries, $E = f _ { \mathrm { s u m } } ( D ^ { ( g ) } )$ , which are stored and organized within structural backends such as vector databases or knowledge graphs to enable long-term retention; (c) Many systems incorporate an updating mechanism to mitigate issues such as context conflicts or outdated information, $M ^ { \prime } = f _ { \mathrm { u p d a t e } } ( M , R ; U )$ , where $M$ denotes the existing memory bank, $R$ represents newly generated memory entries, and $U$ specifies the update or forgetting policy. (II) Retrieval and Usage. When a new user query arrives, the system retrieves relevant entries from the memory bank, integrates them with the query to construct the final prompt, and then invokes the model to produce a response.

# 2.2 ATKINSON–SHIFFRIN HUMAN MEMORY MODEL

Following the Atkinson–Shiffrin human memory model (Atkinson & Shiffrin, 1968), raw environmental information in human brain is first briefly retained in sensory memory, which enables rapid pre-attentive feature extraction and filtering, effectively serving as a form of pre-compression. The processed input can then enter short-term memory (STM), where information and interaction sequences are preserved for tens of seconds to minutes, supporting secondary filtering and more deliberate processing. In contrast, long-term memory (LTM) provides durable storage and undergoes continuous reorganization through updating, abstraction, and forgetting. Importantly, Rasch & Born (2013) highlight that sleep plays a critical role in this reorganization, as oscillatory activity during sleep facilitates the integration and consolidation of memory systems.

# 2.3 LIMITATIONS OF EXISTING LLM MEMORY SYSTEMS

Compared to human memory, current LLM memory systems are burdened by high maintenance costs, mainly due to three limitations: 1) Redundant Sensory Memory. In current systems, $f _ { \mathrm { s u m } } ( )$ and $f _ { \mathrm { g r a n } } ( ; g = \mathrm { t o p i c } )$ are typically executed by calling stronger LLMs. Feeding raw data $D$ directly wastes resources and even weakens in-context learning due to redundancy. A key challenge is to design lightweight mechanisms that pre-compress inputs and apply pre-attention strategies to capture semantic units at different granularities efficiently. 2) Balancing Effectiveness and Efficiency in STM. As shown in Figure 1, when input granularity is fixed, $D ^ { ( g ) }$ must pass through the entire pipeline. Excessively fine granularity increases latency and underutilizes STM capacity, whereas overly coarse granularity without semantic constraints or grouping may cause mixed or entangled semantics and topics, leading to inaccurate memory construction and loss of fine-grained details in subsequent processes. This calls for strategies that better balance effectiveness and efficiency in STM. 3) Inefficient LTM Updating. Current $f _ { \mathrm { u p d a t e } } ( )$ mechanisms face two main issues: (i) enforcing strict real-time updates at test time incurs significant latency, whereas STM can provide short-term context without immediate LTM updates; (ii) memory banks are updated sequentially due to ordering constraints (read-after-write/write-after-read), rather than being triggered dynamically. These limitations raise a research question: Can we design LLM memory that is both efficient and lightweight, inspired by human memory mechanisms?

![](images/216c5aada4205112b4c29da0c0b25217c5ee1c24ea8c75a3ea781ae9a9ca0383.jpg)  
Figure 2: The LightMem architecture. LightMem consists of three modules: a) An efficient Sensory Memory Module, b) a topic aware STM Module, and c) an LTM module updated in sleep time.

# 3 LIGHTMEM ARCHITECTURE

Analogous to the human memory, we design LightMem as shown in Figure 2, which consists of three light modules: Light1 implements an efficient Sensory Memory Module that selectively preserves salient information from raw input (§3.1), Light2 realizes a topic-aware STM Module for transient information processing (§3.2), and Light3 provides an LTM module designed to minimize test time update latency (§3.3) with a sleep time update mechanism. The overall pipeline framework of LightMem, its specific models, and comparisons with other memory frameworks are presented in Appendix A.1. The complexity analysis for LightMem’s efficiency gains is in Section 4.

# 3.1 LIGHT1: COGNITIVE-INSPIRED SENSORY MEMORY

In long horizon interaction scenarios, such as user–assistant dialogues, a large portion of the information is redundant. Therefore, we design a Pre-Compressing Submodule to eliminate redundant tokens, followed by the Topic Segmentation Submodule that forms semantic topic-based segments for following faster and more accurate memory construction.

Pre-Compressing Submodule. This module leverages a compression model $\theta$ to eliminate redundant tokens, tailored for compatibility with the downstream memory construction phase:

$$
\hat {\mathbf {x}} = \left\{x _ {i} \in \mathbf {x} \mid P (\text {r e t a i n} x _ {i} \mid \mathbf {x}; \theta) > \tau \right\}, \tau = \text {P e r c e n t i l e} \left(\left\{x _ {j} \right\}, r\right),
$$

Following Xia et al. (2025), we use LLMLingua-2 (Pan et al., 2024b) as our compression model $\theta$ . Let x be the raw input tokens, $\theta$ the model, and $r$ the compression ratio. The threshold $\tau$ is set to the $r$ -th percentile of retention scores, keeping only tokens above $\tau$ . For $P ( \operatorname { r e t a i n } x _ { i } \mid \mathbf { x } )$ , we treat the compression process as a binary token classification task (“retain” or “discard”). For each token $x _ { i }$ in a sequence x, the model $\theta$ outputs a logit vector $\ell _ { i }$ , and the retention probability is given by:

$$
P (\text {r e t a i n} x _ {i} \mid \mathbf {x}; \theta) = \operatorname {s o f t m a x} \left(\ell_ {i}\right) _ {1},
$$

where the subscript 1 denotes the “retain” class. Tokens with probabilities above a dynamic threshold are included in the compressed sequence. In addition, LightMem can also employ more general generative LLM as the pre-compression model. We further implement a token filtering mechanism based on the cross-entropy between the model’s predicted distribution and the true token labels:

$$
P (\text {r e t a i n} x _ {i} \mid \mathbf {x}; \theta) = - \sum_ {x _ {i} \in \mathcal {V}} q (x _ {i}) \log P (x _ {i} \mid \mathbf {x}; \theta)
$$

where $q ( x _ { i } )$ denotes the true token label distribution. Tokens with higher conditional entropy under a given context are more uncertain and less predictable, indicating greater informational uniqueness and a more critical role in semantic expression, such distinctive tokens are essential for subsequent memory construction and are therefore retained.

Topic Segmentation Submodule. Existing works indicate that topic-granular input facilitates improved performance in memory systems (Pan et al., 2025; Tan et al., 2025). As shown in Figure 2, LightMem maintains a sensory memory buffer to temporarily store information after precompression. When the accumulated information reaches the buffer’s maximum capacity, a hybrid topic segmentation operation based on attention and similarity is triggered. We use the compression model $\theta$ and an embedding model to compute attention matrices and semantic similarities, respectively. We define the final segmentation boundaries as the intersection of attention-based boundaries $\boldsymbol { B } _ { 1 }$ and similarity-based boundaries $B _ { 2 }$ :

$$
\mathcal {B} _ {1} = \{k \mid M _ {k, k - 1} > M _ {k - 1, k - 2}, M _ {k, k - 1} > M _ {k + 1, k}, 1 <   k <   n \},
$$

$$
\mathcal {B} _ {2} = \left\{k \mid \operatorname {s i m} \left(s _ {k - 1}, s _ {k}\right) <   \tau , 1 \leq k <   n \right\}, \quad \mathcal {B} = \mathcal {B} _ {1} \cap \mathcal {B} _ {2}.
$$

Specifically, dialogue scenarios possess natural semantic units, namely the conversational turn. We construct a turn-level attention matrix $M \in \mathbb { R } ^ { n \times n }$ . $\boldsymbol { B } _ { 1 }$ are identified as local maxima in the sequence $\{ M _ { k , k - 1 } \}$ , i.e., the sub-diagonal elements of $M$ corresponding to attention between consecutive sentences. The detailed process of $\boldsymbol { B } _ { 1 }$ and illustrative cases are provided in Appendix C.1. To mitigate attention sinks and dilution in attention-based methods, we compute semantic similarity between adjacent turns near each candidate boundary in $\boldsymbol { B } _ { 1 }$ . Boundaries with similarity below threshold $\tau$ form set $B _ { 2 }$ , which helps determine the final topic boundaries $\boldsymbol { B }$ .

# 3.2 LIGHT2: TOPIC-AWARE SHORT-TERM MEMORY

After obtaining individual topic segments, forming an index structure of {topic, message turns}, where message turns $= \{ u s e r _ { i } , m o d e l _ { i } \}$ . These are first placed into the STM buffer. When the token count in the buffer reaches a preset threshold, we invoke LLM $f _ { \mathrm { s u m } }$ to generate concise summaries of every structure. The final index structure stored in LTM is {topic, $\{ s u m _ { i } , u s e r _ { i } , m o d e l _ { i } \} \}$ .

$$
\operatorname {s u m} _ {i} = f _ {\operatorname {s u m}} \left(S _ {i}\right), \quad S _ {i} \subseteq \left\{\text {u s e r} _ {i}, \text {m o d e l} _ {i} \right\}, \quad S _ {i} \neq \varnothing ,
$$

$$
\operatorname {E n t r y} _ {i} = \left\{\text {t o p i c}, \mathbf {e} _ {i} := \operatorname {e m b e d d i n g} \left(\operatorname {s u m} _ {i}\right), \operatorname {u s e r} _ {i}, \operatorname {m o d e l} _ {i} \right\},
$$

where $\mathrm { E n t r y } _ { i }$ denotes the memory entry to be stored in LTM. Compared with inputting at the granularity of a single turn or session, directly feeding multiple sessions can reduce subsequent API calls but often introduces inaccurate memory entries due to excessive topic mixing, leading to performance degradation. In contrast, topic-constrained input granularity minimizes API calls to the greatest extent while preserving summarization accuracy and maintaining stable system performance.

# 3.3 LIGHT3: LONG-TERM MEMORY WITH SLEEP-TIME UPDATE

Soft Updating at Test Time. At test time, when memory entries arrive, LightMem directly inserts them into LTM with soft updates, thereby decoupling the update process from online inference. Due to real-time updates being converted to direct insertions, interaction latency is significantly reduced. After all entries are inserted or when an update trigger arrives, we compute an update queue for every entry in LTM.

$$
\mathcal {Q} (e _ {i}) = \operatorname {T o p} _ {k} \left\{(e _ {j}, \operatorname {s i m} (v _ {i}, v _ {j})) \mid t _ {j} \geq t _ {i}, j \neq i \right\} _ {: n},
$$

where $e _ { i }$ denotes the $i$ -th memory entry with embedding $v _ { i }$ and timestamp $t _ { i }$ , $\mathrm { s i m } ( \cdot , \cdot )$ is the similarity function, and $\mathrm { T o p } _ { k } \{ \cdot \} _ { : n }$ indicates selecting the top- $k$ most similar candidates, with the update queue $Q ( e _ { i } )$ length fixed at $n$ . Consistent with existing work, we select the top- $k$ existing memory entries with the highest semantic similarity as potential update sources. On this basis, we further impose the constraint that only entries with later timestamps are allowed to update earlier ones $( t _ { j } \ \geq \ t _ { i } )$ , which is consistent with realistic temporal dynamics. Here, $\mathcal { Q } ( e _ { i } )$ denotes the queue of other entries that may update $e _ { i }$ . Since this process involves only similarity retrieval, it is fast and lightweight, and can be executed offline in parallel with online inference.

Offline Parallel Update. LightMem does not simply transfer online update latency to offline phases, it substantially reduces the overall update latency. The online update mechanism in existing memory frameworks enforces sequential updates, leading to a total latency that accumulates with each update. As shown in Figure 2, in LightMem, each memory entry maintains a global update queue, with each queue corresponding to a distinct $f _ { \mathrm { u p d a t e } }$ operation. Since the update targets are independent across queues, updates can be executed in parallel, thereby greatly reducing the total latency.

# 4 COMPLEXITY ANALYSIS ABOUT LIGHTMEM

Table 1: Complexity comparison between LightMem and other memory systems. The specific definitions of each symbol are provided in the Appendix A.2.   

<table><tr><td>Method</td><td>Summary Tokens</td><td>Update Tokens</td><td>API Calls</td><td>Runtime</td></tr><tr><td>Baselines</td><td>N(Lsum-in + T + Lsum-out)</td><td>NM1R1(Lup-in + Lup-out)</td><td>N</td><td>O(N)</td></tr><tr><td>LightMem</td><td>NrTxTh(Lsum-in + th + Lsum-out)</td><td>NrTxThM2R2(Lup-in + Lup-out)</td><td>NrTxTh</td><td>O(NrTxTh)</td></tr></table>

As shown in Table 4, we consider a dialogue with $N$ turns, each containing on average $T$ tokens. In conventional memory systems, each turn triggers a summarization call, consuming $L _ { \mathrm { s u m - i n } } + T +$ $L _ { \mathrm { s u m - o u t } }$ tokens and totaling $N ( L _ { \mathrm { s u m - i n } } + T + L _ { \mathrm { s u m - o u t } } )$ tokens with $N$ API calls. Each summarization produces $M _ { 1 }$ memory entries, a fraction $R _ { 1 }$ of which retrieve at least one relevant neighbor and trigger an update, resulting in an update-token cost of $N M _ { 1 } R _ { 1 } ( L _ { \mathrm { u p - i n } } + L _ { \mathrm { u p - o u t } } )$ .

In LightMem, each turn is first passed through iterative pre-compression submodule, retaining only $r ^ { x } T$ tokens after $x$ iterations, and appended to a short-term memory (STM) buffer of capacity $^ { t h }$ . Summarization is triggered only when the buffer reaches capacity, yielding $\textstyle { \frac { N r ^ { x } T } { t h } }$ summarization calls, each consuming $L _ { \mathrm { s u m - i n } } + t h + L _ { \mathrm { s u m - o u t } }$ tokens. Each summarization produces $M _ { 2 }$ memory entries, but stricter retrieval constraints, including semantic similarity and timestamp filtering, reduce the fraction $R _ { 2 }$ that trigger updates. Hence, the update phase involves N rxTth M2R2 calls, with $\frac { N r ^ { x } T } { t h } \bar { M _ { 2 } R _ { 2 } }$ th a total token cost of N rxTth M2R2(Lup-in + Lup-out). $\begin{array} { r } { \frac { N r ^ { x } T } { t h } M _ { 2 } R _ { 2 } ( L _ { \mathrm { u p - i n } } + L _ { \mathrm { u p - o u t } } ) } \end{array}$ th

Overall, LightMem requires only N rxTth API calls for both summarization operations, substantially $\frac { N r ^ { x } T } { t h }$ reducing token usage and call frequency compared to other systems. Correspondingly, the runtime complexity of other memory systems is $O ( \bar { N } )$ , while LightMem achieves a reduced runtime of $\begin{array} { r } { O \left( \frac { N r ^ { x } T } { t h } \right) } \end{array}$ , reflecting the efficiency gain from compressed summarization and selective updates.

# 5 EXPERIMENTS

# 5.1 EXPERIMENTAL SETUP

Experimental Details. (1) Our experiments adopt a realistic Incremental Dialogue Turn Feeding setting, where the entire dialogue history is fed and processed at the turn level, one turn at a time. This reflects practical scenarios where interactions between user and model is incrementally formed turn by turn. (Hu et al., 2025). (2) For considerations of both efficiency and effectiveness, we employ LLMLingua-2 as our pre-compressor throughout all subsequent experiments. (3) The attention scores for topic segmentation are also obtained using LLMLingua-2, the size of the sensory memory buffer is 512 tokens. All specific models used in this paper, can be found in Table 5.

Datasets & Baseline Methods. We use two well-known datasets, LONGMEMEVAL (Wu et al., 2025) (specifically the LongMemEval-S split) and LOCOMO (Maharana et al., 2024) to evaluate memory ability. We compare LightMem against several representative baselines of conversational memory modeling. $\textcircled{1}$ Full Text, $\textcircled{2}$ Naive RAG, $\textcircled{3}$ LangMem (LangChain, 2025), $\textcircled{4}$ A-MEM (Xu et al., 2025), $\textcircled{5}$ MemoryOS (Kang et al., 2025), $\textcircled{6}$ Mem0 (Chhikara et al., 2025). In addition, all methods use GPT-4o-mini and Qwen3-30B-A3B-Instruct-2507 as the LLM backbones. Details on dataset, baselines, and experimental settings are provided in the Appendix D.

Metrics. We evaluate these methods using both effectiveness and efficiency metrics. For effectiveness, we report Accuracy (ACC), defined as the proportion of correctly answered questions. The

Table 2: Effectiveness and efficiency comparison on LONGMEMEVAL-S. The token usage is in thousands. – indicates no value for the metric. Bold denotes the best result, underline the secondbest. $r$ denotes the compression rate. th denotes the capacity threshold of the STM buffer, measured in tokens. Each pair of $r$ and $^ { t h }$ corresponds to two rows: one for online soft update and one for offline update. OP-update denotes the offline parallel update process of LightMem.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">ACC (%)</td><td colspan="2">Summary Tokens (k)</td><td colspan="2">Update Tokens (k)</td><td rowspan="2">Total (k)</td><td rowspan="2">Calls</td><td rowspan="2">Runtime (s)</td></tr><tr><td>In</td><td>Out</td><td>In</td><td>Out</td></tr><tr><td colspan="9">GPT-4o-mini</td></tr><tr><td>FullText</td><td>56.80</td><td>-</td><td>-</td><td>-</td><td>-</td><td>105.07</td><td>-</td><td>-</td></tr><tr><td>NaiveRAG</td><td>61.00</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>867.38</td></tr><tr><td>LangMem</td><td>37.20</td><td>-</td><td>-</td><td>982.68</td><td>119.48</td><td>1,102.16</td><td>520.62</td><td>2,293.70</td></tr><tr><td>A-MEM</td><td>62.60</td><td>214.66</td><td>42.82</td><td>1,157.52</td><td>190.81</td><td>1,605.81</td><td>986.55</td><td>5,132.06</td></tr><tr><td>MemoryOS</td><td>44.80</td><td>2,302.35</td><td>304.18</td><td>350.02</td><td>35.19</td><td>2,991.75</td><td>2,938.41</td><td>8,030.04</td></tr><tr><td>Mem0</td><td>53.61</td><td>424.13</td><td>17.76</td><td>560.17</td><td>150.56</td><td>1,152.62</td><td>811.57</td><td>4,248.49</td></tr><tr><td>LightMem</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>r=0.5, th=256</td><td>64.29</td><td>20.80</td><td>10.01</td><td>-</td><td>-</td><td>30.81</td><td>25.67</td><td>302.69</td></tr><tr><td>(OP-update)</td><td>64.69</td><td>-</td><td>-</td><td>44.46</td><td>2.56</td><td>47.02</td><td>70.23</td><td>342.63</td></tr><tr><td>r=0.6, th=256</td><td>67.78</td><td>24.58</td><td>10.53</td><td>-</td><td>-</td><td>35.11</td><td>30.47</td><td>329.61</td></tr><tr><td>(OP-update)</td><td>65.39</td><td>-</td><td>-</td><td>53.98</td><td>3.18</td><td>57.16</td><td>85.07</td><td>411.56</td></tr><tr><td>r=0.7, th=512</td><td>68.64</td><td>18.88</td><td>9.37</td><td>-</td><td>-</td><td>28.25</td><td>18.43</td><td>283.76</td></tr><tr><td>(OP-update)</td><td>67.07</td><td>-</td><td>-</td><td>79.38</td><td>4.06</td><td>83.44</td><td>125.47</td><td>496.03</td></tr><tr><td colspan="9">Qwen3-30B-A3B-Instruct-2507</td></tr><tr><td>FullText</td><td>54.80</td><td>-</td><td>-</td><td>-</td><td>-</td><td>105.07</td><td>-</td><td>-</td></tr><tr><td>NaiveRAG</td><td>60.80</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>659.09</td></tr><tr><td>LangMem</td><td>50.80</td><td>-</td><td>-</td><td>1,311.96</td><td>118.06</td><td>1,430.02</td><td>495.12</td><td>3,237.16</td></tr><tr><td>A-MEM</td><td>65.20</td><td>219.21</td><td>66.98</td><td>1,260.54</td><td>318.20</td><td>1,864.93</td><td>989.30</td><td>5,367.51</td></tr><tr><td>MemoryOS</td><td>49.60</td><td>2,101.54</td><td>510.88</td><td>305.12</td><td>27.43</td><td>2,944.97</td><td>2,922.28</td><td>8,721.78</td></tr><tr><td>Mem0</td><td>39.51</td><td>424.20</td><td>15.34</td><td>411.50</td><td>111.35</td><td>1001.90</td><td>722.76</td><td>2,239.94</td></tr><tr><td>LightMem</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>r=0.4, th=768</td><td>61.95</td><td>9.01</td><td>16.14</td><td>-</td><td>-</td><td>25.15</td><td>16.54</td><td>357.13</td></tr><tr><td>(OP-update)</td><td>62.34</td><td>-</td><td>-</td><td>111.13</td><td>7.88</td><td>119.01</td><td>176.02</td><td>1036.47</td></tr><tr><td>r=0.6, th=768</td><td>70.20</td><td>13.19</td><td>19.21</td><td>-</td><td>-</td><td>32.40</td><td>19.97</td><td>417.13</td></tr><tr><td>(OP-update)</td><td>65.14</td><td>-</td><td>-</td><td>97.11</td><td>5.92</td><td>103.03</td><td>152.93</td><td>1023.56</td></tr><tr><td>r=0.8, th=1024</td><td>68.69</td><td>14.82</td><td>18.49</td><td>-</td><td>-</td><td>33.31</td><td>9.43</td><td>355.71</td></tr><tr><td>(OP-update)</td><td>67.34</td><td>-</td><td>-</td><td>106.91</td><td>6.20</td><td>113.11</td><td>168.37</td><td>1026.90</td></tr></table>

evaluation is conducted with GPT-4o-mini as an LLM judge, guided by a detailed evaluation prompt (see Appendix E.1). For efficiency, we focus on tracking the computational costs of the LLM invocations in memory bank construction stage (see Section 2.1), all averaged across the entire dataset, as it is the one tied to the design and implementation differences of memory systems. The retrieval and usage stage is not our focus, because for fair comparison, The $f _ { \mathrm { r e t r i e v e } } ( )$ , $f _ { \mathrm { c h a t } } ( )$ and number of retrieved entries are same among all methods. As a result, their costs exhibit only minor differences, and this stage is largely orthogonal to the design of memory systems, as shown in the table. Within the memory bank construction stage, only the two sub-processes Summary and Update involve the use of LLMs, $f _ { \mathrm { s u m / e x t r a c t } } ( )$ and $f _ { \mathrm { u p d a t e } } ( )$ . So for both processes, we report the token consumption from LLM calls, including input tokens, output tokens, and total token usage (in thousands). Additionally, we track API Calls counting the total number of LLM invocations, and Runtime recording the overall execution time for memory bank construction stage.

# 5.2 MAIN RESULTS

As shown in Table 2 and Table 3, LightMem demonstrates superior effectiveness and efficiency on both datasets across both GPT and Qwen backbones. For a fair comparison, all efficiency metrics for LightMem in the following analysis refer to the combined online and offline costs.

Table 3: Effectiveness and efficiency comparison on LOCOMO. Due to space limitations and for ease of comparison, we merge the results before and after LightMem’s offline update into a single row. The ACC reported corresponds to the performance after the offline update.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">ACC (%)</td><td colspan="2">Summary Tokens (k)</td><td colspan="2">Update Tokens (k)</td><td rowspan="2">Total (k)</td><td rowspan="2">Calls</td><td rowspan="2">Runtime (s)</td></tr><tr><td>In</td><td>Out</td><td>In</td><td>Out</td></tr><tr><td colspan="9">GPT-4o-mini</td></tr><tr><td>FullText</td><td>71.83</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NaiveRAG</td><td>63.64</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LangMem</td><td>57.20</td><td>-</td><td>-</td><td>898.27</td><td>111.95</td><td>1010.22</td><td>920.62</td><td>2229.37</td></tr><tr><td>A-MEM</td><td>64.16</td><td>182.74</td><td>49.29</td><td>729.89</td><td>187.52</td><td>1149.43</td><td>1175.47</td><td>6060.73</td></tr><tr><td>MemoryOS(locomo)^1</td><td>58.25</td><td>110.98</td><td>33.40</td><td>78.08</td><td>64.54</td><td>287.00</td><td>553.45</td><td>2422.05</td></tr><tr><td>MemoryOS(regular)</td><td>54.87</td><td>226.86</td><td>46.61</td><td>177.66</td><td>75.34</td><td>526.48</td><td>1016.06</td><td>3332.59</td></tr><tr><td>Mem0</td><td>61.69</td><td>851.32</td><td>20.53</td><td>632.12</td><td>189.42</td><td>1693.39</td><td>1602.20</td><td>4432.87</td></tr><tr><td>LightMem(0.7,512)</td><td>71.95</td><td>73.19</td><td>20.13</td><td>6.05</td><td>0.40</td><td>99.76</td><td>41.65</td><td>848.49</td></tr><tr><td>LightMem(0.7,768)</td><td>70.26</td><td>57.54</td><td>18.92</td><td>3.79</td><td>0.23</td><td>80.48</td><td>29.55</td><td>737.80</td></tr><tr><td>LightMem(0.8,768)</td><td>72.99</td><td>62.82</td><td>17.95</td><td>4.14</td><td>0.28</td><td>85.19</td><td>29.83</td><td>815.32</td></tr><tr><td colspan="9">Qwen3-30B-A3B-Instruct-2507</td></tr><tr><td>FullText</td><td>74.87</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>NaiveRAG</td><td>66.95</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LangMem</td><td>60.53</td><td>-</td><td>-</td><td>1004.35</td><td>138.02</td><td>1142.37</td><td>1005.37</td><td>2268.57</td></tr><tr><td>A-MEM</td><td>56.10</td><td>158.29</td><td>60.85</td><td>924.19</td><td>483.51</td><td>1626.80</td><td>1175.40</td><td>5543.90</td></tr><tr><td>MemoryOS(locomo)</td><td>61.04</td><td>122.21</td><td>53.12</td><td>104.43</td><td>81.75</td><td>361.51</td><td>414.70</td><td>1269.70</td></tr><tr><td>MemoryOS(regular)</td><td>51.30</td><td>228.85</td><td>51.60</td><td>242.27</td><td>143.63</td><td>666.35</td><td>1004.60</td><td>1982.20</td></tr><tr><td>Mem0</td><td>43.31</td><td>827.09</td><td>18.64</td><td>763.88</td><td>189.80</td><td>1799.40</td><td>1614.50</td><td>4540.70</td></tr><tr><td>LightMem(0.6,768)</td><td>71.36</td><td>56.68</td><td>34.14</td><td>8.31</td><td>0.74</td><td>99.87</td><td>29.10</td><td>815.70</td></tr><tr><td>LightMem(0.8,1024)</td><td>72.60</td><td>61.38</td><td>36.33</td><td>9.86</td><td>0.88</td><td>108.45</td><td>32.00</td><td>1079.40</td></tr></table>

LongMemEval. On the LongMemEval benchmark, LightMem consistently outperforms the strongest baseline, A-Mem, in the ACC metric, improving accuracy by $2 . 0 9 \% - 6 . 4 0 \%$ with GPT and up to $7 . 6 7 \%$ with Qwen. In terms of efficiency, for GPT, LightMem reduces total token consumption by $1 0 \times - 3 8 \times$ and API calls by $3 . 6 \times - 3 0 \times$ ; for Qwen, it reduces total tokens by $6 . 9 { \times } { - 2 1 . 8 \times }$ and API calls by $3 . 3 { \times } \mathrm { - } 1 7 . 1 { \times }$ . Regarding runtime, LightMem achieves $2 . 9 { \times } { - } 1 2 . 4 { \times }$ for GPT and $1 . 6 { \times } { - 6 . 3 } { \times }$ for Qwen speedup over other memory baselines.

If considering only online test-time cost, LightMem shows an even larger efficiency advantage. For GPT, LightMem reduces total token consumption by $3 1 . 4 \times - 1 0 5 . 9 \times$ and API calls by $1 7 . 1 \times -$ $1 5 9 . 4 \times$ ; for Qwen, it reduces total tokens by $3 0 . 1 \times - 1 1 7 . 1 \times$ and API calls by $2 4 . 8 \times - 3 0 9 . 9 \times$ .

LoCoMo. On the LoCoMo dataset, LightMem also demonstrates superior performance over other memory baselines. For the GPT backbone, it improves ACC by $6 . 1 0 \% - 1 8 . 1 2 \%$ , achieves a $2 . 8 7 \times -$ $2 0 . 9 2 \times$ improvement in total token efficiency, reduces API calls by $1 3 . 2 9 \times - 3 9 . 7 8 \times$ , and accelerates runtime by $2 . 6 3 \times - 8 . 2 1 \times$ . On the Qwen backbone, LightMem maintains its advantage in both effectiveness and efficiency, with $4 . 4 1 \% - 2 9 . 2 9 \%$ higher ACC, $3 . 3 3 \times - 1 8 . 0 2 \times$ reduction in total token consumption, $1 2 . 9 6 \times - 5 5 . 4 8 \times$ fewer API calls, and $1 . 1 8 \times - 5 . 5 7 \times$ faster runtime.

LightMem achieves superior performance on nearly all metrics and both LLM backbones, while demonstrating robust performance and efficiency on both LongMemEval and LoCoMo, highlighting its generalizability across different models and scenarios.

# 5.3 ANALYSIS OF PRE-COMPRESSING SUBMODULE

Performance and Overhead. LightMem uses an additional model (Pan et al., 2024b; Xia et al., 2025) for pre-compression. We evaluate its performance by randomly sampling 1/5 of LONG-MEMEVAL and compressing it at ratios shown in Figure 3(a), then prompting LLMs for in-context

![](images/6d19e9993e16495f73eda7694f11d1b35b195d7618a510ab1f3078d164727b48.jpg)  
Figure 3: Analysis and Ablation Study of Key Modules. Fig.(a) depicts the QA accuracy when using prompts compressed at different ratios $( r )$ as in-contexts to query the LLM directly. Fig.(b) compares the accuracy of different topic segmentation methods under these varying compression ratios. Fig.(c1) and Fig.(c2) present the ablation study for the topic segmentation module, evaluating its impact on both performance and efficiency for the GPT and Qwen models.

QA. When compression ratio $r$ ranges from $50 \% - 8 0 \%$ , compressed and uncompressed performance are comparable, demonstrating LLMs can effectively understand compressed content and validating LightMem’s approach. The submodule is highly efficient, consuming under 2GB of GPU memory with negligible impact on overall runtime.

Impact of $r$ on Performance. As shown in Tables 8 and 9, The optimal $r$ for ACC is dependent on the STM buffer threshold $^ { t h }$ . For smaller thresholds $( t h \in \{ 0 , \bar { 2 } 5 6 \} )$ ), an $r$ of 0.6 achieves the highest ACC. In contrast, for larger thresholds $( t h \in \{ 5 1 2 , 1 0 2 4 \} )$ , a higher retention rate of $r = 0 . 7$ performs best. This suggests greater buffer capacity enables effective use of richer, less-compressed information, leveraging LLMs’ advanced long-context processing to mitigate the “lost in the middle” phenomenon. On average, the optimal $r$ for ACC is 0.6, reflecting a trade-off between information compression rate and the quantity of information in the STM buffer. In terms of efficiency, a lower $r$ generally leads to higher efficiency, as it triggers the buffer threshold less frequently under the same $^ { t h }$ , resulting in fewer API calls and lower token consumption.

# 5.4 ANALYSIS OF TOPIC SEGMENTATION SUBMODULE

Segmentation Accuracy. To validate the accuracy of our proposed hybrid topic segmentation method, we compare it with segmentation using only a single granularity: attention-only-based and similarity-only-based segmentation. Since the construction process of the LONGMEMEVAL indicates that different sessions naturally serve as topic boundaries, we directly use them as groundtruth labels. The final accuracy is calculated as the number of correctly identified segmentation points divided by the total number of labels. The results in Figure 3(b) validate the effectiveness of our method: it achieves higher accuracy than both individual segmentation methods across all compression ratios, with an absolute accuracy exceeding $80 \%$ .

Ablation Study. As shown in Figure 3(c), removing the topic segmentation submodule slightly improves efficiency but significantly harms accuracy, causing a $6 . 3 \%$ drop for GPT and $5 . 4 \%$ for Qwen. This indicates that the submodule effectively enables models to perceive semantic units in the input, facilitating subsequent memory unit generation.

# 5.5 ANALYSIS OF THE STM THRESHOLD’S IMPACT

As illustrated in the Figure 4, the STM buffer threshold $( t h )$ has a distinct but significant impact on both efficiency and performance metrics. A consistent trend is: as $^ { t h }$ increases, there is a marked improvement in efficiency. In contrast, the effect on QA accuracy is non-monotonic. The optimal threshold for accuracy varies depending on the model and the compression ratio $( r )$ , indicating that a larger buffer does not always yield better performance. This highlights a crucial trade-off: while

a larger STM threshold is consistently better for reducing computational cost, the ideal setting for maximizing task accuracy requires careful tuning.

# 5.6 ANALYSIS OF SLEEP-TIME UPDATE

Why Soft Updates Work. A primary challenge in designing memory systems is handling updates. While powerful, LLMs can be unreliable when tasked with complex real-time update operations. For instance, when presented with two related but not contradictory pieces of information, an LLM might incorrectly interpret them as a conflict and delete the older memory entry, leading to irreversible information loss. Instead, the optimal operations might be to merge the information or simply add the new entry. In contrast, LightMem performs only incremental additions through soft updates during test time, which preserves global information and complete semantics.

# Case Study: Memory Update Mechanism Comparison

History1: {’Monday, 2 PM’: User is planning a trip to Tokyo.}

History2: {’Monday, 4 PM’: User asks about trains to Kyoto.}

Hard Update: Overwrites memory LightMem Soft Update: Appends info

-> "User plans Kyoto trip" -> "Tokyo trip $^ +$ Kyoto inquiry"

. Tokyo context lost

¥ Full context preserved

# 6 RELATED WORK

Hard Prompt Compression for LLMs. Hard prompt compression improves LLM efficiency by removing redundant content from prompts (Li et al., 2025c). Methods recently have evolved from using smaller language models (Jiang et al., 2023; Li et al., 2023; Chuang et al., 2024) to queryaware approaches that preserve task-relevant information (Weston & Sukhbaatar, 2023; Creswell et al., 2023; Jiang et al., 2024). Additionally, lightweight bidirectional encoders have demonstrated strong effectiveness and efficiency (Pan et al., 2024a; Liskavets et al., 2025).

Chunking Strategies in RAG Systems. Retrieval-Augmented Generation (RAG) systems rely on chunking extrernal documents into smaller units for retrieval (Lewis et al., 2020; Gao et al., 2023). Existing chunking strategies include rule-based methods creating fixed-size segments (Lewis et al., 2020; Sarthi et al., 2024; Edge et al., 2024; Gutierrez et al., 2024), semantic-based methods grouping content by topic (Qu et al., 2025), and LLM-driven methods leveraging model knowledge for splitting (Pan et al., 2025; Duarte et al., 2024; Zhao et al., 2024; Liu et al., 2025b). However, all of these chunking strategies for RAG systems are tailored to static scenarios, not applicable to dynamic and open-ended environments.

Memory Systems for LLM Agents. Memory systems help LLM agents move beyond stateless interactions to support flexible reasoning and adaptation in complex and changing environments (Liu et al., 2025a; Mei et al., 2025). The earliest and most straightforward approaches store experiences as linear or sequential streams, sometimes enhanced with hierarchical structures (Liang et al., 2023; Park et al., 2023; Packer et al., 2023; Zhong et al., 2024; Salama et al., 2025; Fang et al., 2025). A more structured class of methods represents memories as nodes and their relationships as edges, using trees, graphs, or temporal knowledge structures to support retrieval and update (Rezazadeh et al., 2025; Chhikara et al., 2025; Rasmussen et al., 2025; Xu et al., 2025; Zhang et al., 2025). The latest trend integrates various types of memory, allowing them to interact and synergistically improve overall performance (Kang et al., 2025; Li et al., 2025b; Wang & Chen, 2025; Nan et al., 2025). Overall, existing memory systems for LLM agents have become increasingly complex and capable, leveraging hierarchical, structured, and multi-type memories. However, most focus on maximizing effectiveness, with limited consideration of efficiency. While some recent works (Guo et al., 2024; Zhao et al., 2025; Dong et al., 2025) share a similar motivation with our work, they focus on lightweight adaptations of GraphRAG where the corpus is predefined and static.

# 7 CONCLUSION

In this work, we introduced LightMem, a lightweight and efficient memory framework designed to address the significant overhead of memory systems for LLM agents. Inspired by the multi-stage Atkinson-Shiffrin human memory model, LightMem’s architecture effectively filters, organizes, and consolidates information. Our empirical evaluation demonstrates that this approach maintains strong task performance while sharply reducing computational costs. In the near future, we plan to accelerate LightMem’s update phase via offline pre-computed KV caches, reducing runtime overhead. We aim to integrate a lightweight knowledge graph memory for explicit multi-hop reasoning and structured retrieval. A multimodal memory extension will enable adaptation to visual, auditory, and textual inputs in embodied and real-world scenarios.

# ETHICS STATEMENT

LightMem enhances LLM agents by creating an external memory of user interactions. While this improves agent coherence, it introduces critical ethical challenges. Storing dialogue histories poses inherent risks to user privacy, as conversations may contain sensitive data. The memory can also absorb and perpetuate biases or misinformation from user input, potentially leading to bad agent behavior. Therefore, any deployment of this technology must prioritize robust safeguards. We strongly advocate for strict privacy protocols, such as data anonymization and user consent, as well as mechanisms to mitigate the effects of biased or false memories. Responsible development is essential to ensure these memory-augmented systems are used in a safe and trustworthy manner.

# REPRODUCIBILITY STATEMENT

To ensure the reproducibility of this work, we introduce the detailed implementations for LightMem are provided in in Section 3, Appendix C. Additionally, we plan to release our source code in the future to further support reproducibility. These measures are intended to facilitate the verification and replication of our results by other researchers in the field.

# REFERENCES

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
Richard C Atkinson and Richard M Shiffrin. Human memory: A proposed system and its control processes. In Psychology of learning and motivation, volume 2, pp. 89–195. Elsevier, 1968.   
Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024.   
Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory. ArXiv, abs/2504.19413, 2025. URL https://api.semanticscholar.org/CorpusID:278165315.   
Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, and Xia Ben Hu. Learning to compress prompt in natural language formats. In Kevin Duh, Helena Gomez-Adorno, and ´ Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 7756–7767. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.NAACL-LONG.429. URL https: //doi.org/10.18653/v1/2024.naacl-long.429.   
Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=3Pf3Wg6o-A4.   
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu,

Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10. 48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948.   
Junnan Dong, Siyu An, Yifei Yu, Qian-Wen Zhang, Linhao Luo, Xiao Huang, Yunsheng Wu, Di Yin, and Xing Sun. Youtu-graphrag: Vertically unified agents for graph retrieval-augmented complex reasoning. arXiv preprint arXiv:2508.19855, 2025.   
Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, and Jeff Z Pan. Rethinking memory in ai: Taxonomy, operations, topics, and future directions. arXiv preprint arXiv:2505.00675, 2025.   
Andre V. Duarte, Jo ´ ao Marques, Miguel Grac¸a, Miguel Freire, Lei Li, and Arlindo L. Oliveira. ˜ Lumberchunker: Long-form narrative document segmentation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pp. 6473–6486. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-EMNLP.377. URL https://doi.org/10.18653/v1/2024.findings-emnlp.377.   
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization. ArXiv, abs/2404.16130, 2024. URL https://api.semanticscholar.org/ CorpusID:269363075.   
Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. Memp: Exploring agent procedural memory, 2025. URL https://arxiv.org/abs/2508.06433.   
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2023.   
Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrievalaugmented generation. arXiv preprint arXiv:2410.05779, 2024.   
Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ 6ddc001d07ca4f319af96a3024f6dbd1-Abstract-Conference.html.   
Yuanzhe Hu, Yu Wang, and Julian McAuley. Evaluating memory in llm agents via incremental multi-turn interactions, 2025. URL https://arxiv.org/abs/2507.05257.   
Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 13358–13376. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.825. URL https://doi.org/10.18653/v1/2023.emnlp-main.825.

Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1658–1677. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.91. URL https: //doi.org/10.18653/v1/2024.acl-long.91.   
Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent. ArXiv, abs/2506.06326, 2025. URL https://api.semanticscholar.org/CorpusID: 279250574.   
LangChain. Langmem sdk for agent long-term memory, 2025. URL https://blog. langchain.com/langmem-sdk-launch/.   
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨ aschel, Sebastian Riedel,¨ and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html.   
Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue, 2025a. URL https://arxiv.org/ abs/2406.05925.   
Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 6342–6353. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.391. URL https://doi.org/ 10.18653/v1/2023.emnlp-main.391.   
Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Ke-Rong Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhiyang Xu, and Feiyu Xiong. Memos: An operating system for memory-augmented generation (mag) in large language models. ArXiv, abs/2505.22101, 2025b. URL https://api.semanticscholar.org/ CorpusID:278960153.   
Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: A survey. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2025 - Volume 1: Long Papers, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 7182–7195. Association for Computational Linguistics, 2025c. doi: 10.18653/V1/2025.NAACL-LONG.368. URL https://doi.org/10.18653/v1/2025.naacl-long.368.   
Xinnian Liang, Bing Wang, Huijia Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Scm: Enhancing large language model with self-controlled memory framework. 2023. URL https://api.semanticscholar.org/CorpusID:258331553.   
Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, and Shane K. Luke. Prompt compression with context-aware sentence encoding for fast and improved LLM inference. In Toby Walsh, Julie Shah, and Zico Kolter (eds.), AAAI-25, Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pp. 24595–24604. AAAI Press, 2025. doi: 10.1609/AAAI.V39I23.34639. URL https://doi. org/10.1609/aaai.v39i23.34639.

Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990, 2025a.   
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. URL https://arxiv.org/abs/2307.03172.   
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Trans. Assoc. Comput. Linguistics, 12:157–173, 2024. doi: 10.1162/TACL\ A\ 00638. URL https://doi.org/ 10.1162/tacl_a_00638.   
Zuhong Liu, Charles-Elie Simon, and Fabien Caspani. Passage segmentation of documents for extractive question answering. In Claudia Hauff, Craig Macdonald, Dietmar Jannach, Gabriella Kazai, Franco Maria Nardini, Fabio Pinelli, Fabrizio Silvestri, and Nicola Tonellotto (eds.), Advances in Information Retrieval - 47th European Conference on Information Retrieval, ECIR 2025, Lucca, Italy, April 6-10, 2025, Proceedings, Part III, volume 15574 of Lecture Notes in Computer Science, pp. 345–352. Springer, 2025b. doi: 10.1007/978-3-031-88714-7\ 33. URL https://doi.org/10.1007/978-3-031-88714-7_33.   
Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents, 2024. URL https: //arxiv.org/abs/2402.17753.   
Lingrui Mei, Jiayu Yao, Yuyao Ge, Yiwei Wang, Baolong Bi, Yujun Cai, Jiazhi Liu, Mingyu Li, Zhong-Zhi Li, Duzhen Zhang, et al. A survey of context engineering for large language models. arXiv preprint arXiv:2507.13334, 2025.   
Jiayan Nan, Wenquan Ma, Wenlong Wu, and Yize Chen. Nemori: Self-organizing agent memory inspired by cognitive science. arXiv preprint arXiv:2508.03341, 2025.   
Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez. Memgpt: Towards llms as operating systems. ArXiv, abs/2310.08560, 2023. URL https: //api.semanticscholar.org/CorpusID:263909014.   
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Dongmei Zhang. Llmlingua-¨ 2: Data distillation for efficient and faithful task-agnostic prompt compression. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 963–981. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.FINDINGS-ACL.57. URL https://doi.org/10.18653/v1/2024.findings-acl.57.   
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Ruhle, Yuqing Yang, Chin-Yew Lin, et al. Llmlingua-2: Data distillation for efficient and ¨ faithful task-agnostic prompt compression. arXiv preprint arXiv:2403.12968, 2024b.   
Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, and Jianfeng Gao. Secom: On memory construction and retrieval for personalized conversational agents. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=xKDZAW0He3.   
Joon Sung Park, Joseph C. O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. In Sean Follmer, Jeff Han, Jurgen Steimle, and Nathalie Henry Riche (eds.), ¨ Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 2023- 1 November 2023, pp. 2:1–2:22. ACM, 2023. doi: 10.1145/3586183. 3606763. URL https://doi.org/10.1145/3586183.3606763.

Renyi Qu, Ruixuan Tu, and Forrest Sheng Bao. Is semantic chunking worth the computational cost? In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pp. 2155–2177. Association for Computational Linguistics, 2025. doi: 10.18653/V1/2025.FINDINGS-NAACL.114. URL https://doi.org/10.18653/v1/ 2025.findings-naacl.114.   
Bjorn Rasch and Jan Born. About sleep’s role in memory. ¨ Physiological reviews, 2013.   
Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: a temporal knowledge graph architecture for agent memory. arXiv preprint arXiv:2501.13956, 2025.   
Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical schemas: Dynamic tree memory representation for llms. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=moXtEmCleY.   
Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yassine Benajiba. Meminsight: Autonomous memory augmentation for llm agents. ArXiv, abs/2503.21760, 2025. URL https://api.semanticscholar.org/CorpusID:277349587.   
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. RAPTOR: recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=GN921JHCRw.   
Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, and Tomas Pfister. In prospect and retrospect: Reflective memory management for long-term personalized dialogue agents, 2025. URL https://arxiv.org/abs/2503.08026.   
Yu Wang and Xi Chen. Mirix: Multi-agent memory system for llm-based agents. arXiv preprint arXiv:2507.07957, 2025.   
Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, et al. Towards lifespan cognitive systems. arXiv preprint arXiv:2409.13265, 2024.   
Jason Weston and Sainbayar Sukhbaatar. System 2 attention (is something you might need too). arXiv preprint arXiv:2311.11829, 2023.   
Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=pZiyCaVuti.   
Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025.   
Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. ArXiv, abs/2502.12110, 2025. URL https://api. semanticscholar.org/CorpusID:276421617.   
Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and Weinan E. Memory3: Language modeling with explicit memory. CoRR, abs/2407.01178, 2024. doi: 10.48550/ARXIV.2407.01178. URL https://doi.org/10.48550/arXiv. 2407.01178.   
Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025.

Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A survey on the memory mechanism of large language model based agents, 2024. URL https://arxiv.org/abs/2404.13501.   
Jihao Zhao, Zhiyuan Ji, Yuchen Feng, Pengnian Qi, Simin Niu, Bo Tang, Feiyu Xiong, and Zhiyu Li. Meta-chunking: Learning text segmentation and semantic completion via logical perception. 2024. URL https://api.semanticscholar.org/CorpusID:278782541.   
Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, and Xiang Li. Eˆ 2graphrag: Streamlining graphbased rag for high efficiency and effectiveness. arXiv preprint arXiv:2505.24226, 2025.   
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 19724–19731. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29946. URL https://doi.org/10.1609/aaai.v38i17.29946.

# A BACKGROUND DETAILS

# A.1 BACKGROUND ABOUT CURRENT MEMORY SYSTEMS

We describe both the mainstream memory architectures and the LightMem pipeline in terms of two major stages. The first is the memory bank construction stage, which can be further decomposed into the three sub-stages (I), (II), and (III) described in the Section 2.1. The second major stage concerns the usage of the memory system, which consists of retrieval and question answering (QA).

Memory Bank Construction As shown in Table 4, we detail the workflows of the three sub-stages (I), (II), and (III) for naive RAG, prevailing memory systems, and our LightMem. It can be observed that baseline memory systems typically perform their update stage during user–model interaction, which introduces substantial test-time latency. In contrast, LightMem decouples this update process from online interaction, thereby significantly reducing test-time latency. All models involved in these processes are listed in Table 5. As shown, LightMem introduces only one additional model, LLMlingua-2,beyond those used by baseline methods. This model follows a lightweight BERT architecture and requires less than 2GB of GPU memory during inference, rendering its overhead negligible. Moreover, for fairness, the latency introduced by this component is fully accounted for in our reported Runtime metric.

Table 4: The mainstream memory architectures and the LightMem pipeline of memory bank construction stage. Black-font processes denote those executed during online test-time interactions, whereas red-font processes denote those executed offline.   

<table><tr><td>Method</td><td>(I) Segment</td><td>(II) Summary/Extract</td><td>(III) Update</td></tr><tr><td>NaiveRAG</td><td>Raw dialog → fseg() →{seg}</td><td>→findex() →{emb}</td><td>\</td></tr><tr><td>Other Memory Systems</td><td>Raw dialog → fseg() →{seg}</td><td>→fsum/extract() →{memory entry} →findex() →{emb}</td><td>→fretrieve() →{related entry} →fupdate() →{add, delete, update, merge...}</td></tr><tr><td>LightMem</td><td>Raw dialog → fseg() →{seg} →fpre_compress() →{comp_seg} → sensory buffer full → ftopic() →{topic-wise comp_seg}</td><td>→STM buffer full →fsum/extract() →{topic, {memory entry}} →findex() →{topic, {emb}}</td><td>Offline update trigger{every entry} →fretrieve() →{related entry} →{update queue} All update queues established → parallel fupdate() →{add, delete, update, merge...}</td></tr></table>

Table 5: Mapping between functions, their roles, and the concrete models used in this paper. Blackfont entries denote models shared by both LightMem and baseline methods, whereas red-font entries denote models unique to LightMem.   

<table><tr><td>Function</td><td>Model / Strategy</td><td>Implementation in This Paper</td></tr><tr><td>fseg()</td><td>Segmentation strategy</td><td>Turn-level granularity input</td></tr><tr><td>findex()</td><td>Embedding model</td><td>all-MiniLM-L6-v2</td></tr><tr><td>fsum/extract()</td><td>System backbone model</td><td>GPT-4o-mini; Qwen3-30B-A3B-Instruct-2507</td></tr><tr><td>fretrieve()</td><td>Retrieval strategy</td><td>Cosine similarity vector retrieval</td></tr><tr><td>fupdate()</td><td>System backbone model</td><td>GPT-4o-mini; Qwen3-30B-A3B-Instruct-2507</td></tr><tr><td>fpre_compress()</td><td>Token compression model</td><td>LLMlingua-2</td></tr><tr><td>ftopic()</td><td>Topic segmentation model</td><td>LLMlingua-2</td></tr><tr><td>fchat()</td><td>Chat model</td><td>GPT-4o-mini; Qwen3-30B-A3B-Instruct-2507</td></tr></table>

Retrieval and Usage After the memory bank construction stage, we obtain an up-to-date memory bank. When a new user query arrives, the memory system use $f _ { \mathrm { r e t r i e v e } } ( )$ to retrieve relevant entries from this repository, appends them to the query, and then prompts the chat model $f _ { \mathrm { c h a t } } ( )$ to produce a response.

# A.2 NOTATION AND COMPLEXITY DETAILS

Table 6: Notation used in complexity analysis (§Section 4).   

<table><tr><td>Symbol</td><td>Definition</td></tr><tr><td>N</td><td>Total number of turns in a dialogue history.</td></tr><tr><td>T</td><td>Average number of tokens per turn.</td></tr><tr><td>r</td><td>Token compression rate (as defined in the main paper). After one compression step, only a fraction r of tokens is retained.</td></tr><tr><td>x</td><td>Number of compression iterations. In LightMem, the pre-compress module may be invoked multiple times for the same message to remove redundancy until the message is sufficiently compact. This occurs frequently in datasets such as LongMemEval. All time costs are included in runtime metrics.</td></tr><tr><td>th</td><td>Capacity of the Short-Term Memory (STM) buffer, as defined in the paper.</td></tr><tr><td>Lsum-in / Lsum-out</td><td>Number of tokens in the input prompt template and output of a single backbone LLM call for summarization. These are similar across memory frameworks.</td></tr><tr><td>M1 / M2</td><td>Number of memory entries produced from a single summarization operation under Other Memory Systems (M1) and LightMem (M2).</td></tr><tr><td>Lup-in / Lup-out</td><td>Number of tokens in the input prompt template and output of a single backbone LLM call for memory update. Similar across frameworks.</td></tr><tr><td>R1 / R2</td><td>Proportion of summary entries that successfully retrieve at least one relevant memory entry (triggering an update) for Other Memory Systems (R1) and LightMem (R2). Some entries do not retrieve any relevant counterparts and thus do not trigger updates.</td></tr></table>

# B USAGE OF LLMS

Throughout the preparation of this manuscript, we used LLMs to assist with improving grammar, clarity, and wording in parts of this work. The use of LLMs was limited to language refinement, with all ideas, analyses, and conclusions solely developed by the authors.

# C METHODOLOGY DETAILS

# C.1 TOPIC SEGMENTATION

In this part, we present the construction of the attention matrix, the underlying rationale for topic segmentation, and representative illustrative cases.

We extract only the user sentences from multi-turn dialogues, as they are generally more concise and the assistant’s responses necessarily remain consistent with the user’s theme within the same turn. Moreover, since the maximum input length of the LLMLingua-2 Pan et al. (2024b) model is 512 tokens, the assistant’s often lengthy sentences cannot be effectively accommodated. Therefore, we sequentially store the user sentences into a buffer and segment them, ensuring that as many sentences as possible are preserved while staying within the token limit. As a practical trick, if a sentence becomes empty after compression, we retain its original uncompressed version; if the token length of a sentence still exceeds the maximum limit, we continue to compress it using the LLMLingua-2 model at a 0.5 compression rate until the token length falls below the threshold. To reduce the effect of attention sinks, we mask out the contributions of the first and last three tokens in each sequence and subsequently normalize the remaining attention values. Attention is derived from the higher layers of LLMLingua-2 (layers 8, 9, 10, and 11). For any two sentences, we first compute token-level pairwise attention and average across tokens to obtain the overall attention of one sentence to the target sentence; we then average across the selected layers to obtain a more robust inter-sentence attention score. For each current sentence, the attention scores directed toward all

![](images/62d3bb066718738e59c8a4307810cefe989f7a4f417a1dd98fc5403e3700178f.jpg)  
Figure 4: Impact of the STM buffer threshold $( t h )$ on performance and efficiency across different compression ratios $( r )$ . Each radar chart represents a specific configuration of a model (GPT-4omini or Qwen3) and a fixed compression ratio. The axes measure six key metrics: Accuracy (ACC), token consumption (Input, Output, Total), API Calls, and Runtime. To facilitate comparison, all values are normalized for visualization on the chart.

preceding sentences are normalized within the sentence, yielding the final attention matrix. Residual fragments that remain after segmentation are carried over to the beginning of the next buffer for further processing, and this procedure continues iteratively until the dialogue ends.

Based on the attention pattern, we focus on the sequence formed by each sentence’s attention scores relative to its immediately preceding sentence, which directly reflects the continuity of local semantics. Therefore, we take the attention scores from the outermost layer of the attention map. When the attention score at a given position is higher than both its preceding and following positions, it is regarded as a local peak. If a sentence is identified as a peak, we set a segmentation point immediately before this sentence, making the peak sentence the beginning of a new segment. The rationale is that the peak sentence exhibits consistently low attention to all earlier sentences overall and reflects a clear transition from an old topic to a new one, indicating that the identified sentence marks the initiation of a new topic.

![](images/003fc368492f57c082202f01353686da7ed3638bdf51231a123543bff65b5202.jpg)

![](images/8c0e8397624c682a00d338ec18c4f68749c3a425d3608982b985cef799c00a54.jpg)  
Figure 5: Example of Topic Segment Attention Matrix.

Figure 5 illustrates three representative examples of reliable segmentation under $50 \%$ compression rate. In the first attention map, local peaks in the adjacent-sentence attention sequence appear at positions 5, 8, and 11, where the actual segmentation boundaries lie between sentences 4–5 and 11–12.

In the second attention map, peaks occur at positions 3, 8, 12, and 14, and the actual boundaries are located between sentences 7–8, 11–12, and 13–14. Overall, our method achieves close alignment with the majority of true boundaries while providing finer-grained segmentation. These examples demonstrate that our segmentation approach enables both fine-grained and reliable detection of topic boundaries, thereby validating its effectiveness.

# C.2 CATEGORY-WISE ACCURACY

As summarized in Table 7, retrieval-augmented and memory-centric methods (e.g., A-MEM, Mem0, MemoryOS) generally outperform Full Text on categories that demand information integration or belief revision, such as Temporal, Multi-Session, and Knowledge-Update. In contrast, categories such as Single-User and Single-Assistant, lightweight retrieval like Naive RAG is often competitive and can be the most reliable option, while Single-Preference shows higher variance due to its smaller sample size.

Table 7: Category-wise Accuracy. Accuracy $( \% )$ by method across question types. Parentheses indicate category proportion and sample size. For GPT, LightMem is configured with parameters $r = 0 . 7$ and $\mathrm { t h } = 5 1 2$ ; for Qwen, LightMem is configured with $r = 0 . 4$ and $\mathrm { t h } = 7 6 8$ .   

<table><tr><td>Method</td><td>Temporal (n=133)</td><td>Multi-Session (n=133)</td><td>Knowledge-Update (n=78)</td><td>Single-User (n=70)</td><td>Single-Assistant (n=56)</td><td>Single-Preference (n=30)</td></tr><tr><td colspan="7">GPT-4o-mini</td></tr><tr><td>Full Text</td><td>31.58</td><td>45.45</td><td>76.92</td><td>87.14</td><td>89.29</td><td>36.67</td></tr><tr><td>Naive RAG</td><td>39.85</td><td>48.48</td><td>67.95</td><td>90.00</td><td>98.21</td><td>53.33</td></tr><tr><td>LangMem</td><td>15.79</td><td>20.30</td><td>66.67</td><td>60.00</td><td>46.43</td><td>60.00</td></tr><tr><td>A-MEM</td><td>47.36</td><td>48.87</td><td>64.11</td><td>92.86</td><td>96.43</td><td>46.67</td></tr><tr><td>MemoryOS</td><td>32.33</td><td>31.06</td><td>48.72</td><td>80.00</td><td>64.29</td><td>30.00</td></tr><tr><td>Mem0</td><td>40.15</td><td>46.21</td><td>70.12</td><td>81.43</td><td>41.07</td><td>60.00</td></tr><tr><td>LightMem</td><td>67.18</td><td>71.74</td><td>83.12</td><td>87.14</td><td>32.14</td><td>68.18</td></tr><tr><td colspan="7">Qwen3-30B-A3B-Instruct-2507</td></tr><tr><td>Full Text</td><td>33.08</td><td>35.61</td><td>76.92</td><td>82.86</td><td>87.50</td><td>50.00</td></tr><tr><td>Naive RAG</td><td>36.84</td><td>47.73</td><td>65.38</td><td>91.43</td><td>98.21</td><td>70.00</td></tr><tr><td>LangMem</td><td>37.60</td><td>38.35</td><td>67.95</td><td>78.57</td><td>42.86</td><td>70.00</td></tr><tr><td>A-MEM</td><td>51.88</td><td>51.12</td><td>76.93</td><td>90.00</td><td>96.43</td><td>40.00</td></tr><tr><td>MemoryOS</td><td>28.57</td><td>36.84</td><td>61.54</td><td>72.86</td><td>92.86</td><td>33.33</td></tr><tr><td>Mem0</td><td>41.94</td><td>28.13</td><td>28.57</td><td>55.32</td><td>26.09</td><td>81.82</td></tr><tr><td>LightMem</td><td>54.20</td><td>51.91</td><td>66.67</td><td>80.00</td><td>31.25</td><td>80.00</td></tr></table>

# C.3 DETAILED PARAMETER ANALYSIS

As Table 9 shows, we report the numerical results of the effects of LightMem parameters (compression ratio $r$ and STM threshold $^ { t h }$ ).

# D EXPERIMENT DETAILS

# D.1 DATASETS AND BASELINES

Datasets The LongMemEval dataset (Wu et al., 2025) is designed to benchmark long-term interactive memory in conversational agents. It comprises 500 evaluation questions built upon extended user-assistant dialogues. It has two different versions with different lengths: the LONGMEMEVAL-S setting contains approximately 115k tokens per problem, while the LONGMEMEVAL-M setting extends up to 1.5 million tokens across 500 sessions. In our work, we adopt the LONGMEMEVAL-S version due to its balance between dialogue length and computational feasibility. The questions are categorized into multiple types: information extraction, multi-session reasoning, knowledge updates, temporal reasoning, and abstention. Overall, the dataset is characterized by extremely long

Table 8: The impact of LightMem compression ratio $r$ and STM buffer threshold $^ { t h }$ is reported here. Due to space limitations, we only present a subset of representative results of the online soft update results, with more results provided in the Figure 9.   

<table><tr><td>Model</td><td>th</td><td>r</td><td>ACC</td><td>Input (k)</td><td>Output (k)</td><td>Total (k)</td><td>Calls</td><td>Time</td></tr><tr><td rowspan="9">GPT</td><td>256</td><td>0.5</td><td>64.29</td><td>20.80</td><td>10.01</td><td>30.81</td><td>25.67</td><td>302.69</td></tr><tr><td>256</td><td>0.6</td><td>67.68</td><td>24.58</td><td>10.53</td><td>35.11</td><td>30.47</td><td>329.61</td></tr><tr><td>256</td><td>0.7</td><td>65.68</td><td>27.66</td><td>9.97</td><td>37.63</td><td>34.26</td><td>403.59</td></tr><tr><td>512</td><td>0.6</td><td>63.74</td><td>16.23</td><td>9.45</td><td>25.68</td><td>15.63</td><td>266.98</td></tr><tr><td>512</td><td>0.7</td><td>68.64</td><td>18.88</td><td>9.37</td><td>28.25</td><td>18.43</td><td>283.76</td></tr><tr><td>512</td><td>0.8</td><td>66.67</td><td>21.55</td><td>8.59</td><td>30.14</td><td>21.11</td><td>268.97</td></tr><tr><td>1024</td><td>0.6</td><td>59.68</td><td>10.34</td><td>7.68</td><td>18.20</td><td>7.69</td><td>177.45</td></tr><tr><td>1024</td><td>0.7</td><td>64.68</td><td>12.93</td><td>6.90</td><td>19.83</td><td>8.25</td><td>209.12</td></tr><tr><td>1024</td><td>0.8</td><td>64.35</td><td>14.86</td><td>6.28</td><td>21.14</td><td>9.43</td><td>216.08</td></tr><tr><td rowspan="9">Qwen</td><td>512</td><td>0.4</td><td>58.57</td><td>11.03</td><td>17.00</td><td>28.03</td><td>10.11</td><td>421.74</td></tr><tr><td>512</td><td>0.6</td><td>66.57</td><td>16.22</td><td>19.50</td><td>35.72</td><td>15.40</td><td>471.09</td></tr><tr><td>512</td><td>0.8</td><td>67.37</td><td>21.35</td><td>19.36</td><td>40.71</td><td>20.98</td><td>461.02</td></tr><tr><td>768</td><td>0.4</td><td>61.95</td><td>9.01</td><td>16.14</td><td>25.15</td><td>6.54</td><td>357.13</td></tr><tr><td>768</td><td>0.6</td><td>73.20</td><td>13.19</td><td>19.21</td><td>32.40</td><td>9.97</td><td>417.13</td></tr><tr><td>768</td><td>0.8</td><td>64.95</td><td>16.94</td><td>19.06</td><td>36.00</td><td>13.09</td><td>420.14</td></tr><tr><td>1024</td><td>0.4</td><td>53.91</td><td>8.02</td><td>15.44</td><td>23.46</td><td>4.83</td><td>300.56</td></tr><tr><td>1024</td><td>0.6</td><td>65.67</td><td>11.50</td><td>18.21</td><td>29.71</td><td>7.18</td><td>396.35</td></tr><tr><td>1024</td><td>0.8</td><td>68.69</td><td>14.82</td><td>18.49</td><td>33.31</td><td>9.43</td><td>355.71</td></tr></table>

histories, wide temporal spans, and diverse question types, making it a comprehensive benchmark for evaluating conversational agents’ memory capabilities. During the experiments, five samples from this dataset contained corrupted characters, which caused LightMem’s compression model to fail to run properly. Consequently, LightMem directly discarded these five samples when processing the dataset. However, their accuracy results were uniformly treated as false. The indices of these five samples in the dataset are 74, 183, 278, 351, and 380.

The LOCOMO benchmark targets the evaluation of long-range conversational memory. It features extremely long dialogues, with each conversation spanning roughly 300 turns and around 9K tokens on average. The accompanying questions fall into four categories—Single-hop, Multi-hop, Temporal, and Open-domain—providing a comprehensive assessment of different dimensions of memory in LLMs.

Baselines We compare our approach against several representative baselines of conversational memory modeling. $\textcircled{1}$ LANGMEM (LangChain, 2025): The Langchain’s long-term memory module. $\textcircled{2}$ A-MEM (Xu et al., 2025): Constructs a memory-centric knowledge graph, encoding each interaction as a structured memory note and linking these notes via LLM-driven reasoning. $\textcircled{3}$ MEM-ORYOS (Kang et al., 2025): Organizes conversational memory in an OS-inspired hierarchy, structuring interactions into short-term, mid-term, and long-term layers via paging and heat-based updating. $\textcircled{4}$ MEM0 (Chhikara et al., 2025): Extracts memories from dialogue turns through a combination of global summaries and recent context, maintaining them via LLM-guided operations.

# D.2 IMPLEMENTATION DETAILS

All the experiments are conducted on hardware equipped with 4×NVIDIA RTX 3090 GPUs, dual Intel Xeon Gold 6133 CPUs (40 cores, 80 threads), and 256 GB of RAM.

Table 9: The impact of LightMem’s compression ratio $( r )$ and STM buffer threshold $( t h )$ .   

<table><tr><td>Model</td><td>th</td><td>r</td><td>ACC</td><td>Input (k)</td><td>Output (k)</td><td>Total (k)</td><td>Calls</td><td>Time</td></tr><tr><td rowspan="23">GPT-40-mini</td><td>0</td><td>0.4</td><td>58.04</td><td>27.70</td><td>8.90</td><td>36.60</td><td>39.91</td><td>500.69</td></tr><tr><td>256</td><td>0.4</td><td>57.78</td><td>16.64</td><td>8.40</td><td>25.04</td><td>20.25</td><td>254.93</td></tr><tr><td>512</td><td>0.4</td><td>55.56</td><td>11.05</td><td>7.66</td><td>18.71</td><td>10.13</td><td>230.59</td></tr><tr><td>768</td><td>0.4</td><td>49.29</td><td>9.05</td><td>6.55</td><td>15.60</td><td>6.57</td><td>157.13</td></tr><tr><td>1024</td><td>0.4</td><td>46.87</td><td>7.75</td><td>5.25</td><td>13.00</td><td>4.82</td><td>118.11</td></tr><tr><td>0</td><td>0.5</td><td>62.89</td><td>30.84</td><td>9.75</td><td>40.59</td><td>43.56</td><td>550.36</td></tr><tr><td>256</td><td>0.5</td><td>64.29</td><td>20.80</td><td>10.01</td><td>30.81</td><td>25.67</td><td>302.69</td></tr><tr><td>512</td><td>0.5</td><td>62.44</td><td>13.49</td><td>8.89</td><td>22.38</td><td>12.70</td><td>250.36</td></tr><tr><td>768</td><td>0.5</td><td>56.12</td><td>10.93</td><td>7.57</td><td>18.50</td><td>8.12</td><td>203.13</td></tr><tr><td>1024</td><td>0.5</td><td>50.36</td><td>8.34</td><td>6.97</td><td>15.31</td><td>6.32</td><td>160.35</td></tr><tr><td>0</td><td>0.6</td><td>70.35</td><td>33.17</td><td>10.20</td><td>43.37</td><td>45.86</td><td>553.07</td></tr><tr><td>256</td><td>0.6</td><td>67.68</td><td>24.58</td><td>10.53</td><td>35.11</td><td>30.47</td><td>329.61</td></tr><tr><td>512</td><td>0.6</td><td>63.74</td><td>16.23</td><td>9.45</td><td>25.68</td><td>15.63</td><td>266.98</td></tr><tr><td>768</td><td>0.6</td><td>64.44</td><td>13.04</td><td>8.10</td><td>21.14</td><td>9.90</td><td>210.05</td></tr><tr><td>1024</td><td>0.6</td><td>59.68</td><td>10.34</td><td>7.68</td><td>18.20</td><td>7.69</td><td>177.45</td></tr><tr><td>0</td><td>0.7</td><td>62.35</td><td>35.36</td><td>9.76</td><td>45.12</td><td>48.08</td><td>573.42</td></tr><tr><td>256</td><td>0.7</td><td>65.68</td><td>27.66</td><td>9.97</td><td>37.63</td><td>34.26</td><td>403.59</td></tr><tr><td>512</td><td>0.7</td><td>68.64</td><td>18.88</td><td>9.37</td><td>28.25</td><td>18.43</td><td>283.76</td></tr><tr><td>1024</td><td>0.7</td><td>64.68</td><td>12.93</td><td>6.90</td><td>19.83</td><td>8.25</td><td>209.12</td></tr><tr><td>0</td><td>0.8</td><td>61.52</td><td>39.32</td><td>9.89</td><td>49.21</td><td>52.97</td><td>622.90</td></tr><tr><td>256</td><td>0.8</td><td>66.37</td><td>30.67</td><td>9.70</td><td>40.37</td><td>41.66</td><td>489.61</td></tr><tr><td>512</td><td>0.8</td><td>66.67</td><td>21.55</td><td>8.59</td><td>30.14</td><td>21.11</td><td>268.97</td></tr><tr><td>1024</td><td>0.8</td><td>64.35</td><td>14.86</td><td>6.28</td><td>21.14</td><td>9.43</td><td>216.08</td></tr><tr><td rowspan="15">Qwen3</td><td>0</td><td>0.4</td><td>56.89</td><td>28.44</td><td>18.30</td><td>46.74</td><td>41.08</td><td>594.94</td></tr><tr><td>256</td><td>0.4</td><td>52.37</td><td>16.82</td><td>17.63</td><td>34.45</td><td>20.48</td><td>450.98</td></tr><tr><td>512</td><td>0.4</td><td>58.57</td><td>11.03</td><td>17.00</td><td>28.03</td><td>10.11</td><td>421.74</td></tr><tr><td>768</td><td>0.4</td><td>61.95</td><td>9.01</td><td>16.14</td><td>25.15</td><td>6.54</td><td>357.13</td></tr><tr><td>1024</td><td>0.4</td><td>53.91</td><td>8.02</td><td>15.44</td><td>23.46</td><td>4.83</td><td>300.56</td></tr><tr><td>0</td><td>0.6</td><td>69.56</td><td>34.90</td><td>20.26</td><td>55.16</td><td>48.63</td><td>642.10</td></tr><tr><td>256</td><td>0.6</td><td>65.37</td><td>24.78</td><td>19.59</td><td>44.37</td><td>30.66</td><td>520.37</td></tr><tr><td>512</td><td>0.6</td><td>66.57</td><td>16.22</td><td>19.50</td><td>35.72</td><td>15.40</td><td>471.09</td></tr><tr><td>768</td><td>0.6</td><td>73.20</td><td>13.19</td><td>19.21</td><td>32.40</td><td>9.97</td><td>417.13</td></tr><tr><td>1024</td><td>0.6</td><td>65.67</td><td>11.50</td><td>18.21</td><td>29.71</td><td>7.18</td><td>396.35</td></tr><tr><td>0</td><td>0.8</td><td>67.68</td><td>37.97</td><td>20.18</td><td>58.15</td><td>50.81</td><td>759.15</td></tr><tr><td>256</td><td>0.8</td><td>64.52</td><td>30.54</td><td>19.77</td><td>50.31</td><td>37.35</td><td>550.98</td></tr><tr><td>512</td><td>0.8</td><td>67.37</td><td>21.35</td><td>19.36</td><td>40.71</td><td>20.98</td><td>461.02</td></tr><tr><td>768</td><td>0.8</td><td>64.95</td><td>16.94</td><td>19.06</td><td>36.00</td><td>13.09</td><td>420.14</td></tr><tr><td>1024</td><td>0.8</td><td>68.69</td><td>14.82</td><td>18.49</td><td>33.31</td><td>9.43</td><td>355.71</td></tr></table>

# E PROMPTS

# E.1 LLM-AS-JUDGE

# Standard Tasks (Single-session-user/assistant Multi-session)

I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no.

Question: {question}

Correct Answer: {answer}

Model Response: {response}

Is the model response correct? Answer yes or no only.

# Temporal Reasoning Tasks

I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response is equivalent to the correct answer or contains all the intermediate steps to get the correct answer, you should also answer yes. If the response only contains a subset of the information required by the answer, answer no. In addition, do not penalize off-by-one errors for the number of days. If the question asks for the number of days/weeks/months, etc., and the model makes off-by-one errors (e.g., predicting 19 days when the answer is 18), the model’s response is still correct.

Question: {question}

Correct Answer: {answer}

Model Response: {response}

Is the model response correct? Answer yes or no only.

# Knowledge Update Tasks

I will give you a question, a correct answer, and a response from a model. Please answer yes if the response contains the correct answer. Otherwise, answer no. If the response contains some previous information along with an updated answer, the response should be considered as correct as long as the updated answer is the required answer.

Question: {question}

Correct Answer: {answer}

Model Response: {response}

Is the model response correct? Answer yes or no only.

# Single-session Preference Tasks

I will give you a question, a rubric for desired personalized response, and a response from a model. Please answer yes if the response satisfies the desired response. Otherwise, answer no. The model does not need to reflect all the points in the rubric. The response is correct as long as it recalls and utilizes the user’s personal information correctly.

Question: {question}

Rubric: {answer}

Model Response: {response}

Is the model response correct? Answer yes or no only.

# Abstention Tasks

I will give you an unanswerable question, an explanation, and a response from a model. Please answer yes if the model correctly identifies the question as unanswerable. The model could say that the information is incomplete, or some other information is given but the asked information is not.

Question: {question}

Explanation: {answer}

Model Response: {response}

Does the model correctly identify the question as unanswerable? Answer yes or no only.