# Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity

Soyeong Jeong $^{1}$ Jinheon Baek $^{2}$ Sukmin Cho $^{1}$ Sung Ju Hwang $^{1,2}$ Jong C. Park $^{1*}$

School of Computing Graduate School of AI

Korea Advanced Institute of Science and Technology<sup>1,2</sup>

{starsuzi, jinheon.baek, nellllpic, sjhwang82,jongpark}@kaist.ac.kr

# Abstract

Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and inherent inductive biases in datasets. This approach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no-retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, covering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to relevant baselines including the adaptive retrieval approaches. Code is available at: https://github.com/starsuzi/Adaptive-RAG.

# 1 Introduction

Recent Large Language Models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023; Anil et al., 2023) have shown overwhelming performances across diverse tasks, including question-

![](images/8433104f72047eedb176249a6d968290fa3161e89772eccacc1543a8e50751e7.jpg)  
Figure 1: QA performance (F1) and efficiency (Time/Query) for different retrieval-augmented generation approaches. We use the GPT-3.5-Turbo-Instruct as the base LLM.

answering (QA) (Yang et al., 2018; Kwiatkowski et al., 2019). However, they still generate factually incorrect answers since their knowledge solely relies on their parametric memory (Kasai et al., 2022; Mallen et al., 2023). Meanwhile, memorizing all the (ever-changing) world knowledge may not be possible. To address this problem, retrieval-augmented LLMs (Borgeaud et al., 2022; Izacard et al., 2023; Shi et al., 2023), which incorporate non-parametric knowledge into LLMs with additional retrieval modules, have gained much increasing attention. Specifically, these models access a knowledge base, which serves as an extensive repository of information across various subjects and disciplines, to retrieve information relevant to the given input, and then incorporate the retrieved information into LLMs, which enables them to stay accurate and current with the world knowledge.

A particularly salient application of retrieval-augmented LLMs is to handling QA tasks, whose goal is to provide correct answers in response to user queries, especially those of high complexity. Early work on retrieval-augmented LLMs focuses primarily on single-hop queries (Lazaridou et al., 2022; Ram et al., 2023), whose answers are typically found within a single document; therefore, this approach involves retrieving a relevant document based on the query and subsequently integrating this information into QA models to formulate a response. However, unlike this single-hop QA, some queries require connecting and aggregating multiple documents, which are, furthermore,

![](images/3022ca0981a47f3d69e5bf6b8df9c599ca5e4b9dfcdefa3d829df03d1f8bc9cd.jpg)  
Figure 2: A conceptual comparison of different retrieval-augmented LLM approaches to question answering. (A) In response to a query, this single-step approach retrieves relevant documents and then generates an answer. However, it may not be sufficient for complex queries that require multi-step reasoning. (B) This multi-step approach iteratively retrieves documents and generates intermediate answers, which is powerful yet largely inefficient for the simple query since it requires multiple accesses to both LLMs and retrievers. (C) Our adaptive approach can select the most suitable strategy for retrieval-augmented LLMs, ranging from iterative, to single, to even no retrieval approaches, based on the complexity of given queries determined by our classifier.

often not answerable through a single-step process of retrieval-and-response. An example query is 'When did the people who captured Malakoff come to the region where Philipsburg is located?', which requires four reasoning steps to solve. Therefore, to effectively handle such complex queries, recent studies have concentrated largely on multi-step and multi-reasoning QA, which requires iterative accesses to both LLMs and retrievers multiple times (Press et al., 2023; Trivedi et al., 2023), at the cost of heavy computational overheads.

Yet, we should rethink: In a real-world scenario, are all the requests from users complex? Instead, users might often ask simple and straightforward questions, while only occasionally asking complex ones. Specifically, a query such as 'Paris is the capital of what?' is likely to be asked more frequently, compared to the aforementioned multi-step query, and this simpler query might also be easily answered by the LLMs themselves, without accessing external knowledge. In other words, a multi-step QA approach could give rise to unnecessary computational overhead for simple queries, even though it would be vital for complex queries (see Figure 2 (A)). On the other hand, handling complex queries with single-step-retrieval or even non-retrieval strategies would be largely insufficient (Figure 2 (B)). This suggests the need for an adaptive QA system, which can dynamically adjust the operational strategies of retrieval-augmented LLMs based on the query complexity. While some recent approaches are capable of doing this based on the frequency of entities in queries (Mallen et al., 2023; Zhao et al., 2023) or on the generated outputs from models for multi-step QA (Trivedi et al., 2023), they are still suboptimal: the former methods are overly simplistic, failing to consider multi-hop queries; meanwhile, the latter are excessively complex, terminating answer solving steps after several rounds of module access.

In this work, considering diverse complexity levels of real-world queries, we argue that previous one-size-fits-all approaches might be inadequate to cover all of them. Instead, we propose to select the most suitable strategy from a range of (retrieval-augmented) LLMs, each of which is tailored to the specific complexity of the input query. Notably, a critical step in this process is pre-defining the query complexity, which is instrumental in determining the most fitting model to it. In this work, we operationalize this process with a novel classifier, which is a smaller model trained to predict the complexity level of incoming queries (see Figure 2 (c)). Moreover, we automatically collect its training datasets without human labeling, by leveraging the predicted outcomes (i.e., which models accurately respond to which queries) as well as by capitalizing on the inherent biases in existing datasets (i.e., samples in the datasets are designed either for single-step or for multi-step QA scenarios). This proposed method can offer a robust middle ground among the iterative LLM augmentation methods for complex queries, single-step methods for simpler queries, and even no-retrieval-augmented methods for the most straightforward queries (answerable by LLMs themselves), thus significantly enhancing the overall efficiency and accuracy, as shown in Figure 1. We refer to our framework as Adaptive Retrieval-Augmented Generation (Adaptive-RAG).

We validate Adaptive-RAG using benchmark open-domain QA datasets, covering a wide range of query complexity from single-hop (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019) to multi-hop (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022b) queries. The experimental results show that ours significantly improves the overall accuracy and efficiency, compared to the prior adaptive strategies, on multiple LLMs, such as GPT-3.5 (Brown et al., 2020) and FLAN-T5 series (Chung et al., 2022).

Our contributions and findings are threefold:

- We point out the realistic scenario of queries of varying complexities, and find out that existing retrieval-augmented generation approaches tend to be overly simple or complex.   
- We adapt retrieval-augmented LLMs to the query complexity assessed by the classifier, which enables the utilization of the most suitable approach tailored to each query.   
- We show that our Adaptive-RAG is highly effective and efficient, balancing between the complexity and the simplicity for diverse queries.

# 2 Related Work

Open-domain QA Open-domain QA is the task of accurately answering a query by sourcing for query-relevant documents, and then interpreting them to provide answers (Chen et al., 2017; Zhu et al., 2021), which, thus, generally involves two modules: a retriever (Karpukhin et al., 2020; Xiong et al., 2021) and a reader (Yang et al., 2019; Izacard and Grave, 2021; Jeong et al., 2023). Along with the emergence of LLMs with superior reasoning capabilities thanks to their billion-sized parameters (Wei et al., 2022a), a synergy between LLMs and retrievers has led to significant advancements (Lazaridou et al., 2022; Ram et al., 2023). Specifically, this integration has been shown to enhance Open-domain QA by mitigating the hallucination problem from LLMs through strengthened reasoning abilities of the reader, as well as utilizing the retrieved, external documents (Cho et al., 2023). Despite these advancements for single-hop retrieval-augmented LLMs, however, the complexity of some queries needs a more complex strategy.

Multi-hop QA Multi-hop QA is an extension of conventional Open-domain QA, which additionally requires the system to comprehensively gather and contextualize information from multiple documents (often iteratively), to answer more complex queries (Trivedi et al., 2022a; Yang et al., 2018). In the realm of multi-hop QA, the approach to iteratively access both LLMs and the retrieval module is generally employed. Specifically, Khattab et al. (2022), Press et al. (2023), Pereira et al. (2023) and Khot et al. (2023) proposed to first decompose the multi-hop queries into simpler single-hop queries, repeatedly access the LLMs and retriever to solve these sub-queries, and merge their solutions to formulate a complete answer. In contrast

to this decomposition-based approach, other recent studies, such as Yao et al. (2023) and Trivedi et al. (2023), explored the interleaving of Chain-of-Thought reasoning (Wei et al., 2022b) — a method where a logical sequence of thoughts is generated — with document retrieval, repeatedly applying this process until the reasoning chain generates the answer. In addition, Jiang et al. (2023) introduced an approach to repeatedly retrieving new documents if the tokens within generated sentences have low confidence. However, the aforementioned methods overlooked the fact that, in real-world scenarios, queries are of a wide variety of complexities. Therefore, it would be largely inefficient to iteratively access LLMs and retrievers for every query, which might be simple enough with a single retrieval step or even only with an LLM itself.

Adaptive Retrieval To handle queries of varying complexities, the adaptive retrieval strategy aims to dynamically decide whether to retrieve documents or not, based on each query's complexity. In this vein, Mallen et al. (2023) proposed to decide the query's complexity level based on the frequency of its entities and suggested using the retrieval modules only when the frequency falls below a certain threshold. However, this approach, focusing solely on the binary decision of whether to retrieve or not, may not be sufficient for more complex queries that require multiple reasoning steps. Additionally, Qi et al. (2021) proposed an approach that performs a fixed set of operations (retrieving, reading, and reranking) multiple times until the answer is derived for the given query, which is built upon traditional BERT-like LMs. However, unlike our Adaptive-RAG which pre-determines the query complexity and adapts the operational behavior of any off-the-shelf LLMs accordingly, this approach applies the same fixed operations to every query regardless of its complexity but also necessitates additional specific training to LMs. Concurrent to our work, Asai et al. (2024) suggested training a sophisticated model to dynamically retrieve, critique, and generate the text. Nevertheless, we argue that all the aforementioned adaptive retrieval methods that rely on a single model might be suboptimal in handling a variety of queries of a range of different complexities since they tend to be either overly simple or complex for all the input queries, which demands a new approach that can select the most suitable strategy of retrieval-augmented LLMs tailored to the query complexity.

# 3 Method

In this section, we describe our approach to adapting retrieval-augmented LLMs, by pre-determining the query complexity and then selecting the most fitting strategies for retrieval-augmented LLMs.

# 3.1 Preliminaries

We begin with preliminaries, formally introducing different strategies of retrieval-augmented LLMs.

Non Retrieval for QA Let us first define an LLM as a model LLM, which takes a sequence of tokens $\pmb{x} = [x_{1}, x_{2}, \dots, x_{n}]$ as an input and then generates a sequence of tokens $\pmb{y} = [y_{1}, y_{2}, \dots, y_{n}]$ as an output, which is formalized as follows: $\pmb{y} = \mathsf{LLM}(\pmb{x})$ . Then, in our problem setup for QA, $\pmb{x}$ and $\pmb{y}$ become the input query $(\pmb{q})$ from the user and the generated answer $(\bar{\pmb{a}})$ from the LLM, respectively: $\pmb{q} = \pmb{x}$ and $\bar{\pmb{a}} = \pmb{y}$ . Also, subsequently, the most naive LLM-powered QA model can be represented as follows: $\bar{\pmb{a}} = \mathsf{LLM}(\pmb{q})$ . Ideally, $\bar{\pmb{a}}$ should match the actual correct answer $\pmb{a}$ . This non-retrieval-based QA method is highly efficient and could be a somewhat promising approach to handling easy queries, as the size of LLMs becomes extremely large with its effect on storing a large amount of knowledge. However, this approach is largely problematic on queries that require precise or concurrent knowledge of specific people, events, or any subjects beyond the LLMs' internal knowledge.

Single-step Approach for QA To address the aforementioned scenarios where LLM may struggle with queries that are not answerable by LLM itself, we can utilize the external knowledge $\pmb{d}$ , which includes useful information for queries, retrieved from the external knowledge source $\mathcal{D}$ that could be an encyclopedia (e.g., Wikipedia) consisting of millions of documents. Specifically, to obtain such $\pmb{d}$ from $\mathcal{D}$ , a specific retrieval model is necessary, which returns documents based on their relevance with the given query. This process can be formulated as follows: $\pmb{d} = \text{Retriever}(\pmb{q}; D)$ , where $\text{Retriever}$ is the retrieval model, with $\pmb{d} \in \mathcal{D}$ . Here, we can use any off-the-shelf retriever (Robertson et al., 1994; Karpukhin et al., 2020).

After the retrieval step is done, we now have a pair of query $\mathbf{q}$ and its relevant documents $d$ . Then, in order to augment LLMs with this retrieved external knowledge, we can incorporate it into the input of LLMs, represented as follows: $\bar{a} = \mathsf{LLM}(q,d)$ .

This process allows LLMs to gain access to external information contained in $d$ , which can provide the supplementary context that the internal knowledge of LLM lacks, which can subsequently improve the accuracy and concurrency of LLMs for QA.

Multi-step Approach for QA Even though the aforementioned single-step approach offers significant improvements over non-retrieval for $\mathbf{q}$ that requires external knowledge, it encounters notable limitations, particularly when dealing with complex queries that necessitate synthesizing information from multiple source documents and reasoning over them. This is where a multi-step approach and reasoning for QA become essential.

In this multi-step approach, LLM interacts with Retriever in several rounds, progressively refining its understanding of $\mathbf{q}$ , until it formulates the final answer from findings accumulated across these multiple steps. Specifically, the process begins with the initial query $\mathbf{q}$ , and at every retrieval step $i$ , new documents $\mathbf{d}_i$ are retrieved from $\mathcal{D}$ and then incorporated into the input of LLMs, as follows: $\bar{\mathbf{a}}_i = \mathsf{LLM}(\mathbf{q},\mathbf{d}_i,\mathbf{c}_i)$ , where the additional context $\mathbf{c}_i$ can be composed of previous documents and outcomes $(\mathbf{d}_1,\mathbf{d}_2,\dots,\mathbf{d}_{i - 1},\bar{\mathbf{a}}_1,\bar{\mathbf{a}}_2,\dots,\bar{\mathbf{a}}_{i - 1})$ , and $\mathbf{d}_i = \mathrm{Retriever}(\mathbf{q},\mathbf{c}_i;D)^1$ . We would like to note that this iterative, multi-step process enables LLM to construct a more comprehensive and extensive foundation to solve queries effectively, specifically adept at complex multi-hop queries where answers depend on interconnected pieces of information. However, it is important to recognize that this multi-step approach can be resource-intensive due to the repeated accesses to Retriever and LLM, which entail substantial computational costs.

# 3.2 Adaptive-RAG: Adaptive Retrieval-Augmented Generation

We now introduce our adaptive retrieval-augmented LLMs, which are built upon three different strategies described in the previous section, and which are designed to select the most suitable strategy according to the complexity of queries.

Adapting Retrieval-Augmented LLMs Note that in real-world scenarios, not all $\mathbf{q}$ from users have the same level of complexity, necessitating

tailored strategies for handling each query. In other words, employing the most basic, non-retrieval-based approach $\mathsf{LLM}(q)$ to respond to the complex query $q$ would be also ineffective (Figure 2, A); conversely, using a more elaborate multi-step approach $\mathsf{LLM}(q,d,c)$ for simple $q$ would be inefficient (Figure 2, B). Therefore, our adaptive framework is designed to dynamically adjust the query-handling strategy of retrieval-augmented LLMs, which is achieved by determining the complexity of each query before attempting a solution. Notably, this framework can offer a robust middle ground with a range of solutions, from the simplest approach for the most straightforward queries, to the one-step approach for moderate queries, and up to the most comprehensive and rigorous approach for complex queries. In addition, since the operations of LLM and Retriever remain consistent regardless of inputs to them, our method can seemingly go back and forth across queries of different complexities, without changing the internal model architecture or parameters during adaption.

Query Complexity Assessment To operationalize our adaptive retrieval-augmented LLM framework, we should determine the query complexity, and to achieve this, we propose to model a complexity classifier, whose goal is to return the appropriate complexity level of the given query. Specifically, given the query $\mathbf{q}$ , our classifier can be formulated as follows: $o = \text{Classifier}(\mathbf{q})$ , where Classifier is a smaller Language Model that is trained to classify one of three different complexity levels and $o$ is its corresponding class label. In our classifier design, there are three class labels: 'A', 'B', and 'C', where 'A' indicates that $\mathbf{q}$ is straightforward and answerable by LLM(q) itself, 'B' indicates that $\mathbf{q}$ has the moderate complexity where at least a single-step approach LLM(q,d) is needed, and 'C' indicates that $\mathbf{q}$ is complex, requiring the most extensive solution LLM(q,d,c) $^2$ .

Training Strategy The remaining step is to train the smaller Language Model for Classifier, to accurately predict its complexity $o$ in response to the given query $\pmb{q}$ . Yet, there is no annotated dataset available for query-complexity pairs. Hence, we propose to automatically construct the training dataset with two particular strategies.

To be specific, we first aim at labeling the query

complexity based on the results from three different retrieval-augmented LLM strategies, in order to determine the label by its needs. For example, if the simplest non-retrieval-based approach correctly generates the answer, the label for its corresponding query is assigned 'A'. Also, to break the tie between different models in providing the label to the query, we provide a higher priority to a simpler model. In other words, if both single-step and multi-step approaches produce the same correct answer while the non-retrieval-based approach fails, we assign label 'B' to its corresponding query.

However, this labeling strategy has a limitation in that not all the queries are assigned labels, since the three retrieval-augmented approaches may all fail to generate the correct answer. On the other hand, the benchmark datasets may already have meaningful inductive biases about the most appropriate retrieval-augmented LLM strategies for their queries, considering the ways they are created (e.g., QA datasets that require sequential reasoning usually necessitate a multi-step approach; while queries of those with labeled single documents can be ideally answerable with the single-step approach). Therefore, for those queries that remain unlabeled after the first labeling step, we assign 'B' to queries in single-hop datasets and 'C' to queries in multi-hop datasets. Finally, we train Classifier with these automatically-collected query-complexity pairs<sup>3</sup>, by using a cross-entropy loss. Then, at inference, we can determine the complexity of the query, which is one of $\{\mathbf{A}', \mathbf{B}', \mathbf{C}'\}$ , by forwarding it to Classifier: $o = \text{Classifier}(\mathbf{q})$ .

# 4 Experimental Setups

In this section, we explain datasets, models, metrics, and implementation details. We provide additional details in Appendix A.

# 4.1 Datasets

In order to simulate a realistic scenario, where different queries have varying complexities, we use both the single-hop and multi-hop QA datasets simultaneously, in the unified experimental setting.

Single-hop QA For simpler queries, we use three benchmark single-hop QA datasets, which consist

Table 1: Averaged results on a collection of benchmark datasets for open-domain question answering including the single-hop and multi-hop queries, with different LLMs. Self-RAG* is trained with a different base LLM, namely LLaMA2 (Touvron et al., 2023); therefore, we compare the results of FLAN-T5-XL (3B) with the results from Self-RAG with LLaMA2 (7B) and the results of others with the results from Self-RAG with LLaMA2 (13B). We emphasize our results in bold, for easy comparisons.   

<table><tr><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">FLAN-T5-XL (3B)</td><td colspan="5">FLAN-T5-XXL (11B)</td><td colspan="5">GPT-3.5 (Turbo)</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="2">Simple</td><td>No Retrieval</td><td>14.87</td><td>21.12</td><td>15.97</td><td>0.00</td><td>0.11</td><td>17.83</td><td>25.14</td><td>19.33</td><td>0.00</td><td>0.08</td><td>35.77</td><td>48.56</td><td>44.27</td><td>0.00</td><td>0.71</td></tr><tr><td>Single-step Approach</td><td>34.83</td><td>44.31</td><td>38.87</td><td>1.00</td><td>1.00</td><td>37.87</td><td>47.63</td><td>41.90</td><td>1.00</td><td>1.00</td><td>34.73</td><td>46.99</td><td>45.27</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>23.87</td><td>32.24</td><td>26.73</td><td>0.50</td><td>0.56</td><td>26.93</td><td>35.67</td><td>29.73</td><td>0.50</td><td>0.54</td><td>35.90</td><td>48.20</td><td>45.30</td><td>0.50</td><td>0.86</td></tr><tr><td>Self-RAG*</td><td>9.90</td><td>20.79</td><td>31.57</td><td>0.72</td><td>0.43</td><td>10.87</td><td>22.98</td><td>34.13</td><td>0.74</td><td>0.23</td><td>10.87</td><td>22.98</td><td>34.13</td><td>0.74</td><td>1.50</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>37.17</td><td>46.94</td><td>42.10</td><td>2.17</td><td>3.60</td><td>38.90</td><td>48.62</td><td>43.77</td><td>1.35</td><td>2.00</td><td>37.97</td><td>50.91</td><td>48.97</td><td>1.03</td><td>1.46</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>39.00</td><td>48.85</td><td>43.70</td><td>4.69</td><td>8.81</td><td>40.13</td><td>50.09</td><td>45.20</td><td>2.13</td><td>3.80</td><td>38.13</td><td>50.87</td><td>49.70</td><td>2.81</td><td>3.33</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>45.00</td><td>56.28</td><td>49.90</td><td>1.28</td><td>2.11</td><td>47.17</td><td>58.60</td><td>52.20</td><td>0.84</td><td>1.10</td><td>47.70</td><td>62.80</td><td>58.57</td><td>0.50</td><td>1.03</td></tr></table>

of queries and their associated documents containing answers, namely 1) SQuAD v1.1 (Rajpurkar et al., 2016), 2) Natural Questions (Kwiatkowski et al., 2019), and 3) TriviaQA (Joshi et al., 2017).

Multi-hop QA To consider more complex query scenarios, we use three benchmark multi-hop QA datasets, which require sequential reasoning over multiple documents, namely 1) MuSiQue (Trivedi et al., 2022a), 2) HotpotQA (Yang et al., 2018), and 3) 2WikiMultiHopQA (Ho et al., 2020).

# 4.2 Models

We compare our Adaptive-RAG against relevant models, including three retrieval-augmented LLM strategies (in Section 3.1) and the adaptive retrieval approaches (Mallen et al., 2023; Asai et al., 2024), which can be grouped into one of three categories: Simple, Adaptive, and Complex. Specifically, Simple approaches include the 1) No Retrieval and 2) Single-step Approach-based methods. Adaptive approaches include the 3) Adaptive Retrieval (Mallen et al., 2023), 4) Self-RAG (Asai et al., 2024), and our 5) Adaptive-RAG, which can adaptively perform retrieval based on the question complexity. For the 6) Multi-step Approach, we use the most sophisticated state-of-the-art method (Trivedi et al., 2023), iteratively accessing both the retriever and LLM with Chain-of-Thought reasoning (Wei et al., 2022b), for every query. Note that models across different categories are not directly comparable. Yet, in the ideal setting, Adaptive approaches should be more effective than those in the Simple category while simultaneously being more efficient than the Complex one. Therefore, we also report the performance in an ideal scenario, 7) Adaptive-RAG w/ Oracle, using the oracle classifier with our Adaptive-RAG.

# 4.3 Evaluation Metrics

When it comes to evaluating adaptive models, it is essential to simultaneously consider both the

task performance and efficiency along with their trade-offs. Thus, we report the results with five metrics, where three of them measure the effectiveness and the other two measure the efficiency. In particular, for effectiveness, we use F1, EM, and Accuracy (Acc), following the standard evaluation protocol (Mallen et al., 2023; Baek et al., 2023; Asai et al., 2024), where F1 measures the number of overlapping words between the predicted answer and the ground truth, EM measures whether they are the same, and Acc measures whether the predicted answer contains the ground-truth answer. For efficiency, we measure the number of retrieval-and-generate steps and the average time for answering each query relative to the one-step approach.

# 4.4 Implementation Details

For a fair comparison and following Mallen et al. (2023) and Trivedi et al. (2023), we use the same retriever, a term-based sparse retrieval model known as BM25 (Robertson et al., 1994), across all different models. For the external document corpus, we use different sources depending on the dataset type: the Wikipedia corpus preprocessed by Karpukhin et al. (2020) for single-hop datasets, and the preprocessed corpus by Trivedi et al. (2023) for multi-hop datasets. Regarding the LLMs that are used to generate answers, we use the FLAN-T5 series models (Chung et al., 2022) of XL with 3B parameters and XXL with 11B parameters, and the GPT-3.5 model (gpt-3.5-turbo-instruct). For the retrieval-augmented LLM design, we follow the implementation details from Trivedi et al. (2023), which include input prompts, instructions, and the number of test samples for evaluation (e.g., 500 samples per dataset). In our Adaptive-RAG, for the query-complexity classifier, we use and train the T5-Large model (Raffel et al., 2020). Specifically, the classifier is trained using the epoch that shows the best performance until 100 training iterations from the validation set, with the learning rate of 3e

Table 2: Results on each of a collection of datasets with FLAN-T5-XL (3B) as the LLM. We emphasize our results in bold.   

<table><tr><td rowspan="2">Data</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">SQuAD</td><td colspan="5">Natural Questions</td><td colspan="5">TriviaQA</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="7">Single-step</td><td rowspan="2">Simple</td><td>No Retrieval</td><td>3.60</td><td>10.50</td><td>5.00</td><td>0.00</td><td>0.11</td><td>14.20</td><td>19.00</td><td>15.60</td><td>0.00</td><td>0.13</td><td>25.00</td><td>31.80</td><td>27.00</td><td>0.00</td><td>0.13</td></tr><tr><td>Single-step Approach</td><td>27.80</td><td>39.30</td><td>34.00</td><td>1.00</td><td>1.00</td><td>37.80</td><td>47.30</td><td>44.60</td><td>1.00</td><td>1.00</td><td>53.60</td><td>62.40</td><td>60.20</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>13.40</td><td>23.10</td><td>17.60</td><td>0.50</td><td>0.55</td><td>28.20</td><td>36.00</td><td>33.00</td><td>0.50</td><td>0.56</td><td>38.40</td><td>46.90</td><td>42.60</td><td>0.50</td><td>0.56</td></tr><tr><td>Self-RAG*</td><td>2.20</td><td>11.20</td><td>18.40</td><td>0.63</td><td>0.50</td><td>31.40</td><td>39.00</td><td>33.60</td><td>0.63</td><td>0.17</td><td>12.80</td><td>29.30</td><td>57.00</td><td>0.68</td><td>0.45</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>26.80</td><td>38.30</td><td>33.00</td><td>1.37</td><td>2.02</td><td>37.80</td><td>47.30</td><td>44.60</td><td>1.00</td><td>1.00</td><td>52.20</td><td>60.70</td><td>58.20</td><td>1.23</td><td>1.54</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>24.40</td><td>35.60</td><td>29.60</td><td>4.52</td><td>9.03</td><td>38.60</td><td>47.80</td><td>44.20</td><td>5.04</td><td>10.18</td><td>53.80</td><td>62.40</td><td>60.20</td><td>5.28</td><td>9.22</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>32.00</td><td>45.60</td><td>38.20</td><td>1.24</td><td>1.60</td><td>47.40</td><td>57.10</td><td>53.60</td><td>1.10</td><td>1.55</td><td>61.60</td><td>70.20</td><td>66.40</td><td>0.79</td><td>1.10</td></tr></table>

<table><tr><td rowspan="2">Data</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">MuSiQue</td><td colspan="5">HotpotQA</td><td colspan="5">2WikiMultiHopQA</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="7">Multi-step</td><td rowspan="2">Simple</td><td>No Retrieval</td><td>2.40</td><td>10.70</td><td>3.20</td><td>0.00</td><td>0.11</td><td>16.60</td><td>22.71</td><td>17.20</td><td>0.00</td><td>0.11</td><td>27.40</td><td>32.04</td><td>27.80</td><td>0.00</td><td>0.10</td></tr><tr><td>Single-step Approach</td><td>13.80</td><td>22.80</td><td>15.20</td><td>1.00</td><td>1.00</td><td>34.40</td><td>46.15</td><td>36.40</td><td>1.00</td><td>1.00</td><td>41.60</td><td>47.90</td><td>42.80</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>6.40</td><td>15.80</td><td>8.00</td><td>0.50</td><td>0.55</td><td>23.60</td><td>32.22</td><td>25.00</td><td>0.50</td><td>0.55</td><td>33.20</td><td>39.44</td><td>34.20</td><td>0.50</td><td>0.55</td></tr><tr><td>Self-RAG*</td><td>1.60</td><td>8.10</td><td>12.00</td><td>0.73</td><td>0.51</td><td>6.80</td><td>17.53</td><td>29.60</td><td>0.73</td><td>0.45</td><td>4.60</td><td>19.59</td><td>38.80</td><td>0.93</td><td>0.49</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>23.60</td><td>31.80</td><td>26.00</td><td>3.22</td><td>6.61</td><td>42.00</td><td>53.82</td><td>44.40</td><td>3.55</td><td>5.99</td><td>40.60</td><td>49.75</td><td>46.40</td><td>2.63</td><td>4.68</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>23.00</td><td>31.90</td><td>25.80</td><td>3.60</td><td>7.58</td><td>44.60</td><td>56.54</td><td>47.00</td><td>5.53</td><td>9.38</td><td>49.60</td><td>58.85</td><td>55.40</td><td>4.17</td><td>7.37</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>24.80</td><td>38.50</td><td>27.00</td><td>1.98</td><td>3.99</td><td>51.20</td><td>64.00</td><td>54.80</td><td>1.59</td><td>2.77</td><td>53.00</td><td>62.30</td><td>59.40</td><td>1.01</td><td>1.69</td></tr></table>

![](images/aefc3c148919263a51765a854777ef3588054460b3dfcfd830a26f7d12dff0ad.jpg)

![](images/bcf1791fb0aaa851b3ba45bd06118d6ab9c0cea209bae1ca963664765b842453.jpg)

![](images/24f99ccca2c6fad8349bb35f8982dda45133c0fbc7527f5296cb91e0ac03d1e4.jpg)  
Figure 3: Performance on QA and query-complexity assessment of different adaptive approaches for retrieval-augmented LLMs with FLAN-T5 XL (Left) and XXL (Center). For labeling the complexity of queries, we use the silver data annotated from the prediction outcomes of models (described in Section 3.2). We also provide the confusion matrix across three labels (Right).

5 and the AdamW (Loshchilov and Hutter, 2019) as an optimizer. Regarding its training data, we sample and annotate 400 queries from 6 datasets based on its inductive bias (single-hop for one-step approach and multi-hop for multi-step). In addition, we use predicted outcomes of three different strategies over 400 queries sampled from each dataset. Note that those queries used for classifier training do not overlap with the testing queries for QA.

# 5 Experimental Results and Analyses

In this section, we show the overall experimental results and offer in-depth analyses of our method.

Main Results First of all, Table 1 shows our main results averaged over all considered datasets, which corroborate our hypothesis that simple retrieval-augmented strategies are less effective than the complex strategy, while the complex one is significantly more expensive than the simple ones. In addition, we report the more granular results with FLAN-T5-XL on each of the single-hop and multi-hop datasets in Table 2 (and more with different LLMs in Table 7 and Table 8 of Appendix), which are consistent with the results observed in Table 1.

However, in a real-world scenario, not all users ask queries with the same level of complexity, which emphasizes the importance of the need for adaptive strategies. Note that among the adaptive strategies, our Adaptive-RAG shows remarkable

effectiveness over the competitors (Table 1). This indicates that merely focusing on the decision of whether to retrieve or not is suboptimal. Also, as shown in Table 2, such simple adaptive strategies are particularly inadequate for handling complex queries in multi-hop datasets, which require aggregated information and reasoning over multiple documents. Meanwhile, our approach can consider a more fine-grained query handling strategy by further incorporating an iterative module for complex queries. Furthermore, in a realistic setting, we should take into account not only effectiveness but also efficiency. As shown in Table 1, compared to the complex multi-step strategy, our proposed adaptive strategy is significantly more efficient across all model sizes. This is meaningful in this era of LLMs, where the cost of accessing them is a critical factor for practical applications and scalability. Finally, to see the upper bound of our Adaptive-RAG, we report its performances with the oracle classifier where the classification performance is perfect. As shown in Table 1 and Table 2, we observe that it achieves the best performance while being much more efficient than our Adaptive-RAG without the oracle classifier. These results support the validity and significance of our proposal for adapting retrieval-augmented LLM strategies based on query complexity, and further suggest the direction to develop more improved classifiers to achieve optimal performance.

Table 3: The exact elapsed time per query and the percentage of the predicted labels from the classifier over all samples.   

<table><tr><td>Labels</td><td>Time/Query (Sec.)</td><td>Percentage (%)</td></tr><tr><td>No (A)</td><td>0.35</td><td>8.60</td></tr><tr><td>One (B)</td><td>3.08</td><td>53.33</td></tr><tr><td>Multi (C)</td><td>27.18</td><td>38.07</td></tr></table>

Classifier Performance To understand how the proposed classifier works, we analyze its performance across different complexity labels. As Figure 3 (Left and Center) shows, the classification accuracy of our Adaptive-RAG is better than those of the other adaptive retrieval baselines, which leads to overall QA performance improvements. In other words, this result indicates that our Adaptive-RAG is capable of more accurately classifying the complexity levels with various granularities, which include not performing retrieval, performing retrieval only once, and performing retrieval multiple times. In addition to the true positive performance of our classifier averaged over all those three labels in Figure 3 (Left and Center), we further report its confusion matrix in Figure 3 (Right). We note that the confusion matrix reveals some notable trends: 'C (Multi)' is sometimes misclassified as 'B (One)' (about $31\%$ ) and 'B (One)' as 'C (Multi)' (about $23\%$ ); 'A (No)' is misclassified often as 'B (One)' (about $47\%$ ) and less frequently as 'C (Multi)' (about $22\%$ ). While the overall results in Figure 3 show that our classifier effectively categorizes the three labels, further refining it based on such misclassification would be a meaningful direction for future work.

Analyses on Efficiency for Classifier While Table 1 shows the relative elapsed time for each of the three different RAG strategies, we further provide the exact elapsed time per query for our AdaptiveRAG and the distribution for predicted labels from our query-complexity classifier in Table 3. Similar to the results of the elapsed time in Table 1 (relative time), Table 3 (exact time) shows that efficiency can be substantially improved by identifying simple or straightforward queries.

Analyses on Training Data for Classifier We have shown that the classifier plays an important role in adaptive retrieval. Here, we further analyze the different strategies for training the classifier by ablating our full training strategy, which includes two approaches: generating silver data from predicted outcomes of models and utilizing inductive

Table 4: Results on QA and complexity classification with varying the data annotation strategies for training the classifier.   

<table><tr><td rowspan="2">Training Strategies</td><td colspan="2">QA</td><td colspan="4">Classifier (Accuracy)</td></tr><tr><td>F1</td><td>Step</td><td>All</td><td>No</td><td>One</td><td>Multi</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>46.94</td><td>1084</td><td>54.52</td><td>30.52</td><td>66.28</td><td>65.45</td></tr><tr><td>w/o Binary</td><td>43.43</td><td>640</td><td>60.30</td><td>62.19</td><td>65.70</td><td>39.55</td></tr><tr><td>w/o Silver</td><td>48.79</td><td>1464</td><td>40.00</td><td>0.00</td><td>53.98</td><td>75.91</td></tr></table>

bias in datasets (see Section 3.2). As Table 4 shows, compared to the training strategy relying solely on the data derived from inductive bias, ours is significantly more efficient. This efficiency is partly because ours also takes into account the case that does not consider any documents at all, as also implied by the classification accuracy; meanwhile, queries in the existing datasets do not capture the information on whether the retrieval is required or not. On the other hand, in the case of only using the silver data annotated from the correct predictions, while its overall classification accuracy is high, the overall QA performance implies that relying on the silver data may not be optimal. This may be because this silver data does not cover complexity labels over incorrectly predicted queries, which leads to lower generalization effect on queries relevant to them. Meanwhile, by also incorporating complexity labels from dataset bias (single-hop vs multi-hop), the classifier becomes more accurate in predicting multi-hop queries, leading to the better performance. It is worth noting that our automatic labeling strategies are two particular instantiations for training the classifier, and that there could be other instantiations, which we leave as future work.

Analyses on Classifier Size To investigate the sensitivity of our classifier according to its varying sizes, we conducted further experiments. As shown in Table 6, we observe no significant performance differences among classifiers of various sizes, even with reduced complexity and fewer parameters in smaller classifiers. This indicates that our proposed classifier can contribute to resource-efficient settings in real-use cases with smaller sizes without compromising the performance.

Case Study We conduct a case study to qualitatively compare our Adaptive-RAG against Adaptive Retrieval. Table 5 shows the classified complexity and the query handling patterns for both simple and complex questions. First, for the simple single-hop question, our Adaptive-RAG identifies that it is answerable by only using the LLM's parametric knowledge about 'Google'. By contrast,

Table 5: Case study with GPT-3.5 (Turbo), where we represent the factual error in red and the accurate information in blue.   

<table><tr><td>Dataset</td><td>Question</td><td>Adaptive Retrieval</td><td>Adaptive-RAG (Ours)</td></tr><tr><td>NQ (Single-hop)</td><td>Which famous corporate logo changed to a flat colour/color sans serif font in its first major change since 1999?</td><td>Query Type: B (Single-step Approach) Reasoning: The famous corporate logo that changed to a flat color sans-serif font in its first major change since 1999 is Microsoft. Microsoft changed its logo in 2012, replacing the italicized, multi-colored font with a simple, flat, grey font. So the answer is: Microsoft.</td><td>Query Type: A (Non Retrieval) Reasoning: The famous corporate logo that changed to a flat color sans serif font in its first major change since 1999 is Google. Google changed its logo in 2015. So the answer is: Google.</td></tr><tr><td>MuSiQue (Multi-hop)</td><td>Who is the child of the Italian navigator who explored the eastern coast of the continent César Gaytan was born in for the English?</td><td>Query Type: A (Non Retrieval) Reasoning: César Gaytan was born in the continent of North America. The Italian navigator who explored the eastern coast of North America for the English was Giovanni Caboto, also known as John Cabot. So the answer is: Giovanni Caboto/John Cabot.</td><td>Query Type: C (Multi-step Approach) Reasoning: Césarr Gaytan was born in Guadalajara, Jalisco, Mexico. The Italian navigator who explored the eastern coast of the continent for the English is John Cabot. John Cabot&#x27;s son is Sebastian Cabot. So the answer is: Sebastian Cabot.</td></tr></table>

Table 6: Results with varying model sizes for classifiers.   

<table><tr><td rowspan="2">Sizes</td><td colspan="2">QA</td><td colspan="4">Classifier (Accuracy)</td></tr><tr><td>F1</td><td>Step</td><td>All</td><td>No</td><td>One</td><td>Multi</td></tr><tr><td>Small (60M)</td><td>45.83</td><td>964</td><td>53.48</td><td>26.65</td><td>70.62</td><td>53.18</td></tr><tr><td>Base (223M)</td><td>45.97</td><td>983</td><td>53.41</td><td>26.42</td><td>69.46</td><td>56.82</td></tr><tr><td>Large (770M)</td><td>46.94</td><td>1084</td><td>54.52</td><td>30.52</td><td>66.28</td><td>65.45</td></tr></table>

Adaptive Retrieval fetches additional documents, leading to longer processing times and occasionally producing incorrect responses due to the inclusion of partially irrelevant information about 'Microsoft'. Meanwhile, faced with a complex question, Adaptive-RAG seeks out relevant information, including details like 'a son of John Cabot', which may not have been stored in LLMs, while Adaptive Retrieval fails to request such information from external sources, resulting in inaccurate answers.

# 6 Conclusion

In this work, we proposed the Adaptive Retrieval-Augmented Generation framework, referred to as Adaptive-RAG, to handle queries of various complexities. Specifically, Adaptive-RAG is designed to dynamically adjust its query handling strategies in the unified retrieval-augmented LLM based on the complexity of queries that they encounter, which spans across a spectrum of the nonretrieval-based approach for the most straightforward queries, to the single-step approach for the queries of moderate complexity, and finally to the multi-step approach for the complex queries. The core step of our Adaptive-RAG lies in determining the complexity of the given query, which is instrumental in selecting the most suitable strategy for its answer. To operationalize this process, we trained a smaller Language Model with query-complexity pairs, which are automatically annotated from the predicted outcomes and the inductive biases in datasets. We validated our Adaptive-RAG

on a collection of open-domain QA datasets, covering the multiple query complexities including both the single- and multi-hop questions. The results demonstrate that our Adaptive-RAG enhances the overall accuracy and efficiency of QA systems, allocating more resources to handle complex queries while efficiently handling simpler queries, compared to the existing one-size-fits-all approaches that tend to be either minimalist or maximalist over varying query complexities.

# Limitations

While our Adaptive-RAG shows clear advantages in effectiveness and efficiency by determining the query complexity and then leveraging the most suitable approach for tackling it, it is important to recognize that there still exist potential avenues for improving the classifier from the perspectives of its training datasets and architecture. Specifically, as there are no available datasets for training the query-complexity classifier, we automatically create new data based on the model prediction outcomes and the inductive dataset biases. However, our labeling process is one specific instantiation of labeling the query complexity, and it may have the potential to label queries incorrectly despite its effectiveness. Therefore, future work may create new datasets that are annotated with a diverse range of query complexities, in addition to the labels of question-answer pairs. Also, as the performance gap between the ideal classifier in Table 1 and the current classifier in Figure 3 indicates, there is still room to improve the effectiveness of the classifier. In other words, our classifier design based on the smaller LM is the initial, simplest instantiation for classifying the query complexity, and based upon it, future work may improve the classifier architecture and its performance, which will positively contribute to the overall QA performance.

# Ethics Statement

The experimental results on Adaptive-RAG validate its applicability in realistic scenarios, where a wide range of diverse user queries exist. Nonetheless, given the potential diversity of real-world user inputs, it is crucial to also consider scenarios where these inputs might be offensive or harmful. We should be aware that such inputs could lead to the retrieval of offensive documents and the generation of inappropriate responses by the retrieval-augmented LLMs. To address this challenge, developing methods to detect and manage offensive or inappropriate content in both user inputs and retrieved documents within the retrieval-augmented framework is essential. We believe that this is a critical area for future work.

# Acknowledgements

This work was supported by Institute for Information and communications Technology Promotion (IITP) grant funded by the Korea government (No. 2018-0-00582, Prediction and augmentation of the credibility distribution via linguistic analysis and automated evidence document collection), Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (RS-2023-00275747), and the Artificial intelligence industrial convergence cluster development project funded by the Ministry of Science and ICT (MSIT, Korea) & Gwangju Metropolitan City.

# References

Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403.

Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and

Hannaneh Hajishirzi. 2024. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations.

Jinheon Baek, Soyeong Jeong, Minki Kang, Jong Park, and Sung Ju Hwang. 2023. Knowledge-augmented language model verification. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1720-1736. Association for Computational Linguistics.

Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 2206-2240. PMLR.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.

Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1870-1879. Association for Computational Linguistics.

Sukmin Cho, Jeongyeon Seo, Soyeong Jeong, and Jong C. Park. 2023. Improving zero-shot reader by reducing distractions from irrelevant documents in open-domain question answering. In *Findings of the Association for Computational Linguistics: EMNLP* 2023, Singapore, December 6-10, 2023, pages 3145-3157. Association for Computational Linguistics.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan

Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.   
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 6609-6625. International Committee on Computational Linguistics.   
Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 874-880. Association for Computational Linguistics.   
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. J. Mach. Learn. Res., 24:251:1-251:43.   
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong Park. 2023. Test-time self-adaptive small language models for question answering. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 15459-15469. Association for Computational Linguistics.   
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Active retrieval augmented generation. In EMNLP 2023.   
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 1601-1611. Association for Computational Linguistics.   
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, November 16-20, 2020. Association for Computational Linguistics.   
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir R. Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui.

2022. Realtime QA: what's the answer right now? arXiv preprint arXiv:2207.13332.   
Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive NLP. arXiv preprint arXiv.2212.14024, abs/2212.14024.   
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2023. Decomposed prompting: A modular approach for solving complex tasks. In *The Eleventh International Conference on Learning Representations*, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.   
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.   
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. 2022. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115.   
Belinda Z. Li, Sewon Min, Srinivasan Iyer, Yashar Mehdad, and Wen-tau Yih. 2020. Efficient one-pass end-to-end entity linking for questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 6433-6441. Association for Computational Linguistics.   
Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.   
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 9802-9822. Association for Computational Linguistics.   
OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.   
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Z.

Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, pages 8024-8035.   
Jayr Alencar Pereira, Robson do Nascimento Fidalgo, Roberto de Alencar Lotufo, and Rodrigo Frassetto Nogueira. 2023. Visconde: Multi-document QA with GPT-3 and neural reranking. In Advances in Information Retrieval - 45th European Conference on Information Retrieval, ECIR 2023, Dublin, Ireland, April 2-6, 2023, Proceedings, Part II, volume 13981 of Lecture Notes in Computer Science, pages 534-543. Springer.   
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In *Findings of the Association for Computational Linguistics: EMNLP* 2023.   
Peng Qi, Haejun Lee, Tg Sido, and Christopher D. Manning. 2021. Answering open-domain questions of varying reasoning steps from text. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 3599-3614. Association for Computational Linguistics.   
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.   
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100, $000+$ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016, pages 2383-2392. The Association for Computational Linguistics.   
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics.   
Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text Retrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994, volume 500-225 of NIST Special Publication, pages 109-126. National Institute of Standards and Technology (NIST).   
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. REPLUG: retrieval

augmented black-box language models. arXiv preprint arXiv:2301.12652.   
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288.   
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022a. Musique: Multi-hop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539-554.   
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022b. MuSiQue: Multi-hop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539-554.   
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 10014-10037. Association for Computational Linguistics.   
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022.   
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.   
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,

Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2020 - Demos, pages 38-45. Association for Computational Linguistics.   
Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul N. Bennett, Junaid Ahmed, and Arnold Overwijk. 2021. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.   
Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with bertserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations, pages 72-77. Association for Computational Linguistics.   
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics.   
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R. Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net.   
Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu, and Jianshu Chen. 2023. Thrust: Adaptively propels large language models with external knowledge. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.   
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. 2021. Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774.

![](images/41ae5019f98e001a4d694e449f0a312680c98d2b77498ff06462c3a7af052574.jpg)  
Figure 4: QA performance (F1) and efficiency (Time/Query) for different retrieval-augmented generation approaches. We use the FLAN-T5-XL (3B) as the base LLM.

# A Additional Experimental Setups

# A.1 Datasets

We use publicly open datasets for both single-hop and multi-hop QA datasets, referring to as Karpukhin et al. (2020) and Trivedi et al. (2023), respectively. We describe the characteristics of each dataset:

1) SQuAD v1.1 (Rajpurkar et al., 2016) is created through a process where annotators write questions based on the documents they read.   
2) Natural Questions (Kwiatkowski et al., 2019) is constructed by real user queries on Google Search.   
3) TriviaQA (Joshi et al., 2017) comprises trivia questions sourced from various quiz websites.   
4) MuSiQue (Trivedi et al., 2022a) is collected by compositing multiple single-hop queries, to form queries spanning 2-4 hops.   
5) HotpotQA (Yang et al., 2018) is constructed by having annotators create questions that link multiple Wikipedia articles.   
6) 2WikiMultiHopQA (Ho et al., 2020) is derived from Wikipedia and its associated knowledge graph path, needing 2-hops.

# A.2 Models

We describe the details of models as follows:

1) No Retrieval. This approach uses only the LLM itself, to generate the answer to the given query.   
2) Single-step Approach. This approach first retrieves the relevant knowledge with the given query from the external knowledge sources and then augments the LLM with this retrieved knowledge to generate the answer, which iterates only once.   
3) Adaptive Retrieval. This baseline (Mallen et al., 2023) adaptively augments the LLM with the retrieval module, only when the entities appearing in queries are less popular. To extract entities, we use the available entity-linking method (Li et al., 2020), namely BLINK, for questions.   
4) Self-RAG. This baseline (Asai et al., 2024)

![](images/dd2c9235ebaa094ab3537cbecd46cf3a4597217eaeed4b826069b007d3a6da37.jpg)  
Figure 5: QA performance (F1) and efficiency (Time/Query) for different retrieval-augmented generation approaches. We use the FLAN-T5-XXL (11B) as the base LLM.

trains the LLM to adaptively perform retrieval and generation, where the retrieval is conducted once it predicts the special retrieval token above a certain threshold, and the answer generation follows.

5) Adaptive-RAG. This is our model that adaptively selects the retrieval-augmented generation strategy, smoothly oscillating between the nonretrieval, single-step approach, and multi-step approaches without architectural changes, based on the query complexity assessed by the classifier.   
6) Multi-step Approach. This approach (Trivedi et al., 2023) is the multi-step retrieval-augmented LLM, which iteratively accesses both the retriever and LLM with interleaved Chain-of-Thought reasoning (Wei et al., 2022b) repeatedly until it derives the solution or reaches the maximum step number. 7) Adaptive-RAG w/ Oracle This is an ideal scenario of our Adaptive-RAG equipped with an oracle classifier that perfectly categorizes the query complexity.

# A.3 Implementation Details

For computing resources, we use A100 GPUs with 80GB memory. In addition, due to the significant costs associated with evaluating retrieval-augmented generation models, we perform experiments with a single run. Finally, we implemented models using PyTorch (Paszke et al., 2019) and Transformers library (Wolf et al., 2020).

# B Additional Experimental Results

Performance vs Time We further provide a comparison of different retrieval-augmented generation approaches with FLAN-T5-XL and FLAN-T5-XXL models in Figure 4 and Figure 5, respectively, in the context of performance and efficiency trade-offs. Similar to the observation made from the GPT-3.5 model in Figure 1, our proposed Adaptive-RAG is significantly more effective as well as efficient.

Table 7: Results on each of a collection of datasets with FLAN-T5-XXL (11B) as the LLM. We emphasize our results in bold.   

<table><tr><td rowspan="2">Data</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">SQuAD</td><td colspan="5">Natural Questions</td><td colspan="5">TriviaQA</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="7">Single-step</td><td rowspan="2">Simple</td><td>No Retrieval</td><td>7.00</td><td>14.40</td><td>8.40</td><td>0.00</td><td>0.08</td><td>18.80</td><td>25.50</td><td>20.40</td><td>0.00</td><td>0.08</td><td>32.80</td><td>39.20</td><td>35.40</td><td>0.00</td><td>0.08</td></tr><tr><td>Single-step Approach</td><td>28.80</td><td>40.80</td><td>35.00</td><td>1.00</td><td>1.00</td><td>41.40</td><td>51.20</td><td>47.60</td><td>1.00</td><td>1.00</td><td>56.00</td><td>64.70</td><td>61.80</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>15.60</td><td>25.60</td><td>20.00</td><td>0.50</td><td>0.54</td><td>31.00</td><td>39.70</td><td>35.00</td><td>0.50</td><td>0.54</td><td>44.80</td><td>52.20</td><td>48.60</td><td>0.50</td><td>0.54</td></tr><tr><td>Self-RAG*</td><td>1.60</td><td>11.90</td><td>20.80</td><td>0.59</td><td>0.31</td><td>39.20</td><td>47.10</td><td>42.40</td><td>0.75</td><td>0.09</td><td>14.60</td><td>33.70</td><td>60.20</td><td>0.76</td><td>0.22</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>27.80</td><td>39.80</td><td>34.00</td><td>1.17</td><td>1.50</td><td>41.20</td><td>51.00</td><td>47.40</td><td>1.00</td><td>1.00</td><td>52.00</td><td>60.30</td><td>57.20</td><td>1.03</td><td>1.33</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>24.60</td><td>36.90</td><td>30.20</td><td>2.13</td><td>3.83</td><td>39.60</td><td>49.60</td><td>46.40</td><td>2.16</td><td>3.94</td><td>52.60</td><td>61.10</td><td>59.40</td><td>2.17</td><td>4.03</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>32.80</td><td>46.90</td><td>38.20</td><td>0.85</td><td>0.94</td><td>51.20</td><td>61.00</td><td>57.00</td><td>0.71</td><td>0.91</td><td>63.40</td><td>71.30</td><td>68.20</td><td>0.51</td><td>0.60</td></tr><tr><td rowspan="2">Data</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">MuSiQue</td><td colspan="5">HotpotQA</td><td colspan="5">2WikiMultiHopQA</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="7">Multi-step</td><td rowspan="2">Simple</td><td>No Retrieval</td><td>4.20</td><td>13.40</td><td>5.40</td><td>0.00</td><td>0.08</td><td>17.40</td><td>25.44</td><td>18.40</td><td>0.00</td><td>0.09</td><td>26.80</td><td>32.93</td><td>28.00</td><td>0.00</td><td>0.08</td></tr><tr><td>Single-step Approach</td><td>16.80</td><td>25.70</td><td>19.20</td><td>1.00</td><td>1.00</td><td>37.60</td><td>49.27</td><td>39.60</td><td>1.00</td><td>1.00</td><td>46.60</td><td>54.13</td><td>48.20</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>8.40</td><td>17.80</td><td>10.20</td><td>0.50</td><td>0.54</td><td>26.60</td><td>36.01</td><td>27.80</td><td>0.50</td><td>0.54</td><td>35.20</td><td>42.68</td><td>36.80</td><td>0.50</td><td>0.54</td></tr><tr><td>Self-RAG*</td><td>1.20</td><td>8.20</td><td>11.80</td><td>0.68</td><td>0.27</td><td>5.60</td><td>17.86</td><td>30.60</td><td>0.76</td><td>0.26</td><td>3.00</td><td>19.14</td><td>39.00</td><td>0.90</td><td>0.25</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>20.60</td><td>28.50</td><td>23.20</td><td>1.89</td><td>3.12</td><td>44.20</td><td>54.78</td><td>46.80</td><td>1.58</td><td>2.53</td><td>47.60</td><td>57.36</td><td>54.00</td><td>1.46</td><td>2.55</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>19.40</td><td>27.50</td><td>21.80</td><td>2.09</td><td>3.66</td><td>47.00</td><td>57.81</td><td>49.40</td><td>2.08</td><td>3.73</td><td>57.60</td><td>67.65</td><td>64.00</td><td>2.17</td><td>3.63</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>24.20</td><td>37.20</td><td>26.60</td><td>1.22</td><td>1.71</td><td>52.20</td><td>64.80</td><td>54.60</td><td>0.92</td><td>1.33</td><td>59.20</td><td>70.40</td><td>68.60</td><td>0.82</td><td>1.14</td></tr></table>

Table 8: Results on each of a collection of datasets with GPT-3.5 (Turbo) as the LLM. We emphasize our results in bold.   

<table><tr><td rowspan="2">Data</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">SQuAD</td><td colspan="5">Natural Questions</td><td colspan="5">TriviaQA</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="7">Single-step</td><td rowspan="2">Simple</td><td>No Retrieval</td><td>16.00</td><td>29.20</td><td>23.80</td><td>0.00</td><td>0.62</td><td>39.80</td><td>55.70</td><td>55.00</td><td>0.00</td><td>0.56</td><td>64.00</td><td>75.60</td><td>75.80</td><td>0.00</td><td>0.68</td></tr><tr><td>Single-step Approach</td><td>18.00</td><td>33.80</td><td>29.20</td><td>1.00</td><td>1.00</td><td>32.40</td><td>46.80</td><td>54.80</td><td>1.00</td><td>1.00</td><td>55.20</td><td>66.50</td><td>65.80</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>15.40</td><td>30.00</td><td>24.40</td><td>0.50</td><td>0.81</td><td>36.40</td><td>51.20</td><td>56.60</td><td>0.50</td><td>0.78</td><td>62.00</td><td>71.90</td><td>72.20</td><td>0.50</td><td>0.84</td></tr><tr><td>Self-RAG*</td><td>1.60</td><td>11.90</td><td>20.80</td><td>0.59</td><td>1.91</td><td>39.20</td><td>47.10</td><td>42.40</td><td>0.75</td><td>0.52</td><td>14.60</td><td>33.70</td><td>60.20</td><td>0.76</td><td>1.59</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>19.80</td><td>34.40</td><td>30.00</td><td>0.87</td><td>1.21</td><td>36.80</td><td>52.00</td><td>56.60</td><td>0.68</td><td>0.86</td><td>62.40</td><td>73.80</td><td>73.80</td><td>0.22</td><td>0.79</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>17.40</td><td>31.50</td><td>26.20</td><td>2.50</td><td>3.24</td><td>35.60</td><td>49.70</td><td>57.80</td><td>2.58</td><td>3.79</td><td>54.80</td><td>67.10</td><td>68.00</td><td>2.30</td><td>2.65</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>28.00</td><td>45.90</td><td>39.40</td><td>0.54</td><td>0.93</td><td>50.00</td><td>65.40</td><td>67.00</td><td>0.28</td><td>0.8</td><td>70.80</td><td>81.00</td><td>80.00</td><td>0.11</td><td>0.73</td></tr><tr><td rowspan="2">Data</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="5">MuSiQue</td><td colspan="5">HotpotQA</td><td colspan="5">2WikiMultiHopQA</td></tr><tr><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td><td>EM</td><td>F1</td><td>Acc</td><td>Step</td><td>Time</td></tr><tr><td rowspan="7">Multi-step</td><td rowspan="2">Simple</td><td>No Retrieval</td><td>20.40</td><td>31.30</td><td>24.40</td><td>0.00</td><td>0.81</td><td>37.40</td><td>51.04</td><td>43.20</td><td>0.00</td><td>0.74</td><td>37.00</td><td>48.50</td><td>43.40</td><td>0.00</td><td>0.90</td></tr><tr><td>Single-step Approach</td><td>16.40</td><td>26.70</td><td>23.60</td><td>1.00</td><td>1.00</td><td>39.60</td><td>50.44</td><td>45.60</td><td>1.00</td><td>1.00</td><td>46.80</td><td>57.69</td><td>52.60</td><td>1.00</td><td>1.00</td></tr><tr><td rowspan="3">Adaptive</td><td>Adaptive Retrieval</td><td>18.80</td><td>30.30</td><td>24.80</td><td>0.50</td><td>0.90</td><td>38.60</td><td>50.70</td><td>43.20</td><td>0.50</td><td>0.87</td><td>44.20</td><td>55.11</td><td>50.60</td><td>0.50</td><td>0.95</td></tr><tr><td>Self-RAG*</td><td>1.20</td><td>8.20</td><td>11.80</td><td>0.68</td><td>1.66</td><td>5.60</td><td>17.86</td><td>30.60</td><td>0.76</td><td>1.67</td><td>3.00</td><td>19.14</td><td>39.00</td><td>0.90</td><td>1.81</td></tr><tr><td>Adaptive-RAG (Ours)</td><td>21.80</td><td>32.60</td><td>29.60</td><td>1.90</td><td>2.29</td><td>40.40</td><td>52.56</td><td>47.00</td><td>0.93</td><td>1.48</td><td>46.60</td><td>60.09</td><td>56.80</td><td>1.59</td><td>2.23</td></tr><tr><td>Complex</td><td>Multi-step Approach</td><td>23.00</td><td>32.50</td><td>31.60</td><td>3.41</td><td>3.61</td><td>45.80</td><td>58.36</td><td>52.20</td><td>2.73</td><td>3.18</td><td>52.20</td><td>66.08</td><td>62.40</td><td>3.36</td><td>3.35</td></tr><tr><td>Oracle</td><td>Adaptive-RAG w/ Oracle</td><td>29.60</td><td>44.70</td><td>35.60</td><td>0.90</td><td>1.45</td><td>55.60</td><td>69.90</td><td>62.80</td><td>0.54</td><td>1.08</td><td>52.20</td><td>69.90</td><td>66.60</td><td>0.65</td><td>1.21</td></tr></table>

Performance per Dataset In addition to detailing the performance of each dataset with the FLAN-T5-XL model, as shown in Table 2, we also present the results for each dataset with the FLAN-T5-XXL and GPT-3.5 models in Table 2 and Table 8, respectively. The experimental results show that our Adaptive-RAG consistently balances between efficiency and accuracy. It is worth noting that while the GPT-3.5 model performs effectively in addressing straightforward queries even without document retrieval, it benefits significantly from our Adaptive-RAG in terms of effectiveness when solving complex multi-hop queries.