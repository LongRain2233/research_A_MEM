{
    "title": "AgentFold: Long-Horizon Web Agents with Proactive Context Management",
    "background_and_problem": "【一、研究背景与问题核心】\n\n**§1 领域背景与研究动机（150字以上）**\n本研究聚焦于基于大语言模型（LLM）的网页智能体（Web Agent）领域，特别是针对**长视野（Long-Horizon）信息检索任务**。这类任务要求智能体在复杂、多步骤的网页交互中（例如，查找一家同时在墨西哥和加利福尼亚设有分店的特种食品店）持续导航、推理和整合信息。随着智能体交互步数（Turn）的增加，如何有效管理其内部上下文（Context）成为核心瓶颈。现有主流方法（如ReAct）在长任务中表现不佳，促使研究者探索更高效的上下文管理范式，以释放智能体进行数百步深度探索的潜力。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法主要分为两类，均在长视野任务中表现出具体失败模式：\n1.  **基于ReAct范式的智能体**（如WebThinker、WebDancer、WebSailor）：采用**仅追加（append-only）** 的策略，将每一步的推理-行动-观察三元组完整累积到上下文中。**当任务步数超过100步时**，上下文会被大量原始网页数据噪声淹没，导致关键信号被掩盖，智能体做出次优决策。具体表现为，在BrowseComp等长视野基准测试上，其性能远低于本文方法（例如WebThinker-32B在BrowseComp上仅得2.8分）。\n2.  **固定式全历史摘要方法**（如MEM1、MemAgent）：在每一步都对完整历史进行摘要压缩以保持上下文简洁。**当任务涉及多步子任务且中间细节对后续推理至关重要时**，这种**刚性、每步必做的摘要**可能导致关键信息的**不可逆丢失**。作者量化了这种风险：假设每次摘要丢失关键细节的概率为1%，经过100步后，该细节的存活概率仅为36.6%；经过500步后，存活概率骤降至0.66%。\n3.  **更大规模的开源模型**（如DeepSeek-V3.1-671B）：虽然参数规模巨大（671B），但其采用的上下文管理策略（如ReAct）存在根本缺陷，**当允许的交互步数超过其上下文窗口容量（如64步）时**，性能达到饱和并无法继续提升，无法胜任数百步的探索任务。\n\n**§3 问题的根本难点与挑战（200字以上）**\n该问题的根本难点在于**上下文管理的动态性与信息保真度之间的固有矛盾**。从工程角度看，LLM的上下文长度有限（如128K），而长视野任务需要数百步交互，原始历史记录会线性增长，迅速耗尽上下文窗口并引入噪声。从理论或认知科学角度看，人类解决问题时并非保留所有信息，也非每步都进行强制摘要，而是进行**有纪律的、回顾性的巩固（retrospective consolidation）**。然而，让AI智能体学会这种动态的、多尺度的“回顾”机制极具挑战性，因为它需要智能体具备评估自身历史轨迹、识别已完成子任务、并决定哪些细节需要保留、哪些可以抽象的能力。这本质上是一个**元认知（meta-cognition）** 问题，即让智能体学会管理自己的“工作记忆”。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**将智能体的上下文视为一个动态的认知工作空间（cognitive workspace）进行主动塑造，而非被动填充的日志**。其核心假设受到人类认知过程的启发：人类在解决问题时，会在关键时刻进行回顾性巩固，丢弃无关步骤，提炼中间发现，抽象关键见解。基于此，本文提出**主动上下文折叠（proactive context folding）** 范式。具体技术假设是：通过赋予智能体一个结构化的上下文（多尺度状态摘要 + 最新交互）以及一个可学习的“折叠”操作，智能体能够学会在任务执行过程中，自主决定**何时**以及**如何**（进行细粒度压缩还是深度整合）去管理其历史轨迹。该假设的理论依据源于认知心理学中的工作记忆理论和信息瓶颈原则，旨在平衡信息的完整性与处理的简洁性。",
    "core_architecture": "【二、核心架构与技术机制】\n\n**§1 系统整体架构概览（200字以上）**\nAgentFold系统整体架构围绕**感知→推理→折叠→行动**的循环构建。其上下文被明确划分为四个组件：用户问题（Q）、可用工具列表（T）、**多尺度状态摘要（Multi-Scale State Summaries, S）** 和**最新交互（Latest Interaction, I）**。整体数据流如下：\n1.  **输入**：在步骤t，智能体接收上下文 \\(C_t = (Q, T, S_{t-2}, I_{t-1})\\)。\n2.  **处理**：模型 \\(\\theta\\) 基于 \\(C_t\\) 进行深度推理，生成一个包含四个部分的响应 \\(R_t = (th_t, f_t, e_t, a_t)\\)，分别是：思考过程（thinking）、折叠指令（folding）、解释（explanation）和行动（action）。\n3.  **折叠更新**：折叠指令 \\(f_t\\) 被立即执行，更新多尺度状态摘要 \\(S_{t-2} \\rightarrow S_{t-1}\\)。\n4.  **环境交互与上下文构建**：行动 \\(a_t\\) 被执行为工具调用，产生观察 \\(o_t\\)。解释 \\(e_t\\)、行动 \\(a_t\\) 和观察 \\(o_t\\) 共同构成新的最新交互 \\(I_t = (e_t, a_t, o_t)\\)。\n5.  **输出与循环**：新的上下文 \\(C_{t+1} = (Q, T, S_{t-1}, I_t)\\) 被构建，用于下一步循环，直到智能体决定给出最终答案。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：结构化上下文（Structured Context）\n-   **模块名**：Context Representation Module\n-   **输入**：原始用户问题字符串、工具描述列表、上一步的多尺度状态摘要序列、上一步的最新交互三元组。\n-   **核心处理逻辑**：将上下文组织为公式化的四元组 \\(C_t = (Q, T, S_{t-2}, I_{t-1})\\)。其中，多尺度状态摘要 \\(S_t\\) 是一个有序的摘要块序列：\\(S_t = (s_{x_1, y_1}, s_{x_2, y_2}, \\dots, s_{x_m, y_m})\\)，每个 \\(s_{x, y}\\) 是对从步骤 \\(x\\) 到 \\(y\\) 的连续步骤块的文本摘要。\\(I_{t-1} = (e_{t-1}, a_{t-1}, o_{t-1})\\) 是上一步的完整记录。\n-   **输出**：格式化后的上下文字符串，作为模型输入。\n-   **设计理由**：这种设计模仿了人类认知：稳定目标（Q）、工具能力（T）、巩固的知识（S）和易变的工作记忆（I）。它分离了高保真的近期细节（I）和经过整理的长期记忆（S），旨在直接缓解上下文全面性与简洁性之间的权衡。\n\n#### 模块二：折叠操作（Folding Operation）\n-   **模块名**：Folding Directive Module\n-   **输入**：模型生成的折叠指令 \\(f_t\\)，其格式为JSON对象：`{\"range\": [k, t-1], \"summary\": \"σ_t\"}`，其中 \\(k\\) 是折叠起始ID，\\(σ_t\\) 是替换摘要文本。\n-   **核心处理逻辑**：根据指令中的范围 \\([k, t-1]\\)，对多尺度状态摘要 \\(S_{t-2}\\) 执行更新。具体操作是：**撤回（retract）** 所有步骤范围落在 \\([k, t-1]\\) 内的摘要块，并用一个新的摘要块 \\(s_{k, t-1} = σ_t\\) **替换**它们。这支持两种模式：\n    1.  **细粒度压缩（Granular Condensation）**：当 \\(k = t-1\\) 时，仅将最新交互 \\(I_{t-1}\\) 折叠成一个新的细粒度摘要块。\n    2.  **深度整合（Deep Consolidation）**：当 \\(k < t-1\\) 时，将最新交互 \\(I_{t-1}\\) 与一系列先前的摘要块（从第 \\(k\\) 步开始）融合成一个单一的、粗粒度的摘要。\n-   **输出**：更新后的多尺度状态摘要序列 \\(S_{t-1}\\)。\n-   **设计理由**：赋予智能体主动管理其长期记忆的能力。细粒度压缩用于保留可能至关重要的细节，避免被全历史摘要 indiscriminately 压缩；深度整合用于在子任务完成后抽象掉无关的中间步骤，对抗ReAct中的噪声累积。\n\n#### 模块三：响应生成与解析（Response Generation and Parsing）\n-   **模块名**：AgentFold Model (Qwen3-30B-A3B-Instruct-2507)\n-   **输入**：格式化后的上下文 \\(C_t\\)。\n-   **核心处理逻辑**：模型经过有监督微调（SFT），学习生成一个单一的、连贯的文本块 \\(R_t\\)。该文本块被设计为可解析为四部分：思考过程（\\(th_t\\)）、折叠指令（\\(f_t\\)）、解释（\\(e_t\\)）和行动（\\(a_t\\)）。思考过程是详细的链式推理独白，从中推导出其他三个结构化部分。\n-   **输出**：解析后的四元组 \\((th_t, f_t, e_t, a_t)\\)。\n-   **设计理由**：将上下文管理（折叠）作为推理过程的一个核心、可学习的组成部分，而非被动副产品。这种设计迫使智能体批判性地评估其轨迹并提炼最显著的信息，从而做出更明智的后续行动。同时，规划新行动的过程也为决定保留哪些信息提供了实时信号。\n\n**§3 关键公式与算法（如有）**\n论文中给出了上下文和折叠操作的形式化定义：\n1.  **上下文定义**：\\(C_{t} = \\left(Q, T, S_{t - 2}, I_{t - 1}\\right)\\)\n2.  **多尺度状态摘要序列**：\\(S_{t} = \\left(s_{x_{1}, y_{1}}, s_{x_{2}, y_{2}}, \\dots , s_{x_{m}, y_{m}}\\right)\\)，其中 \\(x_{1} = 1\\)，\\(y_{m} = t - 2\\)，且 \\(x_{i + 1} = y_{i} + 1\\)。\n3.  **响应生成**：\\(R_{t} = \\operatorname {AgentFold} \\left(C_{t}; \\theta\\right)\\rightarrow \\left(th_{t}, f_{t}, e_{t}, a_{t}\\right)\\)\n4.  **平均上下文令牌数计算**：用于分析轨迹长度动态，\\(A_{t} = \\frac{1}{|T_{t}|} \\sum_{j \\in \\mathcal{T}_{t}} \\text{TokenCount} \\left(C_{j, t}\\right)\\)，其中 \\(\\mathcal{T}_{t}\\) 是存活超过 \\(t\\) 步的轨迹集合。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n原文未明确描述不同的方法变体。AgentFold本身包含两种折叠操作模式（细粒度压缩和深度整合），但这并非独立的变体，而是同一模型在推理时根据情况选择的两种操作。论文未提供消融实验对比仅使用一种模式的效果。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上存在本质区别：\n1.  **与ReAct范式智能体（如WebThinker, WebDancer）的差异**：ReAct采用**仅追加（append-only）** 的上下文，导致线性增长和噪声累积。AgentFold则引入了**主动的、多尺度的折叠操作**，可以动态压缩或整合历史，使上下文增长呈**次线性（sub-linear）**，显著减少令牌消耗（100步后比ReAct少84K令牌，节省约7GB内存）。\n2.  **与固定式全历史摘要方法（如MEM1, MemAgent）的差异**：这些方法在**每一步**都对**完整历史**进行强制摘要，存在信息丢失风险。AgentFold的折叠是**回顾性的、选择性的**，可以延迟整合直到子任务完成，并且可以选择仅压缩单步（细粒度）或整合多步（深度），避免了每步强制摘要的刚性。\n3.  **与外部上下文增强方法（如基于用户档案或过去对话的方法）的差异**：那些方法专注于从任务轨迹外部注入相关知识。而AgentFold专注于**任务内部上下文管理（Intra-Task Context Curation）**，管理任务本身生成的内容，以在长视野中保持相关性和效率。",
    "methodology_and_formulas": "【三、方法论细节与关键公式】\n\n**§1 完整算法流程（伪代码级描述）**\n论文未提供明确的算法伪代码框，但可以从描述中重构出以下流程：\nStep 1: **初始化**。设置用户问题 \\(Q\\)，可用工具列表 \\(T\\)，多尺度状态摘要 \\(S_0 = \\emptyset\\)，最新交互 \\(I_0 = \\emptyset\\)。设置最大工具调用次数 \\(T_{max} = 100\\)。\nStep 2: **对于每一步 \\(t = 1, 2, ...\\) 直到任务完成或达到 \\(T_{max}\\)**：\n    a. **构建上下文**：\\(C_t = (Q, T, S_{t-2}, I_{t-1})\\)。对于 \\(t=1\\)，\\(C_1 = (Q, T, \\emptyset, \\emptyset)\\)；对于 \\(t=2\\)，\\(C_2 = (Q, T, \\emptyset, I_1)\\)。\n    b. **模型推理**：将 \\(C_t\\) 输入AgentFold模型 \\(\\theta\\)，生成响应文本 \\(R_t\\)。\n    c. **解析响应**：将 \\(R_t\\) 解析为四元组 \\((th_t, f_t, e_t, a_t)\\)。\n    d. **执行折叠**：如果 \\(t > 1\\)（第一步无历史可折叠），则根据折叠指令 \\(f_t = \\{\"range\": [k, t-1], \"summary\": σ_t\\}\\) 更新多尺度状态摘要：\n        i. 从 \\(S_{t-2}\\) 中移除所有步骤范围在 \\([k, t-1]\\) 内的摘要块。\n        ii. 将新的摘要块 \\(s_{k, t-1} = σ_t\\) 插入到相应位置，形成 \\(S_{t-1}\\)。\n    e. **执行行动**：如果 \\(a_t\\) 是工具调用，则执行工具并获得观察结果 \\(o_t\\)；如果 \\(a_t\\) 是最终答案，则终止循环并返回答案。\n    f. **更新最新交互**：\\(I_t = (e_t, a_t, o_t)\\)。\n    g. **准备下一步**：设置 \\(t = t + 1\\)，返回步骤a。\n\n**§2 关键超参数与配置**\n-   **最大工具调用次数（Max Tool Call Number）**：设置为100。任何超过此步数的轨迹将被强制终止。作者在扩展实验中将其增加到256和500以测试扩展性。\n-   **底座模型（Base Model）**：使用Qwen3-30B-A3B-Instruct-2507，总参数量30B，推理时激活参数量3B。\n-   **上下文窗口（Context Window）**：底层模型支持128K令牌上下文长度。\n-   **折叠指令格式**：JSON对象，包含`range`（列表，如`[k, t-1]`）和`summary`（字符串）两个键。\n-   **数据收集中的拒绝采样（Rejection Sampling）**：用于生成训练数据时，丢弃任何**不严格遵循所需格式**的生成步骤，或包含**过多环境错误**的轨迹。\n\n**§3 训练/微调设置（如有）**\n-   **训练方法**：**有监督微调（Supervised Fine-Tuning, SFT）**，未使用持续预训练或强化学习（RL）。\n-   **训练数据构造**：使用**Fold-Generator**数据收集流水线生成。该流水线基于强大的开源LLM，使用与WebSailor工作相同的问题集。通过拒绝采样机制确保数据质量，生成高质量的交互对集合 \\(\\{(C_t, R_t^*)\\}_N\\)，其中 \\(C_t\\) 是结构化上下文，\\(R_t^*\\) 是经过验证的黄金标准响应。\n-   **训练目标**：将流水线复杂的、多步骤的、经过验证的推理过程提炼到模型权重中，使模型能够通过单次前向传播生成整个结构化输出。\n-   **具体优化器、学习率、批次大小、训练轮数**：原文未提供。\n\n**§4 推理阶段的工程细节**\n-   **并行化策略**：原文未提供。\n-   **缓存机制**：原文未提供。\n-   **向量数据库选型**：未使用，因为上下文管理完全在模型内部通过文本摘要完成。\n-   **关键实现细节**：在推理时，模型需要生成一个可解析为四部分的单一文本块。这要求SFT训练确保模型输出严格遵循预定格式。折叠操作在收到模型响应后立即执行，以更新上下文供下一步使用。",
    "experimental_design": "【四、实验设计】\n\n**§1 数据集详情（每个数据集单独列出）**\n1.  **BrowseComp** (Wei et al., 2025)\n    -   **规模**：原文未提供具体样本数。\n    -   **领域类型**：网页信息检索。\n    -   **评测问题类型**：评估智能体在**定位难以找到的信息**方面的能力，属于长视野、复杂搜索任务。\n    -   **特殊处理**：对于样本数少于200的数据集，报告3次试验的平均结果。\n2.  **BrowseComp-ZH** (Zhou et al., 2025a)\n    -   **规模**：原文未提供具体样本数。\n    -   **领域类型**：中文网页信息检索。\n    -   **评测问题类型**：与BrowseComp类似，评估**定位难以找到的信息**的能力，但针对中文环境。\n    -   **特殊处理**：对于样本数少于200的数据集，报告3次试验的平均结果。\n3.  **WideSearch-en** (Wong et al., 2025)\n    -   **规模**：原文未提供具体样本数。\n    -   **领域类型**：网页信息检索。\n    -   **评测问题类型**：强调**广泛搜索**能力，使用最详细的指标Item-F1进行评估。\n    -   **特殊处理**：原文未提及。\n4.  **GAIA** (Mialon et al., 2023)\n    -   **规模**：原文未提供具体样本数，使用的是text-only子集。\n    -   **领域类型**：通用AI助手基准。\n    -   **评测问题类型**：评估AI代理的**通用能力**，不特定于网页搜索。\n    -   **特殊处理**：原文未提及。\n\n**§2 评估指标体系（全量列出）**\n-   **准确性指标**：\n    -   **BrowseComp/BrowseComp-ZH/WideSearch/GAIA得分**：论文中报告的是每个基准测试的总体得分（百分比形式），但未明确说明具体计算方式（如成功率、F1等）。从上下文推断，BrowseComp等可能是基于任务成功率的评分。WideSearch明确使用**Item-F1**。\n-   **效率/部署指标**：\n    -   **平均上下文令牌数（Average Context Token Count）**：\\(A_t\\)，用于分析上下文长度随步数的增长。\n    -   **上下文块数量（Number of Blocks）**：定义为单位条目数（多尺度状态摘要中的每个摘要块算一个，加上最新交互算一个），用于衡量上下文的结构复杂性。\n    -   **内存节省估计**：通过与ReAct基线对比上下文令牌减少量，估算每次推理实例的内存节省（如100步后节省约7GB）。\n    -   **最大交互步数（Max Interaction Turns）**：测试智能体在增加允许步数上限时的性能扩展性。\n-   **其他自定义指标**：原文未提出新的评估维度。\n\n**§3 对比基线（完整枚举）**\n**开源智能体**：\n1.  WebThinker-32B (Li et al., 2025c)：基于ReAct范式。\n2.  WebDancer-32B (Wu et al., 2025)：基于ReAct范式。\n3.  WebSailor-32B / 72B (Li et al., 2025b)：基于ReAct范式。\n4.  ASearcher-Web-32B (Gao et al., 2025)。\n5.  MiroThinker-32B-DPO-v0.2 (MiroMind AI Team, 2025)。\n6.  WebExplorer-8B (Liu et al., 2025)。\n7.  DeepDive-32B (Lu et al., 2025)。\n8.  DeepDiver-V2-38B (OpenPangu Team, 2025)。\n9.  Kimi-K2-Instruct-1T (Team et al., 2025)。\n10. GLM-4.5-355B-A32B (Zeng et al., 2025)：参数量355B。\n11. DeepSeek-V3.1-671B-A37B (DeepSeek Team, 2025)：参数量671B。\n**专有（闭源）智能体**：\n1.  Claude-4-Sonnet / Claude-4-Opus (anthropic, 2025)。\n2.  OpenAI-o4-mini / OpenAI-o3 (OpenAI, 2025b)。\n3.  OpenAI Deep Research (OpenAI, 2025a)。\n**代表性**：涵盖了当前主流的ReAct范式智能体、更大规模的开源模型以及业界领先的专有模型，提供了全面的性能对比。\n\n**§4 实验控制变量与消融设计**\n原文**未进行**系统的消融实验（例如，移除折叠操作、仅使用一种折叠模式、或与其他上下文管理策略对比）。主要的对比实验是AgentFold与所有列出的基线在标准基准测试上的性能比较，以及AgentFold与ReAct在上下文长度和块数量增长动态上的对比。扩展性实验（增加最大步数）可视作一种控制变量的消融，展示了不同交互步数上限下的性能变化。",
    "core_results": "【五、核心实验结果】\n\n**§1 主实验结果全景（表格式呈现）**\n根据论文表1，整理核心结果如下（所有数值均为百分比得分）：\n`方法名 | BrowseComp | BrowseComp-ZH | WideSearch (Item-F1) | GAIA`\n`Claude-4-Sonnet | 14.7 | 22.5 | 62.0 | 68.3`\n`Claude-4-Opus | 18.8 | 37.4 | - | -`\n`OpenAI-o4-mini | 28.3 | 44.3 | - | -`\n`OpenAI-o3 | 49.7 | 58.1 | 60.0 | 70.5`\n`OpenAI Deep Research | 51.5 | 42.9 | - | 67.4`\n`WebThinker-32B | 2.8 | 7.3 | - | 48.5`\n`WebDancer-32B | 3.8 | 18.0 | - | 51.5`\n`WebSailor-32B | 10.5 | 25.5 | - | 53.2`\n`WebSailor-72B | 12.0 | 30.1 | - | 55.4`\n`ASearcher-Web-32B | 5.2 | 15.6 | - | 52.8`\n`MiroThinker-32B-DPO-v0.2 | 13.0 | 17.0 | - | 64.1`\n`WebExplorer-8B | 15.7 | 32.0 | - | 50.0`\n`DeepDive-32B | 14.8 | 25.6 | - | -`\n`DeepDiver-V2-38B | 13.4 | 34.6 | - | -`\n`Kimi-K2-Instruct-1T | 14.1 | 28.8 | 59.9 | 57.3`\n`GLM-4.5-355B-A32B | 26.4 | 37.5 | - | 66.0`\n`DeepSeek-V3.1-671B-A37B | 30.0 | 49.2 | - | 63.1`\n`AgentFold-30B-A3B (Ours) | 36.2 | 47.3 | 62.1 | 67.0`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n-   **BrowseComp（定位难寻信息）**：AgentFold-30B-A3B得分**36.2**，显著超过所有开源基线，包括参数量大22倍的DeepSeek-V3.1-671B（30.0，相对提升20.7%），甚至超越了专有模型OpenAI-o4-mini（28.3，相对提升27.9%）。这表明在需要深度、持久搜索的任务中，主动上下文管理比单纯增大模型规模更有效。\n-   **BrowseComp-ZH（中文难寻信息）**：AgentFold得分**47.3**，仅次于DeepSeek-V3.1-671B（49.2，差距1.9个点），但超越了GLM-4.5-355B（37.5，相对提升26.1%）和OpenAI-o4-mini（44.3，相对提升6.8%）。这证明了其方法在多语言场景下的有效性。\n-   **WideSearch（广泛搜索）**：AgentFold以**62.1**的Item-F1得分取得**最佳总体成绩**，超越了所有列出的专有模型（OpenAI-o3: 60.0, Claude-4-Sonnet: 62.0）和开源模型（Kimi-K2-Instruct-1T: 59.9）。这表明其上下文管理策略在需要广泛覆盖而非深度挖掘的任务中同样优势明显。\n-   **GAIA（通用能力）**：AgentFold得分**67.0**，与顶级专有模型如OpenAI Deep Research（67.4）和Claude-4-Sonnet（68.3）相当，并超越了所有开源基线（最佳为GLM-4.5-355B的66.0）。这表明其架构改进带来的益处不局限于网页搜索，能泛化到更通用的AI助手任务。\n\n**§3 效率与开销的定量对比**\n-   **上下文长度（Token数）**：在BrowseComp的200条轨迹上分析，AgentFold的平均上下文令牌数呈**次线性增长**，从约**3.5K**令牌（初始）增长到100步时的约**7K**令牌，**仅翻了一倍**。相比之下，ReAct基线呈现**近线性增长**，在100步时上下文巨大。具体地，在第100步，AgentFold的上下文比ReAct平均**少84K令牌（少92%）**。\n-   **内存节省**：上述令牌减少量转化为每次推理实例**节省近7GB内存**（在100步轨迹长度下）。\n-   **上下文块数量**：AgentFold的上下文块（摘要块+最新交互）数量增长缓慢（次线性），而ReAct的块数量**线性增长**（每一步增加一个块）。这证明了深度整合操作有效控制了结构复杂性。\n-   **扩展性（交互步数）**：当最大交互步数从64增加到256时，GLM-4.5-355B（基于ReAct）的性能达到饱和并失败，而AgentFold-30B的性能**持续提升**。在500步的扩展实验中，AgentFold的上下文长度大部分保持在**20K令牌以下**且非单调增长，展示了处理极长视野任务的潜力。\n\n**§4 消融实验结果详解**\n原文**未提供**标准的组件消融实验（如移除折叠指令、仅使用一种折叠模式等）。因此无法量化每个组件对性能的具体贡献。\n\n**§5 案例分析/定性分析（如有）**\n论文图5提供了一个案例研究，展示了AgentFold在复杂任务第17步的操作。智能体识别出从第6步到第16步的一系列尝试是死胡同（dead end）。随后，它执行了一次**深度整合（Deep Consolidation）**，将这11步的失败序列折叠成一个单一的总结性摘要（例如：“多次尝试获取麦当劳GPS坐标失败...”）。基于这个新整合的洞察，智能体在解释（motivation）中动态计划转向新的调查方向，并体现在随后的工具调用中。这个案例**定性**证明了AgentFold能够推理自身轨迹、从扩展失败中学习，并通过主动管理其认知工作空间来战略性地重新规划。",
    "conclusion_and_future_work": "【六、结论与未来工作】\n\n**§1 本文核心贡献总结**\n1.  **提出AgentFold新范式**：引入了一种以**主动上下文管理**为核心的智能体新范式，将上下文视为可主动塑造的动态认知工作空间，而非被动日志。\n2.  **设计多尺度折叠操作**：定义了**细粒度压缩**和**深度整合**两种折叠操作，使智能体能够自主决定保留精细细节或抽象掉已完成子任务，从根本上解决了仅追加策略的上下文饱和和固定摘要的信息丢失问题。\n3.  **实现卓越性能与效率**：仅使用30B参数模型（激活3B），通过简单的SFT训练，在多个基准测试上**超越或匹配**参数量大20倍以上的开源模型（如DeepSeek-V3.1-671B）以及领先的专有模型（如OpenAI-o4-mini），同时上下文长度增长呈次线性，效率显著提升。\n4.  **展示长视野扩展潜力**：实验证明AgentFold能够支持数百步的交互，且性能随步数增加持续提升，而传统方法在64步后即饱和，为处理极复杂任务打开了新可能。\n\n**§2 局限性（作者自述）**\n作者在文中明确承认的局限性：\n1.  **训练方法未充分优化**：本文优先展示AgentFold范式的潜力，因此采用了**简单的有监督微调（SFT）方法，未进行大量优化**，也未使用强化学习（RL）。\n2.  **时间限制**：对于扩展到500步等极端长视野场景的详细探索，作者表示**由于时间限制而推迟到未来工作**。\n\n**§3 未来研究方向（全量提取）**\n1.  **利用强化学习优化折叠策略**：作者明确指出，清晰的下一步是**利用强化学习（RL）使智能体能够通过直接优化任务成功率，自主发现最优且可能非显而易见的折叠策略**。这有望超越SFT学到的策略。\n2.  （隐含方向）**探索更极端的交互步数**：基于其展示的扩展潜力，未来可以系统性地探索AgentFold在**数百甚至上千步**的超长视野任务中的表现，并研究其上下文管理的极限。\n3.  （隐含方向）**优化训练数据与流程**：可以进一步优化Fold-Generator数据收集流水线，生成更多样、更高质量的轨迹，或探索其他训练范式（如课程学习）来提升模型性能。",
    "research_contributions": "【七、研究贡献与学术价值】\n\n**§1 核心学术贡献（按重要性排序）**\n1.  **理论新颖性**：提出了**“主动上下文折叠”** 这一核心概念，将智能体的上下文管理从静态、被动的策略（如仅追加或固定摘要）提升为动态、可学习、多尺度的**元认知操作**。这受人类回顾性巩固认知过程的启发，为构建更类人的、可持续推理的AI智能体提供了新的理论框架。\n2.  **实验验证充分性**：在**BrowseComp、BrowseComp-ZH、WideSearch、GAIA**四个具有挑战性的基准测试上进行了全面验证，不仅展示了**SOTA性能**（超越更大规模模型），还通过**上下文长度、块数量、扩展性**等多维度定量分析，实证了其方法在**效率**和**长视野能力**上的显著优势。案例研究提供了定性证据。\n3.  **对领域的影响**：这项工作可能**重塑长视野网页智能体的设计范式**。它证明了通过精巧的架构设计和上下文管理，较小的模型可以媲美甚至超越巨型模型，为资源受限的研究者和开发者提供了新的方向。其开源代码和模型将推动该领域的进一步研究和应用。\n\n**§2 工程与实践贡献**\n-   **开源代码与模型**：论文提供了项目主页和GitHub仓库（https://github.com/Alibaba-NLP/DeepResearch），促进了可复现性和社区发展。\n-   **新的数据收集流水线**：开发了**Fold-Generator**，一个专门用于生成训练AgentFold所需的高质量、结构化轨迹数据的流水线，包含拒绝采样机制，这为后续类似研究提供了数据构造的参考。\n-   **高效的推理系统**：通过将复杂的“生成-过滤”策略提炼到模型权重中，AgentFold在推理时比用于数据收集的通用模型**效率更高**，展示了从数据生成到模型部署的完整工程链路。\n\n**§3 与相关工作的定位**\nAgentFold在当前技术路线图中处于**开辟新路线**的位置。它既不是对ReAct范式的简单改进，也不是对固定摘要方法的微调。它提出了一种**根本性的范式转变**：将上下文管理作为智能体推理的核心组成部分和可学习技能。它属于“**任务内部上下文管理（Intra-Task Context Curation）**”这一新兴研究方向，但通过引入多尺度、主动、延迟的折叠操作，与MEM1、MemAgent等每步强制摘要的方法形成了鲜明对比，为解决长视野任务中的上下文权衡问题提供了更灵活、更强大的解决方案。",
    "professor_critique": "【八、隐性缺陷与教授锐评】\n\n**§1 实验设计与评估体系的缺陷**\n1.  **基准覆盖不全**：实验仅在**4个**基准测试上进行，且主要侧重于**信息检索**任务（BrowseComp, BrowseComp-ZH, WideSearch）。缺乏在**其他类型长视野任务**（如多轮对话、复杂规划、代码生成）上的验证，无法全面证明其泛化能力。\n2.  **评估指标单一**：主实验仅报告了**总体得分百分比**，缺乏对**具体错误类型**（如检索失败、推理错误、折叠决策失误）的细粒度分析。对于WideSearch使用了Item-F1，但其他数据集的具体评分细则未说明，可能存在“指标幸运”问题。\n3.  **基线对比不公**：与ReAct的上下文效率对比是**概念性的**，并未在**相同模型、相同任务**下进行严格的A/B测试。文中ReAct基线的具体实现和模型规模未明确，对比可能不公平。\n4.  **缺少消融实验**：**致命缺陷**。论文没有进行任何消融实验来分离“折叠操作”本身与“更好的训练数据/模型”带来的收益。无法证明性能提升主要归功于折叠架构，而非其独特的SFT数据（Fold-Generator）。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **折叠决策的可靠性**：模型如何学会做出“正确”的折叠决策（何时细粒度压缩，何时深度整合）？**缺乏理论保证或鲁棒性分析**。在嘈杂或对抗性环境中，错误的折叠（如过早整合关键细节）可能导致任务不可恢复的失败，且这种错误会随着步数累积。\n2.  **摘要质量依赖**：折叠操作的核心是生成摘要文本 \\(σ_t\\)。摘要的质量完全依赖于底层语言模型的能力。**如果模型生成有偏差或不完整的摘要，错误信息将被固化到“长期记忆”中，并影响所有后续步骤**，存在错误传播放大的风险。\n3.  **真实部署的扩展性**：虽然上下文令牌数增长慢，但**每一步都需要模型生成包含折叠指令的复杂响应，并进行JSON解析和上下文更新操作**。对于超长任务（如500步），这些额外开销的累积影响未被评估。在真实系统中，延迟和计算成本可能仍然很高。\n4.  **对高质量训练数据的依赖**：方法严重依赖于Fold-Generator生成的**高质量、格式正确的轨迹数据**。该流水线本身基于强大的开源LLM和拒绝采样，其构建成本和可扩展性存疑。对于新领域或新工具，需要重新收集数据。\n\n**§3 未经验证的边界场景**\n1.  **领域外或对抗性查询**：当用户提出**领域外知识**或**恶意对抗性查询**（旨在诱导错误折叠）时，AgentFold的折叠决策机制是否稳健？例如，故意提供误导性信息让智能体过早折叠关键线索。\n2.  **多模态输入**：当前工作仅处理文本。如果任务涉及**多模态输入**（如图片、表格），如何折叠这些非文本信息？架构需要根本性修改。\n3.  **动态变化的环境**：在**实时变化**的网页环境中（如价格更新、新闻滚动），智能体早期折叠的信息可能很快过时。当前架构缺乏**记忆更新或重新激活**机制来处理动态信息。\n4.  **极端多跳推理**：对于需要**数十甚至上百跳**的复杂推理链任务，深度整合可能过度抽象，丢失中间推理步骤，导致最终答案无法追溯或验证。\n\n**§4 可复现性与公平性问题**\n1.  **训练数据未公开**：Fold-Generator生成的数据集**未开源**。其他研究者无法在完全相同的数据上复现或改进该方法，严重影响了可复现性。\n2.  **依赖特定底座模型**：实验基于**Qwen3-30B-A3B-Instruct-2507**。虽然开源，但其性能可能部分归因于该模型本身的强大能力。未在其他开源模型（如Llama、Gemma）上验证架构的通用性。\n3.  **超参数调优细节缺失**：SFT训练的关键超参数（学习率、批次大小、epoch数等）**完全未提供**，增加了复现难度。\n4.  **对比基线版本不明**：与诸多基线对比时，未说明使用的是其官方发布的哪个版本或检查点，可能存在版本差异导致对比不公平。",
    "zero_compute_opportunity": "【九、零算力研究契机】\n\n#### 蓝图一：探究轻量级模型的上下文折叠能力：一个消融研究与效率基准\n-   **核心假设**：AgentFold的性能提升主要源于其“折叠”架构，而非其使用的30B参数底座模型。更小的模型（如7B或13B）在采用相同架构后，能否在效率（上下文长度、推理速度）上获得更大优势，并在性能上逼近甚至超越更大的ReAct基线？\n-   **与本文的关联**：基于本文未进行模型规模消融的不足，验证架构有效性是否与模型规模解耦。\n-   **所需资源**：\n    -   **模型**：Hugging Face上免费的Qwen2.5-7B/14B-Instruct或Llama-3.1-8B-Instruct。\n    -   **数据**：使用本文开源代码中的**Fold-Generator逻辑**，但用上述小模型作为“教师”生成训练数据（质量可能较低，但成本极低）。或使用公开的网页交互轨迹数据集（如WebShop）进行改编。\n    -   **计算**：Google Colab免费T4 GPU（16GB）即可进行SFT微调。预计API调用费用：0（完全本地）。\n-   **执行步骤**：\n    1.  复现AgentFold的上下文表示和折叠逻辑。\n    2.  使用小规模LLM（如Qwen2.5-7B）和简化版的Fold-Generator（例如，仅使用规则或少量示例提示）生成小型训练数据集（~1k条轨迹）。\n    3.  在消费级GPU上对7B模型进行SFT微调，得到“Mini-AgentFold”。\n    4.  在BrowseComp或WideSearch的**子集**（如50个样本）上评估Mini-AgentFold，并与相同底座的ReAct基线、以及原文的AgentFold-30B进行对比。重点记录：性能得分、平均上下文长度、推理延迟。\n    5.  进行消融：训练一个相同数据但**移除折叠指令**的版本，对比性能。\n-   **预期产出**：一篇4-6页的短论文或技术报告，证明轻量级模型结合折叠架构的可行性，提供详细的效率基准。可投稿至EMNLP Findings、AACL-IJCNLP或arXiv。\n-   **潜在风险**：小模型生成的数据质量差，导致训练失败。应对：使用规则模板生成简单的折叠示例，或利用少量GPT-3.5-Turbo API（低成本）生成高质量种子数据再进行扩充。\n\n#### 蓝图二：基于规则与检索的折叠决策代理：降低对LLM摘要的依赖\n-   **核心假设**：折叠决策（何时、如何折叠）可以部分由基于**规则**（如步骤数、工具调用类型）和**检索**（相似度判断）的轻量级模块决定，而非完全依赖LLM生成，从而降低计算开销和错误摘要风险。\n-   **与本文的关联**：针对本文方法摘要质量依赖LLM和折叠决策缺乏理论保证的局限性，探索更可控、可解释的替代方案。\n-   **所需资源**：\n    -   **工具**：SentenceTransformers（免费）用于计算文本相似度，简单的规则引擎。\n    -   **数据**：公开的网页交互轨迹日志（如WebGPT数据集）。\n    -   **计算**：个人笔记本电脑CPU即可运行。\n-   **执行步骤**：\n    1.  设计一组启发式规则：例如，“连续3步搜索未找到答案”触发深度整合；“上一步是点击链接且包含关键信息”触发细粒度压缩。\n    2.  设计检索模块：将当前最新交互与历史摘要块进行嵌入相似度计算，如果相似度高，则建议合并。\n    3.  构建一个混合决策器：结合规则和检索结果，输出折叠建议（范围、摘要模板）。摘要文本仍可由小LLM生成，但指令更简单（如“总结以下失败步骤”）。\n    4.  在模拟环境或现有轨迹数据上离线评估该决策器的准确率（与人工标注的“理想折叠”对比）。\n    5.  将该决策器与一个7B的Action模型结合，构建一个混合系统，在小型基准上测试性能。\n-   **预期产出**：一个可复用的、轻量级的上下文管理模块代码库，以及一篇分析规则/检索方法在折叠任务中有效性的论文。可投稿至EMNLP（系统演示）、或AAMAS等智能体会议。\n-   **潜在风险**：规则和检索方法过于僵化，无法处理复杂多变的真实场景。应对：将规则设计为可学习的参数（如强化学习中的策略网络），但初始阶段先验证简单方法的可行性。\n\n#### 蓝图三：长视野对话任务中的上下文折叠：超越网页搜索的泛化性测试\n-   **核心假设**：AgentFold的主动上下文管理范式可以泛化到其他长视野序列决策任务，如**多轮开放域对话**，其中同样面临上下文累积和关键信息淹没的问题。\n-   **与本文的关联**：验证本文方法在非网页搜索场景下的有效性，回应审稿人关于任务泛化性的质疑。\n-   **所需资源**：\n    -   **数据集**：公开的长对话数据集，如**Multi-Session Chat**（MSC）或**LongChat**。\n    -   **模型**：Hugging Face上免费的7B对话模型（如Zephyr-7B-beta）。\n    -   **评估**：使用LLM-as-a-Judge（如免费/低成本的Claude Haiku或GPT-3.5-Turbo API）进行对话连贯性、信息留存度的评估。\n    -   **费用**：少量API调用费用（预计< $50）。\n-   **执行步骤**：\n    1.  将对话任务形式化：用户查询+对话历史（多轮）作为上下文，模型生成下一轮回复。将“折叠”操作定义为对历史对话的摘要/压缩。\n    2.  构建训练数据：从长对话数据集中，人工标注或使用LLM标注哪些历史轮次可以被折叠/摘要，并生成摘要文本。\n    3.  微调一个7B对话模型，使其在生成回复时，同时输出对历史对话的折叠指令和摘要。\n    4.  评估：与标准的“完整历史”对话模型、以及“每轮固定摘要”的基线对比。评估指标：对话质量（LLM评分）、上下文长度、在超长对话（>50轮）中关键事实的回忆准确率。\n-   **预期产出**：一篇证明上下文折叠范式可迁移到对话领域的论文，并提出适用于对话的折叠策略。可投稿至SIGDIAL、INLG或EMNLP。\n-   **潜在风险**：对话中的信息重要性更主观，难以定义“正确”的折叠策略。应对：聚焦于事实性对话（如知识问答对话），使用事实回忆准确率作为客观指标。",
    "source_file": "AgentFold Long-Horizon Web Agents with Proactive Context Management.md"
}