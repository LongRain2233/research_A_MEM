{
    "title": "DeepSeek-OCR: Contexts Optical Compression",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n当前大型语言模型（LLMs）在处理长文本内容时面临二次方复杂度带来的显著计算挑战。本文探索一个潜在的解决方案：利用视觉模态作为文本信息的高效压缩媒介。一个包含文档文本的单张图像，可以用比等效数字文本少得多的token来表示丰富的信息，这表明通过视觉token进行光学压缩可以实现更高的压缩比。这一洞见促使作者从LLM中心视角重新审视视觉语言模型（VLMs），聚焦于视觉编码器如何提升LLMs处理文本信息的效率，而非基础视觉问答任务。光学字符识别（OCR）任务作为连接视觉与语言的中间模态，为这种视觉-文本压缩范式提供了理想的试验场，因为它建立了视觉与文本表示之间自然的压缩-解压缩映射，同时提供了定量评估指标。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有开源VLMs主要采用三类视觉编码器，各有其具体失败模式：\n1.  **双塔架构（如Vary）**：使用并行SAM编码器增加视觉词汇参数以处理高分辨率图像。当需要部署时，其**双重图像预处理**流程复杂化部署，并且在训练期间**编码器流水线并行化**极具挑战。\n2.  **分块处理方法（如InternVL2.0）**：通过将图像分割成小块进行并行计算，在高分辨率设置下减少激活内存。然而，由于其**原生编码器分辨率通常较低（低于512×512）**，导致大图像被过度分割，产生**大量视觉token**，从而拖慢推理的预填充和生成阶段。\n3.  **自适应分辨率编码（如Qwen2-VL）**：采用NaViT范式直接通过基于patch的分割处理完整图像，无需分块并行化。但当处理大图像时，由于**巨大的激活内存消耗**可能导致GPU内存溢出，且序列打包在训练期间需要**极长的序列长度**，同样面临效率瓶颈。\n\n**§3 问题的根本难点与挑战（200字以上）**\n从理论与工程角度看，实现高效视觉-文本压缩的难点在于：\n1.  **计算复杂度与内存占用的矛盾**：高分辨率输入对于准确OCR至关重要，但直接处理会导致视觉token数量激增（与图像面积成正比），进而导致Transformer注意力机制的二次方复杂度爆炸，以及激活内存的线性增长，使得在有限GPU内存下处理大文档变得不可行。\n2.  **压缩比与信息保真度的权衡**：压缩比越高（视觉token越少），每个视觉token需要承载的文本信息密度越大，解码模型（语言模型）的推理难度呈指数级上升，可能导致信息丢失或解码错误率急剧增加。\n3.  **架构设计的兼容性**：理想的编码器需要同时满足**高分辨率处理能力、低激活内存、少量视觉token、多分辨率输入支持、适中的参数量**这五个条件，现有编码器架构无法同时满足，需要进行根本性的创新。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文作者的核心切入点是：**将视觉模态重新定位为LLMs处理长文本上下文的一种高效压缩媒介**。其核心技术假设是：**紧凑的语言模型能够有效地学习从压缩的视觉表示中解码出原始文本信息**。该假设的理论依据在于，OCR任务在视觉和文本表示之间建立了天然的压缩-解压缩映射关系，可以作为研究这种范式的理想测试平台。作者进一步假设，通过**专门的编码器设计（DeepEncoder）**，可以在高分辨率输入下保持低激活内存和少量视觉token，从而在可控的计算开销下实现高压缩比。这一假设的验证基于一个观察：单张图像中的文档文本信息密度远高于其对应的数字文本token序列，因此通过优化视觉编码和语言解码的联合训练，有可能实现接近无损的高倍率压缩。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\nDeepSeek-OCR是一个统一的端到端VLM架构，由编码器（DeepEncoder）和解码器（DeepSeek-3B-MoE）两部分构成。整体数据流向为：**输入高分辨率文档图像 → DeepEncoder进行特征提取与token化压缩 → 输出压缩后的潜在视觉token序列 → DeepSeek-3B-MoE解码器基于视觉token和提示词生成目标文本**。DeepEncoder是系统的核心引擎，参数量约380M，主要由80M的SAM-base和300M的CLIP-large串联组成，中间通过一个16倍下采样的卷积压缩模块连接。解码器采用DeepSeek-3B-MoE架构，推理时激活64个路由专家中的6个以及2个共享专家，激活参数量约570M。该设计旨在实现高分辨率输入下的低激活内存、高压缩比以及可控的视觉token数量。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### DeepEncoder（视觉编码器）\n-   **输入**：高分辨率图像（如1024×1024）。\n-   **核心处理逻辑**：采用**串行双组件设计**。首先，图像被分割为patch（如1024/16=4096个patch token），输入到以**窗口注意力（window attention）**为主的视觉感知特征提取组件（SAM-base，80M参数）进行处理。然后，4096个token经过一个**2层卷积压缩模块**（每层kernel size=3, stride=2, padding=1，通道数从256增加到1024）进行16倍下采样，得到256个token。最后，这些token输入到具有**密集全局注意力（dense global attention）**的视觉知识特征提取组件（CLIP-large，300M参数）。\n-   **输出**：压缩后的潜在视觉token序列，维度为 \\( \\mathbb{R} ^ {n \\times d _ {\\mathrm {latent}}} \\)，其中n是压缩后的token数量（如256）。\n-   **设计理由**：此设计将计算密集的全局注意力层限制在处理少量（压缩后）的token上，而让计算较轻的窗口注意力层处理大量原始patch token，从而在保持高分辨率处理能力的同时，将总体激活内存控制在可接受范围内（4096窗口注意力token + 256全局注意力token），解决了现有编码器无法兼顾高分辨率与低激活的难题。\n\n#### 多分辨率支持模块\n-   **输入**：任意尺寸的原始图像。\n-   **核心处理逻辑**：通过**位置编码的动态插值**实现多分辨率支持。设计了两种主要输入模式：**原生分辨率模式**（Tiny:512×512/64 tokens, Small:640×640/100 tokens, Base:1024×1024/256 tokens, Large:1280×1280/400 tokens）和**动态分辨率模式**（Gundam: n×640×640 tiles + 1024×1024全局视图；Gundam-M: n×1024×1024 tiles + 1280×1280全局视图）。对于Tiny/Small模式，图像直接缩放到目标尺寸；对于Base/Large模式，图像通过填充（padding）保持原始宽高比，有效视觉token数按公式 \\( N_{\\text{valid}} = \\lceil N_{\\text{actual}} \\times [1 - ((\\max(w, h) - \\min(w, h)) / (\\max(w, h)))] \\rceil \\) 计算。动态分辨率模式遵循InternVL2.0的分块方法，tile数量n控制在2到9之间。\n-   **输出**：适配不同压缩比需求的视觉token序列，token数量可变（64, 100, 256, 400, n×100+256等）。\n-   **设计理由**：为了系统研究视觉-文本压缩的可行性边界，模型需要支持可变的视觉token数量以测试不同压缩比下的解码精度。多模式联合训练使单个模型能灵活应对不同复杂度和信息密度的文档。\n\n#### DeepSeek-3B-MoE解码器\n-   **输入**：来自DeepEncoder的压缩潜在视觉token序列 \\( \\mathbf{Z} \\in \\mathbb{R} ^ {n \\times d _ {\\mathrm {latent}}} \\) 以及文本提示（prompt）。\n-   **核心处理逻辑**：解码器学习一个非线性映射函数 \\( f_{\\mathrm{dec}}: \\mathbb{R} ^ {n \\times d_{\\text{latent}}} \\rightarrow \\mathbb{R} ^ {N \\times d_{\\text{text}}} \\)，从压缩的视觉表示中重建出文本表示 \\( \\hat{\\mathbf{X}} \\)。它基于混合专家（MoE）架构，在推理时激活6/64个路由专家和2个共享专家，以约570M的激活参数量获得接近3B模型的表达能力。\n-   **输出**：解码生成的文本序列。\n-   **设计理由**：选择MoE架构是为了在面向OCR的VLM研究中，**以较小模型（500M级别）的推理效率，获得较大模型（3B级别）的表达能力**，这对于需要处理复杂布局和多语言文档的密集视觉感知任务至关重要。\n\n**§3 关键公式与算法（如有）**\n1.  **有效视觉token计算公式**（用于Base/Large填充模式）：\n    \\[\n    N _ {\\text {valid}} = \\lceil N _ {\\text {actual}} \\times [ 1 - ((\\max (w, h) - \\min (w, h)) / (\\max (w, h))) ] \\rceil \\tag{1}\n    \\]\n    其中 \\(w\\) 和 \\(h\\) 代表原始输入图像的宽度和高度，\\(N_{\\text{actual}}\\) 是填充后的总token数。\n2.  **解码器映射函数**：\n    \\[\n    f _ {\\mathrm {d e c}}: \\mathbb {R} ^ {n \\times d _ {\\text {l a t e n t}}} \\rightarrow \\mathbb {R} ^ {N \\times d _ {\\text {t e x t}}}; \\quad \\hat {\\mathbf {X}} = f _ {\\mathrm {d e c}} (\\mathbf {Z}) \\quad \\text {w h e r e} \\ n \\leq N \\tag{2}\n    \\]\n    其中 \\(\\mathbf{Z}\\) 是来自DeepEncoder的压缩潜在视觉token，\\(\\hat{\\mathbf{X}}\\) 是重建的文本表示。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\nDeepSeek-OCR本身是一个统一模型，但通过不同的**分辨率模式**实现了性能与效率的变体，这些模式在训练时是联合学习的：\n1.  **Tiny模式**：分辨率512×512，输出64个视觉token。适用于文本token较少（<700）的简单文档，压缩比最高，速度最快，但精度较低。\n2.  **Small模式**：分辨率640×640，输出100个视觉token。平衡了压缩比和解码精度，在10倍压缩比内能达到高精度。\n3.  **Base模式**：分辨率1024×1024，输出256个视觉token（实际有效token约182）。通过填充保持宽高比，适用于大多数文档，提供较好的精度。\n4.  **Large模式**：分辨率1280×1280，输出400个视觉token（实际有效token约285）。提供更高精度，适用于复杂文档。\n5.  **Gundam模式**：动态分辨率，由n个640×640的局部视图（tiles）和一个1024×1024的全局视图组成，输出n×100+256个token。专为超高分辨率图像（如报纸）设计，通过分块二次降低激活内存。\n6.  **Gundam-M模式**：在预训练的DeepSeek-OCR模型上继续训练得到，由n个1024×1024的局部视图和一个1280×1280的全局视图组成，输出n×256+400个token。性能最强，但训练和推理开销最大。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上的本质区别如下：\n-   **与Vary相比**：Vary采用**双塔并行架构**（SAM+CLIP），需要双重图像预处理，部署复杂且训练并行化困难。DeepSeek-OCR采用**串行架构**（SAM→卷积压缩器→CLIP），将CLIP的输入从原始图像改为SAM的输出token，并插入16倍下采样压缩模块，实现了**单流处理**，简化了部署，并通过token压缩显著降低了后续全局注意力层的计算和内存开销。\n-   **与InternVL2.0相比**：InternVL2.0使用**分块（tiling）方法**处理高分辨率，但其原生编码器分辨率低（<512×512），导致图像被过度分割，产生大量视觉token（如6790个）。DeepSeek-OCR的**原生分辨率更高（最高1280×1280）**，并且通过串行压缩设计，即使在高分辨率下也能保持较少的视觉token（如Base模式256个），避免了过度碎片化，同时Gundam模式仅在必要时才启用分块。\n-   **与Qwen2-VL相比**：Qwen2-VL采用**自适应分辨率编码（NaViT）**，直接处理完整图像，但面临大图像下**激活内存爆炸**和**训练序列过长**的问题。DeepSeek-OCR通过**窗口注意力处理大量原始patch，再压缩token后进入全局注意力**的设计，从根本上限制了高分辨率下全局注意力的token数量，从而控制了内存消耗。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\nStep 1: 输入用户查询（Query）q和文档图像I。\nStep 2: 根据任务需求（压缩比、精度）选择DeepEncoder的分辨率模式（Tiny/Small/Base/Large/Gundam）。\nStep 3: 对图像I进行预处理：\n    - 若为Tiny/Small模式：将I resize至目标分辨率（512×512或640×640）。\n    - 若为Base/Large模式：将I padding至目标分辨率（1024×1024或1280×1280），并计算有效视觉token数（公式1）。\n    - 若为Gundam模式：将I分割为n个局部tile（每个tile resize至640×640）并保留一个全局视图（resize至1024×1024）。\nStep 4: 预处理后的图像（或tile集合）输入DeepEncoder：\n    - 组件1（SAM-base，窗口注意力）：处理原始高分辨率patch token（如4096个）。\n    - 组件2（16倍卷积压缩器）：对SAM的输出进行16倍下采样，减少token数量（如4096→256）。\n    - 组件3（CLIP-large，全局注意力）：处理压缩后的token序列，输出最终的潜在视觉token序列Z。\nStep 5: 将视觉token序列Z与文本提示（如“<image>\\nFree OCR.”或“<image>\\n<|grounding|>Convert the document to markdown.”）拼接，形成解码器的输入序列。\nStep 6: 输入序列送入DeepSeek-3B-MoE解码器，通过自回归生成（autoregressive generation）输出目标文本序列。\nStep 7: （可选，深度解析模式）如果输出中包含需要进一步解析的图表、几何图形或化学公式图像，可以再次调用DeepSeek-OCR模型，输入特定的解析提示（如“Parse the figure.”）进行二次解析。\n\n**§2 关键超参数与配置**\n-   **卷积压缩器**：2层卷积，每层kernel size=3，stride=2，padding=1，通道数从256增加到1024。下采样倍率固定为16倍。\n-   **分辨率模式与对应token数**：Tiny（512×512，64 tokens）、Small（640×640，100 tokens）、Base（1024×1024，256 tokens）、Large（1280×1280，400 tokens）、Gundam（n×100+256 tokens）、Gundam-M（n×256+400 tokens）。选择依据是权衡压缩比（视觉token数）与解码精度，通过实验确定不同文档类型所需的最优模式。\n-   **解码器专家激活数**：DeepSeek-3B-MoE在推理时激活6个路由专家和2个共享专家，总激活参数量约570M。此配置在表达能力和推理效率间取得平衡。\n-   **训练序列长度**：4096（DeepEncoder预训练）和8192（DeepSeek-OCR整体训练）。\n-   **Gundam模式tile数n**：控制在2到9之间，根据输入图像尺寸动态决定。\n\n**§3 训练/微调设置（如有）**\n-   **训练数据构造**：混合多种数据源：\n    - OCR 1.0数据（70%）：3000万页多语言PDF文档（中英文约2500万页，其他语言约500万页），包含粗标注（直接使用fitz提取）和细标注（使用PP-DocLayout、MinuerU、GOT-OCR2.0等模型构造检测与识别交错数据）。300万Word数据用于公式和HTML表格。自然场景OCR数据（中英文各1000万样本，使用PaddleOCR标注）。\n    - OCR 2.0数据（包含在70%内）：图表数据（1000万张，使用pyecharts/matplotlib渲染，标签为HTML表格格式）、化学公式数据（500万张，SMILES格式，使用RDKit渲染）、平面几何数据（100万张，遵循Slow Perception生成，使用字典格式标签）。\n    - 通用视觉数据（20%）：遵循DeepSeek-VL2，生成描述、检测、定位等相关任务数据。\n    - 纯文本数据（10%）：内部预训练数据，处理为8192 tokens长度。\n-   **优化器与调度**：使用AdamW优化器。DeepEncoder预训练使用cosine annealing调度器，学习率5e-5。DeepSeek-OCR整体训练使用step-based调度器，初始学习率3e-5。\n-   **批次大小与训练轮数**：DeepEncoder预训练批次大小1280，训练2个epoch。DeepSeek-OCR整体训练使用20个节点（每个节点8张A100-40G GPU），数据并行（DP）度为40，全局批次大小640。纯文本数据训练速度90B tokens/天，多模态数据训练速度70B tokens/天。\n-   **并行策略**：采用流水线并行（PP），将整个模型分为4部分：DeepEncoder的SAM和压缩器作为视觉tokenizer置于PP0并冻结参数；CLIP部分作为输入嵌入层置于PP1，权重不冻结；语言模型部分（DeepSeek3B-MoE，12层）平均分配到PP2和PP3（各6层）。\n\n**§4 推理阶段的工程细节**\n-   **并行化策略**：训练时采用的流水线并行（PP）和数据并行（DP）策略在推理时可能进行调整以适应部署环境，但论文未详细说明推理时的具体并行配置。\n-   **缓存机制**：未明确提及特定的键值（KV）缓存优化策略。\n-   **向量数据库**：不涉及向量数据库检索，是纯粹的端到端生成模型。\n-   **多分辨率模式切换**：推理时根据输入图像尺寸和任务要求（压缩比 vs 精度）动态选择DeepEncoder的分辨率模式（Tiny/Small/Base/Large/Gundam）。对于Gundam模式，需要实现图像分块（tiling）逻辑。\n-   **生产部署**：论文提到在生产中，DeepSeek-OCR使用20个节点（每个节点8张A100-40G GPU）每天可生成3300万页数据用于LLM/VLM预训练。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n1.  **Fox Benchmark [21]**：\n    - **用途**：用于**视觉-文本压缩研究**，验证压缩-解压缩能力。\n    - **规模**：100页英文文档。\n    - **领域类型**：多样化文档布局。\n    - **评测问题类型**：文档OCR，评估解码精度（Precision）。\n    - **数据筛选**：仅使用英文文档部分，并**筛选出ground truth文本token数在600-1300之间的文档**，恰好100页。文本使用DeepSeek-OCR的分词器（词汇量约129k）进行分词。\n2.  **OmniDocBench [27]**：\n    - **用途**：用于评估**实际文档解析性能**。\n    - **规模**：未明确说明总页数，但包含多种文档类别。\n    - **领域类型**：涵盖书籍、幻灯片、财务报告、教科书、试卷、杂志、学术论文、笔记、报纸等多种类型。\n    - **评测问题类型**：端到端文档OCR，评估编辑距离（Edit Distance）。\n    - **特殊处理**：对于dots.ocr+200dpi基线，使用fitz将原始图像插值到200dpi。对于DeepSeek-OCR的Base和Large模式，报告了有效视觉token数（根据公式1计算）。\n\n**§2 评估指标体系（全量列出）**\n-   **准确性指标**：\n    1.  **解码精度（Precision）**：在Fox Benchmark上使用，计算模型输出文本与ground truth文本的匹配精度百分比。\n    2.  **编辑距离（Edit Distance）**：在OmniDocBench上使用，值越小表示性能越好。分别报告**总体（overall）**、**文本（text）**、**公式（formula）**、**表格（table）**、**顺序（order）**五个子指标的编辑距离。\n-   **效率/部署指标**：\n    1.  **视觉token数量（Tokens）**：模型处理每页文档平均使用的视觉token数。这是衡量**压缩比**和**计算开销**的核心指标。对于DeepSeek-OCR，同时报告总token数和有效token数（Base/Large模式）。\n    2.  **生产吞吐量**：在20个节点（每个节点8张A100-40G GPU）上，每天可生成**3300万页**训练数据用于LLM/VLM。\n-   **其他自定义指标**：\n    1.  **压缩比（Compression Ratio）**：在Fox Benchmark上计算，定义为 `ground truth文本token数 / 模型使用的视觉token数`。用于量化视觉模态对文本的压缩效率。\n\n**§3 对比基线（完整枚举）**\n-   **Pipeline Models（流水线模型）**：\n    - Dolphin [11]\n    - Marker [1]\n    - Mathpix [2]\n    - MinerU-2.1.1 [34]\n    - MonkeyOCR-1.2B [18]\n    - PPstructure-v3 [9]\n    - **类型**：传统OCR流水线系统，通常包含独立的检测和识别专家模型。\n-   **End-to-end Models（端到端模型）**：\n    - Nougat [6]：使用2352个视觉token。\n    - SmolDocling [25]：使用392个视觉token。\n    - InternVL2-76B [8]：使用6790个视觉token。\n    - Qwen2.5-VL-7B [5]：使用3949个视觉token。\n    - OLMOCR [28]：使用3949个视觉token。\n    - GOT-OCR2.0 [38]：使用256个视觉token。\n    - OCRFlux-3B [3]：使用3949个视觉token。\n    - GPT4o [26]：未报告视觉token数（闭源API）。\n    - InternVL3-78B [42]：使用6790个视觉token。\n    - Qwen2.5-VL-72B [5]：使用3949个视觉token。\n    - dots.ocr [30]：使用3949个视觉token。\n    - Gemini2.5-Pro [4]：未报告视觉token数（闭源API）。\n    - MinerU2.0 [34]：使用6790个视觉token。\n    - dots.ocr+200dpi [30]：使用5545个视觉token。\n    - **类型**：端到端视觉语言模型，直接输入图像输出文本。\n-   **代表性说明**：Baseline涵盖了从传统流水线到最新大型端到端VLM的全谱系，包括开源和闭源模型，以及专门为文档OCR设计的模型（如GOT-OCR2.0、Nougat）和通用VLM（如InternVL、Qwen-VL系列）。这确保了对比的全面性。\n\n**§4 实验控制变量与消融设计**\n论文未设计传统的组件消融实验（如移除压缩模块）。其核心的消融设计体现在**不同分辨率模式的性能对比**上，这本质上是对**压缩比（视觉token数）** 这一核心变量的系统研究。通过固定解码器（DeepSeek-3B-MoE）和训练数据，仅改变DeepEncoder的分辨率模式（从而改变视觉token数量），可以清晰观察到压缩比对解码精度的影响。此外，通过对比不同模式在各类文档（书籍、报纸等）上的表现，分析了不同文档类型对压缩比的敏感性。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n**Fox Benchmark（压缩比与精度）**\n`文本Token范围 | Vision Tokens=64 精度 | Vision Tokens=64 压缩比 | Vision Tokens=100 精度 | Vision Tokens=100 压缩比 | 页数`\n`600-700 | 96.5% | 10.5× | 98.5% | 6.7× | 7`\n`700-800 | 93.8% | 11.8× | 97.3% | 7.5× | 28`\n`800-900 | 83.8% | 13.2× | 96.8% | 8.5× | 28`\n`900-1000 | 85.9% | 15.1× | 96.8% | 9.7× | 14`\n`1000-1100 | 79.3% | 16.5× | 91.5% | 10.6× | 11`\n`1100-1200 | 76.4% | 17.7× | 89.8% | 11.3× | 8`\n`1200-1300 | 59.1% | 19.7× | 87.1% | 12.6× | 4`\n\n**OmniDocBench（端到端模型性能，编辑距离越小越好）**\n`模型 | 视觉Token数 | 英文-总体 | 英文-文本 | 英文-公式 | 英文-表格 | 英文-顺序 | 中文-总体 | 中文-文本 | 中文-公式 | 中文-表格 | 中文-顺序`\n`DeepSeek-OCR (Tiny) | 64 | 0.386 | 0.373 | 0.469 | 0.422 | 0.283 | 0.361 | 0.307 | 0.635 | 0.266 | 0.236`\n`DeepSeek-OCR (Small) | 100 | 0.221 | 0.142 | 0.373 | 0.242 | 0.125 | 0.284 | 0.24 | 0.53 | 0.159 | 0.205`\n`DeepSeek-OCR (Base) | 256(182) | 0.137 | 0.054 | 0.267 | 0.163 | 0.064 | 0.24 | 0.205 | 0.474 | 0.1 | 0.181`\n`DeepSeek-OCR (Large) | 400(285) | 0.138 | 0.054 | 0.277 | 0.152 | 0.067 | 0.208 | 0.143 | 0.461 | 0.104 | 0.123`\n`DeepSeek-OCR (Gundam) | 795 | 0.127 | 0.043 | 0.269 | 0.134 | 0.062 | 0.181 | 0.097 | 0.432 | 0.089 | 0.103`\n`DeepSeek-OCR (Gundam-M+200dpi) | 1853 | 0.123 | 0.049 | 0.242 | 0.147 | 0.056 | 0.157 | 0.087 | 0.377 | 0.08 | 0.085`\n`GOT-OCR2.0 | 256 | 0.287 | 0.189 | 0.360 | 0.459 | 0.141 | 0.411 | 0.315 | 0.528 | 0.52 | 0.28`\n`MinerU2.0 | 6790 | 0.133 | 0.045 | 0.273 | 0.15 | 0.066 | 0.238 | 0.115 | 0.506 | 0.209 | 0.122`\n`dots.ocr+200dpi | 5545 | 0.125 | 0.032 | 0.329 | 0.099 | 0.04 | 0.16 | 0.066 | 0.416 | 0.092 | 0.067`\n`Gemini2.5-Pro | - | 0.148 | 0.055 | 0.356 | 0.13 | 0.049 | 0.212 | 0.168 | 0.439 | 0.119 | 0.121`\n`InternVL3-78B | 6790 | 0.218 | 0.117 | 0.38 | 0.279 | 0.095 | 0.296 | 0.21 | 0.533 | 0.282 | 0.161`\n`Qwen2.5-VL-72B | 3949 | 0.214 | 0.092 | 0.315 | 0.341 | 0.106 | 0.261 | 0.18 | 0.434 | 0.262 | 0.168`\n`GPT4o | - | 0.233 | 0.144 | 0.425 | 0.234 | 0.128 | 0.399 | 0.409 | 0.606 | 0.329 | 0.251`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n-   **压缩比与精度关系（Fox Benchmark）**：当压缩比低于10倍（即文本token数在视觉token数的10倍以内）时，模型解码精度可达97%以上（Small模式在文本token 600-1000范围内精度为96.8%-98.5%）。当压缩比超过10倍后，精度开始下降，在20倍压缩比（Tiny模式，文本token 1200-1300）时，精度仍能保持在59.1%。这表明**光学上下文压缩在10倍以内接近无损，在20倍时仍保留大部分信息**。性能下降可能源于长文档布局更复杂，或文本在低分辨率下变得模糊。\n-   **实际OCR性能（OmniDocBench）**：\n    - **效率优势**：仅用100个视觉token（Small模式），DeepSeek-OCR的总体编辑距离（英文0.221，中文0.284）**优于使用256个token的GOT-OCR2.0（英文0.287，中文0.411）**。用400个token（Large模式，实际有效285个），性能与SOTA模型（如使用6790个token的MinerU2.0）**持平甚至略有超越**（英文0.138 vs 0.133，中文0.208 vs 0.238）。用少于800个token（Gundam模式），**超越需要近7000个token的MinerU2.0**（英文0.127 vs 0.133，中文0.181 vs 0.238）。\n    - **分文档类型性能**（表4）：幻灯片（Slides）仅需64个token即可达到0.147的编辑距离。书籍（Book）和报告（Financial Report）用100个token即可取得良好性能（0.085, 0.079）。报纸（Newspaper）需要Gundam或Gundam-M模式（编辑距离0.153/0.1），因为其文本token数高达4000-5000，远超其他模式的10倍压缩极限。\n-   **与基线的对比**：DeepSeek-OCR在**视觉token使用效率上显著优于所有对比的端到端模型**。在相近或更少的token消耗下，其性能匹配或超越参数量大得多的模型（如InternVL3-78B、Qwen2.5-VL-72B）。虽然绝对性能上略逊于某些使用数千token的SOTA模型（如dots.ocr+200dpi），但其**单位token带来的性能增益最高**，证明了其架构在压缩表示上的有效性。\n\n**§3 效率与开销的定量对比**\n-   **视觉token消耗对比**：在OmniDocBench上，DeepSeek-OCR的Small模式（100 tokens）相比GOT-OCR2.0（256 tokens）**视觉token消耗减少60.9%**，同时性能更优。Base模式（256 tokens，有效182 tokens）相比InternVL2-76B（6790 tokens）**视觉token消耗减少96.2%**，性能相近甚至更好（英文0.137 vs 0.44）。Gundam模式（<800 tokens）相比MinerU2.0（6790 tokens）**视觉token消耗减少超过88%**，性能更优。\n-   **生产吞吐量**：使用20个节点（每个节点8张A100-40G GPU），DeepSeek-OCR每天可生成**3300万页**训练数据用于LLM/VLM预训练。\n-   **延迟与显存占用**：论文未提供具体的推理延迟（ms）和显存占用（GB）数据。\n\n**§4 消融实验结果详解**\n论文未进行传统的组件移除消融实验。其核心的“消融”体现在不同分辨率模式的性能阶梯上，这验证了**视觉token数量（压缩比）是性能的关键决定因素**：\n-   从Tiny（64 tokens）到Small（100 tokens），英文总体编辑距离从0.386**大幅提升至0.221（相对提升42.7%）**，中文从0.361提升至0.284（相对提升21.3%）。\n-   从Small（100 tokens）到Base（256 tokens），英文总体编辑距离从0.221**进一步提升至0.137（相对提升38.0%）**，接近SOTA水平。\n-   从Base到Large（400 tokens）性能提升不明显（英文0.137→0.138），表明在256个token附近可能存在**收益递减点**。\n-   对于极端复杂的报纸文档，必须使用Gundam模式（795 tokens）或Gundam-M模式（1853 tokens）才能获得可接受的性能，这验证了**动态分辨率分块机制对于超高分辨率、高信息密度文档的必要性**。\n\n**§5 案例分析/定性分析（如有）**\n论文通过图7-11展示了DeepSeek-OCR的**深度解析（Deep Parsing）** 和**多语言识别**能力：\n-   **成功案例**：模型能够统一解析文档内的图表（输出HTML表格）、化学公式（输出SMILES格式）、简单平面几何图形（输出结构化字典）以及自然图像（输出密集描述）。例如，图7展示了金融研究报告中的图表结构化提取；图9展示了化学文档中公式到SMILES的转换；图10展示了平面几何图形的复制（结构化）能力。这证明了模型在OCR 2.0任务上的泛化能力。\n-   **多语言支持**：图11展示了模型对阿拉伯语和僧伽罗语等近100种语言文档的OCR能力，支持带布局和不带布局两种输出格式，证明了其数据工程（特别是对小语种数据的处理）的有效性。\n-   **失败模式分析**：论文未明确展示失败案例，但指出在Fox Benchmark上，由于输出格式无法完全匹配基准格式，实际性能会略高于测试结果。同时，压缩比超过10倍后性能下降，可能源于长文档布局复杂化或低分辨率下文本模糊，这暗示了方法在**极高压缩比或极低分辨率下信息丢失**的潜在风险。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出了视觉-文本压缩的定量分析框架**：首次系统研究了视觉token数量与文本解码精度之间的关系，实证表明在10倍压缩比内可实现~97%的解码精度，在20倍压缩比下仍能保持~60%精度，为LLM的长上下文压缩和遗忘机制研究提供了新思路。\n2.  **设计了DeepEncoder新型视觉编码器**：通过串联窗口注意力（SAM）和全局注意力（CLIP）组件，中间插入16倍卷积压缩器，实现了高分辨率输入下的低激活内存和少量视觉token输出，解决了现有编码器无法兼顾高分辨率与低激活的难题。\n3.  **构建了高效实用的DeepSeek-OCR系统**：基于DeepEncoder和DeepSeek-3B-MoE解码器，在OmniDocBench上以最少的视觉token数取得了端到端模型中的SOTA性能（100个token超越GOT-OCR2.0的256个token，少于800个token超越MinerU2.0的近7000个token），并具备深度解析和多语言识别能力，生产环境下单日可处理3300万页数据。\n\n**§2 局限性（作者自述）**\n原文中作者未明确列出“局限性”章节，但从讨论部分可推断：\n1.  **任务聚焦**：本文主要作为概念验证（proof-of-concept）聚焦于OCR任务，尚未将视觉-文本压缩范式推广到更广泛的文本处理场景。\n2.  **压缩边界**：实验表明，当压缩比超过10倍（文本token数超过视觉token数的10倍）时，解码精度开始下降，尤其是在布局复杂的超长文档（如报纸）上，需要启用计算开销更大的动态分辨率模式（Gundam）。\n3.  **输出格式对齐**：在Fox Benchmark上的评测因输出格式无法完全匹配ground truth，实际性能可能被低估。\n\n**§3 未来研究方向（全量提取）**\n1.  **实现近乎无损的10倍上下文压缩**：作者展望未来可能通过文本到图像（text-to-image）的方法，实现接近10倍的无损上下文压缩。这需要进一步优化编码器和解码器的联合表示学习。\n2.  **探索压缩比与遗忘机制的关系**：当压缩比达到20倍时精度仍保持~60%，这可能成为LLM遗忘机制的一个特征。未来研究可以探索高压缩比下信息的选择性保留与丢弃，将其建模为一种可控的遗忘机制。\n3.  **优化VLM的token分配**：本文的定量分析为VLM中视觉token分配的优化提供了经验指导。未来工作可以基于此，为不同任务和输入类型动态分配最优的视觉token预算。\n4.  **拓展到更广泛的文本处理与智能体系统**：虽然以OCR为切入点，但视觉-文本压缩范式为重新思考视觉与语言模态如何协同增强大规模文本处理和智能体系统的计算效率开辟了新的可能性。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **范式创新：提出并定量验证了“视觉作为文本压缩媒介”的新范式**\n    - **理论新颖性**：首次系统性地将OCR任务重新定义为视觉-文本的压缩-解压缩问题，并提供了压缩比与解码精度之间的定量关系曲线（10倍压缩比内~97%精度，20倍压缩比~60%精度），为LLM长上下文处理提供了全新的技术路线。\n    - **实验验证充分性**：在Fox和OmniDocBench两个基准上进行了详尽的实验，覆盖了从低压缩比到高压缩比、从简单文档到复杂报纸的多种场景，数据扎实。\n    - **对领域的影响**：可能改变VLM和长上下文LLM的研究方向，引导社区关注视觉模态在信息压缩和高效表示方面的潜力，而非仅仅作为感知模态。\n2.  **架构创新：设计并实现了DeepEncoder，解决了高分辨率与低激活的内存矛盾**\n    - **理论新颖性**：创新性地将窗口注意力（处理大量高分辨率patch）与全局注意力（处理少量压缩后token）通过卷积下采样模块串联，在架构层面实现了计算负载的分离，是解决VLM高分辨率输入痛点的有效方案。\n    - **实验验证充分性**：通过多分辨率模式（Tiny/Small/Base/Large/Gundam）验证了该架构在不同计算预算和精度要求下的灵活性与有效性。\n    - **对领域的影响**：为后续设计高效VLM编码器提供了可借鉴的蓝图，特别是其“先局部感知，再全局理解，中间压缩”的思想。\n3.  **工程与实践贡献：发布了高性能、高效率的DeepSeek-OCR模型与代码**\n    - **理论新颖性**：相对较少，更多是前述范式和架构的工程实现。\n    - **实验验证充分性**：在OmniDocBench上取得了极具竞争力的结果，并以远少于基线模型的视觉token消耗实现了可比甚至更优的性能，证明了其工程优越性。\n    - **对领域的影响**：开源了模型权重和代码，为社区提供了一个强大的OCR工具和后续研究的基础模型。其每天3300万页的数据生成能力，对LLM/VLM预训练数据构造有直接实用价值。\n\n**§2 工程与实践贡献**\n-   **开源模型与代码**：完整开源DeepSeek-OCR的模型权重和训练/推理代码，地址为 http://github.com/deepseek-ai/DeepSeek-OCR。\n-   **大规模数据生成能力**：在生产环境中，使用20个节点（160张A100-40G GPU）可实现每天3300万页训练数据的生成吞吐量，为LLM/VLM预训练提供了高效的数据生产流水线。\n-   **统一的多功能模型**：不仅支持传统OCR（OCR 1.0），还支持图表、化学公式、几何图形解析（OCR 2.0）以及通用视觉理解，提供了一个集成的工具。\n-   **多语言支持**：通过精心构建的数据工程，模型支持近100种语言的文档OCR，提升了实用性。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于一个**交叉创新**的位置：它既是**端到端文档理解VLM**技术路线上的一个重要进展（在效率上显著超越了Nougat、GOT-OCR2.0等），也是**LLM长上下文压缩与高效表示**这一新兴方向上的开创性探索。它没有沿着单纯增大模型参数量或使用更多视觉token的路径前进，而是另辟蹊径，从信息压缩的角度重新思考视觉-语言的交互，为解决LLM的长上下文瓶颈提供了一种全新的、与现有VLM基础设施兼容的解决方案。因此，它既是对现有VLM for OCR方向的深化和效率优化，也是开辟了一条名为“光学上下文压缩”的新研究路线。",
    "professor_critique": "**§1 实验设计与评估体系的缺陷**\n1.  **数据集覆盖不全**：压缩比研究仅在**Fox Benchmark的100页英文文档**上进行，样本量小且语言单一，结论的普适性存疑。未在更大规模、多语言、多领域的长文档数据集（如arXiv论文、法律文书）上验证高压缩比下的性能。\n2.  **评估指标单一**：主实验仅使用**编辑距离（Edit Distance）和精度（Precision）**，缺乏对**结构化信息保留度**（如表格格式、数学公式语义、图表数据准确性）的细粒度评估。高压缩比下模型可能“猜对”了文字但丢失了排版和语义结构。\n3.  **基线对比不全面**：未与最新的**专门为长文档OCR设计的模型**（如UDOP、DocFormerv2等）进行对比。也未与**纯文本压缩方法**（如Perplexity-based pruning、LLMLingua等）进行跨模态的压缩效率对比，无法证明视觉压缩相比文本压缩的优势。\n4.  **“指标幸运”风险**：OmniDocBench上“Tokens”指标仅统计视觉token，但**解码器（DeepSeek-3B-MoE）本身有3B参数（激活570M）**，总计算开销并未与参数量小得多的纯OCR模型（如PPstructure-v3）进行公平比较。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **信息瓶颈理论应用不彻底**：虽然提出了压缩思想，但**16倍下采样卷积压缩器是启发式设计**，缺乏理论指导（如基于信息瓶颈的最优压缩率）。当输入图像信息密度极高（如超小字体密集排版）时，固定16倍压缩可能导致关键细节丢失，且模型无法自适应调整压缩率。\n2.  **动态分辨率模式的工程隐患**：Gundam模式的分块（tiling）策略可能导致**跨块上下文丢失**。例如，一个表格或公式被切割到两个tile中，模型可能无法正确重建其结构。论文未测试这种边界情况。\n3.  **对真实部署场景的考虑不足**：模型依赖**高分辨率输入**（最高1280×1280），在实际部署中，图像预处理（缩放、填充）和编码本身会带来显著延迟，论文未报告端到端延迟数据。此外，多分辨率模式切换需要额外的逻辑判断，增加了系统复杂性。\n4.  **解码器能力瓶颈**：实验表明，在Base模式（256 tokens）之后，增加视觉token（Large模式，400 tokens）带来的性能提升微乎其微。这可能暗示**解码器（570M激活参数）的能力上限**，而非编码器信息不足。要突破更高压缩比，可能需要更强解码器，但这会抵消压缩带来的效率收益。\n\n**§3 未经验证的边界场景**\n1.  **极端排版与低质量图像**：未测试**手写体、艺术字体、严重遮挡、低对比度、拍照扭曲**的文档。在这些场景下，视觉编码器的感知能力可能崩溃，导致压缩-解压缩完全失败。\n2.  **多模态混合文档**：文档中包含大量**自然图像与文本交错**的版面（如杂志），模型在深度解析时可能混淆文本区域和图像区域，导致错误的信息提取。\n3.  **对抗性攻击与安全**：未测试模型对**对抗性扰动**的鲁棒性。恶意制作的、对人类可读但对模型不可解码的“对抗图像”可能轻易破坏整个压缩-解压缩流程。\n4.  **超长文档的序列建模**：模型处理的是单页图像。对于**多页PDF**，需要逐页处理，模型**缺乏跨页的长期依赖建模能力**，无法进行真正的“长上下文”压缩（如整本书的摘要）。\n5.  **领域外知识与符号系统**：对于包含**罕见符号、专业领域 notation（如音乐乐谱、电路图）** 的文档，模型可能因训练数据缺乏而无法正确编码和解码。\n\n**§4 可复现性与公平性问题**\n1.  **计算资源依赖**：训练使用了**20个节点（160张A100-40G GPU）**，并依赖HAI-LLM平台，这远超普通研究者的资源预算，导致完整训练流程难以复现。\n2.  **数据依赖**：模型使用了**3000万页私有PDF数据**和大量合成数据，数据构造流程复杂（涉及fitz、PP-DocLayout、MinuerU、GOT-OCR2.0、PaddleOCR等多个工具），完整数据流水线难以复现。\n3.  **超参数调优公平性**：论文未说明是否为DeepSeek-OCR和每个Baseline都进行了同等的超参数调优（如提示工程、分辨率选择）。对于DeepSeek-OCR，可以选择最优分辨率模式，而其他Baseline可能只有固定输入尺寸，这造成了对比的不公平。\n4.  **闭源Baseline的对比**：与GPT4o、Gemini2.5-Pro等闭源API模型的比较存在不确定性，因为其内部视觉token数未知，且API可能存在速率限制和后处理，并非严格的端到端对比。",
    "zero_compute_opportunity": "**§1 零算力研究契机**\n为资源受限的研究者（无GPU集群、无大额API预算）提供3个可立即执行的研究蓝图，每个蓝图必须具备可操作性。\n\n#### 蓝图一：探究小语言模型在视觉-文本压缩中的极限与瓶颈\n-   **核心假设**：当前DeepSeek-OCR使用570M激活参数的MoE解码器，在Base模式（256视觉token）后出现性能瓶颈。我们假设，**更小（如100M）或更大（如1B）的纯解码器语言模型**，在固定编码器（DeepEncoder）下，其解码精度随压缩比变化的曲线形状不同，从而揭示解码器能力与压缩效率的关系。\n-   **与本文的关联**：基于本文发现“在10倍压缩比内解码精度可达97%”，但未系统研究解码器规模的影响。本研究将深化对压缩-解压缩映射函数 \\(f_{\\mathrm{dec}}\\) 学习难度的理解。\n-   **所需资源**：\n    1.  使用**公开的DeepSeek-OCR模型权重**（已开源），冻结DeepEncoder。\n    2.  使用**Fox Benchmark**（100页文档，已公开）或自构建的小规模多语言文档数据集（可从arXiv、Wiki等获取）。\n    3.  替换解码器为**不同规模的公开小语言模型**（如Phi-2、Qwen2.5-0.5B、TinyLlama等），在单张消费级GPU（如RTX 4090）上进行轻量微调或评估。\n    4.  **预计成本**：主要成本为电费和少量云GPU租赁（如需微调），可控制在100美元以内。\n-   **执行步骤**：\n    1.  下载DeepSeek-OCR代码和权重，提取其DeepEncoder部分。\n    2.  准备Fox Benchmark数据，并使用DeepEncoder提取不同分辨率模式下的视觉token特征。\n    3.  将视觉token特征与提示词拼接，输入到不同规模的小语言模型（保持输入输出维度适配），进行**仅解码器的微调**（LoRA或全参数微调），目标为文本重建。\n    4.  在不同压缩比（视觉token数）下，评估各解码器模型的精度（Precision）和编辑距离。绘制“解码器参数量-压缩比-精度”三维关系图。\n    5.  分析曲线，确定解码器规模对压缩边界的影响，并尝试理论解释（如表征能力、泛化误差）。\n-   **预期产出**：一篇揭示视觉-文本压缩中解码器能力瓶颈的短论文，可能发表在**EMNLP、ACL Findings或TMLR**。结论可为社区选择合适规模的解码器提供指导。\n-   **潜在风险**：小语言模型可能无法拟合复杂的压缩表示，导致实验失败。**应对方案**：先尝试在极低压缩比（如2倍）下验证可行性，再逐步提高；使用更强的PEFT方法（如QLoRA）来增强小模型的适配能力。\n\n#### 蓝图二：基于公开API的低成本视觉-文本压缩基准构建与评测\n-   **核心假设**：现有评测（Fox, OmniDocBench）未能全面反映视觉-文本压缩在真实长文档、多模态、对抗性场景下的表现。我们可以利用**公开可获取的文档数据集和低成本VLM API**，构建一个更全面、低成本的评测基准。\n-   **与本文的关联**：针对本文实验设计的缺陷（§1），构建一个补充性基准，推动该研究方向的公平比较与健康发展。\n-   **所需资源**：\n    1.  **数据集**：公开数据集如**arXiv论文**（长文本、公式）、**PubLay",
    "source_file": "DeepSeek-OCR Contexts Optical Compression.md"
}