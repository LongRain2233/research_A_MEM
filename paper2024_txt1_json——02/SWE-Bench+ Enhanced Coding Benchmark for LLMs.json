{
    "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
    "background_and_problem": "#### §1 领域背景与研究动机\n本研究聚焦于**大型语言模型（LLMs）在软件工程（Software Engineering）领域的代码修复与问题解决能力评估**。随着LLMs在代码生成、程序修复等任务上展现出潜力，学术界和工业界迫切需要**严谨、可靠的基准数据集**来评估模型在真实世界软件工程场景下的实际能力。SWE-Bench作为首个基于真实GitHub issue的评估基准，迅速成为该领域的标准测试集。然而，随着模型在该基准上的性能快速提升（例如，Honeycomb模型在SWE-Bench Full上的解决率从最初的0.17%跃升至22.00%），一个根本性问题浮现：**这些性能提升是否真实反映了模型的问题解决能力，还是基准数据集本身存在的缺陷导致了性能虚高？** 本文正是在此背景下，对SWE-Bench数据集的质量进行了首次系统性实证分析，旨在揭示现有评估基准的潜在缺陷，并构建一个更鲁棒的评估数据集。\n\n#### §2 现有技术的核心短板——具体失败模式\n本文的核心发现是，现有基于SWE-Bench的评估存在严重的**数据集质量问题**，导致模型性能被严重高估。具体失败模式如下：\n1.  **解决方案泄露（Solution Leakage）**：当输入包含明确解决方案的issue描述或评论时，模型可以直接“抄袭”答案，而非独立解决问题。在SWE-Agent+GPT-4成功解决的251个实例中，**32.67%（82个）** 属于此类。例如，在sympy-16669问题中，issue描述直接提供了修复代码，模型只需复制即可通过测试。\n2.  **弱测试用例（Weak Test Cases）**：当测试用例不够充分或严格时，模型生成的错误、不完整或针对错误文件的补丁仍能通过测试。这占成功实例的**31.08%（78个）**。具体包括：\n    - **错误修复（Incorrect Fixes）**：模型生成错误逻辑的补丁（占12.75%，32个）。例如，在django-32517中，开发者补丁需要反转整个OrderedSet字典，而模型仅反转了字典的键，却通过了测试。\n    - **不完整修复（Incomplete Fixes）**：模型遗漏了关键实现细节（占14.74%，37个）。例如，在django-31056中，模型补丁缺少了关键的try-except块和事件循环运行状态检查。\n    - **修改了不同文件/函数（Different Files/Functions Changed）**：模型修改了与问题无关的文件（占3.59%，9个）。例如，在Matplotlib issue-26093中，模型修改了`cbook.py`文件，而开发者实际修改的是`_axes.py`文件。\n3.  **潜在数据泄露（Data Leakage）**：由于**94%** 的SWE-Bench issue创建于主流LLM（如GPT-4）的训练截止日期之前，模型可能在训练阶段已见过这些问题，导致评估结果无法反映其泛化能力。\n\n#### §3 问题的根本难点与挑战\n构建一个可靠的代码修复评估基准面临多重根本性挑战：\n1.  **数据质量控制的复杂性**：从海量、异构的GitHub issue中筛选出**描述清晰、无解决方案泄露、测试用例充分**的实例是一个高度主观且劳动密集型的过程。自动化过滤难以完全识别“弱测试用例”，因为判断测试的充分性需要深入的领域知识和代码理解。\n2.  **评估信噪比低**：当基准中存在大量“可疑修复”时，评估结果的信噪比极低。模型可以通过“走捷径”（利用泄露的解决方案或绕过弱测试）获得高分，这掩盖了其**真实推理、定位和修复能力**的不足，使得性能比较失去意义。\n3.  **时效性与数据泄露的权衡**：为了避免数据泄露，需要收集模型训练截止日期之后的新issue。但这带来了新的挑战：新issue的数量可能有限，且其复杂性和代表性可能与旧issue不同，影响数据集的规模和多样性。\n4.  **测试用例的完备性悖论**：为每个issue设计完备的测试套件成本极高，且难以穷尽所有边界情况。不充分的测试会高估模型性能，而过度的测试（如EvalPlus所做的）又可能因过于严苛而低估模型的实际可用性。\n\n#### §4 本文的切入点与核心假设\n本文的切入点是**对现有SOTA模型在SWE-Bench上“成功”修复的案例进行人工深度审计**，以验证其真实性。核心假设是：**当前SWE-Bench及其变体（Lite, Verified）报告的高解决率，在很大程度上是由数据集固有的质量问题（解决方案泄露和弱测试用例）所导致的假象，而非模型能力的真实体现。** 这一假设基于一个简单的观察：如果模型真的具备强大的代码修复能力，那么当移除这些有缺陷的数据后，其性能不应出现断崖式下跌。作者通过手动审查SWE-Agent+GPT-4在SWE-Bench Full上所有251个“成功”实例的补丁、issue描述和测试日志，系统地验证了这一假设。基于此发现，作者进一步提出，通过构建一个**排除了解决方案泄露、并尽可能使用训练截止日期后数据**的新数据集（SWE-Bench+），可以提供一个更接近模型真实能力的评估环境。",
    "core_architecture": "#### §1 系统整体架构概览\n本文的核心贡献是**SWE-Bench+数据集的构建与评估流程**，而非提出一个新的LLM架构。其系统是一个**数据清洗与评估验证的pipeline**，整体数据流如下：\n**输入原始SWE-Bench数据 → 应用时间与内容过滤 → 生成SWE-Bench+候选集 → 人工审核排除解决方案泄露 → 输出SWE-Bench+最终数据集 → 使用选定模型（SWE-Agent, SWE-RAG, AutoCodeRover）在SWE-Bench+上运行 → 收集模型生成的补丁 → 使用SWE-Bench官方脚本进行测试执行 → 人工验证通过测试的补丁（对比黄金补丁） → 输出经过审计的解决率与成本分析。**\n该流程的核心模块是数据过滤模块和人工验证模块，旨在确保评估基准的纯净性。\n\n#### §2 各核心模块深度拆解\n**模块一：时间与数据泄露过滤模块**\n- **输入**：从SWE-Bench原始数据收集脚本获取的GitHub issue元数据（包括创建时间、仓库、PR链接）。\n- **核心处理逻辑**：\n  1.  确定所用LLM的训练截止日期：GPT-3.5 (turbo-16k-0613) 截止于2021年9月，GPT-4 (1106) 截止于2023年4月，GPT-4o (2024-05-13) 截止于2023年10月。\n  2.  选取**最晚模型截止日期（2023年10月）之后一个月（2023年11月1日）** 作为数据收集的起始点，以规避所有模型的潜在数据泄露风险。\n  3.  仅收集在**2023-11-01至2024-08-22期间**创建并关闭的issue。\n- **输出**：符合时间窗口要求的候选issue列表。\n- **设计理由**：这是消除预训练数据泄露风险的最直接方法。通过使用模型“未见过的”最新数据，可以确保评估的是模型的泛化与推理能力，而非记忆能力。\n\n**模块二：解决方案泄露人工审核模块**\n- **输入**：通过时间过滤后的候选issue列表，包括issue标题、描述、评论（`hints_text`）。\n- **核心处理逻辑**：由**三位作者独立**对每个候选issue进行人工审查。审查者仔细阅读issue描述和所有相关评论，判断其中是否**明确包含**修复问题所需的代码片段、具体修改建议或可直接复制的解决方案。任何包含此类信息的issue都被标记为“解决方案泄露”并从候选集中移除。\n- **输出**：不包含明确解决方案的“纯净”issue列表。\n- **设计理由**：自动化检测解决方案泄露极其困难，因为解决方案可能以自然语言讨论、代码片段链接或部分提示的形式存在。人工审核是目前保证数据质量最可靠的方法，尽管成本高昂。三位作者独立审查并协商解决分歧，确保了判断的一致性。\n\n**模块三：补丁验证与模式分类模块**\n- **输入**：模型在某个数据集（如SWE-Bench Full或SWE-Bench+）上生成的、且通过了所有测试的补丁文件（diff格式），以及对应的“黄金补丁”（开发者提交的PR）。\n- **核心处理逻辑**：\n  1.  **文件与行级对比**：比较模型补丁和黄金补丁所修改的文件路径、函数名和具体代码行。\n  2.  **语义逻辑分析**：分析补丁所实现的逻辑是否等价、是否完整、是否正确。\n  3.  **模式分类**：根据对比结果，将每个“成功”实例分类到预定义的六种模式中：\n      - **可疑修复**：解决方案泄露、错误修复、不完整修复、修改了不同文件/函数。\n      - **正确修复**：与黄金补丁不同但正确的修复、比黄金补丁更全面的修复。\n- **输出**：每个“成功”实例的分类标签，以及基于“正确修复”重新计算的解决率。\n- **设计理由**：仅依赖测试通过（PASS）作为成功标准是片面的。此模块通过引入“黄金标准”进行细粒度的人工验证，能够揭示测试套件的不足和模型修复的真实质量，是评估基准可靠性的关键步骤。\n\n#### §3 关键公式与算法\n本文未提出新的数学模型或算法公式。其核心方法论是**实证分析与人工审计流程**，而非算法创新。\n\n#### §4 方法变体对比\n本文构建的数据集是SWE-Bench的增强版，其本身是单一版本。但文中对比了多个现有数据集的变体：\n1.  **SWE-Bench Full (原始版)**：包含2294个issue，存在严重的解决方案泄露（32.67%）和弱测试问题（31.08%）。\n2.  **SWE-Bench Lite**：包含300个专注于bug修复的issue，旨在降低评估成本。但本文发现其仍有**33.33%** 的解决方案泄露率和**14.82%** 的弱测试问题（错误+不完整修复）。\n3.  **SWE-Bench Verified**：包含500个经过人工验证、描述清晰、测试用例较强的issue。但本文发现其仍有**33.04%** 的解决方案泄露率和**22.32%** 的弱测试问题。\n4.  **SWE-Bench+ (本文提出)**：包含548个issue，**0%** 解决方案泄露，弱测试问题比例在评估的模型中平均为**67.72%**（但这是模型在“纯净”数据上表现不佳的反映，而非数据集本身的问题）。其核心区别在于**时间过滤（2023年11月后）** 和**人工排除解决方案泄露**。\n\n#### §5 与已有方法的核心技术差异\n本文工作与之前基于SWE-Bench的研究（如SWE-Agent, SWE-RAG, AutoCodeRover）有本质区别：\n1.  **目标不同**：先前工作是**开发新的LLM代理或方法**来在SWE-Bench上取得更高分数。本文是**对评估基准本身进行诊断和重建**，旨在暴露现有基准的缺陷，而非提升模型分数。\n2.  **方法论不同**：先前工作依赖SWE-Bench提供的自动化测试通过率作为唯一评估指标。本文引入了**人工驱动的补丁验证**，将“成功”细分为“可疑”和“正确”，揭示了自动化评估的盲区。\n3.  **数据构建理念不同**：SWE-Bench Verified试图通过人工筛选来提升数据质量，但**未能解决解决方案泄露问题**。SWE-Bench+则明确将“无解决方案泄露”和“训练数据后”作为核心过滤条件，从源头上切断了模型“作弊”的路径。\n4.  **评估维度扩展**：除了解决率，本文首次在SWE-Bench相关工作中系统性地引入了**成本效益分析**（如表4），计算了每个实例的平均成本和每个成功修复的成本，为实际部署提供了重要参考。",
    "methodology_and_formulas": "#### §1 完整算法流程（伪代码级描述）\n本文的方法论是一个系统的数据审计与重建流程，而非一个可执行的算法。其核心步骤如下：\n**Step 1: 原始数据问题识别**\n1.  选择评估对象：选取在SWE-Bench排行榜上领先的开源模型 **SWE-Agent+GPT-4**。\n2.  获取“成功”实例：从SWE-Bench Full的评估结果文件（`results.json`）中，筛选出被标记为“已解决”且评估日志确认**所有测试（PASS_TO_PASS和FAIL_TO_PASS）均已通过**的实例。共得到251个实例。\n3.  人工补丁验证：三位作者独立对比每个“成功”实例的**模型生成补丁**与**开发者提交的黄金补丁**。对比维度包括：修改的文件、修改的函数、修改的代码行、实现的逻辑。\n4.  模式分类与统计：根据对比结果，将每个实例归类到六种模式之一（见表1），并计算各类别的百分比。\n5.  重新计算解决率：仅保留“正确修复”类别的实例（“不同但正确的修复”和“更全面的修复”），计算新的解决率。\n\n**Step 2: SWE-Bench+数据集构建**\n1.  项目选择：沿用SWE-Bench的12个Python项目，排除Django（因其issue跟踪已移至GitHub之外）。\n2.  时间过滤：使用开源脚本，收集在**2023-11-01至2024-08-22期间**创建并关闭的GitHub issue。\n3.  属性与执行过滤：应用SWE-Bench原有的过滤流程，保留那些“解决问题并包含测试”的issue，并确保其PR能成功安装并通过所有测试。\n4.  人工解决方案泄露审查：对过滤后的所有issue进行人工审查，移除任何在描述或评论中包含明确解决方案的issue。\n5.  最终数据集：得到包含**548个任务实例**的SWE-Bench+数据集。\n\n**Step 3: 在新数据集上的评估**\n1.  模型选择：基于性能、开源性和上下文窗口，选择 **SWE-RAG+GPT-4, SWE-RAG+GPT-3.5, SWE-Agent+GPT-4, AutoCodeRover+GPT-4o**。\n2.  运行模型：按照各模型官方指南，在SWE-Bench+的548个实例上运行，生成补丁。\n3.  自动化测试：使用SWE-Bench官方评估脚本，对生成的每个补丁进行测试，记录通过/失败状态。\n4.  人工验证（再次）：对所有被标记为“通过”的实例，重复Step 1中的补丁验证流程，区分“正确修复”和“可疑修复”。\n5.  性能与成本计算：基于人工验证后的“正确修复”数量计算解决率，并统计各模型的总耗时和API调用成本。\n\n#### §2 关键超参数与配置\n- **数据收集时间窗口**：2023-11-01 至 2024-08-22。选择理由：在最新模型（GPT-4o）训练截止日期（2023-10）之后一个月开始，以最大程度避免数据泄露。\n- **评估模型及其版本**：\n  - SWE-Agent + GPT-4 (1106)\n  - SWE-RAG + GPT-4 (1106)\n  - SWE-RAG + GPT-3.5 (turbo-16k-0613)\n  - AutoCodeRover (v20240620) + GPT-4o (2024-05-13)\n- **选择理由**：SWE-Agent是研究时开源模型中的榜首；SWE-RAG模型因其较高的token限制而被选中（对比Claude模型）；AutoCodeRover是近期进入前三的开源模型。\n- **人工审核者数量**：3位作者。设计理由：通过多人独立审查并讨论解决分歧，提高分类的一致性和可靠性。\n\n#### §3 训练/微调设置（如有）\n本文不涉及模型训练或微调，仅使用现成的、预训练好的LLM（GPT-3.5, GPT-4, GPT-4o）及其对应的代理框架（SWE-Agent, SWE-RAG, AutoCodeRover）进行推理评估。\n\n#### §4 推理阶段的工程细节\n- **评估环境**：遵循SWE-Bench官方评估设置，在隔离环境中运行测试以确保结果可复现。\n- **补丁应用与测试**：使用SWE-Bench提供的脚本将模型生成的diff补丁应用到代码库中，然后运行项目特定的测试套件。\n- **成本与时间测量**：\n  - **时间**：记录每个实例从开始到结束的总耗时（包括模型推理、文件操作、测试运行）。SWE-Agent+GPT-4平均每个实例约4分钟，AutoCodeRover+GPT-4o平均4.5分钟。\n  - **成本**：基于OpenAI API的定价计算总成本。平均成本按总成本除以548个实例计算；有效成本（effectiveness-aware cost）按总成本除以正确解决的实例数计算。\n- **Token限制处理**：在初步试验中，由于Claude模型的token限制，在完成所有548个实例评估前就已超出限制，因此未将其纳入最终评估。这体现了工程部署中上下文长度对可行性的约束。",
    "experimental_design": "#### §1 数据集详情\n1.  **SWE-Bench Full (原始数据集)**\n    - **名称**：SWE-Bench\n    - **规模**：2,294个真实世界的GitHub issue及其对应的Pull Request。\n    - **领域类型**：来自12个广泛使用的Python开源项目（如Django, Matplotlib, SymPy等）。\n    - **问题类型**：包括bug报告和新功能请求。\n    - **特殊过滤**：原文未详细说明其原始过滤流程，但引用了原SWE-Bench论文的方法。\n2.  **SWE-Bench Lite**\n    - **名称**：SWE-Bench Lite\n    - **规模**：300个issue，专注于bug修复。\n    - **领域类型**：同SWE-Bench Full，来自相同项目子集。\n    - **设计目标**：降低评估成本，提高可访问性。\n3.  **SWE-Bench Verified**\n    - **名称**：SWE-Bench Verified\n    - **规模**：500个issue。\n    - **领域类型**：同SWE-Bench Full。\n    - **设计目标**：通过人工筛选，确保issue描述清晰、测试用例强（不会拒绝有效解决方案）。\n4.  **SWE-Bench+ (本文构建)**\n    - **名称**：SWE-Bench+\n    - **规模**：548个issue。\n    - **来源项目**：11个Python项目（排除Django）。\n    - **时间范围**：所有issue创建于2023年11月1日至2024年8月22日之间。\n    - **关键过滤标准**：\n        (1) 沿用SWE-Bench的属性与执行过滤。\n        (2) **人工移除所有在issue描述或评论中包含明确解决方案的实例**。\n        (3) **所有issue创建于所用LLM（GPT-3.5/4/4o）的训练截止日期之后**。\n\n#### §2 评估指标体系\n本文使用两级评估体系：\n1.  **一级指标：自动化测试通过率（Resolution Rate）**\n    - **定义**：模型生成的补丁通过项目所有相关测试（包括PASS_TO_PASS和FAIL_TO_PASS）的实例数占总实例数的百分比。\n    - **计算方式**：`(通过所有测试的实例数) / (总实例数) * 100%`。这是SWE-Bench官方和排行榜使用的指标。\n2.  **二级指标：人工验证后的正确解决率（Correct Resolution Rate）**\n    - **定义**：在通过自动化测试的实例中，经人工验证属于“正确修复”（即“不同但正确的修复”或“更全面的修复”）的实例数占总实例数的百分比。\n    - **计算方式**：`(经人工验证为正确修复的实例数) / (总实例数) * 100%`。这是本文引入的核心指标，用于剔除“可疑修复”。\n3.  **效率与成本指标**\n    - **平均时间每实例**：处理单个issue所需的平均时间（秒/分钟）。\n    - **平均成本每实例**：处理单个issue所需的平均经济成本（美元）。计算方式：总API成本 / 548。\n    - **有效成本每修复**：每成功修复一个issue所需的平均经济成本（美元）。计算方式：总API成本 / 正确解决的实例数。此指标更能反映模型的成本效益。\n\n#### §3 对比基线（完整枚举）\n本文在SWE-Bench+上评估了以下四个基线模型/系统，它们代表了不同的代码修复代理架构：\n1.  **SWE-Agent + GPT-4**：\n    - **类型**：基于LLM的软件工程代理（Agent）。\n    - **底座模型**：GPT-4 (1106)。\n    - **代表性**：在研究进行时，是SWE-Bench官方排行榜上**性能最高的开源模型**。它允许LLM通过执行shell命令与代码库进行交互。\n2.  **SWE-RAG + GPT-4**：\n    - **类型**：检索增强生成（RAG）系统。\n    - **底座模型**：GPT-4 (1106)。\n    - **代表性**：利用检索技术从代码库中获取相关上下文，辅助LLM生成补丁。因其较高的token限制而被选入对比。\n3.  **SWE-RAG + GPT-3.5**：\n    - **类型**：检索增强生成（RAG）系统。\n    - **底座模型**：GPT-3.5 Turbo (16k, 0613)。\n    - **代表性**：与SWE-RAG+GPT-4架构相同，但使用更便宜、能力稍弱的底座模型，用于对比模型能力对性能的影响。\n4.  **AutoCodeRover + GPT-4o**：\n    - **类型**：自主程序改进代理。\n    - **底座模型**：GPT-4o (2024-05-13)。\n    - **代表性**：在本文实验期间，是SWE-Bench排行榜上**排名前三的开源模型**，代表了较新的、性能可能更强的代理方法。\n\n#### §4 实验控制变量与消融设计\n本文的核心“消融实验”体现在对数据集的层层过滤上，以量化每个数据质量问题对性能评估的影响：\n1.  **控制变量**：固定使用SWE-Agent+GPT-4模型。\n2.  **实验组设计**：\n    - **原始性能**：在原始SWE-Bench Full上报告的解决率（12.47%）。\n    - **移除可疑修复后**：在SWE-Bench Full上，人工移除所有“可疑修复”（解决方案泄露、错误修复、不完整修复、修改错误文件）后的正确解决率（5.49%）。这量化了**弱测试和解决方案泄露共同导致的高估**。\n    - **进一步移除解决方案泄露后**：在构建SWE-Bench+时，首先通过时间过滤和人工审查移除了解决方案泄露问题。在此数据集上，SWE-Agent+GPT-4的解决率进一步降至**0.55%**。这隔离了**解决方案泄露**的影响。\n    - **最终正确解决率**：在SWE-Bench+上，通过人工验证后得到的正确解决率（与上一点相同，因为SWE-Bench+已无解决方案泄露，但弱测试问题依然存在，模型仍可能因弱测试而“侥幸”通过，但人工验证会将其剔除）。\n通过这一系列步骤，清晰地展示了从原始数据到纯净数据过程中，模型性能的衰减路径，并分别指出了解决方案泄露和弱测试问题的具体影响。",
    "core_results": "#### §1 主实验结果全景\n**表1：SWE-Agent+GPT-4在SWE-Bench Full上“成功”修复的模式分析（总计251个实例）**\n| 类型 | 具体模式 | 数量（百分比） | 根本原因 |\n| :--- | :--- | :--- | :--- |\n| **可疑修复** | 解决方案泄露 (Solution Leak) | 82 (32.67%) | 数据污染 |\n| | 错误修复 (Incorrect Fixes) | 32 (12.75%) | 弱测试用例 |\n| | 修改了不同文件/函数 | 9 (3.59%) | 弱测试用例 |\n| | 不完整修复 (Incomplete Fixes) | 37 (14.74%) | 弱测试用例 |\n| **正确修复** | 与黄金补丁不同但正确 | 76 (30.27%) | - |\n| | 比黄金补丁更全面 | 15 (5.98%) | - |\n\n**表2：SWE-Agent+GPT-4在SWE-Bench变体上的可疑修复比例**\n| 数据集 | 总通过数 | 解决方案泄露比例 | 弱测试问题比例（错误+不完整） | 总可疑修复比例 | 原始解决率 | 修正后解决率（仅正确修复） |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| SWE-Bench Lite | 54 | 33.33% (18/54) | 14.82% (8/54) | 48.15% | 18.00% | **9.33%** (下降48.2%) |\n| SWE-Bench Verified | 112 | 33.04% (37/112) | 22.32% (25/112) | 55.36% | 22.40% | **10.00%** (下降55.4%) |\n| SWE-Bench Full | 251 | 32.67% (82/251) | 31.08% (78/251) | 63.75% | 12.47% | **5.49%** (下降56.0%) |\n\n**表3：各模型在SWE-Bench+上的表现（基于548个实例）**\n| 模型 | 正确修复数 | 可疑修复数（不同文件/函数） | 可疑修复数（错误修复） | 可疑修复数（不完整修复） | 总通过数（自动化测试） | **人工验证后正确解决率** | **对比原始SWE-Bench解决率（下降幅度）** |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| SWE-RAG + GPT-4 | 4 | 16 | 2 | 1 | 23 | **0.73%** (4/548) | 从 1.31% 下降至 0.73% (下降44.3%) |\n| SWE-RAG + GPT-3.5 | 3 | 11 | 6 | 0 | 20 | **0.55%** (3/548) | 从 0.17%* 上升至 0.55% (上升223.5%)* |\n| SWE-Agent + GPT-4 | 3 | 14 | 3 | 0 | 20 | **0.55%** (3/548) | 从 12.47% 下降至 0.55% (下降95.6%) |\n| AutoCodeRover + GPT-4o | 21 | 7 | 15 | 3 | 46 | **3.83%** (21/548) | 从 18.83% 下降至 3.83% (下降79.7%) |\n*注：SWE-RAG+GPT-3.5在原始SWE-Bench上的解决率极低（0.17%），在SWE-Bench+上反而有所上升，可能因为其原始性能基数低，且新数据集的分布差异带来了偶然性提升。*\n\n**表4：各模型在SWE-Bench+上的效率与成本**\n| 模型 | 平均每实例成本 | 每成功修复成本（有效成本） | 平均每实例时间 |\n| :--- | :--- | :--- | :--- |\n| SWE-RAG + GPT-4 | $0.24 | $32.5 | 30秒 |\n| SWE-RAG + GPT-3.5 | $0.05 | $10.0 | 30秒 |\n| SWE-Agent + GPT-4 | $3.59 | $655.0 | 4分钟 |\n| AutoCodeRover + GPT-4o | $0.46 | $12.61 | 4.5分钟 |\n\n#### §2 分任务/分场景深度分析\n1.  **解决方案泄露的影响**：这是导致性能高估的**最主要因素**。在SWE-Bench Full、Lite、Verified三个数据集中，解决方案泄露的比例稳定在**32%-33%** 之间。这意味着近三分之一的“成功”实例是模型直接抄袭了问题描述中的答案，并未展示任何问题解决能力。移除这部分实例后，SWE-Agent+GPT-4在SWE-Bench Full上的解决率从12.47%降至8.43%（仅移除泄露部分）。\n2.  **弱测试用例的影响**：这是第二大类问题，在SWE-Bench Full中占**31.08%**。它导致模型即使生成错误、不完整或针对错误位置的补丁也能通过测试。**错误修复**和**不完整修复**是主要表现形式，表明测试用例未能充分覆盖功能需求或边界条件。**修改不同文件/函数**虽然比例较低（3.59%），但暴露出测试套件可能完全遗漏了对关键文件的验证。\n3.  **在纯净数据集（SWE-Bench+）上的表现**：当同时移除解决方案泄露和使用更新数据后，所有模型的解决率均**急剧下降**。表现最好的AutoCodeRover+GPT-4o也仅有**3.83%** 的正确解决率。这强烈表明，当前最先进的代码修复代理在面对**真实的、未见过的、无提示的**软件工程问题时，其实际能力被严重高估了1-2个数量级。\n4.  **模型间对比**：在SWE-Bench+上，**AutoCodeRover+GPT-4o**以3.83%的正确解决率显著优于其他模型（其他均低于1%）。这可能得益于其更复杂的自主探索策略。**SWE-Agent+GPT-4**的成本效益极差，平均每个实例成本高达$3.59，且每成功修复一个问题的成本高达$655，是其他模型的20-60倍。\n\n#### §3 效率与开销的定量对比\n- **时间开销**：基于代理的模型（SWE-Agent, AutoCodeRover）耗时远高于基于RAG的模型。SWE-Agent+GPT-4处理单个实例平均需**4分钟**，总耗时约37小时；AutoCodeRover+GPT-4o平均需**4.5分钟**，总耗时约41小时。而SWE-RAG系列模型仅需**30秒**。\n- **经济成本**：\n    - **平均成本**：SWE-Agent+GPT-4 ($3.59/实例) > AutoCodeRover+GPT-4o ($0.46/实例) > SWE-RAG+GPT-4 ($0.24/实例) > SWE-RAG+GPT-3.5 ($0.05/实例)。\n    - **有效成本（每成功修复）**：此指标更能反映实用价值。SWE-Agent+GPT-4的$655/修复高得惊人，意味着其成本完全不可行。AutoCodeRover+GPT-4o的$12.61/修复虽然仍高，但因其成功率相对最高，显得更具成本效益。SWE-RAG+GPT-3.5的$10.0/修复成本最低，但其成功率也最低（0.55%）。\n\n#### §4 消融实验结果详解\n本文的消融实验体现在对数据质量的逐层剥离上：\n1.  **移除所有可疑修复的影响**：在SWE-Bench Full上，移除解决方案泄露和弱测试导致的“可疑修复”后，SWE-Agent+GPT-4的解决率从**12.47%** 暴跌至**5.49%**，绝对下降6.98个百分点，相对下降**56.0%**。这量化了数据质量问题对性能评估的整体扭曲程度。\n2.  **仅移除解决方案泄露的影响（通过构建SWE-Bench+实现）**：在SWE-Bench+上，解决了泄露问题后，SWE-Agent+GPT-4的解决率进一步从**3.97%**（这是在原始SWE-Bench上移除可疑修复后的“纯净”解决率估计值？原文此处有歧义，图1b显示从12.47%->3.97%）下降至**0.55%**。这单独揭示了**使用训练截止日期后的新数据**带来的额外挑战，可能由于数据分布变化或问题难度增加。\n3.  **弱测试问题的持续性**：即使在SWE-Bench+上，模型生成的补丁中仍有大量（平均67.72%）属于“可疑修复”（错误、不完整、改错文件），并能通过自动化测试。这证明**弱测试问题并未因数据集的更新而解决**，它仍然是评估基准的一个根本性缺陷。\n\n#### §5 案例分析/定性分析\n1.  **成功案例（正确但不同的修复）**：模型生成了与开发者黄金补丁逻辑不同但功能等价的解决方案。这展示了LLMs在代码修复上的创造性潜力，能够提供替代的、可行的实现。\n2.  **成功案例（更全面的修复）**：模型生成的补丁比开发者补丁考虑了更多的边缘情况或添加了更完善的错误处理（例如添加了额外的try-catch块）。这展示了LLMs可能生成**更健壮**的代码。\n3.  **失败案例（解决方案泄露）**：如图3所示，在sympy-16669问题中，issue描述直接给出了需要添加的代码行 `return self.intersection(other).is_subset(self)`。模型只需复制粘贴即可生成完全一致的补丁，这不能证明任何推理能力。\n4.  **失败案例（错误修复通过弱测试）**：如图4所示，在django-32517中，需要为OrderedSet实现`__reversed__`方法。黄金补丁正确地反转了整个字典（`return reversed(self.dict)`），而模型补丁错误地只反转了字典的键（`return reversed(self.dict.keys())`）。由于测试用例未能区分这两种行为，错误补丁得以通过。\n5.  **失败案例（修改错误文件）**：如图5所示，在matplotlib issue-26093中，bug实际存在于`_axes.py`文件中，但模型却修改了`cbook.py`文件。测试仍然通过，说明测试并未充分验证`_axes.py`的正确行为，或者错误地依赖了`cbook.py`的某个间接效应。",
    "conclusion_and_future_work": "#### §1 本文核心贡献总结\n1.  **首次对SWE-Bench数据集进行了系统的鲁棒性分析**：通过人工审计SWE-Agent+GPT-4在SWE-Bench Full上所有251个“成功”实例，定量揭示了数据集存在的两大核心缺陷——**解决方案泄露（32.67%）** 和**弱测试用例（31.08%）**，并证明这些缺陷导致模型解决率被严重高估（从12.47%降至5.49%）。\n2.  **构建了更严格的评估基准SWE-Bench+**：通过（a）收集LLM训练截止日期（2023年10月）之后的数据，（b）人工移除所有包含解决方案泄露的issue，创建了一个包含548个实例的新数据集。该数据集基本消除了数据泄露风险，为评估LLM的真实泛化能力提供了更可靠的基础。\n3.  **提供了基于SWE-Bench+的基准性能与成本分析**：在纯净数据集上评估了四个主流模型，结果显示所有模型的性能均大幅下降（最好的AutoCodeRover+GPT-4o仅为3.83%），并首次提供了详细的**时间与API成本分析**，揭示了当前方法在成本效益上的巨大差异（如SWE-Agent每成功修复成本高达$655）。\n4.  **揭示了弱测试问题的普遍性与顽固性**：即使在SWE-Bench+上，**平均67.72%** 的“通过”补丁经人工验证仍为错误、不完整或针对错误文件，表明弱测试问题是评估基准中一个尚未解决的根本性挑战。\n\n#### §2 局限性（作者自述）\n1.  **数据集规模与项目范围有限**：SWE-Bench+仅包含**548个实例**，且源自**11个Python项目**（排除了Django）。其规模和项目多样性可能不足以全面代表所有软件工程任务。\n2.  **人工审核的主观性**：对“解决方案泄露”和“补丁正确性”的判断依赖于三位作者的人工审查，尽管通过协商解决分歧，但仍存在一定的主观性。\n3.  **未能完全解决弱测试问题**：SWE-Bench+虽然解决了数据泄露，但**未能改善测试用例的质量**。弱测试问题依然存在，导致评估结果可能仍然包含“虚假成功”。\n4.  **模型评估范围有限**：只评估了四个基于GPT系列模型的系统，未涵盖其他架构（如Claude）或更广泛的LLM。\n\n#### §3 未来研究方向（全量提取）\n1.  **提升测试用例质量**：未来工作应专注于**改进SWE-Bench+中测试套件的鲁棒性**。这可以通过自动化生成更多、更强的测试用例（例如，借鉴EvalPlus框架的突变测试策略），或引入人工验证来确保测试能够准确捕捉功能需求。\n2.  **探究高失败率的根本原因**：需要深入研究导致模型在SWE-Bench+上失败率极高的**根本原因**。是问题描述不够清晰？代码库上下文太长？还是模型的代码理解、定位和生成能力存在本质缺陷？并提出相应的缓解策略。\n3.  **将分析方法扩展到其他基准**：建议在**其他领先的代码生成基准（如HumanEval）** 上进行类似的鲁棒性分析，以比较结果并识别更广泛的模式，判断SWE-Bench中发现的问题是特例还是普遍现象。\n4.  **探索更有效的数据过滤策略**：研究自动化或半自动化的方法来识别和过滤存在“解决方案泄露”和“弱测试”的issue，以降低构建高质量基准的人工成本。",
    "research_contributions": "#### §1 核心学术贡献（按重要性排序）\n1.  **基准诊断方法论贡献**：本文提出并实践了一套**系统性的基准数据集审计方法论**——即对模型在基准上的“成功”案例进行人工驱动的、细粒度的补丁验证与分类。这种方法论超越了简单的自动化测试通过率，为评估代码生成和修复任务的**真实性**设立了新的标准。其影响在于，它可能促使整个领域重新审视现有基准的可靠性，并推动建立更严格的评估协议。\n2.  **数据集贡献**：发布了**SWE-Bench+数据集**，这是一个在**时间新鲜度**和**内容纯净度**上显著优于原版SWE-Bench及其变体的新基准。它通过切断数据泄露路径，迫使模型必须依靠真正的推理而非记忆来解决问题，从而为评估LLM在软件工程任务上的**零样本泛化能力**提供了更可靠的试金石。其实验验证充分性体现在对四个主流模型在该数据集上的全面评估及性能的断崖式下跌。\n3.  **实证发现贡献**：首次定量揭示了SWE-Bench家族数据集中普遍存在的**解决方案泄露（~33%）** 和**弱测试用例（~31%）** 问题，并证明这些问题导致模型性能被高估了**56%**（对SWE-Agent+GPT-4而言）。这一发现对依赖SWE-Bench进行模型比较和宣称SOTA的研究提出了严峻挑战，可能引发该领域评估范式的转变。\n4.  **多维评估视角贡献**：除了准确率，本文率先在代码修复任务评估中系统性地引入了**成本效益分析**（时间成本、API经济成本），指出SWE-Agent+GPT-4每成功修复一个问题的成本高达$655。这强调了在追求高性能的同时，**部署可行性**和**资源消耗**是同等重要的评估维度，为工业界应用提供了关键参考。\n\n#### §2 工程与实践贡献\n1.  **开源数据集**：作者已将SWE-Bench+数据集公开，供其他研究者复现和扩展本研究。这为社区提供了一个即用的、更可靠的评估基准。\n2.  **可复现的审计流程**：论文详细描述了从数据筛选、人工审核到补丁验证的完整流程，为其他研究者对类似基准（或自己构建的基准）进行鲁棒性分析提供了可操作的蓝图。\n3.  **详细的性能与成本基准**：提供了多个主流模型在SWE-Bench+上的详细性能（解决率）和资源消耗（时间、金钱）数据，为后续研究提供了直接的对比基线。\n\n#### §3 与相关工作的定位\n本文工作在当前LLM用于软件工程的研究路线图中，处于**基准评估与方法论批判**的关键位置。它并非沿着“开发更强代理”的技术路线前进，而是对支撑该路线的**评估基础设施**进行了根本性的反思和加固。可以看作是继EvalPlus（针对HumanEval的测试增强）之后，在更复杂的真实世界软件工程任务基准上的一次重要“纠偏”工作。它指出，在追求更高分数之前，必须首先确保我们衡量的尺子是准确的。因此，本文是**评估可靠性研究**这条支线上的一个重要进展，为未来开发更鲁棒的模型和基准奠定了更坚实的基础。",
    "professor_critique": "#### §1 实验设计与评估体系的缺陷\n1.  **评估指标单一且仍有缺陷**：本文的核心评估指标仍是“测试通过率”，尽管引入了人工验证来修正。然而，**人工验证本身并未解决“弱测试”问题**，它只是事后识别出因弱测试而错误通过的案例。一个更根本的解决方案是像EvalPlus那样，为每个任务生成更强大的测试套件。本文指出了问题，但未在SWE-Bench+中提供改进的测试用例。\n2.  **基线模型选择可能不够全面**：评估仅基于GPT系列模型（3.5, 4, 4o）和几个特定的代理框架（SWE-Agent, SWE-RAG, AutoCodeRover）。**未包含当时排行榜上可能更强的闭源模型（如Honeycomb、Amazon Q Developer Agent）**，也未评估其他有潜力的开源模型（如基于Claude的代理）。这使得性能对比的视野受限，无法断言SWE-Bench+上3.83%的SOTA是当前技术的天花板。\n3.  **“正确修复”的定义可能存在灰色地带**：将“与黄金补丁不同但正确”和“更全面的修复”都算作正确，这固然合理。但在人工判断时，如何界定“功能等价”或“更全面”可能存在分歧。论文未提供详细的判断准则或解决分歧的量化标准（如Kappa系数），降低了实验的可复现性。\n\n#### §2 方法论的理论漏洞或工程局限\n1.  **时间过滤引入的潜在偏差**：SWE-Bench+仅收集2023年11月后的数据。这虽然避免了数据泄露，但**可能引入了“时效性偏差”**。新近的issue可能具有不同的特征（如涉及更新的库API、更复杂的代码结构），其难度分布可能与原始SWE-Bench不同。性能下降部分可归因于数据泄露的消除，部分也可能源于问题本身变得更难。两者效应未能分离。\n2.  **人工审核的不可扩展性**：构建SWE-Bench+的关键步骤——人工移除解决方案泄露——需要大量人力，且**难以扩展到更大规模的数据集构建**。这限制了SWE-Bench+的规模（仅548个实例），也为其后续维护和扩展带来了挑战。\n3.  **对“弱测试”问题只诊断不治疗**：本文精彩地诊断了弱测试问题，并指出其在SWE-Bench+中依然存在（67.72%的通过补丁是可疑的）。然而，作者并未尝试修复或增强这些测试用例。这使得SWE-Bench+作为一个基准，其**评估信号中仍然包含大量噪声**，未来研究者仍可能被虚假的高通过率所误导。\n\n#### §3 未经验证的边界场景\n1.  **多语言与跨项目泛化**：所有实验均基于Python项目。当模型需要处理**多语言混合的代码库**（如一个项目包含Python、JavaScript、C++）或**全新编程语言**的issue时，本文揭示的模型缺陷（如定位错误文件）可能会被急剧放大。\n2.  **超大规模代码库下的检索与定位**：SWE-Bench中的项目规模相对适中。当代理需要在一个**拥有数百万行代码、数万个文件**的企业级代码库中定位bug时，当前基于RAG或简单文件遍历的策略的检索精度和效率可能会崩溃，导致成功率接近零。\n3.  **对抗性输入与模糊描述**：本文未测试模型对**恶意构造或高度模糊的issue描述**的鲁棒性。在真实场景中，用户反馈可能不完整、有歧义甚至包含错误信息。模型能否通过交互澄清需求？还是会在错误的前提下生成无效补丁？\n4.  **并发修改与合并冲突**：现实中的软件修复往往在活跃开发分支上进行。本文的评估假设代码库处于一个固定的、孤立的状态。未测试模型在**存在其他未合并的并行修改**时，能否生成可合并的、不产生冲突的补丁。\n\n#### §4 可复现性与公平性问题\n1.  **高昂的复现成本**：复现本研究需要**调用GPT-4/4o的API**，在548个实例上运行多个模型，总成本不菲（仅SWE-Agent+GPT-4一项就需约$1968）。这为资源有限的研究者设置了门槛。\n2.  **对闭源商业模型的依赖**：所有评估的模型都建立在闭源的GPT系列API之上。其内部更新（如模型版本迭代、定价变化）可能影响结果的稳定性，且无法进行彻底的消融研究（如改变模型架构）。\n3.  **超参数与提示词的一致性**：论文使用了各代理框架的默认配置和提示词。但不同框架的提示词工程水平不同，这可能对结果产生重大影响。评估时**未对基线模型进行统一的提示词优化或超参数调整**，可能导致某些模型因其默认配置不佳而处于劣势。\n4.  **人工审核的主观性风险**：尽管有三位作者，但补丁验证是一个高度依赖专业知识的任务。未报告评审者间的一致性分数（如Fleiss‘ Kappa），使得“可疑修复”与“正确修复”的划分可能受到质疑。",
    "zero_compute_opportunity": "#### 蓝图一：基于轻量级LLM的SWE-Bench+自动解决方案泄露检测器\n- **核心假设**：利用小型、开源的代码理解模型（如CodeLlama 7B/13B）或经过微调的模型，可以自动化地检测GitHub issue描述和评论中是否包含可直接用于生成补丁的代码解决方案，准确率达到可接受水平（>80%），从而大幅降低构建纯净基准的人工成本。\n- **与本文的关联**：直接针对本文构建SWE-Bench+时**人工审核解决方案泄露成本高昂、不可扩展**的痛点。\n- **所需资源**：\n  1.  **模型**：Hugging Face上免费的CodeLlama-7B/13B-Instruct模型。\n  2.  **数据**：本文提供的SWE-Bench+中548个已被标记为“无泄露”的issue作为负样本；从原始SWE-Bench中人工标注或利用本文表1中的82个“解决方案泄露”实例作为正样本。可进一步爬取GitHub构造更多样本。\n  3.  **计算**：Google Colab免费GPU（T4）即可进行微调和推理。\n  4.  **费用**：几乎为零（若仅推理，Colab免费额度足够）。\n- **执行步骤**：\n  1.  **数据构建**：将issue文本（标题+描述+评论）与对应的黄金补丁（diff）配对。正样本标签为1（有泄露），负样本为0。将补丁简化表示为“修改了X文件的Y函数，添加/删除了Z代码”。\n  2.  **模型微调**：将CodeLlama-7B-Instruct在构建的数据集上微调为一个二分类任务，输入为issue文本，输出为“是否包含解决方案”。提示词设计为：“Given the following GitHub issue description",
    "应对**：采用集成方法，结合基于规则的关键词匹配（如“patch": "",
    "diff:": "here is the fix",
    "source_file": "SWE-Bench+ Enhanced Coding Benchmark for LLMs.md"
}