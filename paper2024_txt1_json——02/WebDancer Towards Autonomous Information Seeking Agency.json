{
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n本研究聚焦于**信息寻求智能体（Information Seeking Agent）**领域，旨在构建能够自主进行多步网络探索以解决复杂问题的智能系统。该领域的发展脉络始于利用提示工程（Prompt Engineering）引导大型语言模型（LLM）或大型推理模型（LRM）执行任务，随后演进为通过监督微调（SFT）或强化学习（RL）将搜索/浏览能力内化到智能体中。研究动机在于，当前诸如Deep Research、Grok DeepSearch等闭源系统已展示了通过端到端RL训练实现深度信息寻求的潜力，但开源社区尚缺乏一个**系统化、可复现的构建指南**。本文旨在提供一个从零开始构建此类智能体的完整方法论，其核心应用场景是**需要多轮、多工具交互的深度信息检索任务**，例如回答需要跨多个网页、执行复杂推理链才能解决的问题。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法主要分为两类，均存在具体失败模式：\n1.  **基于提示工程的无训练方法**：直接引导LLM/LRM执行任务。当面对需要长序列推理和精确工具调用的复杂任务时，该方法无法有效利用推理模型的能力，导致**任务分解失败**和**工具调用序列错误**。例如，在需要超过5步交互的GAIA Level 3任务上，仅使用提示的基线方法（如GPT-4o Base）准确率仅为8.3%。\n2.  **基于SFT或RL的训练方法**：虽然内化了部分信息寻求能力，但其训练和评估数据集相对简单，未能捕捉真实世界的复杂性。具体失败模式包括：\n    *   **数据集浅薄**：现有数据集（如2Wiki）问题通常只需2-3步即可解决，导致模型在需要**长视野（long-horizon）推理**的任务上泛化能力差。例如，在2Wiki数据集上性能已超过80%，但无法应对更复杂的场景。\n    *   **训练范式低效**：当前的SFT或RL训练范式未能充分利用信息寻求行为的潜力。例如，在多工具设置中，早期RL训练步骤往往只专注于学习工具使用，导致**探索效率低下**和**稀疏奖励问题**。\n    *   **数据规模不足**：挑战性的网络QA数据集（如GAIA仅466条，WebWalkerQA仅680条）规模过小，不足以进行有效训练，导致模型容易**过拟合**到有限的模式上。\n\n**§3 问题的根本难点与挑战（200字以上）**\n构建自主信息寻求智能体面临的根本挑战源于其**动态、开放、长序列**的特性：\n1.  **高质量数据获取难**：真实世界的网络信息寻求涉及多样化的用户意图和丰富的交互上下文。自动合成既能体现任务复杂性（如多跳推理），又能保证答案正确性和逻辑连贯性的QA对及轨迹数据，在技术上非常困难。\n2.  **长序列决策优化难**：智能体需要在多轮（Thought-Action-Observation）交互中做出连续决策，这带来了**信用分配（Credit Assignment）** 难题。在RL训练中，最终答案的正确与否这一稀疏奖励信号难以有效回传到序列中早期的关键决策步骤。\n3.  **环境非平稳性**：网络环境是动态演化的，网页内容可能随时变化。这导致智能体的部署环境是**非平稳（Non-stationary）** 的，要求智能体具备强大的**领域外（Out-of-Distribution）泛化**和**上下文适应**能力，而不仅仅是拟合静态的数据分布。\n4.  **推理模式迁移难**：强推理模型（LRM）的思维模式（如长链思维链）难以直接迁移到较小的指令微调模型上，存在**知识迁移的脆弱性**，这限制了利用强模型为弱模型生成高质量监督信号的效果。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**从数据中心（Data-centric）和训练阶段（Training-stage）两个视角，系统化地抽象出构建端到端网络智能体的四阶段管道**。其核心假设是：通过精心设计的**数据合成**与**轨迹采样**方法，可以生成大规模、高质量、具有挑战性的训练数据；进而，通过**两阶段训练策略（SFT冷启动 + RL泛化优化）**，能够有效地将复杂的信息寻求与多步推理能力内化到模型中。\n具体而言，本文假设：\n1.  **数据假设**：通过模拟人类浏览行为（CRAWLQA）和从易到难的QA对演化（E2HQA），可以自动合成出能够激发长视野推理和丰富交互的QA数据。\n2.  **轨迹假设**：结合使用强指令LLM（生成Short-CoT轨迹）和强推理LRM（生成Long-CoT轨迹）进行拒绝采样，可以获得高质量、多样化的决策轨迹，为SFT提供有效的监督信号。\n3.  **训练假设**：先进行SFT（学习ReAct格式和基础工具使用）再进行RL（优化决策以获得最终答案奖励）的两阶段策略，比单一的SFT或RL训练更有效。SFT为RL提供了良好的**冷启动（Cold Start）**，避免了RL早期探索的无效性；而RL则能进一步**提升泛化能力和决策一致性**。该假设基于强化学习中利用预训练策略进行初始化的常见实践，并针对智能体任务的特殊性进行了定制。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\nWebDancer的整体架构遵循ReAct范式，其构建管道抽象为四个关键阶段，数据流如下：\n**输入用户问题Q** → **阶段I：QA对合成**（生成复杂QA数据CRAWLQA/E2HQA）→ **阶段II：轨迹拒绝采样**（使用GPT-4o/QwQ-Plus生成Short/Long-CoT轨迹，并经过三层过滤）→ **阶段III：智能体监督微调（SFT）**（使用格式化轨迹训练策略模型π_θ，掩码观察损失）→ **阶段IV：智能体强化学习（RL）**（使用DAPO算法，在未用于SFT的QA对上 rollout，优化策略以获得格式和答案奖励）→ **输出最终答案**。\n系统核心是一个基于Transformer的策略模型π_θ，它接收包含历史轨迹的上下文，并依次生成Thought、Action（工具调用）、最终Answer。外部工具环境包括**搜索（search）** 和**访问（visit）** 两种动作，分别返回搜索结果和网页摘要。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：QA对合成模块（CRAWLQA & E2HQA）\n-   **输入**：根URL列表（来自arxiv、github、wiki等知识性网站）或简单的SimpleQA风格QA对。\n-   **核心处理逻辑**：\n    *   **CRAWLQA**：递归爬取根URL的子页面链接，收集网页内容。使用GPT-4o，通过上下文学习（In-context Learning）提示，根据预定义的问题类型（如COUNT, MULTI-HOP, INTERSECTION）合成QA对。\n    *   **E2HQA**：从简单问题Q_n中选取实体E_n，用LLM构造查询搜索相关信息C_n，再用LLM π将C_n重构成新的查询R_n来替换原问题中的E_n，生成更复杂的问题Q_{n+1}。迭代次数n控制问题复杂度。公式：\\(R_n = \\pi(S(C_n))\\)。\n-   **输出**：合成的复杂QA对。\n-   **设计理由**：直接解决现有数据集规模小、难度低的问题。CRAWLQA基于真实网页内容，确保信息真实性；E2HQA通过可控的迭代演化，系统性地增加问题推理步长，构建难度谱系。\n\n#### 模块二：轨迹拒绝采样与过滤模块\n-   **输入**：合成的QA对。\n-   **核心处理逻辑**：\n    1.  **采样**：使用两种策略生成ReAct格式轨迹：(a) **Short-CoT**：直接使用GPT-4o在ReAct框架下生成轨迹。(b) **Long-CoT**：向QwQ-Plus顺序提供历史动作和观察（排除之前的Thought），让其自主决定下一步动作，并将其推理过程`<reasoning_content>`记录为当前Thought。每个QA实例进行最多N次拒绝采样以确保质量。\n    2.  **三层漏斗过滤**：\n        *   **有效性控制**：丢弃不遵循ReAct格式指令的数据。\n        *   **正确性验证**：使用GPT-4o作为评判模型，只保留答案正确的轨迹。\n        *   **质量评估**：先应用规则过滤（动作数>2、无幻觉、无严重重复），再通过提示保留满足**信息非冗余性、目标对齐性、逻辑推理与准确性**三个标准的轨迹。\n-   **输出**：高质量的ReAct格式轨迹，用于SFT；被过滤的QA对保留用于RL阶段。\n-   **设计理由**：利用强模型生成高质量监督信号。分层过滤确保轨迹在格式、答案正确性和推理逻辑上均达到高标准，为SFT提供干净、有效的训练数据。\n\n#### 模块三：两阶段训练模块（SFT + RL）\n-   **输入**：SFT阶段使用格式化轨迹；RL阶段使用未用于SFT的QA对。\n-   **核心处理逻辑**：\n    *   **SFT阶段**：在轨迹数据上训练策略模型π_θ。**关键设计**：在损失计算中掩码（Mask）掉来自外部环境的Observation tokens，只对模型自主生成的Thought和Action计算损失。损失函数为：\n        \\[ L = - \\frac{1}{\\sum_{i=1}^{|\\mathcal{H}|} \\mathbb{I}[x_i \\neq o]} \\sum_{i=1}^{|\\mathcal{H}|} \\mathbb{I}[x_i \\neq o] \\cdot \\log \\pi_{\\theta}(x_i \\mid \\mathbf{tc}, x_{< i}) \\]\n        其中\\(\\mathbb{I}[x_i \\neq o]\\)是指示函数，过滤掉observation。\n    *   **RL阶段**：使用**解耦裁剪与动态采样策略优化（DAPO）**算法。在QA对上使用当前策略进行Rollout，生成候选执行序列{o_i}。优化目标是最大化经过裁剪的重要性采样比与优势函数的乘积之和。动态采样机制会过采样并过滤掉准确率为1和0的提示，专注于学习具有不确定性的样本。奖励函数结合格式奖励和答案奖励：\\(R(\\hat{y}_i, y) = 0.1 * \\text{score}_{\\text{format}} + 0.9 * \\text{score}_{\\text{answer}}\\)。\n-   **输出**：训练好的策略模型π_θ。\n-   **设计理由**：SFT提供冷启动，教会模型ReAct格式和基础工具使用模式。掩码观察损失避免模型学习不可控的外部反馈。RL阶段利用结果奖励进一步优化决策序列，DAPO算法提高数据利用效率和策略鲁棒性。奖励函数以答案正确性为主（权重0.9），格式为辅（权重0.1）。\n\n**§3 关键公式与算法（如有）**\n1.  **SFT损失函数（掩码观察）**：\n    \\[ L = - \\frac{1}{\\sum_{i=1}^{|\\mathcal{H}|} \\mathbb{I}[x_i \\neq o]} \\sum_{i=1}^{|\\mathcal{H}|} \\mathbb{I}[x_i \\neq o] \\cdot \\log \\pi_{\\theta}(x_i \\mid \\mathbf{tc}, x_{< i}) \\]\n2.  **RL目标函数（DAPO）**：\n    \\[ \\begin{array}{l} \\mathcal{J}_{\\mathrm{DAPO}}(\\theta) = \\mathbb{E}_{(q, a) \\sim \\mathcal{D}, \\{o_i\\}_{i=1}^{G} \\sim \\pi_{\\theta_{\\mathrm{old}}}(\\cdot | context)} \\ \\left[ \\frac{1}{\\sum_{i=1}^{G} |o_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|o_i|} \\min \\left(r_{i, t}(\\theta) \\hat{A}_{i, t}, \\operatorname{clip}\\left(r_{i, t}(\\theta), 1-\\varepsilon_{\\text{low}}, 1+\\varepsilon_{\\text{high}}\\right) \\hat{A}_{i, t}\\right) \\right] \\end{array} \\]\n    其中，\\(r_{i, j}(\\theta) = \\frac{\\pi_{\\theta}\\left(o_{i} \\mid q_{i}, o_{i, < t}\\right)}{\\pi_{\\theta_{\\mathrm{old}}}\\left(o_{i} \\mid q_{i}, o_{i, < t}\\right)}\\), \\(\\hat{A}_{i, j} = \\frac{R_{i} - \\operatorname{mean}\\left(\\left\\{R_{i}\\right\\}\\right)}{\\operatorname{std}\\left(\\left\\{R_{i}\\right\\}\\right)}\\)。\n3.  **奖励函数**：\n    \\[ R\\left(\\hat{y}_{i}, y\\right) = 0.1 * \\text{score}_{\\text{format}} + 0.9 * \\text{score}_{\\text{answer}} \\]\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n本文主要变体体现在**轨迹类型**和**训练阶段**：\n1.  **Short-CoT WebDancer**：使用GPT-4o生成的短思维链轨迹进行SFT。适用于Qwen-2.5-7B/32B等非推理模型。\n2.  **Long-CoT WebDancer**：使用QwQ-Plus生成的长思维链轨迹进行SFT。适用于QwQ-32B等推理模型。实验表明，对非推理模型使用Long-CoT会导致更高的无效率（如重复、超长上下文）。\n3.  **仅SFT版本**：仅进行第一阶段监督微调，不进行RL。作为消融实验对比，性能低于完整两阶段训练。\n4.  **仅RL版本（无SFT冷启动）**：直接在基础模型上应用RL。在QwQ模型上的实验显示，仅RL时Pass@3性能极低（仅5%），验证了SFT冷启动的必要性。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n1.  **与纯提示工程方法（如直接使用GPT-4o）**：本质差异在于**训练范式**。本文通过端到端的SFT+RL训练，将信息寻求和工具使用能力**内化**到模型参数中，而提示工程仅依赖模型已有的上下文学习能力。这导致了在复杂任务（如GAIA Level 2/3）上性能的显著差距（WebDancer (QwQ-32B) Avg. 51.5% vs. GPT-4o Base 17.5%）。\n2.  **与现有SFT方法（如WebThinker-Base）**：核心差异在于**数据构建管道**和**训练策略**。本文提出了系统化的QA合成（CRAWLQA/E2HQA）和轨迹过滤流程，旨在生成更大规模、更复杂、更高质量的训练数据。而许多现有工作依赖从开源数据集中过滤或使用相对简单的合成数据。此外，本文采用了**掩码观察的SFT损失**，专注于优化模型自主决策部分。\n3.  **与现有RL方法（如WebThinker-RL, R1-Searcher）**：主要差异在于**RL算法**和**两阶段训练设计**。本文采用DAPO算法，其动态采样机制能更高效地利用数据，特别是过滤后剩余的、可能包含噪声的QA对。同时，本文强调了SFT作为RL**冷启动**的不可或缺性，并提供了实验验证（仅RL效果差）。而一些工作可能直接进行RL或使用不同的RL算法（如PPO）。此外，本文的奖励函数设计（0.9权重给答案正确性）也更侧重于最终任务目标。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n**WebDancer训练管道：**\nStep 1: **数据合成**：通过CRAWLQA（爬取网页+GPT-4o生成QA）和E2HQA（简单QA迭代演化）生成复杂QA对集合D。\nStep 2: **轨迹采样与过滤**：对于D中的每个QA对(q",
    "2.1": "使用GPT-4o（Short-CoT）或QwQ-Plus（Long-CoT）进行最多N次拒绝采样，生成候选轨迹τ。\n    Step 2.2: **三层过滤**：\n        (a) 有效性控制：检查τ是否符合ReAct格式，丢弃无效格式。\n        (b) 正确性验证：使用GPT-4o判断τ的最终答案是否正确，丢弃错误答案。\n        (c) 质量评估：规则过滤（动作数≤2，无幻觉/重复）后，用LLM提示评估信息非冗余、目标对齐、逻辑准确，保留高质量轨迹。\n    Step 2.3: 将通过过滤的轨迹加入SFT数据集D_sft，将被过滤的QA对加入RL数据集D_rl。\nStep 3: **监督微调（SFT）**：使用D_sft训练策略模型π_θ。对于每个轨迹样本，计算损失时掩码掉Observation tokens（公式2）。训练至收敛，得到冷启动策略π_θ_sft。\nStep 4: **强化学习（RL）**：初始化策略π_θ = π_θ_sft。对于D_rl中的每个QA对(q",
    "4.1": "Rollout**：使用当前策略π_θ与环境交互，生成G个候选执行序列{o_i",
    "source_file": "WebDancer Towards Autonomous Information Seeking Agency.md"
}