{
    "title": "Deep Research: A Survey of Autonomous Research Agents",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n本综述论文聚焦于**自主研究智能体（Autonomous Research Agents）**领域，这是一个由大语言模型（LLMs）能力提升所催生的新兴研究方向。随着GPT-4、Qwen3、DeepSeek-R1等模型在通用语言理解与生成任务上取得显著进展，研究者开始探索如何让LLMs超越其固有的静态知识边界，自主完成复杂的、需要外部知识支撑的研究任务，例如撰写分析报告、进行文献综述或事实核查。该研究的核心应用场景是**基于网络证据的深度信息探索与综合报告生成**。其研究动机在于，传统的检索增强生成（RAG）范式虽然能提供外部知识，但其被动、单次检索的模式无法满足复杂研究任务所需的主动规划、迭代探索和多源信息综合推理的需求。因此，需要一种新的范式——**深度研究（Deep Research）**——来构建能够主动规划、探索、检索并生成可信赖分析报告的智能体系统。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有技术主要分为三类，每类在特定场景下存在明确的失败模式：\n1.  **传统RAG系统（如基于向量检索的问答系统）**：当面对需要多步推理、信息分散且需要主动探索的开放研究问题时，RAG系统会失败。其失败模式表现为：**被动检索**，仅能根据初始查询返回静态结果，无法根据中间发现动态调整搜索策略；**缺乏规划**，无法将高层研究目标分解为有序的子任务序列；**浅层综合**，仅能对检索到的片段进行拼接或简单总结，无法生成结构化的、有深度的分析报告。例如，在撰写关于“量子计算对密码学影响”的报告时，RAG可能无法主动规划并依次检索“量子计算原理”、“Shor算法”、“后量子密码学标准”等关联子主题。\n2.  **反应式智能体（Reactive Agents）**：这类智能体基于逐步提示（step-by-step prompting）执行任务。当任务步骤长、依赖关系复杂时，其失败模式表现为：**短视行为**，缺乏前瞻性规划，容易陷入局部最优或重复操作；**错误传播**，早期步骤的错误决策会直接影响后续所有步骤，缺乏全局纠错机制；**缺乏可解释性**，其决策过程是隐式的，难以追踪和调试。例如，在探索一个多层级网站时，反应式智能体可能因点击错误的链接而迷失，无法回溯并调整策略。\n3.  **静态查询生成方法**：现有方法（如基于模板或简单提示的查询生成）在应对模糊或宽泛的研究问题时，其失败模式表现为：**查询质量对子目标清晰度高度敏感**。一旦子目标定义模糊，生成的查询就会变得宽泛或无关，导致检索到的信息质量急剧下降，相关性召回率（Recall@K）可能降低超过30%。此外，**缺乏上下文连贯性**，生成的查询之间相互独立，无法有效利用先前查询的历史结果，导致信息冗余或覆盖不全。\n\n**§3 问题的根本难点与挑战（200字以上）**\n构建深度研究智能体面临的根本难点与挑战来自多个层面：\n- **规划与决策的复杂性**：研究任务本质上是开放式的，目标模糊且路径不确定。如何将高层、模糊的用户意图（如“分析某技术趋势”）自动分解为一系列具体、可执行、逻辑连贯的子目标，是一个组合优化问题，搜索空间巨大。这要求智能体具备**任务感知的推理能力**和**对未知环境的模拟能力**。\n- **信息检索的动态性与噪声**：网络信息源是动态、异构且充满噪声的。智能体需要像人类研究员一样，能够**主动导航**（如点击链接、填写表单）、**评估信息可信度**、并**从海量、稀疏分布的内容中提取关键证据**。这涉及到对网页结构（HTML/DOM）、视觉布局（图表、界面）以及文本语义的多模态理解，计算复杂度和工程实现难度极高。\n- **报告生成的忠实性与结构性**：将碎片化的多源证据综合成一篇**结构连贯、逻辑清晰且忠实于原始证据**的长篇报告是核心挑战。这要求生成模型不仅要有强大的语言能力，还要能进行**篇章级规划**（section-level planning）、**证据选择与融合**，并处理**证据间的潜在冲突**。现有LLM在长文本生成中容易产生事实性幻觉（hallucination）和结构性漂移（structural drift）。\n- **端到端优化的困难**：深度研究流程包含规划、查询、检索、生成等多个模块，形成一个复杂的决策链。对这样一个多阶段、部分可观察的序列决策过程进行**联合优化（joint optimization）** 极其困难。奖励信号稀疏（仅在最终报告质量上体现）、不同模块间的误差会逐级放大，导致训练不稳定和样本效率低下。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**采用能力中心（capability-centric）的模块化视角**，对深度研究智能体进行系统性解构与分析。与以往综述（如Huang等人关注系统架构，Xu等人枚举任务与工具）不同，本文的核心假设是：**深度研究系统的整体效能取决于其核心子能力（规划、查询开发、网络探索、报告生成）的独立优化与有效协同**。\n\n基于此，本文的综述框架围绕以下核心假设展开：\n1.  **模块化可优化性**：每个核心阶段（如规划模块）所面临的技术挑战和解决方案可以独立进行分析、分类和评估。通过深入理解每个模块的内部机制，可以更有效地提升整体系统性能。\n2.  **能力集成是关键**：单纯提升单个模块的性能（如更精准的检索）不足以解决复杂研究任务。关键在于如何设计机制（如通过强化学习奖励、结构化中间表示）使这些模块能够**有效协同工作**，实现信息流的顺畅传递与决策的闭环反馈。例如，报告生成模块需要接收来自规划模块的结构化大纲和来自检索模块的证据集。\n3.  **学习与适应是趋势**：深度研究智能体的高级能力（如自适应规划、查询策略优化）不应是静态预设的，而应通过**与环境的交互（如强化学习）** 或**大规模任务分布上的训练（如元学习）** 来学习和进化。本文综述了大量将规划、查询生成等过程视为**可学习过程（Learnable Process）** 的工作，这构成了本文的一个重要技术叙事线索。",
    "core_architecture": "【二、核心架构与技术机制】最低字数要求：400字。\n\n这是最重要的字段，必须达到可复现/可实现的描述深度。\n\n**§1 系统整体架构概览（200字以上）**\n根据论文描述，一个典型的深度研究（Deep Research）系统遵循一个四阶段的模块化流水线。整体数据流向如下：\n**输入用户研究问题q₀ → 规划模块（Planning Module）→ 输出结构化研究计划P（包含子目标序列[s₁, s₂, ..., sₙ]）→ 查询开发模块（Question Developing Module）→ 针对每个子目标sᵢ，输出一系列搜索查询Qᵢ = {qᵢ,₁, qᵢ,₂, ...} → 网络探索模块（Web Exploration Module）→ 针对每个查询qᵢ,ⱼ，从网络语料库H中检索并输出相关文档集合D → 报告生成模块（Report Generation Module）→ 综合初始问题q₀、计划P、查询Q和证据D，输出最终的结构化报告Y。**\n\n整个系统可以以**单智能体（Single-Agent）** 或**多智能体（Multi-Agent）** 的形式实现。单智能体系统中，一个统一的LLM负责所有阶段，内部进行端到端推理。多智能体系统则将不同阶段分配给专门的智能体（如规划器、查询器、检索器、撰写器），实现模块化分工与并行执行。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：规划模块（Planning Module）\n- **输入**：初始研究问题q₀，智能体上下文知识K（可能包含任务历史、领域知识等）。\n- **核心处理逻辑**：规划模型M^plan（参数θ）将输入映射为一个结构化的研究计划P。具体策略多样：1) **基于模拟的规划**：如Simulate Before Act框架，在行动前对候选行动轨迹进行心理推演和可行性评估。2) **基于模块化的规划**：如WebPilot，使用战略规划器协调多个专门子智能体。3) **基于适应的规划**：如Meta-Plan Optimization (MPO)，通过元学习跨不同网络环境自适应调整规划策略。关键输出是一个子目标或工具调用步骤的序列。\n- **输出**：结构化研究计划P = [s₁, s₂, ..., sₙ]。\n- **设计理由**：显式规划旨在克服纯反应式提示的局限性，通过前瞻性分解任务来系统性地指导后续检索和推理，减少试错导致的失败。将规划作为独立模块，也提高了系统的可解释性和可控性。\n\n#### 模块二：查询开发模块（Question Developing Module）\n- **输入**：来自规划模块的结构化计划P，当前子目标sᵢ，以及从先前查询中积累的证据集ε。\n- **核心处理逻辑**：查询生成模型M^ask（参数θ）根据输入生成针对子目标sᵢ的查询集Qᵢ。论文将方法分为两大类：\n  1.  **奖励优化方法（Reward-Optimized Methods）**：通过与搜索环境交互，利用任务特定奖励（如答案准确性、检索覆盖率、效率惩罚）来优化查询生成策略。例如，DeepResearcher使用基于格式正确性和答案F1分数的奖励，通过GRPO算法训练。InForage则引入信息增益和效率惩罚等多维奖励。\n  2.  **监督驱动方法（Supervision-Driven Methods）**：利用监督微调、基于规则的分解或多智能体工作流。例如，ManuSearch使用具有明确角色（规划子问题、网络搜索、提取证据）的多智能体系统，基于确定性规则进行协调。\n- **输出**：针对子目标sᵢ的一系列搜索查询Qᵢ。\n- **设计理由**：单一静态查询无法满足复杂研究任务的信息需求。该模块旨在将抽象的子目标转化为具体、情境化且多样化的查询，以引导检索过程获取全面且相关的信息，是连接规划与检索的关键桥梁。\n\n#### 模块三：网络探索模块（Web Exploration Module）\n- **输入**：来自查询开发模块的查询Qᵢ，网络检索器R，开放网络语料库H。\n- **核心处理逻辑**：网络智能体M^web（参数θ）执行检索。主要分为两种范式：\n  1.  **基于网络智能体的检索**：模拟人类浏览行为，通过程序控制浏览器（如使用Selenium）进行动态导航、点击链接、解析HTML。例如，WebGPT将HTML转换为结构化表示，让LLM发出高级命令。**多模态网络智能体**（如WebVoyager, MM-ReAct）进一步整合视觉感知，分析网页截图以理解图表和复杂界面。\n  2.  **基于API的检索**：直接调用成熟的搜索引擎API（如Bing、Google Search、Perplexity Sonar API）获取排名后的文档或摘要。例如，OpenAI DeepResearch利用Bing基础设施进行查询、提取段落和生成后续查询。\n- **输出**：最终检索到的文档集合D。\n- **设计理由**：网络信息稀疏且异构，需要主动、智能的探索策略。基于智能体的方法能访问未索引或交互式内容，灵活性高；基于API的方法则快速、可靠，适合大规模集成。两者互补，分别解决深度探索和高效获取的挑战。\n\n**§3 关键公式与算法（如有）**\n论文给出了三个核心模块的形式化定义：\n1.  **规划模块**：\\(\\mathcal{P} = \\mathcal{M}^{\\text{plan}} \\left(q_{0}, \\mathcal{K}; \\theta\\right)\\)，其中 \\(\\mathcal{P} = [s_{1}, s_{2}, ..., s_{n}]\\)。\n2.  **查询开发模块**：\\(Q_{i} = \\mathcal{M}^{\\text{ask}} \\left(\\mathcal{P}, s_{i}, \\mathcal{E}; \\theta\\right)\\)，其中 \\(Q_{i} = \\{ q_{i,1}, q_{i,2}, ... \\}\\)，\\(\\mathcal{E}\\) 是累积的证据集。\n3.  **网络探索模块**：\\(\\mathcal{D} = \\mathcal{M}^{\\mathrm{web}} \\left(\\mathcal{R}, Q_{i}, \\mathcal{H}; \\theta\\right)\\)。\n4.  **报告生成模块**：\\(\\mathcal{Y} = \\mathcal{M}_{\\theta} (q_{0}, \\mathcal{P}, Q, \\mathcal{D})\\)。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n论文未提出单一的具体方法变体，而是对每个模块下的多种代表性方法进行了分类综述。例如，在查询开发模块中，明确对比了**奖励优化方法**与**监督驱动方法**两大变体。在奖励优化方法内部，又进一步区分了**仅基于格式和准确性的奖励**（如DeepResearcher, R1-Searcher）和**多维奖励**（如InForage, OTC-PO）等子类别。这些变体的核心差异在于优化目标、训练范式以及对环境交互的依赖程度。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文综述的深度研究范式与已有方法的核心技术差异主要体现在与**传统RAG**和**反应式智能体**的对比上：\n1.  **与传统RAG的差异**：传统RAG是**被动、单次检索**。给定查询，检索固定文档集，然后生成答案。深度研究智能体则是**主动、迭代探索**。它首先进行任务规划，然后动态生成查询，在探索过程中根据中间结果调整策略，并进行多源信息综合。技术本质从“检索-生成”的**两阶段管道**，演变为“规划-查询-探索-生成”的**多阶段、闭环决策过程**。\n2.  **与反应式智能体的差异**：反应式智能体（如Chain-of-Thought prompting）是**无显式规划、逐步推理**。它根据当前状态和提示决定下一步动作，缺乏全局视野。深度研究智能体强调**显式、结构化的前期规划**（如生成子目标序列），这提供了任务执行的路线图，提高了决策的系统性和可解释性。规划模块的输出（如推理链、搜索命令）是显式的，便于人类检查和调试。\n3.  **与简单查询生成的差异**：简单的查询生成通常是静态的或基于简单规则。深度研究中的查询开发模块是**情境化、自适应且可优化的**。它考虑整个研究计划、当前子目标和历史证据，生成多样化的查询。并且，可以通过强化学习从任务结果中学习优化查询策略，这是静态方法无法实现的。",
    "methodology_and_formulas": "【三、方法论细节与关键公式】最低字数要求：200字。\n\n**§1 完整算法流程（伪代码级描述）**\n论文作为综述，未提供单一系统的完整算法流程。但基于其描述的四阶段范式，可以重构一个典型的深度研究智能体的高层执行流程：\n**Step 1：接收输入**。接收用户的高层研究问题 \\(q_0\\)。\n**Step 2：规划阶段**。规划模块 \\(M^{plan}\\) 基于 \\(q_0\\) 和上下文 \\(\\mathcal{K}\\)，生成结构化研究计划 \\(\\mathcal{P} = [s_1, s_2, ..., s_n]\\)。\n**Step 3：迭代执行子目标**。对于计划中的每个子目标 \\(s_i \\)（\\(i\\) 从1到 \\(n\\)）：\n  - **Step 3.1：查询开发**。查询开发模块 \\(M^{ask}\\) 基于计划 \\(\\mathcal{P}\\)、当前子目标 \\(s_i\\) 和已积累的证据集 \\(\\mathcal{E}\\)，生成一组查询 \\(Q_i = \\{q_{i,1}, q_{i,2}, ...\\}\\)。\n  - **Step 3.2：网络探索**。对于 \\(Q_i\\) 中的每个查询 \\(q_{i,j}\\)，网络探索模块 \\(M^{web}\\) 通过检索器 \\(\\mathcal{R}\\) 从网络语料库 \\(\\mathcal{H}\\) 中检索相关文档，并更新证据集 \\(\\mathcal{E} = \\mathcal{E} \\cup \\mathcal{D}_{i,j}\\)。\n**Step 4：报告生成**。在所有子目标执行完毕后，报告生成模块 \\(M_\\theta\\) 综合初始问题 \\(q_0\\)、计划 \\(\\mathcal{P}\\)、所有查询 \\(Q\\) 和最终证据集 \\(\\mathcal{D}\\)（即所有 \\(\\mathcal{D}_{i,j}\\) 的并集），生成最终的结构化报告 \\(\\mathcal{Y}\\)。\n**Step 5：输出报告** \\(\\mathcal{Y}\\)。\n\n**§2 关键超参数与配置**\n原文作为综述，未指定具体系统的超参数。但可以从所述方法中推断出常见超参数类型：\n- **检索相关**：检索器返回的Top-K文档数量（K值），是平衡召回率与噪声的关键参数。\n- **规划相关**：规划深度（子目标数量n）、模拟推演的步数（在Simulate Before Act中）。\n- **强化学习相关**：奖励函数中的权重（如格式奖励、答案F1奖励、效率惩罚项的系数）、折扣因子、探索率（ε）。\n- **生成相关**：解码温度（temperature）、核采样（top-p）参数，用于控制报告生成的多样性和确定性。\n- **多智能体系统**：智能体数量、通信机制（如广播、集中式协调器）。\n作者未提供选择这些值的具体理由，但通常通过网格搜索、贝叶斯优化或在验证集上的消融实验来确定。\n\n**§3 训练/微调设置（如有）**\n论文综述了不同方法的训练范式：\n1.  **强化学习（RL）训练**：用于奖励优化类查询开发方法。例如，DeepResearcher、R1-Searcher使用**GRPO（Group Relative Policy Optimization）** 或**PPO（Proximal Policy Optimization）** 算法，在模拟或真实搜索环境中进行训练。奖励信号包括格式正确性（二进制奖励）和答案质量（基于F1或精确匹配的奖励）。训练可能需要数百万个交互步骤。\n2.  **监督微调（SFT）**：用于监督驱动类方法。使用人工标注的（查询， 正确检索结果/答案）配对数据，或通过规则合成的数据，对基础LLM进行微调。例如，ReasonRAG使用**直接偏好优化（DPO）** 对从蒙特卡洛树搜索（MCTS）探索中得到的推理轨迹进行排名学习。\n3.  **元学习（Meta-Learning）**：用于规划策略的跨环境适应。例如，MPO（Meta-Plan Optimization）通过高阶梯度更新，使智能体能够快速适应新的网络环境。\n4.  **大规模预训练**：一些工作（如Trabucco et al.）提出在互联网规模的任务分布上进行训练，以获取通用的规划行为。\n具体的优化器（如AdamW）、学习率（如1e-5到5e-5）、批次大小和训练轮数因具体工作而异，原文未提供统一标准。\n\n**§4 推理阶段的工程细节**\n原文未详细描述特定系统的推理工程细节，但提到了相关技术点：\n- **工具调用与API集成**：智能体需要与搜索引擎API（Bing, Google）或浏览器自动化工具（Selenium）进行交互。这涉及将自然语言指令转换为API调用或浏览器操作命令。\n- **多模态处理**：对于视觉丰富的网页，需要将截图与HTML/DOM树结合，输入给多模态LLM（如GPT-4V）进行分析和决策。\n- **缓存机制**：可能对频繁检索的查询结果进行缓存，以减少API调用成本和延迟。\n- **并行化**：在多智能体系统中，不同的智能体（规划器、检索器）可以并行执行，以提高系统吞吐量。\n- **向量数据库**：虽然未明确强调，但在基于API检索后，可能使用向量数据库对检索到的文档片段进行本地存储和快速相似性检索，以供报告生成时参考。",
    "experimental_design": "【四、实验设计】最低字数要求：200字。\n\n**§1 数据集详情（每个数据集单独列出）**\n本文是综述性论文，未进行原创实验，因此未列出具体的实验数据集。但文中提及了用于评估深度研究智能体的**基准测试（Benchmarks）**，例如：\n- **DeepResearch Bench [16]**：用于评估报告忠实性和引用准确性的标准化基准。\n- **WebArena [101]**：一个用于评估网络智能体在多样化网页界面模式上性能的综合性评估框架。\n这些基准通常包含复杂的、需要多步交互的网络任务或开放域研究问题，但论文未提供其具体规模、样本数或领域类型等细节。\n\n**§2 评估指标体系（全量列出）**\n论文中提及的评估指标可分为以下几类：\n- **准确性指标**：\n  - **任务成功率（Task Success Rate）**：例如，WebVoyager在真实世界基准上达到了59%的任务成功率。\n  - **答案质量**：如基于**F1分数**、**精确匹配（Exact Match）** 的奖励，用于评估最终答案的正确性。\n  - **报告忠实性（Report Fidelity）** 与**引用准确性（Citation Accuracy）**：在DeepResearch Bench等基准中用于评估生成报告的事实正确性和引用来源的准确性。\n- **检索性能指标**：\n  - **检索相关性指标**：如**Recall@K**、**NDCG@K**，用于评估查询开发模块检索到的文档的相关性。\n- **效率指标**：\n  - **工具使用成本/效率**：如OTC-PO中引入的成本感知奖励，用于惩罚过多的外部API调用，旨在最小化外部调用次数。\n  - **推理延迟**：虽未明确给出数字，但效率是规划策略（如Katz et al.的工作）的考虑因素之一。\n- **结构控制评估**：\n  - **关键点召回率（Key-point Recall）**：如Long2RAG中提出，评估检索到的证据是否被充分纳入长文本回答。\n  - **布局特定完整性（Layout-specific Completeness）** 与**可解释性（Explainability）**：如ExPerT中提出的评估策略。\n- **事实性评估**：\n  - **归因一致性（Attribution Consistency）**：如Face4RAG中用于中文RAG系统的细粒度指标。\n  - **上下文归因指标（Contextual Attribution Indicators）**：如SFR-RAG中在段落级别衡量对齐的指标。\n\n**§3 对比基线（完整枚举）**\n作为综述，本文未进行统一的对比实验，而是分类综述了各类方法。因此，没有统一的基线列表。但在各个模块的讨论中，隐含的基线是：\n1.  **规划模块**：**无显式规划的 reactive prompting**（逐步提示）方法。\n2.  **查询开发模块**：**静态查询生成**或**基于规则/模板的查询**方法。\n3.  **网络探索模块**：**传统单次检索的RAG系统**（仅使用API进行一次性检索）。\n4.  **报告生成模块**：**标准的、无结构控制或事实性增强的长文本生成模型**。\n\n**§4 实验控制变量与消融设计**\n原文未描述具体的消融实验设计，但可以从方法描述中推断常见的消融维度：\n- **模块消融**：例如，在端到端系统中，移除规划模块，观察性能下降；或比较使用不同查询生成策略（RL vs. 监督）的效果。\n- **奖励函数消融**：在强化学习方法中，比较仅使用格式奖励、仅使用答案奖励以及组合奖励的效果。\n- **检索源消融**：比较仅使用API检索、仅使用浏览器智能体检索以及两者混合的效果。\n- **结构控制消融**：在报告生成中，比较有无规划引导生成、有无约束解码对输出结构连贯性的影响。",
    "core_results": "【五、核心实验结果】最低字数要求：400字。\n\n⚠️ 此字段是最关键的字段，必须包含所有定量数字，严禁使用任何模糊描述！\n\n**§1 主实验结果全景（表格式呈现）**\n本文是综述论文，未报告统一的定量实验结果。文中引用的具体数字仅来自个别被综述的工作。例如：\n`WebVoyager | 真实世界基准任务成功率 | 59% | (对比文本基线未给出具体数值，但称“显著优于”)`\n`(其他被综述的工作，如DeepResearcher, InForage等，其具体实验数据未在本文中汇总呈现)`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n由于是综述，本文未提供分任务/分场景的定量分析。但文中对不同方法在不同挑战下的表现进行了定性比较：\n- **在规划鲁棒性方面**：基于模拟（如Simulate Before Act）和基于元学习（如MPO）的规划方法，被认为在应对动态、不确定的网络环境时，比静态规划更具**适应性和鲁棒性**。然而，当前LLM生成的计划仍然**脆弱（brittle）**，对模糊的研究问题或目标不明确的情况缺乏稳健性。\n- **在查询有效性方面**：**奖励优化方法**（如DeepResearcher, InForage）通过与环境交互，能学习到更有效的查询策略，尤其在答案准确性和检索覆盖率上有潜力取得更好表现。**监督驱动方法**（如ManuSearch, ReasonRAG）则更**可控、稳定且易于解释**，但在面对未见过的子目标或新颖推理上下文时，其泛化能力受限于示范数据的质量和覆盖范围。\n- **在网络探索能力方面**：**多模态网络智能体**（如WebVoyager）在处理包含图表、复杂界面的网页时，比纯文本浏览器智能体（如WebGPT）有显著优势，任务成功率更高。**基于API的检索系统**在速度和可靠性上占优，但无法访问未索引或需要交互的动态内容。\n- **在报告生成质量方面**：结合了**规划引导生成**（如Agent Laboratory, AI Scientist v2）和**事实性建模**（如RAGSynth, BRIDGE）的方法，在生成**结构连贯、事实准确**的长篇报告方面，被认为优于标准的、无约束的生成方法。然而，处理多文档、多跳上下文中的证据冲突仍是重大挑战。\n\n**§3 效率与开销的定量对比**\n文中未提供统一的效率对比数据。仅在讨论个别工作时提及效率考量：\n- **OTC-PO** 方法引入了**成本感知奖励**，旨在**惩罚过多的工具使用（外部API调用）**，以在保持正确性的同时最小化外部调用。但未给出具体的调用次数减少百分比或延迟降低数据。\n- **Katz et al.** 的工作**量化了推理操作的计算成本**，并提出了**效率感知的规划策略**，以平衡深思深度与资源使用。但未提供具体的量化节省数据。\n- 一般而言，**基于API的检索**比**浏览器智能体检索**更**快速、资源消耗更低**，但后者能获取更丰富的内容。\n\n**§4 消融实验结果详解**\n原文未提供具体的消融实验数值结果。\n\n**§5 案例分析/定性分析（如有）**\n原文未提供具体的成功或失败案例分析。",
    "conclusion_and_future_work": "【六、结论与未来工作】最低字数要求：150字。\n\n**§1 本文核心贡献总结**\n1.  **提出了系统性的能力中心综述框架**：首次以**规划（Planning）、查询开发（Question Developing）、网络探索（Web Exploration）、报告生成（Report Generation）** 四个核心能力模块为纲，对深度研究智能体领域进行了系统性梳理，超越了以往按系统架构或任务枚举的综述方式。\n2.  **深度剖析了各模块的技术挑战与解决方案**：对每个模块下的代表性方法进行了分类（如规划分为基于结构化世界知识和可学习过程；查询开发分为奖励优化和监督驱动），并分析了其技术原理、优势与局限。\n3.  **强调了模块协同与端到端学习的重要性**：指出当前方法大多孤立优化子技能，而有效的深度研究需要各模块紧密协同，未来方向在于**联合优化（joint optimization）** 与**学习型工作流（learnable workflows）** 的设计。\n4.  **指明了评估与基准的不足**：指出当前评估过于依赖端任务准确性，缺乏对中间过程（如规划质量、查询有效性）的细粒度评估，并呼吁建立更全面的基准（如DeepResearch Bench）。\n\n**§2 局限性（作者自述）**\n本文作为综述，总结的是当前领域的局限性，而非本文方法的局限性。作者指出的领域局限性包括：\n1.  **规划脆弱性**：当前LLM生成的计划对模糊或定义不清的研究目标缺乏鲁棒性，内部一致性无法保证，幻觉步骤会导致错误传播。\n2.  **评估粗糙性**：规划模块的评估通常依赖于端任务准确性，而非计划本身的质量，难以诊断规划失败或进行有意义的策略比较。\n3.  **任务孤立性**：许多系统将每个研究问题视为孤立问题，未能利用共享结构或可迁移策略，限制了跨任务积累可泛化规划知识的能力。\n4.  **报告生成的早期阶段**：当前报告生成方法大多针对孤立子技能，缺乏与上游组件（规划、查询、探索）的联合优化。结构控制方法缺乏灵活性，事实性方法难以处理多文档、多跳上下文中的冲突。\n\n**§3 未来研究方向（全量提取）**\n1.  **处理模糊性与长程一致性**：开发能够处理模糊研究问题、并在长序列任务中保持计划一致性的更鲁棒的规划机制。\n2.  **细粒度与可转移的规划评估**：建立超越端任务准确性的、针对规划质量本身的细粒度评估指标和基准，以促进规划策略的跨任务迁移和比较。\n3.  **模块间的联合优化与端到端学习**：研究如何将规划、查询、检索、生成等模块进行更紧密的集成和端到端优化，例如通过强化学习框架（如DeepResearcher）促进迭代自我反思和改进。\n4.  **混合网络探索架构**：结合基于API检索的效率与基于智能体检索的深度，开发混合架构，并集成证据提取、正确性验证和内容质量评估等专用模块。\n5.  **报告生成的灵活结构与冲突推理**：开发能够适应任务特定复杂性、证据分布和推理流程的灵活结构控制方法，以及能够建模多文档、多跳上下文中一致性的高级事实性保证机制。\n6.  **多模态与实时验证**：在网络探索中进一步发展多模态处理能力，并建立实时验证框架，以应对快速变化的网络环境。",
    "research_contributions": "【七、研究贡献与学术价值】最低字数要求：150字。\n\n**§1 核心学术贡献（按重要性排序）**\n1.  **理论框架创新**：提出了一个以**核心能力模块**为中心的深度研究智能体分类与综述框架。这一框架具有高度的**理论新颖性**，它不再局限于描述具体系统，而是抽象出共通的、可独立分析与优化的能力单元（规划、查询、探索、生成），为理解该领域提供了清晰的概念地图。\n2.  **技术脉络梳理**：对每个能力模块下的技术路线（如规划中的“基于知识”与“可学习过程”；查询开发中的“奖励优化”与“监督驱动”）进行了系统性的梳理和对比，**实验验证充分性**体现在引用了大量前沿工作（超过100篇文献）并进行了深入分析，揭示了技术演进的内在逻辑。\n3.  **领域影响与指导**：通过指出当前技术的局限性（如规划脆弱性、评估粗糙、模块割裂）和明确的未来方向，本文对**领域的发展具有重要的指导意义**。它为研究人员指明了关键挑战和潜在突破口，有助于推动深度研究从概念验证走向成熟、可靠的实际应用。\n\n**§2 工程与实践贡献**\n本文的工程与实践贡献主要体现在**系统化归纳与开源生态指引**上：\n- **开源代码与工具集梳理**：文中提及了大量开源框架和工具（如Scrapy, Selenium, DuckSearch, BraveSearch），并讨论了它们在深度研究流水线中的应用，为实践者提供了技术选型参考。\n- **评测基准汇总**：总结并提到了新兴的评测基准，如DeepResearch Bench和WebArena，为社区评估智能体性能提供了目标。\n- **架构设计参考**：通过对单智能体与多智能体工作流的分析，为系统工程师设计可扩展、高效的深度研究系统提供了架构层面的见解。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于**承上启下的系统性总结与前瞻位置**。\n- 它是**对“检索增强生成（RAG）”和“智能体（Agent）”两大技术路线融合产物（即深度研究）的首次全面综述**。\n- 相较于之前侧重于系统架构（Huang et al.）或宽泛任务枚举（Xu et al.）的综述，本文开辟了**以“能力形成与集成”为核心的新分析视角**，更深入地揭示了方法间的内在联系和设计范式的演变。\n- 因此，本文并非在某一具体技术路线上的简单延伸，而是为这个新兴交叉领域建立了一个系统化的认知框架和研发路线图。",
    "professor_critique": "【八、隐性缺陷与教授锐评】最低字数要求：200字。\n\n**§1 实验设计与评估体系的缺陷**\n本文作为综述，最大的缺陷在于**缺乏对所述方法性能的定量、横向比较**。文中引用了大量工作，但仅零星提及个别数据点（如WebVoyager的59%成功率），并未提供一个统一的、覆盖各模块核心指标的对比表格。这使得读者无法判断：在规划能力上，Simulate Before Act比模块化规划（WebPilot）具体好多少？在查询生成上，RL方法（DeepResearcher）比监督方法（ManuSearch）在检索召回率上能提升几个百分点？这种**数据缺失**严重削弱了综述的指导价值。此外，文中提到的评估指标（如任务成功率、F1）可能存在“指标幸运”问题：一个智能体可能通过“投机取巧”完成简单任务拉高平均分，但在需要深度推理的复杂任务上表现糟糕。缺乏对任务难度、领域覆盖度的细致划分。\n\n**§2 方法论的理论漏洞或工程局限**\n综述中描述的方法存在以下潜在漏洞：\n1.  **规划模块的“闭门造车”风险**：当前规划大多基于LLM的内部知识或对世界的简单模拟生成。当研究问题涉及**快速变化的领域（如新兴科技、金融市场）** 或**高度专业化的知识**时，LLM的先验知识可能过时或不足，导致规划从一开始就偏离正确方向。规划阶段缺乏与实时信息源的初步交互来“校准”方向。\n2.  **检索阶段的“信任危机”与“效率瓶颈”**：基于智能体的浏览器导航虽然灵活，但**严重依赖网页结构的稳定性**。一个微小的前端更新就可能导致自动化脚本失效。同时，递归点击链接的深度探索模式，在**记忆库（访问过的页面）超过数千条时，检索精度和效率可能会崩溃**，因为智能体难以记住所有历史并避免循环。基于API的检索则受限于搜索引擎的索引范围和排名算法，可能遗漏关键的非公开或长尾信息。\n3.  **报告生成的“事实性幻觉”与“冲突回避”**：尽管有事实性建模和冲突推理的研究，但当前方法在处理**多源、相互矛盾证据**时，往往倾向于**生成一个平滑的、看似合理的叙述，而非明确指出现有知识的冲突与不确定性**，这与真实学术研究的要求相悖。模型更倾向于“编造”一致性，而非承认无知。\n\n**§3 未经验证的边界场景**\n1.  **对抗性输入与信息污染**：当用户提供包含**误导性前提或包含矛盾事实的研究问题**时，智能体的规划、检索和生成流程如何应对？当前系统很可能被“带偏”，检索支持错误前提的“证据”，并生成看似合理实则错误的报告。\n2.  **跨语言与跨文化研究**：研究问题涉及**多语言混合资料**（如中英文文献）或需要**文化背景知识**进行解读时，当前以英文为主、依赖通用搜索引擎的智能体性能将严重退化。例如，检索中文学术数据库（如CNKI）并综合中英文观点生成报告。\n3.  **高度创造性或探索性研究**：对于“提出一个新的理论框架”或“预测未来十年趋势”这类**没有明确答案、需要高度创造性综合**的研究任务，当前依赖于检索已有知识的范式可能失效。智能体可能陷入信息复述，无法产生真正的洞见。\n\n**§4 可复现性与公平性问题**\n1.  **依赖昂贵模型与API**：许多先进方法（如WebVoyager使用GPT-4V，DeepResearcher依赖与真实搜索环境的RL交互）**严重依赖闭源、昂贵的超大模型和商业API**，使得普通研究者和机构难以复现实验结果或进行后续研究。这造成了资源壁垒。\n2.  **基线对比不公平**：不同工作使用不同的底座模型（GPT-4 vs. Claude vs. 开源模型）、不同的检索后端（Bing vs. Google vs. 模拟器）、以及不同的评估数据集。综述中未能，也无法进行**控制变量的公平比较**。因此，声称某方法“更优”的结论可能完全源于其使用了更强的底座模型或更优质的检索服务，而非方法本身。\n3.  **超参数调优差异**：各方法在强化学习奖励权重、规划深度、检索数量K等超参数上的调优策略未公开或不一致，进一步影响比较的公平性。",
    "zero_compute_opportunity": "【九、零算力研究契机】最低字数要求：300字。\n\n为资源受限的研究者（无GPU集群、无大额API预算）提供3个可立即执行的研究蓝图，每个蓝图必须具备可操作性。\n\n#### 蓝图一：基于轻量级模型与规则引擎的模块化深度研究智能体原型\n- **核心假设**：通过精心设计的规则引擎和提示工程，结合小型开源LLM（如Qwen2.5-7B, Llama 3.1-8B），可以构建一个功能完整、可解释性强的深度研究智能体原型，其核心模块（规划、查询）的性能接近甚至超过依赖黑盒大模型的端到端方法。\n- **与本文的关联**：基于本文对**监督驱动方法**和**模块化系统**（如ManuSearch）的分析，证明规则与多智能体协作的有效性。挑战在于如何用轻量模型实现类似效果。\n- **所需资源**：\n  1.  **模型**：Hugging Face上的免费小型LLM（如Qwen2.5-7B-Instruct）。\n  2.  **API**：免费的搜索引擎API（如DuckDuckGo Instant Answer API, SerpAPI免费额度）或开源检索库（如BM25）。\n  3.  **数据集**：公开的复杂问答或报告生成数据集（如HotpotQA, ELI5）用于验证。\n  4.  **费用**：主要成本为少量云GPU/CPU时间（Google Colab免费版可支撑），API调用费用近乎为零（利用免费额度）。\n- **执行步骤**：\n  1.  **规划模块**：设计基于模板和规则的研究计划分解器。针对特定领域（如科技新闻综述），预定义常见的子目标类型（背景、现状、挑战、趋势），让小型LLM根据输入问题选择并排序。\n  2.  **查询开发模块**：实现一个多智能体模拟系统。使用三个轻量级LLM实例分别扮演“分解者”（将子目标转化为关键词）、“扩展者”（生成同义词和相关查询）、“筛选者”（基于简单规则去重和排序）。\n  3.  **检索与生成**：使用免费API进行检索，用BM25对结果重排序。报告生成采用“大纲填充”模式：先让LLM根据证据生成要点，再根据预定义的结构模板（引言、主体、结论）进行组装。\n  4.  **评估**：在HotpotQA等需要多跳推理的数据集上，评估最终答案的F1分数，并与仅使用原始查询的基线对比。\n- **预期产出**：一篇展示如何用极低成本构建可解释研究智能体的技术报告或系统演示论文，可投递**EMNLP/ACL的Demo track或arXiv**。核心结论是：模块化设计和规则先验能有效弥补小模型能力不足。\n- **潜在风险**：小模型的推理和规划能力有限，可能导致计划不完整或查询不准确。**应对方案**：严格限制问题领域，提供更详细的规则和模板；利用检索结果对规划进行迭代修正。\n\n#### 蓝图二：深度研究智能体规划阶段的仿真评估基准与轻量级评估器\n- **核心假设**：规划质量是深度研究成败的关键，但缺乏独立评估。可以构建一个**仿真的、无需真实网络交互的评估环境**，通过预定义的“世界模型”（知识图谱）和任务集，定量评估不同规划策略（如模拟规划 vs. 模块化规划）的有效性、鲁棒性和效率。\n- **与本文的关联**：直接回应本文指出的**评估粗糙性**缺陷，即当前评估依赖端任务精度，难以诊断规划失败。\n- **所需资源**：\n  1.  **知识库**：利用现有结构化知识库（如DBpedia, ConceptNet）的子集构建一个仿真“网络”。\n  2.  **任务生成**：利用LLM（可用免费API如OpenAI GPT-3.5-Turbo的免费额度）自动生成需要多步检索的研究问题及其黄金规划路径。\n  3.  **计算**：本地CPU即可运行评估脚本。\n- **执行步骤**：\n  1.  **构建仿真环境**：将知识库中的实体和关系映射为一个图数据库。设计一个简单的“检索器”，当输入查询命中某个实体或关系时，返回相连的节点信息。\n  2.  **定义评估指标**：\n     - **规划完整性**：生成的子目标序列覆盖黄金路径关键节点的比例。\n     - **规划效率**：达成任务目标所需的模拟“检索”步骤数。\n     - **规划鲁棒性**：对问题表述添加噪声（如替换同义词）后，规划质量的变化程度。\n  3.  **实现基线规划器**：实现2-3种简单的规划策略作为基线，例如：贪婪搜索（每次查询与当前信息最相关的实体）、基于规则的分解（预定义模板）。\n  4.  **评估与分析**：在生成的测试集上运行不同规划器，计算上述指标，分析各类策略在不同复杂度任务上的表现。\n- **预期产出**：一个开源的深度研究规划仿真评估工具包及一份评估报告，可投递**LREC或NLP相关会议的Resource track**。核心结论是：显式规划在复杂任务上优于反应式策略，但当前LLM规划器在鲁棒性上存在缺陷。\n- **潜在风险**：仿真环境过于简化，无法反映真实网络的复杂性和噪声。**应对方案**：在环境中引入随机噪声（返回无关信息）和部分可观察性（每次检索只返回部分信息），增加评估的逼真度。\n\n#### 蓝图三：基于检索反馈的查询生成策略迭代优化——一个离线强化学习研究\n- **核心假设**：查询开发模块的强化学习训练需要昂贵的环境交互。可以利用**公开的检索日志数据集**（如MS MARCO, TREC）构建离线训练环境，通过离线强化学习（Offline RL）或模仿学习来优化查询生成策略，完全避免与实时搜索API的交互成本。\n- **与本文的关联**：针对**奖励优化方法**（如DeepResearcher）依赖在线交互成本高的问题，探索低成本的替代训练范式。\n- **所需资源**：\n  1.  **数据集**：MS MARCO等包含查询、检索文档及相关性标注的数据集。\n  2.  **模型**：小型开源LLM作为查询生成策略网络。\n  3.  **计算**：需要GPU进行模型微调，但可在Google Colab Pro等低成本平台完成。\n- **执行步骤**：\n  1.  **构建离线MDP**：将检索日志中的（查询，返回文档列表，相关性标签）序列建模为马尔可夫决策过程（MDP）轨迹。状态是累积的检索信息，动作是生成下一个查询，奖励是基于后续检索到的文档相关性计算的（如NDCG@10的提升）。\n  2.  **训练策略**：采用离线RL算法（如Conservative Q-Learning, Implicit Q-Learning）或行为克隆（Behavior Cloning），利用日志数据训练查询生成模型。目标是学习到能生成比日志中原始查询更有效（获得更高相关性文档）的新查询。\n  3.  **评估**：在留出的日志数据或新的查询集上，评估新策略生成的查询所对应的检索指标（Recall@K, NDCG@K），与日志中的原始查询进行对比。\n  4.  **分析**：分析学到的策略是否发现了新的、有效的查询模式（如更具体、包含更多实体）。\n- **预期产出**：一篇关于利用离线数据优化信息检索智能体查询策略的研究论文，可投递**SIGIR或EMNLP/ACL的信息检索相关方向**。证明离线学习能在低成本下有效提升查询生成质量。\n- **潜在风险**：离线数据存在选择偏差，学到的策略可能过于保守，无法探索出数据分布外更优的查询。**应对方案**：结合不确定性估计或引入轻量的在线探索（如使用免费API的有限额度）进行微调。",
    "source_file": "Deep Research A Survey of Autonomous Research Agents.md"
}