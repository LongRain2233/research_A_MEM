{
    "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n该研究位于大语言模型（LLM）的长上下文处理领域，核心应用场景是长文档问答（QA）、多文档摘要和自主智能体（Deep Search）。随着LLM上下文窗口的不断扩展，管理冗长的上下文已成为关键瓶颈。冗长的输入不仅带来高昂的计算成本（如API调用费用和GPU显存开销），还会引入显著的噪声信息，淹没模型并导致性能下降。在当前时间点，虽然已有多种上下文压缩技术，但它们往往在保留文本的**全局结构**和**细粒度细节**上存在根本性缺陷，导致下游推理任务（如多跳问答）出现幻觉。因此，研究一种既能压缩长度又能保持文本结构和忠实性的方法，对于提升LLM在复杂、真实世界任务中的实用性至关重要。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法主要分为两类，均存在具体失败模式：\n1.  **显式压缩方法（如LLMLingua, Selective-Context）**：这类方法通过删除不重要的词元或句子来压缩。其失败模式在于：当输入是具有复杂逻辑结构的文档（如学术论文、政府报告）时，基于词元或固定句子边界的删除会**破坏文本的局部连贯性**。例如，在多跳问答任务中，删除看似不重要的连接词或修饰性句子，可能导致证据链断裂，使模型无法进行正确的推理。它们通常只关注保留最重要的全局信息，而**忽略了原文的结构信息和细粒度细节**。\n2.  **隐式压缩方法（如AutoCompressor, ICAE, Glyph）**：这类方法将长文本编码为连续的潜在向量。其失败模式在于：当输入上下文包含大量信息时，这些方法存在**位置偏见**，即倾向于忽略上下文开头或中间部分的信息，只关注最显眼的内容。例如，在处理一篇长论文时，模型可能只记住了摘要和结论，而忽略了中间章节的关键方法论细节。此外，这些方法**缺乏灵活性**，通常需要专门设计的后训练过程或使用潜在向量作为新输入，这限制了它们与基于API的闭源模型（如GPT-4.1）的兼容性。\n3.  **基于检索的方法（标准RAG）**：这类方法在拼接检索到的离散文本块时，会**丢失话语连贯性**。当检索到的证据块来自文档的不同部分时，简单的拼接会破坏其间的逻辑关系（如因果、对比），导致模型无法理解完整的论证过程。\n\n**§3 问题的根本难点与挑战（200字以上）**\n上述问题的根本难点源于几个方面：\n- **结构信息与压缩率的权衡**：如何在高度压缩文本的同时，不丢失对复杂推理至关重要的**层次化结构关系**（如章节嵌套、论点-论据关系）是一个核心挑战。传统的摘要或词元删除方法本质上是线性的，难以保留这种树状结构。\n- **忠实性与抽象性的矛盾**：压缩过程必然涉及信息的抽象和取舍。隐式压缩方法（潜在向量）的“黑箱”特性使得其输出难以追溯源头，极易产生**幻觉**，即生成与原文不符的内容。确保压缩后的内容严格忠实于原文，需要一种可追溯的机制。\n- **计算效率与模型兼容性**：对于超长文档（如5万个词），实时进行高质量的语义理解和结构分析计算成本极高。同时，方法必须与广泛使用的闭源API模型兼容，不能依赖特定的模型架构或训练过程。\n- **噪声环境下的鲁棒性**：在真实场景（如网页搜索）中，输入文本充满广告、导航栏等无关内容（“结构死枝”），如何自动识别并滤除这些噪声，同时保留核心逻辑单元，是一个工程上的严峻挑战。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**将上下文压缩重新定义为“先结构分解，后选择”的过程**。其核心假设是：将线性的、无结构的文本序列转换为一个**严格锚定于源文本坐标的层次化关系树**，可以同时实现高效压缩、结构保留和忠实性保证。该假设的理论依据来源于**修辞结构理论**，该理论认为文本由基本话语单元（EDU）构成，单元之间存在修辞关系（如详述、对比）。本文假设，以EDU为节点构建的结构关系树，是保留文本逻辑流和细粒度细节的最小、最合适的表示形式。通过将压缩过程分解为两个阶段：1) **结构分解**：将文档解析为EDU关系树；2) **子树检索与线性化**：根据查询选择相关子树并还原为文本，可以克服现有方法的短板。该方法的关键创新在于其**显式性和可追溯性**：每个树节点都通过坐标指针（起始和结束ID）严格锚定到源文本，从而从根本上杜绝了幻觉。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\nEDU-based Context Compressor 是一个两阶段的即插即用框架，整体数据流如下：\n**输入**：长文档（或文档集） \\(\\mathcal{D}\\) 和用户查询 \\(q\\)。\n**Phase I: 结构分解**：输入文档 \\(\\mathcal{D}\\) 首先被送入 **LingoEDU** 分解模块。该模块将线性文本 \\(\\mathcal{D}\\) 转换为一个**结构关系树** \\(\\mathcal{T} = (\\nu, \\mathcal{E})\\)。其中，节点 \\(\\nu\\) 代表基本话语单元（EDU），边 \\(\\mathcal{E}\\) 代表EDU之间的话语链接和依赖关系（如详述、对比）。每个节点都严格锚定到源文本的字符偏移量（坐标指针）。\n**Phase II: 子树检索与线性化**：结构树 \\(\\mathcal{T}\\) 和查询 \\(q\\) 被送入**轻量级排序模块**。该模块计算查询与每个树节点的相关性分数 \\(\\phi(q, n_j)\\)。然后，采用**预算感知的贪心选择策略**，在给定的上下文预算 \\(B_{\\max}\\)（最大token数）内，选择分数最高的节点集合 \\(\\mathcal{C}\\)。最后，对 \\(\\mathcal{C}\\) 中的节点按其原始起始索引 \\(\\mathrm{id}_{\\mathrm{start}}\\) 进行排序，并**线性化**为连贯的文本序列 \\(\\mathcal{D}^{\\prime}\\)。\n**最终输出**：压缩后的上下文 \\(\\mathcal{D}^{\\prime}\\) 与原始查询 \\(q\\) 一同输入到目标LLM（如GPT-4.1）中，生成最终答案。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### **模块一：LingoEDU (分解模块)**\n- **输入**：原始长文档的线性文本序列。\n- **核心处理逻辑**：\n  1.  **坐标系统构建**：首先将文档分割为一系列基本话语单元（EDU）。每个EDU \\(e_i\\) 是一个三元组 \\(e_i = (t_i, \\operatorname{pos}_i, \\operatorname{id}_i)\\)，其中 \\(t_i\\) 是文本内容，\\(\\operatorname{pos}_i\\) 是字符偏移量，\\(\\operatorname{id}_i\\) 是唯一的顺序索引（从1开始）。这建立了一个可寻址的坐标系统。\n  2.  **可追溯生成**：模型被训练生成**增强的Markdown格式**输出：`## [id_start-id_end] ConceptTitle`。其中，`##` 表示层级深度，`[id_start-id_end]` 是锚定到源EDU序列的闭合索引区间，`ConceptTitle` 是该区间内容的语义摘要。模型通过生成索引区间而非重新生成文本内容来实现压缩。\n  3.  **树实现**：解析模型的Markdown输出，构建层次化语义树 \\(\\tau\\)。每个树节点 \\(n_j = (h_j, l_j, \\sigma_j)\\)，其中 \\(h_j\\) 是语义摘要，\\(l_j\\) 是层级深度，\\(\\sigma_j = [\\mathrm{id}_{\\text{start}}, \\mathrm{id}_{\\text{end}}]\\) 是EDU跨度。\n- **输出**：一个结构关系树 \\(\\mathcal{T}\\)，其中节点代表EDU及其层次关系。\n- **设计理由**：与直接生成摘要不同，生成索引区间确保了**零幻觉**，因为所有内容都严格指向源文本。采用Markdown格式既保证了输出的结构化（易于解析），又保持了token效率。\n\n#### **模块二：Ranking Module (排序模块)**\n- **输入**：用户查询 \\(q\\) 和结构关系树 \\(\\mathcal{T}\\)（包含每个节点的摘要 \\(h_j\\) 和代表文本片段 \\(t_{\\mathrm{rep}}\\)）。\n- **核心处理逻辑**：\n  1.  **相关性评分**：使用一个轻量级模型（如0.6B参数的Qwen3-Reranker）计算每个节点 \\(n_j\\) 与查询 \\(q\\) 的相关性分数：\\(s_j = \\phi_{\\theta}(q, h_j \\oplus t_{\\mathrm{rep}})\\)。其中 \\(\\oplus\\) 表示拼接。\n  2.  **预算感知贪心选择**：将所有节点按分数 \\(s_j\\) 降序排序。初始化候选集 \\(\\mathcal{C} = \\emptyset\\)。遍历排序后的节点，对于每个节点 \\(n_j\\)，计算如果将其加入 \\(\\mathcal{C}\\) 后，通过 `Retrieve(σ_n)` 函数获取的原始文本EDUs的总token长度。如果总长度不超过预算 \\(B_{\\max}\\)，则将该节点加入 \\(\\mathcal{C}\\)。该过程持续直到遍历完所有节点或预算耗尽。公式化表示为：\\(\\mathcal{C} = \\left\\{n_j \\mid \\sum_{n \\in \\mathcal{C}} \\operatorname{Len}(\\operatorname{Retrieve}\\left(\\sigma_n\\right)) \\leq B_{\\max} \\right\\}\\)。\n- **输出**：一个选中的节点集合 \\(\\mathcal{C}\\)，这些节点在预算限制下与查询最相关。\n- **设计理由**：与句子级或文档级检索相比，**节点级检索**能捕获更大的逻辑块（一个节点可能包含多个句子），避免上下文碎片化。使用轻量级排序模型（而非大型LLM API）是为了平衡性能与效率，实现高吞吐量的过滤且延迟可忽略。预算感知策略确保了压缩后的上下文长度适配下游LLM的最佳窗口大小。\n\n#### **模块三：Linearization (线性化模块)**\n- **输入**：选中的节点集合 \\(\\mathcal{C}\\)，其中每个节点包含EDU跨度 \\(\\sigma_j = [\\mathrm{id}_{\\text{start}}, \\mathrm{id}_{\\text{end}}]\\)。\n- **核心处理逻辑**：\n  1.  **重排序协议**：将 \\(\\mathcal{C}\\) 中的节点按其原始起始索引 \\(\\mathrm{id}_{\\text{start}}\\) 进行升序排序。这一步恢复了文本在原始文档中的逻辑顺序。\n  2.  **文本还原**：对于排序后的每个节点，根据其EDU跨度 \\(\\sigma_j\\)，从原始的EDU序列 \\(\\mathcal{U}\\) 中提取出对应的文本内容 \\(t_i\\)。\n  3.  **拼接**：将所有提取出的文本块按顺序拼接，形成最终的压缩上下文 \\(\\mathcal{D}^{\\prime}\\)。\n- **输出**：一个连贯的、压缩后的文本序列 \\(\\mathcal{D}^{\\prime}\\)。\n- **设计理由**：标准RAG方法在拼接离散块时会丢失话语连贯性。本设计利用分解模块提供的**显式坐标**，在拼接前对选中的文本块进行重排序，从而恢复逻辑顺序，使得下游LLM能够对不连续但结构上相关的片段进行有效推理。\n\n**§3 关键公式与算法（如有）**\n1.  **EDU表示**：\\(e_i = \\left(t_i, \\operatorname{pos}_i, \\operatorname{id}_i\\right)\\)\n2.  **树节点表示**：\\(n_j = \\left(h_j, l_j, \\sigma_j\\right), \\quad \\text {where } \\sigma_j = \\left[ \\mathrm{id} _{\\text {start}}, \\mathrm{id} _{\\text {end}} \\right]\\)\n3.  **增强Markdown输出格式**：\\(\\text {Output} = \\underbrace {\\# \\#} _{ \\text {Level}} \\underbrace {[ id _{\\text {start}} - id _{\\text {end}}} _{ \\text {Traceable Anchor}} \\underbrace {\\text {Concept Title}} _{ \\text {Semantic Abstract}}\\)\n4.  **预算感知选择公式**：\\(\\mathcal{C} = \\left\\{n_j \\mid \\sum_{n \\in \\mathcal{C}} \\operatorname{Len}(\\operatorname{Retrieve}\\left(\\sigma_n\\right)) \\leq B _{\\max} \\right\\}\\)\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n论文在消融实验中对比了以下变体：\n1.  **Indices Only**：LingoEDU模块仅输出索引区间 `[id_start-id_end]`，不生成语义摘要 `ConceptTitle`。\n2.  **Indices + Text (Ours)**：完整的LingoEDU，输出索引区间和语义摘要。\n3.  **不同训练数据规模**：使用20%、50%、100%的训练数据训练的模型变体。\n4.  **不同骨干模型规模**：基于Qwen-1.7B、Qwen-4B、Qwen-8B的LingoEDU变体。\n5.  **不同排序策略**：在排序模块中对比了：a) **No Selection**（标准，无过滤）；b) **Random**（随机选择）；c) **BM25**（基于词袋的稀疏检索）；d) **Self-Sum (LLM-Select)**（使用目标LLM自身进行节点选择）；e) **Ours (Qwen3-Reranker 0.6B)**（使用专用的轻量级排序模型）。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n1.  **与显式压缩方法（如LLMLingua, TokenSkip）的区别**：\n    - **操作单元**：现有方法操作于离散的词元或句子，破坏局部连贯性。本文方法操作于**基本话语单元（EDU）**，EDU是可变长度的最小连贯语义单元，能更好地保留逻辑流。\n    - **结构保留**：现有方法主要关注信息重要性，忽略原文结构。本文方法显式地构建**层次化关系树**，保留了章节嵌套、论点-论据等全局结构信息。\n    - **可追溯性**：现有方法生成新的摘要文本，可能引入幻觉。本文方法生成**锚定到源文本坐标的索引**，输出是原文片段的精确引用，确保了忠实性。\n2.  **与隐式压缩方法（如AutoCompressor, Glyph）的区别**：\n    - **表示形式**：隐式方法将文本压缩为**潜在向量**，是黑箱且不可解释的。本文方法输出**显式的、可解析的树结构**，完全透明。\n    - **位置偏见**：隐式方法已被证明存在位置偏见。本文方法基于EDU的坐标系统，对所有位置的文本单元进行平等处理，避免了该问题。\n    - **模型兼容性**：隐式方法通常需要特定架构或后训练，难以应用于闭源API模型。本文方法是**即插即用的**，与任何LLM（包括API模型）兼容，只需将压缩后的文本作为输入即可。\n3.  **与标准RAG的区别**：\n    - **检索粒度**：RAG通常在句子或段落级别检索，导致信息碎片化。本文在**节点（可能包含多个句子/段落）级别**检索，保留了更大的逻辑块。\n    - **上下文整合**：RAG简单拼接检索到的块，丢失连贯性。本文通过**重排序协议**，在拼接前按原始顺序排列选中的文本块，恢复了话语连贯性。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n**算法：EDU-based Context Compressor**\n**输入**：长文档 \\(\\mathcal{D}\\)，用户查询 \\(q\\)，最大上下文预算 \\(B_{\\max}\\)。\n**输出**：压缩后的上下文 \\(\\mathcal{D}^{\\prime}\\)。\n1.  **Step 1: 结构分解**\n    - 使用 **LingoEDU** 模型处理文档 \\(\\mathcal{D}\\)。\n    - LingoEDU 将 \\(\\mathcal{D}\\) 分割为EDU序列 \\(\\mathcal{U} = \\{e_1, e_2, ..., e_N\\}\\)，其中 \\(e_i = (t_i, pos_i, id_i)\\)。\n    - LingoEDU 生成增强Markdown格式的输出，解析后得到结构关系树 \\(\\mathcal{T} = (\\nu, \\mathcal{E})\\)，其中每个节点 \\(n_j = (h_j, l_j, \\sigma_j)\\)，\\(\\sigma_j = [id_{start}, id_{end}]\\)。\n2.  **Step 2: 节点相关性评分**\n    - 对于树 \\(\\mathcal{T}\\) 中的每个节点 \\(n_j\\)：\n        - 提取其语义摘要 \\(h_j\\) 和代表文本片段 \\(t_{\\mathrm{rep}}\\)（来自跨度 \\(\\sigma_j\\) 中的部分文本）。\n        - 使用轻量级排序模型计算相关性分数：\\(s_j = \\phi_{\\theta}(q, h_j \\oplus t_{\\mathrm{rep}})\\)。\n3.  **Step 3: 预算感知贪心选择**\n    - 初始化候选集 \\(\\mathcal{C} = \\emptyset\\)，已用预算 \\(used = 0\\)。\n    - 将所有节点按 \\(s_j\\) 降序排序。\n    - **for** 每个节点 \\(n_j\\)（按排序顺序） **do**：\n        - 计算节点 \\(n_j\\) 对应的原始文本长度 \\(len_j = \\operatorname{Len}(\\operatorname{Retrieve}(\\sigma_j))\\)。\n        - **if** \\(used + len_j \\leq B_{\\max}\\) **then**：\n            - \\(\\mathcal{C}.\\text{add}(n_j)\\)\n            - \\(used = used + len_j\\)\n        - **end if**\n    - **end for**\n4.  **Step 4: 线性化**\n    - 将候选集 \\(\\mathcal{C}\\) 中的节点按其 \\(\\sigma_j.start\\)（即 \\(id_{start}\\)）升序排序。\n    - **for** 每个排序后的节点 \\(n_j \\in \\mathcal{C}\\) **do**：\n        - 根据 \\(\\sigma_j\\) 从EDU序列 \\(\\mathcal{U}\\) 中提取对应的原始文本块 \\(text_j\\)。\n        - 将 \\(text_j\\) 追加到 \\(\\mathcal{D}^{\\prime}\\)。\n    - **end for**\n5.  **Step 5: 输出**\n    - 返回压缩后的上下文 \\(\\mathcal{D}^{\\prime}\\)。\n\n**§2 关键超参数与配置**\n- **最大上下文预算 \\(B_{\\max}\\)**：在排序模块的贪心选择中使用的token数上限。该值需要根据下游LLM的上下文窗口大小和任务需求进行调整。论文中未明确给出具体数值，但指出该策略使检索到的上下文密度与LLM的最佳窗口大小对齐。\n- **LingoEDU模型规模**：主要使用 **Qwen3-4B** 作为骨干模型。消融实验对比了1.7B、4B、8B版本，结果表明4B参数在结构误差（TED）和关系准确性（DLA）之间达到最佳平衡。\n- **排序模型规模**：使用 **Qwen3-Reranker-0.6B** 作为轻量级排序模型。选择0.6B是为了在保持高性能的同时实现高吞吐量和低延迟。\n- **训练数据规模**：LingoEDU的训练涉及两阶段：1) 在约 **100k** 个合成样本上进行持续预训练，以学习布局模式；2) 在数千个精心手动标注的文档上进行监督微调（SFT）。消融实验使用了20%、50%、100%的训练数据。\n\n**§3 训练/微调设置（如有）**\n1.  **数据合成**：由于高质量、细粒度的长上下文层次标注稀缺，论文引入了一个可扩展的自动化流水线来合成训练数据。该流水线利用强大的LLM来提炼“摘要-索引”的逻辑。\n    - **双层任务分解**：将数据生成解耦为两个子任务以避免“指令冲突”：\n        a. **显式布局提取**：模型提取客观的结构线索（如Markdown标题、HTML标签）以形成文档骨架。\n        b. **深度语义分割**：对于缺乏显式格式的大文本块，模型专注于语义转变以划分更细粒度的功能部分。\n    - **求解器-批判器精炼循环**：\n        - **求解器**：提出一个层次分解，尝试将详细内容抽象为高层语义节点。\n        - **批判器**：审计该提案，专门检查生成的摘要（标题）是否准确反映了分配的跨度 \\(\\sigma_j\\)，没有幻觉或语义漂移。\n2.  **模型训练**：\n    - **骨干模型**：Qwen3-4B。\n    - **训练阶段**：\n        a. **持续预训练**：在约100k合成样本上训练，学习布局模式。\n        b. **监督微调（SFT）**：在数千个人工标注的文档上进行微调，以对齐人类意图。\n    - **输出格式训练**：模型被训练生成**增强Markdown模式**（`## [id_start-id_end] ConceptTitle`），以确保可追溯性和token效率。在解码时强制执行约束，确保只生成来自EDU序列 \\(\\mathcal{U}\\) 的有效数字索引。\n\n**§4 推理阶段的工程细节**\n- **部署环境**：实验在运行于高性能服务器的Linux操作系统上进行，配备Intel Xeon 2.3GHz CPU、1960GB内存和8个NVIDIA A100 GPU（每个80GB VRAM）。\n- **延迟**：LingoEDU处理每个文档的平均延迟为 **1.20秒**，比本地部署的Qwen3-32B（10.17秒）快近10倍。\n- **成本**：使用开源模型（Qwen3-4B, Qwen3-Reranker-0.6B）进行本地推理，成本极低。在StructBench上处理整个测试集（约248个文档）的成本为 **0.17美元**，与最便宜的解析器API（Jina Reader: 0.10美元, Firecrawl: 0.17美元）相当。\n- **兼容性**：该框架是即插即用的，与任何LLM兼容，包括闭源API模型（如GPT-4.1, Claude）。它不需要修改目标LLM的架构或进行联合训练。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n1.  **StructBench**（本文构建）：\n    - **规模**：248个文档。\n    - **领域/类型**：覆盖多样格式（网页、PDF）、语言（中文、英文）和体裁。包括10种不同体裁，主要是复杂结构，如学术论文、政府文件、商业报告和技术教程。\n    - **文档长度**：从300词到50,000词不等。\n    - **标注**：由人类专家对文档进行解析、句子分割和话语结构手动标注，以生成真实的结构骨架（顶层层次结构）作为标签。\n    - **用途**：用于评估模型理解和描述上下文结构信息的能力。\n2.  **LongBench**：\n    - **规模**：未在论文中明确给出具体样本数，但这是一个广泛使用的双语多任务长上下文理解基准。\n    - **任务类型**：包含多文档问答（Multi-Doc QA）、摘要（Summarization）和少样本学习（Few-shot）三类任务。\n    - **具体数据集**：\n        - **Multi-Doc QA**: HotpotQA, 2WikiMultihopQA, Musique, DuReader。\n        - **Summarization**: GovReport, QMSum, MultiNews, VCSUM。\n        - **Few-shot**: TREC, TriviaQA, SAMSum, LSHT（Long-Sentence Hyperbolic Test）。\n3.  **Deep Search 基准**：\n    - **HLE**：一个用于评估深度搜索中学术推理能力的基准。\n    - **BrowseComp-ZH**：一个专注于中文网页环境、噪声密集的深度搜索基准。\n\n**§2 评估指标体系（全量列出）**\n1.  **结构完整性评估（StructBench）**：\n    - **Tree Edit Distance (TED)**：微观指标。计算将预测树转换为真实树所需的最小编辑操作（插入、删除、替换）次数。**值越低越好**，表示结构对齐更精确。\n    - **Document Level Accuracy (DLA)**：宏观指标。定义为 \\(\\mathrm{DLA} = \\frac{|D_{\\mathrm{match}}|}{|D_{\\mathrm{all}}|}\\)，其中 \\(D_{\\mathrm{match}}\\) 代表分解的结构骨架与真实值完全匹配的文档数量。这是一个严格的指标，要求**零结构错误**。值越高越好。\n2.  **下游任务性能评估（LongBench & Deep Search）**：\n    - **准确性指标**：对于QA任务，使用标准指标如F1分数、精确匹配（EM）等。对于摘要任务，使用ROUGE等指标。论文表格中报告的是具体任务的得分（百分比）。\n3.  **效率与成本评估**：\n    - **成本（$）**：处理整个测试集（约248个文档）所需的美元费用。计算基于API调用或本地计算资源消耗。\n    - **延迟（秒）**：处理单个文档所需的平均时间（秒）。\n    - **模型规模**：作为计算开销的间接指标（如0.6B vs. 大型LLM）。\n\n**§3 对比基线（完整枚举）**\n1.  **前沿大语言模型（通过API或本地部署）**：\n    - GPT-4o, GPT-4.1 (OpenAI)\n    - OpenAI o3, o4-mini (OpenAI)\n    - Claude-3.7-Sonnet, Claude-4-Sonnet (Anthropic)\n    - Gemini-2.5-Flash, Gemini-2.5-Pro (Google)\n    - DeepSeek-V3, DeepSeek-R1 (DeepSeek-AI)\n    - Qwen3-32B, Qwen3-235B (Alibaba)\n    - **类型**：通用LLM，使用特定提示指令其输出层次化的JSON/Markdown结构。\n2.  **商业解析API**：\n    - Jina Reader\n    - Firecrawl\n    - **类型**：广泛用于网页到Markdown转换的解析工具。通过URL访问测试文档。\n3.  **上下文压缩基线（LongBench实验）**：\n    - **Standard**：将完整的原始上下文直接输入LLM。\n    - **Self-Sum**：利用LLM自身在处理前生成上下文的抽象摘要。\n    - **Glyph**：应用隐式压缩的基线，作为基于潜在表示方法的参考。\n    - **C3**：使用一个小型LLM将长上下文积极压缩为紧凑的潜在表示，然后用LLM解码。\n4.  **节点排序策略基线（消融实验）**：\n    - **No Selection**：标准基线，无显式过滤。\n    - **Random**：随机选择节点以匹配压缩预算。\n    - **BM25**：基于词袋模型的稀疏检索基线。\n    - **Self-Sum (LLM-Select)**：提示生成器（GPT-4.1）自身在推理前识别相关节点。\n\n**§4 实验控制变量与消融设计**\n1.  **LingoEDU模块消融**：\n    - **输出形式**：对比“仅索引”与“索引+文本”两种输出形式对结构预测性能（TED, DLA）的影响。\n    - **训练数据规模**：对比使用20%、50%、100%训练数据训练的模型性能。\n    - **骨干模型规模**：对比基于Qwen-1.7B、Qwen-4B、Qwen-8B的LingoEDU性能。\n2.  **排序模块消融**：\n    - 固定生成器为GPT-4.1，对比五种节点选择策略对下游任务性能的影响：No Selection, Random, BM25, Self-Sum (LLM-Select), Ours (Qwen3-Reranker 0.6B)。\n3.  **Deep Search实验设计**：\n    - 将LingoEDU模块集成到深度搜索流水线中，评估其在两个基准（HLE, BrowseComp-ZH）上的效果。对比三种配置：Base（无压缩）、Self-Sum（查询聚焦摘要）、Ours (LingoEDU)（结构分解）。\n4.  **通用性验证**：\n    - 在多个不同的LLM骨干（DeepSeek-R1, Qwen3-235B-Thinking, DeepSeek-V3.1, DeepSeek-V3.2, GPT-5, Claude Opus 4.1, Gemini 3 Pro）上测试方法的有效性，证明其作为“推理支架”的普适性。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n**表1：StructBench上的性能对比**\n`方法 | 类型 | TED (↓) | DLA (%) (↑) | 成本 ($) (↓) | 延迟 (s) (↓)`\n`GPT-4o | General LLM* | 6.22 | 29.03 | 5.21 | -`\n`GPT-4.1 | General LLM* | 6.35 | 37.90 | 4.17 | -`\n`OpenAI o3 | General LLM* | 5.51 | 28.63 | 4.17 | -`\n`OpenAI o4-mini | General LLM* | 5.87 | 32.66 | 2.28 | -`\n`Claude-3.7-Sonnet | General LLM* | 6.65 | 35.08 | 7.09 | -`\n`Claude-4-Sonnet | General LLM* | 5.08 | 43.15 | 7.09 | -`\n`Gemini-2.5-Flash | General LLM* | 5.82 | 27.82 | 0.99 | -`\n`Gemini-2.5-Pro | General LLM* | 5.61 | 32.66 | 4.02 | -`\n`DeepSeek-V3 | General LLM* | 6.32 | 33.47 | 0.30 | -`\n`DeepSeek-R1 | General LLM* | 6.26 | 30.65 | 1.14 | -`\n`Qwen3-32B | General LLM* | 9.49 | 24.90 | 0.26 | 10.17†`\n`Qwen3-235B | General LLM* | 9.93 | 17.89 | 0.11 | -`\n`Jina-Reader | Parser API | 17.04 | - | 0.10 | -`\n`Firecrawl | Parser API | 16.81 | - | 0.17 | -`\n`Our Method (LingoEDU) | Specialized | 4.77 | 49.60 | 0.17 | 1.20†`\n\n**表4：LongBench上的结果（部分摘要）**\n`模型/方法 | HotpotQA | 2Wiki | Musique | DuReader | GovRep | QMSum | MultiN | VCSum | TREC | Trivia | SAMSum | LSHT`\n`Gemini-2.5-Pro Standard | 35.20 | 38.10 | 28.55 | 7.15 | 4.10 | 15.80 | 4.05 | 5.80 | 46.50 | 59.85 | 20.45 | 26.10`\n`Gemini-2.5-Pro Self-Sum | 37.78 | 39.90 | 30.77 | 7.79 | 4.34 | 16.53 | 4.44 | 6.17 | 49.00 | 62.31 | 21.89 | 29.50`\n`Gemini-2.5-Pro Ours (LingoEDU) | 40.46 | 40.91 | 31.22 | 8.12 | 4.25 | 16.17 | 4.85 | 6.36 | 57.50 | 63.25 | 23.80 | 35.48`\n`Δ (vs. Standard) | +14.94% | +7.38% | +9.35% | +7.69% | +2.44% | +2.34% | +19.75% | +9.66% | +23.66% | +1.25% | +11.39% | +3.45%`\n`GPT-4.1 Standard | 65.83 | 72.98 | 51.90 | 21.80 | 29.97 | 22.84 | 20.85 | 12.50 | 77.00 | 90.07 | 39.20 | 48.60`\n`GPT-4.1 Self-Sum | 67.89 | 74.39 | 53.48 | 23.51 | 30.98 | 22.53 | 22.06 | 13.71 | 79.00 | 93.69 | 40.79 | 50.50`\n`GPT-4.1 Ours (LingoEDU) | 70.11 | 74.68 | 54.86 | 25.34 | 31.56 | 23.30 | 23.50 | 14.62 | 80.00 | 93.76 | 41.68 | 52.50`\n`Δ (vs. Standard) | +6.50% | +2.33% | +5.70% | +16.24% | +2.94% | +0.61% | +5.80% | +8.96% | +3.90% | +4.10% | +6.33% | +8.02%`\n\n**表6：Deep Search上的结果**\n`模型骨干 | HLE (Base) | HLE (Self-Sum) | HLE (Ours) | HLE Δ | BrowseComp-ZH (Base) | BrowseComp-ZH (Self-Sum) | BrowseComp-ZH (Ours) | BrowseComp-ZH Δ`\n`DeepSeek-R1 | 9.0 | 9.5 | 13.6 | +51.11% | 18.7 | 19.4 | 20.4 | +9.09%`\n`Qwen3-235B-Thinking | 14.2 | 14.7 | 15.5 | +9.15% | 8.7 | 9.0 | 12.8 | +47.13%`\n`DeepSeek-V3.1 | 14.5 | 14.8 | 15.6 | +7.59% | 29.1 | 29.8 | 38.8 | +33.33%`\n`DeepSeek-V3.2 | 20.0 | 20.6 | 21.2 | +6.00% | 31.1 | 32.2 | 34.6 | +11.25%`\n`GPT-5 | 25.0 | 25.9 | 27.1 | +8.40% | 29.1 | 29.8 | 31.8 | +9.28%`\n`Claude Opus 4.1 | 14.0 | 14.8 | 15.5 | +10.71% | 20.8 | 21.5 | 23.2 | +11.54%`\n`Gemini 3 Pro (w/o Deep Think) | 26.1 | 26.7 | 30.1 | +15.33% | 47.4 | 48.1 | 48.8 | +2.95%`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n- **结构理解任务（StructBench）**：本文的LingoEDU在TED（4.77）和DLA（49.60%）上均显著优于所有基线。最强的通用LLM基线Claude-4-Sonnet的TED为5.08，DLA为43.15%。LingoEDU相比其DLA绝对提升了6.45个百分点。这表明，**专门化的结构理解监督**超越了通用LLM的涌现提示或推理能力。商业解析API（Jina, Firecrawl）表现最差（TED > 16），因为它们依赖浅层HTML标签，无法捕获复杂PDF中的隐式话语结构。\n- **多文档问答任务（LongBench）**：在Gemini-2.5-Pro上，本文方法在HotpotQA上相比Standard基线相对提升14.94%（从35.20到40.46），在Musique上提升9.35%（从28.55到31.22）。**提升原因**：抽象摘要方法（Self-Sum）容易丢失多跳推理所需的关键实体；而本文的显式树结构通过原始文本索引保留了精确的证据链，使模型能够查找确切细节而无幻觉。在GPT-4.1上，提升幅度相对较小但依然稳定，表明即使对于强大模型，结构感知压缩也能提供增益。\n- **摘要任务（LongBench）**：本文方法在MultiNews上相比Standard基线提升显著（Gemini-2.5-Pro: +19.75%, GPT-4.1: +5.80%）。虽然Self-Sum本身在摘要任务上很强，但本文方法仍具竞争力，甚至在MultiNews上超过了Self-Sum（GPT-4.1: 23.50 vs. 22.06）。这表明保留层次结构比潜在压缩更能有效保留信息。\n- **深度搜索任务（HLE & BrowseComp-ZH）**：在噪声密集的中文网页环境（BrowseComp-ZH）中，本文方法带来了巨大提升（Qwen3-235B: +47.13%, DeepSeek-V3.1: +33.33%）。**原因**：分解器充当了语义过滤器，通过识别逻辑EDU，有效修剪了“结构死枝”（如广告、导航栏），同时保留了核心内容。在需要跨学科综合多个线索的学术推理任务（HLE）中，DeepSeek-R1获得了+51.11%的相对提升（从9.0到13.6）。这表明提供更清晰、结构感知的上下文充当了“推理支架”，防止模型迷失在无关细节中。\n\n**§3 效率与开销的定量对比**\n- **成本**：在StructBench上，处理248个文档，本文方法的成本为**0.17美元**，与最便宜的解析器API（Firecrawl: 0.17美元）相当，远低于所有通用LLM API（如Claude-4-Sonnet: 7.09美元，GPT-4.1: 4.17美元）。\n- **延迟**：本文方法处理每个文档的平均延迟为**1.20秒**，比本地部署的Qwen3-32B（10.17秒）快**近10倍**。\n- **模型规模与计算开销**：本文方法使用一个4B参数的LingoEDU和一个0.6B参数的排序模型，均为轻量级模型，可在单个GPU上高效运行。相比之下，使用大型LLM（如GPT-4.1）进行Self-Sum或直接处理长上下文，需要高昂的API调用费用或巨大的本地计算资源。\n\n**§4 消融实验结果详解**\n1.  **LingoEDU输出形式**：\n    - “仅索引”变体的TED为8.16，DLA为33.06%。\n    - “索引+文本”的完整方法TED为4.77，DLA为49.60%。\n    - **结论**：显式文本生成作为结构预测的**关键语义锚点**，移除后性能大幅下降（DLA下降16.54个百分点）。\n2.  **训练数据规模**：\n    - 使用20%训练数据时，TED为4.87，DLA为45.16%，保留了完整模型91%的准确率。\n    - 使用50%数据时，TED为4.85，DLA为48.79%。\n    - 使用100%数据时，TED为4.77，DLA为49.60%。\n    - **结论**：模型表现出**显著的数据效率**，即使数据量很少也能获得大部分性能。\n3.  **骨干模型规模**：\n    - Qwen-1.7B: TED 4.99, DLA 48.39%。\n    - Qwen-4B: TED 4.77, DLA 49.60%。\n    - Qwen-8B: TED 4.89, DLA 49.19%。\n    - **结论**：从1.7B扩展到4B带来明显改进，但进一步扩展到8B导致性能饱和甚至回归。**4B参数范围**是该任务的最佳平衡点，更大的模型可能对刚性输出格式过拟合，而没有相应更大的数据集。\n4.  **排序策略（固定GPT-4.1为生成器）**：\n    - **No Selection**: HotpotQA 65.83, DuReader 21.80。\n    - **Random**: HotpotQA 56.42 (-14.3%), DuReader 21.70 (-0.5%)。\n    - **BM25**: HotpotQA 65.99 (+0.2%), DuReader 26.84 (+23.1%)。\n    - **Self-Sum (LLM-Select)**: HotpotQA 67.89 (+3.1%), DuReader 23.51 (+7.8%)。\n    - **Ours (Qwen3-Reranker)**: HotpotQA 70.11 (+6.5%), DuReader 25.34 (+16.2%)。\n    - **结论**：本文专用的轻量级排序器（0.6B）在大多数数据集上超越了其他策略，包括使用GPT-4.1自身进行选择（Self-Sum）。这表明**专门的、语义感知的排序模型**在证据定位上优于通用LLM的内在选择能力。BM25在DuReader上表现较好，但在HotpotQA上提升有限，验证了语义过滤优于表面匹配的必要性。\n\n**§5 案例分析/定性分析（如有）**\n原文未提供具体的定性案例分析，但从结果分析中可以推断：通用模型（如Qwen3）在结构解析时经常**幻觉出不存在的子章节**或将深层层次结构**扁平化**以节省生成token。商业解析API（如Jina, Firecrawl）由于依赖浅层HTML标签，在解析复杂PDF时**无法捕获隐式话语结构**，导致高TED分数。本文方法通过可追溯的坐标索引，确保了生成的结构严格忠实于源上下文，消除了幻觉。在深度搜索中，该方法通过识别逻辑EDU，有效修剪了网页中的“结构死枝”（噪声），保留了核心内容，从而在噪声密集型任务上获得巨大提升。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出了基于EDU的上下文压缩器**：这是一个新颖的、即插即用的框架，将上下文压缩重新定义为“先结构分解，后选择”的过程。它通过将线性文本转换为严格锚定于源文本坐标的层次化关系树，实现了**全局结构**和**细粒度细节**的同时保留，并且完全兼容闭源API模型。\n2.  **发布了StructBench基准**：一个包含248个多样化文档的手动标注数据集，用于精确评估LLM理解和描述上下文结构信息的能力。该基准填补了该领域公共基准的空白。\n3.  **实现了SOTA的结构理解性能与效率**：在StructBench上，LingoEDU在结构预测准确性（TED: 4.77, DLA: 49.60%）上显著优于前沿LLM（如Claude-4-Sonnet），同时成本（0.17美元）与最便宜的解析器API相当，延迟（1.20秒/文档）比本地部署的32B模型快近10倍。\n4.  **有效减少长上下文任务的幻觉**：在LongBench和Deep Search任务上的实验表明，该方法通过保留精确的证据链和过滤噪声，显著提升了多种下游任务的性能（如HotpotQA上相对提升14.94%，DeepSeek-R1在HLE上相对提升51.11%），并减少了幻觉。\n\n**§2 局限性（作者自述）**\n原文中作者**未明确列出**具体的局限性。但从上下文中可以推断出潜在的局限：该方法依赖于对文档进行EDU分解，这可能对**非结构化或格式极其混乱的文本**（如社交媒体流、极度口语化的对话）效果有限。此外，训练LingoEDU需要合成和手动标注数据，这可能限制了其快速适应新领域或语言的能力。\n\n**§3 未来研究方向（全量提取）**\n原文结论部分明确提出了一个未来方向：\n- **将范式扩展到多模态上下文和动态智能体记忆**：当前工作专注于文本模态。未来工作将探索如何将这种显式的结构建模方法应用于包含图像、表格等多模态信息的上下文压缩。同时，也将研究如何将该框架集成到自主智能体中，用于动态管理和更新其记忆，以支持更复杂的长期推理任务。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **理论新颖性**：提出了“**结构先行**”的上下文压缩新范式。与传统的基于重要性评分或潜在编码的压缩方法不同，本文首次将**修辞结构理论（RST）中的基本话语单元（EDU）** 系统地引入到LLM的上下文管理问题中，将压缩过程形式化为一个可追溯的坐标索引问题。这为理解和管理长文本的语义结构提供了新的理论框架。\n2.  **实验验证充分性**：通过构建**StructBench**基准，首次对LLM的结构理解能力进行了系统性的定量评估，揭示了即使是最先进的通用LLM（如o3）在此任务上的不足。同时，在多个标准长上下文基准（LongBench）和具有挑战性的深度搜索基准上进行了全面实验，证明了该方法在提升任务性能和减少幻觉方面的**普遍有效性**。消融实验深入验证了每个组件（输出形式、数据规模、模型规模、排序策略）的必要性。\n3.  **对领域的影响**：这项工作为长上下文处理领域开辟了一条**显式、可解释、高保真**的技术路线。其提出的框架是即插即用的，与现有LLM生态系统高度兼容，有望被广泛应用于需要处理长文档、进行复杂推理的真实世界应用中，如法律文档分析、学术文献调研、智能客服等。它同时也强调了在追求压缩率的同时，**保留文本结构信息**对于高质量推理的至关重要性。\n\n**§2 工程与实践贡献**\n- **开源代码与模型**：在GitHub上开源了项目代码（https://github.com/DeepLangAI/LingoEDU），促进了该领域的可复现性和后续研究。\n- **新评测基准（StructBench）**：发布了一个手动标注的、覆盖多格式多语言的文档结构分析基准，为社区提供了宝贵的评估资源。\n- **高效的工程实现**：设计了一个轻量级、低延迟的流水线（4B+0.6B模型），在保持高性能的同时，将成本控制在极低水平（0.17美元/248文档），展示了**专精化小模型**在特定任务上超越通用大模型的潜力，为资源受限的研究者和开发者提供了可行的解决方案。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于**显式压缩**和**结构感知建模**的交叉点。它不是在已有的显式压缩（如LLMLingua）或隐式压缩（如Glyph）路线上进行渐进式改进，而是**开辟了一条新路线**：将文本视为由EDU构成的层次化结构，并通过坐标索引实现可追溯的压缩。它继承了RAG中“检索-生成”的思想，但将检索粒度从句子/段落提升到了**结构化的子树**，并引入了重排序以保持连贯性。因此，本文可被视为连接了“压缩”、“结构分析”和“检索增强生成”三个子领域的一座桥梁。",
    "professor_critique": "**§1 实验设计与评估体系的缺陷**\n- **StructBench的覆盖范围有限**：尽管包含了10种体裁，但总共只有248个文档，**样本量相对较小**，可能无法全面代表所有类型的复杂文档结构（如代码仓库、医疗记录、法律合同）。其评估仅关注“结构骨架”的顶层层次，**忽略了更深层次、更细粒度的话语关系**（如EDU间的修辞关系：因果、对比、详述等）的评估。\n- **LongBench评估指标的单一性**：在LongBench上的评估主要使用了任务特定的准确性指标（如F1、ROUGE），但**缺乏对压缩本身质量的直接评估**，例如压缩率、信息保留度（通过人工或自动评估）、压缩后文本的流畅性和连贯性。没有报告压缩前后的平均token减少比例，使得效率提升的量化不够清晰。\n- **基线对比的公平性**：与通用LLM（如GPT-4.1）在StructBench上对比时，使用了“特定提示”指令其输出结构。但**提示工程的质量和一致性**可能极大地影响结果，论文未详细说明这些提示的具体内容，也未进行充分的提示优化，这可能低估了通用LLM的潜力。\n- **缺少与最新SOTA压缩方法的直接对比**：虽然提到了Glyph和C3，但在主实验中（LongBench Table 4）只将Glyph和C3作为参考，并未将其与本文方法在相同设置下进行**端到端的性能比较**。特别是缺少与同样强调结构保留的最新方法（如果有）的对比。\n\n**§2 方法论的理论漏洞或工程局限**\n- **EDU分割的模糊性与误差传播**：EDU的定义（“最小可变长度单元”）本身存在一定主观性。LingoEDU的EDU分割步骤是自动化的，任何分割错误都会**传播到后续的结构树构建和检索阶段**，导致整个流水线失败。论文未评估EDU分割的准确性，也未分析分割错误对最终下游任务性能的影响。\n- **排序模块的语义表示瓶颈**：排序模块使用节点摘要 \\(h_j\\) 和代表文本片段 \\(t_{\\mathrm{rep}}\\) 来计算相关性。如果摘要 \\(h_j\\) 未能准确概括节点内容，或者 \\(t_{\\mathrm{rep}}\\) 选择不当，**排序准确性将严重下降**。论文未探讨不同摘要生成质量或片段选择策略对排序效果的影响。\n- **预算感知贪心选择的次优性**：贪心选择策略可能在全局上",
    "source_file": "From Context to EDUs Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition.md"
}