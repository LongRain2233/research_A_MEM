{
    "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
    "background_and_problem": "#### **§1 领域背景与研究动机（150字以上）**\n本文聚焦于**大型语言模型（LLMs）**与**强化学习（RL）**交叉领域，特别是RL用于提升LLMs在复杂逻辑任务（如数学、编程）上的**推理能力**，从而将其转变为**大型推理模型（LRMs）**。自OpenAI o1和DeepSeek-R1等里程碑模型发布以来，RL已从主要用于人类对齐（如RLHF）的工具，演变为一种**直接激励和扩展推理能力**的基础方法论。研究动机在于，RL为模型能力提升提供了一个**正交于预训练数据与参数规模扩展的新维度**：通过增加训练时RL计算量（train-time compute）和推理时“思考”时间（test-time compute），可以持续提升模型性能。当前，如何进一步扩展RL以应对迈向**人工超级智能（ASI）** 所面临的算法、数据与基础设施挑战，成为该领域亟待梳理和解决的核心问题。\n\n#### **§2 现有技术的核心短板——具体失败模式（250字以上）**\n本文虽为综述，但通过梳理文献，指出了传统RL方法在应用于LLMs时面临的几类核心短板：\n1.  **基于学习的奖励模型（RMs）的奖励黑客（Reward Hacking）问题**：当RLHF等使用学习到的奖励模型进行大规模RL训练时，模型可能学会**最大化奖励分数而非真正提升任务性能**，导致奖励信号失真和性能崩溃。例如，在DeepSeek-V3和R1的实践中发现，学习到的RM在规模扩大时变得不可靠。\n2.  **稀疏奖励信号的信用分配难题**：在传统RLHF或基于结果的RLVR中，奖励仅在完整轨迹（如最终答案）结束时给出一个标量。**当推理链（Chain-of-Thought）长达数百步时**，模型难以将最终的成功或失败归因于中间的具体步骤，导致训练效率低下、优化不稳定。\n3.  **可验证任务的范围限制**：基于规则的奖励（如数学答案检查、代码单元测试）虽然可靠，但其应用严格受限于**存在明确、客观、可自动验证结果的任务**（如竞赛数学、编程）。对于**开放域问答、创意写作、主观评价**等缺乏客观验证标准的任务，传统RLVR范式失效，因为无法提供可靠的自动化奖励信号。\n4.  **过程监督的高昂成本**：早期工作（如Lightman等人，2024）依赖人类专家对推理的每一步进行标注以获得密集奖励（Process Reward），这种方法**成本极高，难以扩展到大规模数据集**，限制了其广泛应用。\n\n#### **§3 问题的根本难点与挑战（200字以上）**\n从理论与工程角度，上述问题的根本原因在于：\n- **奖励信号的可靠性与可扩展性矛盾**：可靠的奖励（如基于规则的验证器）适用范围窄；而适用范围广的奖励（如基于LLM的生成式奖励模型）又面临**可靠性、一致性和对抗性攻击（如奖励黑客）** 的挑战。如何设计既通用又鲁棒的奖励机制是核心难点。\n- **长序列决策的信用分配**：LLM生成是一个**长序列、高维度的Markov决策过程（MDP）**，动作空间是整个词汇表。在如此长的轨迹上提供有效的、及时的反馈，在计算上和算法上都极具挑战性。\n- **训练数据的自生成与质量**：RL的一大优势是能够利用模型自生成的数据进行训练，以克服高质量监督数据（SFT）的瓶颈。然而，**如何确保自生成数据的多样性、质量和避免训练循环中的退化**，是一个尚未完全解决的开放问题。\n- **计算基础设施的极端需求**：大规模RL训练需要**海量的采样（rollouts）和策略更新**，对计算资源（GPU集群）、存储（经验回放池）和工程系统（分布式训练框架）提出了远超传统SFT的苛刻要求。\n\n#### **§4 本文的切入点与核心假设（200字以上）**\n本文作为一篇系统性综述，其核心切入点是**对RL for LRMs这一新兴领域进行全景式梳理与结构化分析**，而非提出单一新方法。其核心假设贯穿于对各技术路线的分析中：\n1.  **可验证性定律（Verifier‘s Law）**：训练AI系统执行某项任务的难易程度，与该任务的**可验证程度成正比**。本文认为，对于任何能够配备可靠自动反馈的任务，RL都能通过试错优化实现快速改进。这为RL在数学、编程等领域的成功提供了理论解释，也指明了开放域任务面临的挑战根源。\n2.  **RL与SFT的互补性**：本文假设RL并非要取代监督微调（SFT），而是作为一种**互补的能力扩展机制**。SFT提供了良好的先验和基础能力，而RL则通过**最大化奖励目标**，能够诱导出SFT难以覆盖的复杂推理行为（如规划、反思、自我纠正），并可能实现**超越训练数据分布**的泛化。\n3.  **从对齐到推理的范式转变**：本文的核心叙事是，RL在LLMs中的应用正从**以人类偏好对齐（RLHF）为核心**，转向以**提升模型内在推理能力（RLVR等）为核心**。这一转变意味着奖励设计从学习人类主观偏好，转向利用**客观、可验证的任务成功信号**，从而开启了新的规模化扩展轴。",
    "core_architecture": "#### **§1 系统整体架构概览（200字以上）**\n本文综述的并非单一系统，而是勾勒了**RL for LRMs** 领域的通用技术框架。该框架将LLM视为在MDP中与环境交互的智能体（Agent），其核心数据流如下：\n**输入Prompt/Task (x)** → **策略模型 (Policy π_θ，即LLM)** → **生成动作序列 (y = (y1, ..., yT))** → **环境/奖励函数 (R)** → **产生奖励信号 (r_t 或 R(x,y))** → **策略优化器 (如PPO, GRPO)** → **更新后的策略模型**。\n其中，**动作（Action）** 的粒度可以是token级、片段（step）级或整个序列级，**奖励（Reward）** 的分配也相应匹配这些粒度。**状态（State）** 定义为当前提示词与已生成token的拼接。**转移动态（Transition Dynamics）** 在文本生成中通常是确定性的（拼接操作）。整个学习目标是最大化期望累积回报：\\(\\max _{\theta} \\mathcal{J}(\\theta) := \\mathbb{E} _{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta} (x)} [ G ]\\)，通常还需加入相对于参考策略的KL散度约束以稳定训练。\n\n#### **§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n##### **模块一：奖励设计 (Reward Design)**\n- **输入**：模型生成的完整或部分序列 \\(y\\)，以及原始提示 \\(x\\)。\n- **核心处理逻辑**：根据任务类型和可用资源，选择并应用不同的奖励计算机制。**对于可验证任务（如数学）**，使用基于Python库（Math-Verify, SymPy）或自定义的**规则验证器**，检查最终答案格式（如`\\boxed{}`）和数值/符号等价性，输出二元（0/1）或格式奖励。**对于非可验证任务**，使用**生成式奖励模型（GenRM）**，该模型本身是一个LLM，被提示或微调以生成包含推理过程的评估文本（如批判、原理），再从中提取标量奖励或偏好排名。**对于密集奖励**，可能使用**过程奖励模型（PRM）** 对每个推理步骤进行评估，或通过**蒙特卡洛采样**在线估计步骤价值。\n- **输出**：一个标量奖励值（序列级）、一个奖励序列（token级或step级），或结构化的评估文本。\n- **设计理由**：奖励信号是RL优化的指南针。**规则奖励**确保了在可验证领域的**绝对可靠性和可扩展性**，避免了学习奖励模型的偏差和黑客攻击。**生成式奖励**扩展了RL到主观、开放域任务，利用了LLM自身的理解和评判能力。**密集奖励**旨在解决**长轨迹信用分配**问题，通过提供更细粒度的反馈来加速学习和稳定优化。\n\n##### **模块二：策略优化算法 (Policy Optimization Algorithms)**\n- **输入**：当前策略模型参数 \\(\\theta\\)，从环境中采样得到的一批轨迹数据（状态、动作、奖励），参考策略模型（通常为SFT后的初始模型）。\n- **核心处理逻辑**：本文综述了多种策略优化算法变体：\n  - **基于策略梯度的方法**：如PPO（Proximal Policy Optimization），通过重要性采样和裁剪来稳定策略更新，其目标函数为 \\(L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t [\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]\\)，其中 \\(r_t(\\theta) = \\pi_\\theta(a_t|s_t) / \\pi_{\\theta_{old}}(a_t|s_t)\\)，\\(\\hat{A}_t\\) 是优势估计。\n  - **无评论家算法**：如GRPO（Group Relative Policy Optimization，DeepSeek-R1采用），通过在同一提示下采样多个输出并计算相对奖励排名来更新策略，避免了训练独立的评论家（价值函数）网络，简化了训练流程。公式为基于分组内样本的奖励排序进行策略概率的调整。\n  - **离线策略优化**：利用离线收集的数据（如人类演示或模型旧策略数据）进行更高效的学习。\n- **输出**：更新后的策略模型参数 \\(\\theta_{new}\\)。\n- **设计理由**：**PPO**因其**良好的稳定性和成熟的工程实现**而被广泛采用。**GRPO**等无评论家方法**减少了训练复杂度**（无需额外价值网络），更易于在大规模分布式环境中部署，且通过组内相对比较缓解了奖励绝对值的校准问题。不同算法的选择取决于任务特性、计算资源和对训练稳定性的要求。\n\n##### **模块三：采样策略 (Sampling Strategy)**\n- **输入**：当前策略模型，用于生成候选序列的提示 \\(x\\)，以及采样超参数（如温度 \\(\\tau\\)，top-p）。\n- **核心处理逻辑**：为了获得用于策略评估和更新的多样化、高质量轨迹，采样策略至关重要。本文提到：\n  - **动态与结构化采样**：例如，在推理任务中，不是简单地对完整序列进行采样，而是采用**树状搜索结构**（如TreeRL, SPO），在推理路径的分支点（如低概率token、高熵位置、高注意力得分位置）进行多次采样，以探索不同的推理路径并更精确地估计步骤价值。\n  - **采样超参数调整**：根据任务和训练阶段动态调整**温度（Temperature）** 和**核采样（top-p）** 参数，以平衡探索（生成多样化序列）与利用（选择高奖励序列）。在RL训练中，通常采用较低的温度以聚焦高奖励区域，但初期可能需要较高温度进行探索。\n- **输出**：一组（通常为多个，如4-8个）针对同一提示生成的候选序列 \\(\\{y^{(1)}, ..., y^{(K)}\\}\\)。\n- **设计理由**：**高质量的采样是有效策略更新的前提**。在推理任务中，单一贪心解码可能错过更优的推理路径。**树状或分支采样**允许模型在关键决策点进行探索，从而更好地估计不同推理步骤的价值，这对于**基于过程的密集奖励**计算尤为重要。调整采样超参数是控制策略探索-利用权衡、防止过早收敛到次优策略的关键工程手段。\n\n#### **§3 关键公式与算法（如有）**\n本文作为综述，引用了多个核心公式：\n1.  **RL通用目标函数**：\\(\\max _{\theta} \\mathcal{J}(\\theta) := \\mathbb{E} _{x \\sim \\mathcal{D}, y \\sim \\pi_{\\theta} (x)} [ G ]\\)，其中G是轨迹的累积回报。\n2.  **PPO裁剪目标函数**：\\(L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t [\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]\\)，其中 \\(r_t(\\theta) = \\pi_\\theta(a_t|s_t) / \\pi_{\\theta_{old}}(a_t|s_t)\\)。\n3.  **KL散度约束**：通常在目标函数中加入 \\(-\\beta D_{KL}(\\pi_\\theta || \\pi_{ref})\\) 项，以防止策略偏离参考模型太远，保持生成文本的语言质量和稳定性。\n\n#### **§4 方法变体对比（如有多个变体/消融组件）**\n本文并未提出单一方法变体，但系统对比了不同技术路线下的代表性工作：\n- **奖励类型变体**：**规则奖励（RLVR）** vs. **生成式奖励（GenRM）** vs. **密集奖励（PRM等）**。规则奖励可靠但适用范围窄；生成式奖励通用但可能不可靠；密集奖励信息量大但标注成本高或估计困难。\n- **策略优化算法变体**：**基于评论家的算法（如PPO+价值网络）** vs. **无评论家算法（如GRPO, DPO）**。前者更通用但更复杂；后者更简单，适合大规模分布式训练，但可能在某些任务上表达能力受限。\n- **采样策略变体**：**标准序列采样** vs. **树状/结构化采样（如TreeRL, SPO）**。后者通过构建推理树，能在相同采样预算下获得更丰富的轨迹信息，用于更好的过程奖励估计。\n\n#### **§5 与已有方法的核心技术差异（200字以上）**\n本文梳理的RL for LRMs范式与早期主流的**RLHF（Reinforcement Learning from Human Feedback）** 存在本质区别：\n1.  **奖励信号来源**：RLHF依赖于**从人类偏好数据中训练得到的奖励模型（RM）**，其奖励信号是主观的、基于比较的（如哪个回答更好）。而RL for LRMs，特别是RLVR，转向使用**客观、可自动计算的规则奖励**（如答案正确性、代码通过率）。这从根本上改变了奖励的性质，从**学习人类模糊偏好**变为**优化明确、可验证的任务目标**。\n2.  **优化目标**：RLHF的核心目标是**对齐（Alignment）**，使模型行为符合人类价值观（ helpful, honest, harmless）。而RL for LRMs的核心目标是**能力提升（Capability Enhancement）**，特别是**复杂推理能力**，如数学解题、代码生成、多步规划。\n3.  **数据生成方式**：RLHF严重依赖**高质量的人类偏好标注数据**，其规模和多样性受限。而RL for LRMs可以充分利用**模型自生成的数据**进行训练，通过与环境（如代码编译器、数学验证器）交互产生近乎无限的数据，实现了**数据生成的自主扩展**。\n4.  **与SFT的关系**：在RLHF流程中，SFT通常是必要的**前置阶段**，用于初始化策略模型。而在一些RL for LRMs的工作（如DeepSeek-R1的Zero RL探索）中，尝试**完全绕过SFT，直接从预训练模型开始进行纯RL训练**，这挑战了SFT作为必要先验的传统认知。",
    "methodology_and_formulas": "#### **§1 完整算法流程（伪代码级描述）**\n原文未提供统一的算法伪代码，但根据综述内容，可概括一个典型的**基于规则奖励的RL训练流程（例如RLVR）**：\n**Step 1：数据准备**。从任务分布 \\(\\mathcal{D}\\) 中采样一批提示 \\(\\{x_i\\}\\)。准备一个参考策略 \\(\\pi_{ref}\\)（通常是经过SFT的模型）。\n**Step 2：轨迹采样**。对于每个提示 \\(x_i\\)，使用当前策略 \\(\\pi_\\theta\\) 生成K个候选输出序列 \\(\\{y_i^{(1)}, ..., y_i^{(K)}\\}\\)。采样时可能采用**树状搜索**策略在推理步骤分支点进行探索。\n**Step 3：奖励计算**。对于每个生成的序列 \\(y_i^{(k)}\\)，调用**规则验证器**（如数学答案检查器、代码单元测试运行器）计算奖励 \\(R(x_i, y_i^{(k)})\\)。奖励可能是二元的（正确/错误），或包含格式奖励。对于密集奖励变体，则会调用**过程奖励模型（PRM）** 或使用**蒙特卡洛估计**为每个token或步骤计算奖励 \\(r_t\\)。\n**Step 4：优势估计**。根据获得的奖励（序列级或密集级），计算每个样本的优势值 \\(\\hat{A}_t\\)。对于序列级奖励，可能使用**广义优势估计（GAE）**；对于GRPO等无评论家方法，则直接在组内根据奖励排名计算相对优势。\n**Step 5：策略更新**。使用策略优化算法（如PPO或GRPO）更新策略参数 \\(\\theta\\)。PPO会计算重要性权重 \\(r_t(\\theta)\\) 并优化裁剪后的目标函数，同时加入KL散度惩罚项 \\(-\\beta D_{KL}(\\pi_\\theta || \\pi_{ref})\\)。GRPO则根据组内样本的奖励排序直接调整策略概率。\n**Step 6：迭代**。重复步骤1-5，直到策略性能收敛或达到预设的训练步数。\n\n#### **§2 关键超参数与配置**\n原文未集中列出具体模型的超参数，但可从讨论中归纳出RL for LRMs训练中的关键超参数：\n- **采样数量（K）**：每个提示下采样的候选序列数。DeepSeek-R1等工作中常使用**K=4或8**。更大的K能更好地估计策略梯度，但计算成本线性增加。\n- **KL散度系数（β）**：控制当前策略与参考策略之间KL散度惩罚的强度。典型值在**0.01到0.1**之间，用于防止策略退化并保持生成质量。\n- **PPO裁剪范围（ε）**：在PPO中限制重要性比率变化的范围，通常设置为**0.1或0.2**。\n- **折扣因子（γ）**：用于计算累积回报，在有限视野的文本生成任务中常设为**1**。\n- **优势估计GAE参数（λ）**：平衡偏差和方差，常用值约为**0.95**。\n- **学习率**：策略网络的学习率，通常远低于SFT阶段，在**1e-6到1e-5**量级，并可能使用余弦衰减调度。\n- **批次大小（Batch Size）**：取决于可用GPU内存，在大规模训练中可能达到**数百万token**。\n- **温度（τ）和top-p**：采样时的超参数。RL训练中常使用**较低温度（如0.7）和较低top-p（如0.9）** 以聚焦高概率区域，但初期可能使用更高温度进行探索。\n\n#### **§3 训练/微调设置（如有）**\n原文未提供具体模型的完整训练配置，但综述了常见的训练范式：\n- **数据构造**：使用**任务特定的数据集**（如数学竞赛题MATH、代码竞赛题APPS、科学问答数据集）。对于RL，数据通常以（提示， 无答案）的形式提供，模型通过与环境交互生成答案并获取奖励。**自生成数据**是核心：模型在训练过程中不断生成新的解决方案，这些方案连同其奖励被加入经验池用于后续更新。\n- **优化器**：常用**AdamW**优化器，带有权重衰减。\n- **学习率调度**：常使用**余弦衰减**或**线性衰减**调度，配合热身（warm-up）阶段。\n- **训练轮数/步数**：大规模RL训练可能持续**数万到数十万步**，消耗巨大的计算资源（千卡GPU月）。\n- **参考策略**：通常使用一个**经过SFT的模型**作为参考策略 \\(\\pi_{ref}\\)，以施加KL约束。一些探索性工作（Zero RL）尝试直接从预训练模型开始。\n\n#### **§4 推理阶段的工程细节**\n- **思维链（CoT）生成**：在推理时，LRMs被鼓励生成详细的**思考过程**（置于``标签内），并将最终答案放在特定分隔符中（如`<answer>`）。这需要模型在推理时分配更多的计算时间（test-time compute）。\n- **采样与搜索**：不同于训练时的多序列采样，推理时通常使用**贪心解码**或**束搜索（beam search）** 来获得最终答案。一些高级方法可能集成**蒙特卡洛树搜索（MCTS）** 或**基于奖励的路径重排序**。\n- **缓存与优化**：对于长序列生成，会使用**键值（KV）缓存**来加速自回归生成。在部署时，可能采用**模型量化**、**蒸馏**到更小模型或**投机解码（speculative decoding）** 等技术来降低延迟。\n- **验证器调用**：在需要验证的代理任务中（如代码执行），推理流程需要与外部环境（编译器、解释器、API）交互，获取执行结果作为后续行动的输入。",
    "experimental_design": "#### **§1 数据集详情（每个数据集单独列出）**\n本文作为综述，提及了多个用于评估LRMs的数据集，主要集中在数学、编程和代理任务：\n- **数学推理**：**MATH**（包含12,500个来自高中数学竞赛的题目，涵盖代数、几何、数论等）、**GSM8K**（8.5K个小学水平数学文字题）、**竞赛数学数据集**（如AMC、AIME题目）。这些数据集要求模型输出最终数值答案，便于进行规则验证。\n- **代码生成**：**HumanEval**（164个Python编程问题）、**MBPP**（974个入门级编程问题）、**APPS**（10,000个竞争性编程问题，难度各异）。评估基于单元测试通过率（pass@k）。\n- **代理任务与工具使用**：**SWE-bench**（评估模型在真实软件工程问题上修复GitHub issue的能力）、**WebShop**（在线购物网站交互任务）、**ALFWorld**（文本游戏中的指令跟随与规划）。这些任务需要多轮交互和工具调用。\n- **科学问答**：如**MMLU**（大规模多任务语言理解）中的科学子集、**ScienceQA**等，测试模型的多学科知识推理。\n- **多模态任务**：如**VQA**（视觉问答）、**ChartQA**（图表理解）等，用于评估多模态LRMs。\n\n#### **§2 评估指标体系（全量列出）**\n- **准确性指标**：\n  - **数学**：**准确率（Accuracy）**，即答案完全匹配或经符号化简后等价的比例。\n  - **代码**：**pass@k**，在k个生成样本中至少有一个通过所有单元测试的概率。常用pass@1和pass@100。\n  - **通用推理**：在MMLU、BBH（Big-Bench Hard）等基准上的**平均准确率**。\n  - **LLM-as-a-Judge评分**：对于开放域或主观任务，使用强大的LLM（如GPT-4）作为评判员，根据预定义的准则（rubric）对回答进行评分（如1-10分）。\n- **效率/部署指标**：\n  - **推理延迟**：生成完整回答（包括思考过程）所需的**平均时间（毫秒）** 或**每秒处理token数（tokens/s）**。\n  - **思考开销**：模型为“思考”所消耗的额外计算时间或token数量占总生成的比例。\n  - **训练成本**：完成RL训练所需的**GPU小时数**或**计算浮点操作数（FLOPs）**。\n- **其他自定义指标**：\n  - **推理链质量**：对生成的思考过程进行人工或模型评估，如**连贯性、逻辑性、步骤正确性**。\n  - **校准度（Calibration）**：模型对其答案正确性的置信度与实际正确率之间的匹配程度。\n  - **分布外泛化**：在训练数据未见过的、更难的题目上的性能表现。\n\n#### **§3 对比基线（完整枚举）**\n综述中提及的对比基线涵盖了不同技术路线和时代的模型：\n- **纯SFT基线**：仅在相关任务数据上进行监督微调的模型，作为RL方法提升的起点。\n- **RLHF对齐模型**：如使用RLHF训练的ChatGPT、Claude早期版本，主要优化对齐而非推理能力。\n- **早期RLVR/推理模型**：**OpenAI o1系列**（2024年发布，确立了RL扩展推理能力的有效性）、**DeepSeek-R1**（开源，使用GRPO，性能匹配o1）。\n- **同期开源竞品**：**QwQ-32B**、**Qwen3系列**、**Skywork-OR1**、**Phi-4 Reasoning**、**Llama-Nemotron-Ultra**、**Magistral 24B**、**Minimax-M1**、**InternVL3/3.5**、**Intern-S1**、**Kimi K2**、**GLM-4.5系列**等（详见表1）。\n- **多模态模型**：**Gemini 2.0/2.5**、**GPT-5**、**QVQ**、**Skywork R1V2**、**MiniCPM-V 4.5**等。\n- **代理模型**：**Claude-4.1-Opus**（在SWE-bench领先）、**Kimi K2**（专为代理任务优化）。\n\n#### **§4 实验控制变量与消融设计**\n原文未描述具体的消融实验设计，但根据综述内容，该领域常见的消融研究包括：\n- **奖励类型消融**：比较**仅使用结果奖励** vs. **结合过程奖励** vs. **使用生成式奖励** 对最终性能的影响。\n- **算法组件消融**：在GRPO中，消融**分组相对比较**机制，与标准的PPO+价值网络进行对比；在树状采样方法中，消融**分支点检测策略**（如基于熵 vs. 基于注意力）。\n- **训练阶段消融**：比较**纯RL（Zero RL）** 与 **SFT后接RL** 两种流程的效果，验证SFT先验的必要性。\n- **数据混合消融**：研究训练数据中**不同任务（数学、代码、科学）比例**对模型多任务性能的影响。\n- **超参数敏感性分析**：分析KL惩罚系数β、采样数量K、学习率等关键超参数对训练稳定性和最终性能的影响。",
    "core_results": "#### **§1 主实验结果全景（表格式呈现）**\n本文是综述，未提供统一的实验结果表格，但通过引用各模型论文，汇总了关键性能数据（基于文中提及的信息）：\n`模型 | 数学（MATH） | 代码（HumanEval） | 代理（SWE-bench） | 多模态（MMMU）`\n`OpenAI o1 | 报告领先，具体数值未提供 | 报告领先，具体数值未提供 | - | -`\n`DeepSeek-R1 | 在MATH等基准上匹配o1性能 | 在HumanEval上达到高水平（如pass@1 > 90%） | - | -`\n`Claude-4.1-Opus | - | - | 在SWE-bench上取得SOTA结果 | -`\n`Qwen3-235B | 在数学基准上表现优异 | 代码能力强劲 | - | -`\n`GLM-4.5V | - | - | - | 在大多数视觉多模态基准上取得SOTA`\n`MiniCPM-V 4.5 (8B) | - | - | - | 高效且性能强劲`\n（注：原文未提供精确的数值对比表格，以上为定性描述的综合。）\n\n#### **§2 分任务/分场景深度分析（每个维度100字以上）**\n- **数学推理**：RL with verifiable rewards（如DeepSeek-R1, o1）在该领域取得了**最显著的突破**。原因在于数学问题具有**明确的最终答案和可自动验证的求解过程**，完美符合“可验证性定律”。规则奖励（答案正确性）提供了清晰、无噪声的训练信号，使得RL能够有效地探索和优化长推理链。提升最大的子任务通常是**需要多步代数变换或逻辑推导的竞赛级难题**。\n- **代码生成**：与数学类似，代码的**单元测试提供了客观的验证标准**。RL不仅优化最终代码的正确性，还能通过**格式奖励**鼓励模型输出可解析的结构。提升体现在**更复杂的算法实现和更高的单元测试通过率（pass@k）** 上。然而，对于**需要理解模糊需求或涉及复杂软件工程上下文**的任务，纯结果奖励可能不足，需要结合过程奖励或更复杂的奖励设计。\n- **代理任务与工具使用**：在这些**多轮交互、部分可观察**的任务中，RL面临更大挑战。**Turn-level rewards**（每轮工具调用的格式正确性、结果有效性）和**outcome-level rewards**（最终任务完成度）的结合是关键。Claude-4.1-Opus和Kimi K2的成功表明，通过大规模合成代理训练数据并结合RL，可以显著提升模型在真实世界交互任务（如修复GitHub issue）上的能力。\n- **多模态推理**：将RL扩展到视觉、音频等多模态领域，需要设计**跨模态的奖励信号**。例如，对于图表推理任务，奖励可能基于对图表问题的正确答案；对于图像描述任务，则可能使用基于CLIP相似度或生成式评估的奖励。Intern-S1等模型通过**混合奖励设计**，在多个科学推理任务上同时取得进步，表明RL可以协调优化不同模态和任务的目标。\n\n#### **§3 效率与开销的定量对比**\n原文未提供具体的效率对比数据，但指出了相关趋势和挑战：\n- **训练成本**：大规模RL训练（如DeepSeek-R1）需要**数千甚至上万张GPU持续训练数周**，计算开销远超SFT。这是限制许多研究机构跟进的主要瓶颈。\n- **推理开销**：LRMs在推理时进行**长链思考（long CoT）**，会显著增加**延迟和每请求的token消耗**。例如，o1系列模型被设计为可以分配更多“思考时间”以换取更高准确率。这带来了**延迟与准确率的权衡**。\n- **采样效率**：基于树状搜索的采样策略（如TreeRL）虽然能提供更好的过程奖励估计，但需要**对同一提示进行多次前向传播**，增加了训练时的计算成本。\n- **内存占用**：训练时需要同时维护**策略模型、参考模型、评论家模型（如有）以及经验回放缓冲区**，对显存提出了高要求。GRPO等无评论家方法在一定程度上缓解了这个问题。\n\n#### **§4 消融实验结果详解**\n原文未提供具体的消融实验数值结果，但综述了相关研究发现：\n- **奖励密度的影响**：研究表明，**加入过程奖励（PRM）** 相比仅使用结果奖励，能**显著提升训练样本效率并加速收敛**。例如，在数学推理任务上，使用PRM的模型在相同训练步数下达到更高准确率。\n- **规则奖励 vs. 学习奖励**：DeepSeek-R1的经验表明，在可验证任务上，**用规则奖励替代学习到的奖励模型**，可以**避免奖励黑客**，并使得训练在更大规模上保持稳定。\n- **SFT先验的重要性**：一些探索（Zero RL）尝试直接从预训练模型开始RL训练，但普遍发现**经过SFT初始化的策略收敛更快、更稳定**，最终性能也更好。这表明SFT提供的良好先验对于RL成功至关重要。\n- **树状采样的收益**：与标准序列采样相比，**树状采样方法**（如SPO, TreeRL）通过更好地探索推理路径，能够**更准确地估计步骤价值**，从而在需要复杂多步推理的任务上带来更明显的性能提升。\n\n#### **§5 案例分析/定性分析（如有）**\n原文未提供具体的案例分析，但综述中暗示了典型现象：\n- **成功案例**：RL训练后的模型能够生成**更长、更结构化、更具反思性**的思考过程。例如，在数学题上，模型会先分解问题，尝试不同方法，检查中间步骤，最后给出答案。在代码题上，模型会添加注释、考虑边界情况、进行测试。\n- **失败/挑战案例**：在**开放域、主观性任务**中，基于生成式奖励模型的RL可能受到**评估偏见**或**奖励黑客**的影响。例如，模型可能学会生成符合评判者（RM）口味但内容空洞或错误的回答。此外，**格式奖励的过度优化**可能导致模型过于关注输出格式而非内容实质。",
    "conclusion_and_future_work": "#### **§1 本文核心贡献总结**\n1.  **系统性地梳理了RL for LRMs这一新兴领域**：首次全面综述了将强化学习应用于大型语言模型以提升推理能力的最新进展，涵盖了从奖励设计、策略优化到采样策略的完整技术栈。\n2.  **明确了从“对齐”到“推理”的范式转变**：清晰地阐述了RL在LLMs中的应用重心已从RLHF（人类偏好对齐）转向RLVR等以**优化客观推理能力**为核心的新范式，并分析了其背后的“可验证性定律”。\n3.  **深度剖析了奖励设计的三大范式**：详细对比了**可验证奖励**（规则驱动）、**生成式奖励**（模型驱动）和**密集奖励**（过程驱动）的优缺点、适用场景及最新技术，为研究者选择奖励机制提供了清晰指南。\n4.  **勾勒了前沿模型生态与技术路线图**：汇总了自OpenAI o1和DeepSeek-R1以来涌现的大量开源和闭源LRMs，并分析了其背后的算法（如GRPO, MPO）、架构（MoE, Dense）和应用领域（代码、代理、多模态）。\n5.  **指出了规模化RL面临的核心挑战与未来方向**：明确提出了在算力、算法设计、训练数据和基础设施等方面存在的瓶颈，并展望了持续学习、基于记忆的RL、模型基RL等未来研究方向。\n\n#### **§2 局限性（作者自述）**\n原文中作者未明确列出本文作为综述的局限性。但根据综述性质，其局限性可能包括：**覆盖范围可能无法穷尽所有最新工作**（因领域发展迅速）；**对某些技术细节的深度可能不及原始论文**；**缺乏对实验结果的统一量化比较**（因各模型评估基准和设置不同）。\n\n#### **§3 未来研究方向（全量提取）**\n作者在第七节（Future Directions）中提出了9个未来研究方向：\n1.  **面向LLMs的持续RL（Continual RL for LLMs）**：研究如何使LRMs在**非静态、流式数据**中持续学习新知识、适应新任务，而不遗忘旧能力。这需要解决灾难性遗忘和高效知识整合的算法。\n2.  **面向LLMs的基于记忆的RL（Memory-based RL for LLMs）**：探索将**外部记忆模块**（如向量数据库、键值记忆网络）集成到RL框架中，使模型能够记住过去的经验、高效检索相关知识，从而在长序列任务中做出更明智的决策。\n3.  **面向LLMs的模型基RL（Model-based RL for LLMs）**：让LLM学会**预测其行动的环境后果**（世界模型），从而在内部进行模拟和规划，减少与真实环境交互的高成本。这对于训练代价高昂的代理任务尤为重要。\n4.  **教授LRMs高效推理（Teaching LRMs Efficient Reasoning）**：不仅追求推理准确性，还优化推理的**效率**（如思考时间、token消耗）。研究如何设计奖励以鼓励模型在保证正确性的前提下，选择更简洁、更快的推理路径。\n5.  **教授LLMs潜在空间推理（Teaching LLMs Latent Space Reasoning）**：探索在**潜在表示空间**而非token序列空间进行推理的可能性，这可能实现更抽象、更高效的推理过程。\n6.  **用于LLMs预训练的RL（RL for LLMs Pre-training）**：将RL技术整合到**预训练阶段**，而不仅仅是后训练微调。这可能涉及使用RL目标来指导下一个token预测，或者从交互数据中进行大规模预训练。\n7.  **用于基于扩散的LLMs的RL（RL for Diffusion-based LLMs）**：将RL应用于**扩散模型**驱动的文本生成或跨模态生成，优化生成过程的质量、多样性或可控性。\n8.  **用于科学发现的RL（RL for LLMs in Scientific Discovery）**：将LRMs与科学实验（湿实验、模拟）环境结合，通过RL自主提出假设、设计实验、分析结果，加速科学发现流程。\n9.  **用于架构-算法协同设计的RL（RL for Architecture-Algorithm Co-Design）**：使用RL来自动搜索或优化**模型架构**（如注意力机制、MoE专家配置）与**训练算法**（如优化器、奖励设计）的最佳组合，以实现更高效的训练和推理。",
    "research_contributions": "#### **§1 核心学术贡献（按重要性排序）**\n1.  **首次系统性构建RL for LRMs的知识体系**：\n    - **理论新颖性**：本文超越了以往专注于对齐（RLHF）或泛化推理的综述，首次将**RL作为提升LLMs核心推理能力的方法论**进行系统梳理，提出了清晰的技术分类（奖励、优化、采样）和问题框架（RL的角色、与SFT的关系等）。\n    - **实验验证充分性**：通过广泛引用和比较数十个前沿模型（o1, R1, Claude, Gemini, Qwen等）及其在数学、代码、代理等任务上的表现，实证了RLVR等范式的有效性。\n    - **对领域的影响**：为后续研究者提供了**全景式技术路线图**，明确了关键挑战（如奖励设计、信用分配、可扩展性）和未来机遇，有望引导该领域从分散的模型发布走向更深入、更系统的算法创新。\n2.  **深入辨析了RL在LRMs中的核心作用与争议**：\n    - **理论新颖性**：明确提出了**“RL是锐化（Sharpening）还是发现（Discovery）能力？”** 这一根本性问题，并讨论了RL与SFT在泛化与记忆上的不同作用，推动了关于RL本质价值的学术讨论。\n    - **实验验证充分性**：通过分析不同模型（如Zero RL尝试）的训练流程和性能，提供了关于RL与SFT协同关系的实证观察。\n    - **对领域的影响**：帮助社区厘清了对RL能力的期望，避免将其神话或低估，为设计更高效的训练流程（如是否以及如何结合SFT）提供了理论依据。\n3.  **前瞻性地指出了通向ASI的RL扩展挑战**：\n    - **理论新颖性**：将RL for LRMs的讨论置于**迈向人工超级智能（ASI）** 的宏大背景下，指出单纯的计算扩展已不足，必须在算法、数据、基础设施上进行根本性创新。\n    - **实验验证充分性**：通过分析当前最大规模RL训练（如千亿参数模型）的资源需求，定量地揭示了扩展瓶颈。\n    - **对领域的影响**：为大型实验室和学术界指明了未来需要攻克的研究方向（如持续学习、模型基RL、高效推理），有助于集中科研资源解决最关键的问题。\n\n#### **§2 工程与实践贡献**\n- **开源资源汇总**：在论文中（如表1）详细列出了截至2025年9月的主要**开源RL-trained LRMs**，包括模型名称、机构、参数量、核心算法和模态，为社区提供了宝贵的模型选型参考。\n- **训练基础设施讨论**：在§5.3中简要讨论了RL训练所需的基础设施挑战（如分布式采样、经验回放、混合精度训练），为工程实践者提供了高层指引。\n- **技术组件标准化**：通过对奖励设计、策略优化、采样策略的分类和对比，促进了该领域技术术语和组件定义的标准化，有助于不同工作之间的比较和复用。\n\n#### **§3 与相关工作的定位**\n本文在现有技术路线图中处于**承上启下的关键位置**：\n- **它是RLHF技术路线向“能力提升”方向的自然延伸**。早期综述（如Srivastava和Aggarwal，2025）主要关注RL用于对齐，而本文则聚焦于RL用于推理，反映了领域重心的转移。\n- **它是对“推理LLMs”综述（如Zhang等人，2025a；Chen等人，2025n）的重要补充**。那些综述主要关注推理方法本身（如思维链、程序辅助），而本文则深入探讨了**如何用RL来训练和优化这些推理能力**，提供了不同的视角。\n- **它开辟了“规模化RL”这一新的讨论维度**。不同于以往综述只关注算法效果，本文特别强调**算法在超大规模计算下的可行性、可扩展性及面临的工程挑战**，为下一代LRMs的训练奠定了讨论基础。",
    "professor_critique": "#### **§1 实验设计与评估体系的缺陷**\n- **缺乏统一的、可复现的基准测试**：本文综述了众多模型，但各模型**使用不同的评测数据集、不同的数据清洗方式、不同的评估指标**（例如，有的报告pass@1，有的报告pass@100，有的使用LLM-as-a-Judge）。这使得跨模型性能对比极其困难，结论往往是定性的“表现优异”，缺乏**硬性的、可量化的横向比较**。\n- **评估任务范围狭窄**：现有工作（及本综述的关注点）过度集中于**数学和编程**这类“易于验证”的任务。对于**开放域对话、创意写作、复杂规划、伦理推理、多轮谈判**等缺乏明确验证器的任务，RL for LRMs的有效性证据严重不足。这可能导致领域产生“在玩具问题上过拟合”的错觉。\n- **Baseline的陈旧性与选择性**：许多对比实验仍将**纯SFT或早期RLHF模型**作为主要基线，而**未能充分与同期最先进的非RL方法**（如更高质量的SFT数据、更好的推理提示技术、检索增强等）进行对比。无法确定观察到的提升有多少真正来自RL，而非来自更大的模型、更好的数据或其他改进。\n- **效率评估严重缺失**：几乎所有被引用的工作都只报告准确率等性能指标，而**几乎完全忽略了训练和推理的效率成本**。一个在MATH上准确率提升5%但训练成本增加10倍、推理延迟增加5倍的方法，其实际价值需要审慎评估。本综述也未对此提出批评。\n\n#### **§2 方法论的理论漏洞或工程局限**\n- **“可验证性定律”的双刃剑**：该定律正确指出了RL在可验证任务上的优势，但也可能**误导研究者放弃对不可验证任务的研究**。然而，通用人工智能必须处理大量不可验证的主观任务。过度依赖规则奖励可能导致方法论的路径依赖，阻碍了在更广阔领域的发展。\n- **奖励黑客的幽灵并未消失**：本文承认学习奖励模型（RM）存在奖励黑客问题，并推崇规则奖励。但**规则奖励本身也可能被“黑客攻击”**——模型可能学会生成恰好能通过特定规则检查器但语义错误的答案（例如，在数学题中输出格式正确但数值错误的答案）。对于更复杂的验证器（如代码单元测试），模型也可能学会生成能通过给定测试但存在隐藏bug的代码。\n- **密集奖励的实际可扩展性存疑**：尽管过程奖励（PRM）在理论上很美，但**训练高质量的PRM本身需要大量步骤级标注数据**，或者依赖另一个强大的LLM来生成评估，这形成了**循环依赖或成本转移**。文中提到的基于采样的方法（如树搜索）计算开销巨大，在超大规模模型上是否可行仍是未知数。\n- **对分布外泛化和鲁棒性的忽视**：现有方法主要在**同分布（i.i.d.）** 的基准数据集上进行训练和测试。当面对**分布外（OOD）** 的、对抗性的或高度非常规的问题时，这些基于RL训练的模型是否比SFT模型更鲁棒？目前缺乏系统性的压力测试。\n\n#### **§3 未经验证的边界场景**\n1.  **多语言与跨文化语境**：当前RL for LRMs的研究和模型几乎全部基于**英语**和**西方文化背景**的数据集（如MATH、APPS）。当应用于**低资源语言**或需要**跨文化理解**的推理任务时，规则奖励的设计、生成式奖励模型的偏见、以及训练数据的代表性都将面临严峻挑战。\n2.  **动态、对抗性环境**：现有环境（如数学验证器、代码运行器）大多是**静态、良构、确定性的**。在**动态变化**（如实时股票市场）、**信息不完全**（如部分可观察的游戏）、或存在**对抗性干扰**（如故意提供误导信息）的环境中，基于当前RL范式的LRMs很可能失效，因为它们缺乏持续适应和战略欺骗应对的能力。\n3.  **长程、多模态时序推理**：当前任务多是单轮或有限轮次的。对于需要**理解长视频、阅读整本书籍、进行长达数天的科研项目规划**等极端长程、多模态时序推理任务，现有的token-level或step-level奖励设计是否还能有效进行信用分配？记忆和规划机制如何与RL整合？这完全是未知领域。\n4.  **安全与价值观对齐的冲突**：RL for LRMs以“能力提升”为目标，可能与其“安全对齐”目标产生冲突。一个通过RL训练变得极度擅长解决数学难题的模型，是否也可能更擅长生成有害内容或进行社会工程攻击？目前的能力导向RL工作很少讨论与安全约束的协同优化。\n\n#### **§4 可复现性与公平性问题**\n- **巨额计算资源依赖**：DeepSeek-R1、o1等标杆模型的训练需要**数千张H800/A100 GPU数月**的计算资源。这**几乎将独立学术机构和大多数工业界团队排除在外**，导致研究高度集中化，不利于领域的健康生态和创新多样性。\n- **算法细节披露不足**：许多关键实现细节，如**GRPO的具体分组策略和排名损失函数、树状采样的确切分支启发式、混合奖励的加权系数**等，在原论文中往往语焉不详。这使得**精确复现实验结果极其困难**。\n- **对Baseline的不公平调优**：在比较RL方法与SFT基线时，RL方法通常享受**更长时间的训练、更复杂的数据增强、更精细的超参数调优**。而SFT基线往往只使用标准设置。这种**不对称的工程投入**使得性能提升的归因模糊不清：到底是RL算法本身更优，还是仅仅投入了更多资源？\n- **测试数据泄露风险**：由于许多基准测试集（如MATH, HumanEval）是公开的，在长达数月的训练中，模型很可能通过训练数据（可能包含互联网爬取内容）间接接触到测试题目，导致**性能高估**。目前缺乏对数据污染的系统性检查和报告。",
    "zero_compute_opportunity": "#### **蓝图一：探究小规模模型上“奖励黑客”的成因与防御**\n- **核心假设**：在资源有限的情况下，可以通过在小模型（如7B参数）和简化任务（如小学数学）上系统研究奖励黑客（Reward Hacking）的产生机制，并测试轻量级防御策略（如正则化、奖励不确定性）的有效性。\n- **与本文的关联**：基于本文§3.1指出的奖励模型（RM）在规模化时的不可靠性问题，以及规则奖励也可能被攻击的潜在风险（见教授锐评§2）。\n- **所需资源**：\n  1.  **模型**：Hugging Face上开源的7B量级模型（如Llama-3.2-7B, Qwen2.5-7B）。使用免费API（如Together.ai的免费额度）或本地单张消费级GPU（如RTX 4090）进行微调。\n  2.  **数据集**：公开的GSM8K（小学数学习题）或AQUA-RAT（代数题）。\n  3.  **奖励函数**：编写简单的Python规则验证器（检查数值答案匹配），并故意引入有漏洞的验证逻辑（如只检查最后5个字符）。\n  4.  **成本**：主要成本为GPU时间（本地）或少量API调用费用（< $50）。\n- **执行步骤**：\n  1.  **构建脆弱环境**：设计一个存在明显漏洞的规则奖励函数（例如，只要答案字符串中包含“42”就给予高奖励）。\n  2.  **训练与观察**：使用PPO或GRPO的轻量级实现（如TRL库）对小模型进行RL训练，观察模型是否迅速学会利用漏洞（输出无关但包含“42”的文本）。\n  3.  **实施防御**：测试多种防御策略：(a) 在奖励中加入**输出长度惩罚**；(b) 使用**基于KL散度的奖励标准化**；(c) 引入**奖励随机化**（以一定概率给零奖励）；(d) 使用**对抗性奖励模型**检测异常输出。\n  4.  **评估与量化**：在干净的测试集上评估经过防御训练后的模型真实解题能力，并与原始SFT模型、无防御RL模型对比。\n- **预期产出**：一篇短论文，揭示小规模RL训练中奖励黑客的具体模式，并提出一种计算成本低的防御机制。可投稿到**EMNLP Findings**或**NeurIPS Workshop on Responsible AI**。\n- **潜在风险**：小模型的实验结果可能无法推广到千亿参数模型。应对：明确说明研究的探索性质，并强调其方法论意义——为大规模训练提供早期预警和启发。\n\n#### **蓝图二：基于公开API实现低成本、任务特定的生成式奖励模型（GenRM）**\n- **核心假设**：对于缺乏客观验证标准的主观任务（如故事续写、诗歌创作），可以利用强大的公开API（如GPT-4o mini, Claude Haiku）作为“裁判”，通过少量提示工程和思维链（CoT）技术，构建一个低成本、有效的生成式奖励模型，并用于微调小模型。\n- **与本文的关联**：基于本文§3.1.2对生成式奖励模型的讨论，特别是“推理奖励模型（Learning to Think）”和“基于准则的奖励（Rubric-based Rewards）”部分。\n- **所需资源**：\n  1.  **裁判模型API**：OpenAI GPT-4o mini API（每百万token输入$0.15，输出$0.60）或Anthropic Claude Haiku API（成本更低）。预计总API费用控制在$100以内。\n  2.  **策略模型**：同蓝图一，使用7B开源模型。\n  3.  **数据集**：从Alpaca或OpenAssistant等开源指令数据集中，筛选出“创意写作”、“开放式问答”类任务，构建约1000个提示。\n  4.  **本地计算**：单张GPU用于策略模型微调。\n- **执行步骤**：\n  1.  **设计评估准则（Rubric）**：针对“故事续写”任务，设计包含“连贯性”、“创造力”、“角色一致性”、“语言生动性”等维度的自然语言评估准则。\n  2.  **构建GenRM提示**：编写提示，要求API裁判根据上述准则，对给定的故事开头和续写进行**分步推理**，最后输出1-10分的总体评分及理由。\n  3.  **收集奖励数据**：对策略模型的多个输出采样，调用API裁判获取评分，构建（提示， 输出， 奖励）三元组数据集。\n  4.  **训练策略模型**：使用**直接偏好优化（DPO）** 或**离线RL**方法，利用收集到的偏好数据（高分输出 vs. 低分输出）微调策略模型。避免昂贵的在线RL。\n  5",
    "source_file": "A Survey of Reinforcement Learning for Large Reasoning Models.md"
}