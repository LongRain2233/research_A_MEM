{
    "title": "Memento: Fine-tuning LLM Agents without Fine-tuning LLMs",
    "background_and_problem": "【一、研究背景与问题核心】\n\n**§1 领域背景与研究动机（150字以上）**\n本文研究领域为**自适应大型语言模型（LLM）智能体**，具体聚焦于**深度研究（Deep Research）**场景。在该场景下，智能体需要与外部环境（如网络、工具）进行多轮交互，以解决复杂、长视野（long-horizon）的任务，例如多步骤信息检索、分析和综合。当前，开发能够持续学习、适应新任务的通用智能体是迈向开放式技能获取的关键。然而，现有方法要么僵化，要么计算成本高昂，难以实现低成本、实时的持续适应。因此，本文旨在探索一种无需微调底层LLM参数，即可使智能体从环境反馈中持续学习的范式。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法主要分为两类，各自存在明确的失败模式：\n1.  **静态框架与硬编码工作流**：例如基于固定反思流程的ReAct式智能体（Yao et al., 2023）或预定义启发式的多智能体管道（如AutoGen）。当输入是**新颖的、超出预定义工作流范围的任务**时，这些方法无法动态调整其行为，导致任务失败。它们缺乏在线信息整合能力，部署后即保持静态。\n2.  **基于参数微调的方法**：包括监督微调（如Toolformer）和基于强化学习的参数更新（如Agentic RL）。当需要在**开放环境中进行持续、在线学习**时，这些方法面临**高昂的计算成本**和**灾难性遗忘**的风险。例如，为应对长视野复杂任务（如GAIA Level 3），需要大量时间展开轨迹收集训练数据，并依赖大量人工标注问题，导致**部署和适应效率极低**。\n3.  **传统检索增强生成（RAG）系统**：虽然表面类似，但RAG通常查询静态文档语料库。当任务需要**基于过去成功与失败经验进行动态推理和适应**时，RAG缺乏**持续更新记忆内容和调整检索策略的机制**，导致其无法实现真正的终身学习。\n\n**§3 问题的根本难点与挑战（200字以上）**\n从理论和工程角度看，实现无需微调的持续学习面临以下核心挑战：\n- **计算复杂度与成本**：微调大型LLM（尤其是通过强化学习）需要巨大的计算资源（GPU集群）和数据标注开销，这对于频繁更新的在线学习场景是**禁止性**的。\n- **灾难性遗忘**：参数化学习方法在适应新任务时，会覆盖或削弱对旧任务的记忆，这与持续学习的目标相悖。\n- **长视野决策的复杂性**：深度研究任务涉及多步骤工具调用和状态转移，将整个交互序列建模为一个可学习的决策过程非常复杂，需要平衡**探索（尝试新策略）与利用（重用成功经验）**。\n- **非结构化经验的有效利用**：智能体与环境交互产生的是非结构化的自然语言轨迹（状态、动作、奖励）。如何高效地存储、索引、检索这些经验，并从中提炼出可泛化的策略，是一个核心难题。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的突破口源于对人类记忆和学习机制的观察与借鉴。核心假设是：**LLM智能体可以通过模仿人类的案例推理（Case-Based Reasoning, CBR）和情景记忆（Episodic Memory）机制，实现持续学习，而无需修改其固有的参数化知识**。\n具体而言，人类通过（1）将每次经历编码为情景痕迹，（2）在睡眠依赖的巩固过程中将其提炼为抽象规则，（3）通过多巴胺驱动的信用分配进行选择性强化，以及（4）在遇到类似问题时通过案例或类比推理进行检索，从而稳步提升表现。\n本文基于此认知科学理论，提出将智能体的决策过程形式化为一个**记忆增强的马尔可夫决策过程（Memory-augmented MDP, M-MDP）**。其核心假设是：通过维护一个外部**案例库（Case Bank）**来存储过去成功与失败的经验轨迹，并学习一个**神经案例选择策略**来指导检索，智能体可以基于与当前状态相似的过去案例做出更好的决策，从而实现策略的持续改进。该方法的有效性不依赖于LLM参数的更新，而是依赖于记忆的读写机制。",
    "core_architecture": "【二、核心架构与技术机制】\n\n**§1 系统整体架构概览（200字以上）**\nMemento 被实例化为一个**规划器-执行器（Planner-Executor）**架构，在**案例规划**和**工具执行**两个阶段交替运行。整体数据流如下：\n1.  **输入**：用户任务指令（当前状态 \\(s_t\\)）。\n2.  **规划阶段（Planner）**：\n    - 查询**案例记忆（Case Memory）**模块，根据检索策略 \\(\\mu\\) 检索出 Top-K 个相关历史案例 \\(c_t = (s_i, a_i, r_i)\\)。\n    - 将检索到的案例与当前任务指令拼接，输入给规划器LLM（GPT-4.1），生成针对当前任务的行动计划（动作 \\(a_t\\)）。\n    - 生成的计划被分解为子任务，存入**子任务记忆（Subtask Memory）**。\n3.  **执行阶段（Executor）**：\n    - 执行器LLM（o4-mini/o3）从子任务记忆中读取待处理子任务。\n    - 查询**工具记忆（Tool Memory）**获取相关历史，决定调用哪个外部工具（通过MCP协议）。\n    - 执行工具调用，获取结果，并更新工具记忆和子任务记忆中的结果。\n4.  **评估与记忆更新**：\n    - 环境根据执行结果给出奖励 \\(r_t\\)（成功为1，失败为0）和下一个状态 \\(s_{t+1}\\)。\n    - **案例记忆**执行**写操作**，将新经验 \\((s_t, a_t, r_t)\\) 加入案例库 \\(M_t\\)。\n    - 若任务未完成，回到步骤2进行重新规划；否则输出最终结果。\n5.  **最终输出**：任务完成后的答案或结果。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：案例记忆（Case Memory）\n- **模块名**：Case Memory\n- **输入**：当前状态 \\(s_t\\)（自然语言任务描述），当前案例库 \\(M_t = \\{(s_i, a_i, r_i)\\}_{i=1}^{N_t}\\)。\n- **核心处理逻辑**：包含**读（Read）**和**写（Write）**两种操作，有两种实现变体：\n    - **非参数化记忆**：`Write` 仅将新案例 \\((s_t, a_t, r_t)\\) 追加到 \\(M_t\\)。`Read` 使用预训练文本编码器（SimCSE）将 \\(s_t\\) 和所有历史 \\(s_i\\) 编码为向量，计算余弦相似度，返回Top-K个最相似的案例（公式13）。\n    - **参数化记忆**：`Write` 在追加案例的同时，使用新样本在线更新一个Q函数网络（公式15）。`Read` 使用学习到的Q函数 \\(Q(s_t, c_i; \\theta)\\) 为每个候选案例评分，返回Q值最高的Top-K个案例（公式16）。\n- **输出**：Top-K个历史案例 \\((s_i, a_i, r_i)\\)，用于规划器提示。\n- **设计理由**：非参数化版本计算高效，基于相似性假设（相似问题有相似解）。参数化版本通过Q学习能学习更复杂的效用函数，实现**自适应案例选择**，超越简单的语义相似度，能识别高回报案例。\n\n#### 模块二：规划器（Planner - CBR Agent）\n- **模块名**：Planner (CBR Agent)\n- **输入**：当前状态 \\(s_t\\)，从案例记忆检索到的K个案例 \\(c_t\\)。\n- **核心处理逻辑**：根据**案例推理智能体**的总体策略公式：\n    \\[\n    \\pi(a | s, M) = \\sum_{c \\in M} \\mu(c | s, M) p_{\\mathrm{LLM}}(a | s, c)\n    \\]\n    在实现中，检索策略 \\(\\mu\\) 已由案例记忆的Read操作实现（返回Top-K）。规划器LLM的条件概率 \\(p_{\\mathrm{LLM}}(a | s, c)\\) 通过提示工程实现：将 \\(s_t\\) 和K个 \\((s_i, a_i)\\) 拼接成提示，引导LLM生成当前的动作/计划 \\(a_t\\)。\n- **输出**：针对当前状态的动作/计划 \\(a_t\\)（自然语言）。\n- **设计理由**：将LLM作为通用的模式匹配和生成器，利用其强大的上下文学习能力，根据提供的相似案例“类比”生成新计划。这避免了微调LLM，仅通过外部记忆和提示改变其行为。\n\n#### 模块三：执行器与工具集成（Executor & Tool Integration）\n- **模块名**：Executor\n- **输入**：来自子任务记忆的待处理子任务（自然语言描述），来自工具记忆的该子任务相关历史交互记录。\n- **核心处理逻辑**：执行器LLM（o4-mini）根据子任务描述和历史，判断是否需要调用工具。通过**模型上下文协议（Model Context Protocol, MCP）** 客户端接口调用外部工具服务器。工具集包括：\n    1.  **信息获取**：集成Searxng元搜索引擎进行网页搜索，使用Crawl4AI抓取和解析完整网页内容。\n    2.  **多模态处理**：使用VLMs为图像生成描述，使用ASR转录音频，解析PPT、Excel、Word、PDF等多种格式文件。\n    3.  **推理与分析**：提供安全的代码执行沙盒（支持Python常用库）和基础数学计算工具。\n- **输出**：工具调用结果或直接生成的答案，更新子任务记忆和工具记忆。\n- **设计理由**：采用MCP协议实现了工具调用的标准化和模型无关性，使智能体能灵活组合多种异构工具。将复杂任务分解为原子子任务并由专门工具处理，克服了单一LLM在计算、实时信息获取和多模态理解方面的局限。\n\n**§3 关键公式与算法（如有）**\n1.  **案例推理智能体总体策略**：\n    \\[\n    \\pi(a | s, M) = \\sum_{c \\in M} \\mu(c | s, M) p_{\\mathrm{LLM}}(a | s, c)\n    \\]\n2.  **基于最大熵RL的优化目标**（用于学习检索策略 \\(\\mu\\)）：\n    \\[\n    J(\\pi) = \\mathbb{E}_{\\tau \\sim p} \\left[ \\sum_{t=0}^{T-1} \\left[ \\mathcal{R}(s_t, a_t) + \\alpha \\mathcal{H}\\left(\\mu\\left(\\cdot | s_t, M_t\\right)\\right) \\right] \\right]\n    \\]\n    其中 \\(\\alpha\\) 是熵权重超参数，鼓励检索多样性。\n3.  **最优检索策略的封闭解**（Softmax over Q-values）：\n    \\[\n    \\mu^{*}(c | s, M) = \\frac{\\exp\\left(Q^{*}(s, M, c) / \\alpha\\right)}{\\sum_{c^{\\prime} \\in M} \\exp\\left(Q^{*}(s, M, c^{\\prime}) / \\alpha\\right)}\n    \\]\n4.  **参数化记忆的Q函数训练目标**（简化单步、二元奖励）：\n    \\[\n    \\mathcal{L}(\\theta) = \\mathbb{E}_{(s, c, r)} \\left[ - r \\log Q(s, c; \\theta) - (1 - r) \\log \\left(1 - Q(s, c; \\theta)\\right) \\right]\n    \\]\n    其中 \\(Q(s, c; \\theta)\\) 被解释为给定状态 \\(s\\) 和案例 \\(c\\) 时任务成功的概率，\\(r \\in \\{0, 1\\}\\)。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n论文明确提出了两种主要的记忆变体：\n1.  **Memento (非参数化记忆)**：案例记忆的Read操作基于**语义相似度**（SimCSE编码+余弦相似度）。Write操作仅为追加。这是主流CBR范式的实现。\n2.  **Memento (参数化记忆)**：案例记忆的Read操作基于**学习到的Q函数**（一个两层MLP）。Write操作在追加案例的同时，使用交叉熵损失在线更新Q函数网络。这实现了自适应案例选择。\n此外，消融实验中还包含：\n- **Memento w/o CBR**：移除案例推理组件，即规划器不检索任何历史案例，仅基于当前状态和LLM内部知识进行规划。此变体用于衡量案例记忆带来的增益。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上存在本质区别：\n1.  **与静态工作流智能体（如ReAct、AutoGen）的区别**：这些方法依赖预定义的反思或对话流程，是**静态和启发式**的。Memento 的核心是**动态、可学习的检索策略** \\(\\mu\\)，它通过在线强化学习（即使是单步的）不断优化案例选择，从而实现**持续的行为适应**，而非固定流程。\n2.  **与参数微调方法（如Toolformer、Agentic RL）的区别**：这些方法通过梯度更新直接修改LLM的权重。Memento **完全冻结底层LLM（GPT-4.1, o4-mini）的参数**，所有学习都发生在外部的**案例库**和轻量的**检索策略网络（Q函数MLP）** 上。这实现了“微调智能体而不微调LLM”，计算成本极低。\n3.  **与传统RAG系统的区别**：RAG从静态文档库中检索知识片段来增强生成，但**文档库本身不随智能体经验增长，检索策略（通常为相似度搜索）也不更新**。Memento 的案例库是**在线增长**的，存储的是具体的（状态，动作，奖励）轨迹，并且其检索策略（参数化版本）会**根据任务成功与否的反馈进行在线优化**，实现了真正的从经验中学习。\n4.  **与早期情景控制（Episodic Control）算法的区别**：虽然借鉴了核函数估计Q值的想法，但Memento 将其与**现代LLM的规划能力**和**丰富的工具集**相结合，应用于复杂的深度研究任务，而不仅仅是简单的强化学习环境。",
    "methodology_and_formulas": "【三、方法论细节与关键公式】\n\n**§1 完整算法流程（伪代码级描述）**\n论文算法1描述了参数化记忆版本下CBR智能体的微调流程，以下是其核心步骤的还原：\n**Step 0 (初始化)**：初始化核网络参数 \\(\\theta\\)、目标网络 \\(\\bar{\\theta} \\gets \\theta\\)、LLM策略 \\(p_{\\mathrm{LLM}}\\)、初始案例库 \\(M_0 = \\varnothing\\)、情景记忆 \\(\\mathcal{D} = \\varnothing\\)、回放缓冲区 \\(\\mathcal{B} = \\varnothing\\)。设置超参数：熵权重 \\(\\alpha\\)、折扣因子 \\(\\gamma\\)、学习率 \\(\\eta\\)、目标网络更新周期 \\(K\\)、平均权重 \\(\\beta\\)。\n**Step 1 (循环每个时间步 \\(t\\))**:\n1.  **检索（Retrieve - Memory Reading）**：给定当前状态 \\(s_t\\) 和案例库 \\(M_t\\)，根据当前策略 \\(\\mu_\\theta\\)（由公式7和9定义）采样一个案例 \\(c_t\\)。在实现中，使用Top-K选择Q值最高的K个案例。\n2.  **重用与修订（Reuse & Revise）**：以 \\(s_t\\) 和检索到的案例 \\(c_t\\) 为条件，从LLM中采样动作 \\(a_t \\sim p_{\\mathrm{LLM}}(\\cdot | s_t, c_t)\\)。\n3.  **执行（Execute）**：执行动作 \\(a_t\\)，观察环境返回的奖励 \\(r_t\\) 和下一个状态 \\(s_{t+1}\\)。\n4.  **保留（Retain）**：更新案例库 \\(M_{t+1} = M_t \\cup \\{(s_t, a_t, r_t)\\}\\)。\n5.  **存储经验**：将转移 \\((s_t, c_t, r_t, s_{t+1}, M_{t+1})\\) 存入回放缓冲区 \\(\\mathcal{B}\\)。\n6.  **更新情景记忆**：将 \\((s_t, c_t, Q_t)\\) 加入情景记忆 \\(\\mathcal{D}\\)（Memory Writing）。\n7.  **策略更新**：从 \\(\\mathcal{B}\\) 中采样小批量数据，计算损失（公式10），通过梯度下降更新核网络参数 \\(\\theta\\)：\\(\\theta \\gets \\theta - \\eta \\nabla_\\theta \\mathcal{L}_i\\)。\n8.  **目标网络更新（周期性）**：如果 \\(t \\bmod K = 0\\)，则更新目标网络：\\(\\bar{\\theta} \\gets \\beta \\bar{\\theta} + (1 - \\beta)\\theta\\)。\n\n**注意**：在论文的实际实现（深度研究场景）中，为了简化，常采用**单步设置**，即假设规划是单步的，Q学习目标简化为对即时奖励的拟合（公式14, 15），从而避免了多步bootstrapping的复杂性。\n\n**§2 关键超参数与配置**\n- **检索数量 K**：在非参数化检索（公式13）和参数化检索（公式16）中，均使用Top-K操作。论文未明确给出具体K值，但这是控制提示上下文长度和检索范围的关键参数。\n- **熵权重 \\(\\alpha\\)**：在最大熵RL目标（公式3）中，用于平衡奖励最大化和策略随机性（探索）。值越大，检索策略越均匀。\n- **折扣因子 \\(\\gamma\\)**：在完整的M-MDP中用于计算未来奖励的现值。在简化的单步设置中可能被忽略或设为0。\n- **学习率 \\(\\eta\\)**：用于更新Q函数网络参数（公式10更新步骤）。\n- **目标网络更新参数**：更新周期 \\(K\\) 和平滑系数 \\(\\beta\\)，用于稳定Q学习。\n- **文本编码器**：使用**SimCSE**对状态进行编码，用于计算相似度或作为Q函数网络的输入特征。\n- **Q函数网络结构**：参数化记忆中的Q函数实现为一个**两层MLP**。\n\n**§3 训练/微调设置（如有）**\n- **训练数据构造**：数据来源于智能体与环境的**在线交互轨迹**。每个数据样本是一个三元组 \\((s, c, r)\\)，其中 \\(s\\) 是任务状态，\\(c\\) 是检索到的案例，\\(r\\) 是二元奖励（任务成功为1，失败为0）。这些样本被存储在回放缓冲区 \\(\\mathcal{B}\\) 中。\n- **优化器与学习率**：论文未明确指定优化器类型（如Adam）和具体学习率数值。\n- **训练方式**：**在线学习（on-the-fly）**。Q函数网络在智能体执行任务的过程中**持续进行增量更新**，而非离线的批量训练。这符合持续学习的设定。\n- **奖励信号**：奖励是二元的（0/1），来源于环境对任务最终结果的判断（如答案是否精确匹配）。\n\n**§4 推理阶段的工程细节**\n- **并行化策略**：未明确提及。但由于规划器和执行器是顺序交替的，且工具调用可能涉及网络I/O，推测可能采用异步处理来优化流水线。\n- **缓存机制**：案例库中的状态向量可能被缓存以避免重复编码。\n- **向量数据库选型**：论文未指定使用专门的向量数据库（如FAISS）。非参数化检索通过计算当前状态向量与所有历史状态向量的余弦相似度来实现，当案例库很大时，这可能成为瓶颈。\n- **工具调用协议**：统一使用**模型上下文协议（MCP）** 作为与外部工具服务器通信的标准接口，实现了工具集成的模块化和灵活性。\n- **模型部署**：规划器使用GPT-4.1 API，执行器使用o4-mini（GAIA任务用o3）API，图像处理用GPT-4o API，视频代理用Gemini 2.5 Pro API，音频代理用Assembly AI API。这表明系统严重依赖闭源、昂贵的商业API。",
    "experimental_design": "【四、实验设计】\n\n**§1 数据集详情（每个数据集单独列出）**\n1.  **GAIA** (Mialon et al., 2023):\n    - **规模**：共450个问题，其中验证集150个，测试集300个（私有）。\n    - **领域类型**：通用真实世界任务，需要工具使用。\n    - **评测问题类型**：长视野规划与工具调用。分为三个难度等级：Level 1（约5步，单一工具）、Level 2（5-10步，多工具）、Level 3（最多50步，工具无限制）。\n    - **特殊标准**：答案具有唯一性，评估时进行标准化（小写、去除标点/冠词、规范化空白字符）。\n2.  **DeepResearcher Benchmark** (整合自Zheng et al., 2025):\n    - **构成**：来自7个开放域QA数据集的混合。\n        - **Natural Questions (NQ)**: 512个样本。\n        - **TriviaQA (TQ)**: 512个样本。\n        - **HotpotQA**: 512个样本（多跳推理）。\n        - **2Wiki**: 512个样本。\n        - **MusiQue**: 512个样本。\n        - **Bamboogle**: 125个高质量样本（强调基于网络的综合）。\n        - **PopQA**: 512个样本。\n    - **总规模**：约3717个样本。\n    - **领域类型**：开放域事实性问答，涵盖多跳、复杂推理。\n3.  **SimpleQA** (Wei et al., 2024):\n    - **规模**：4,330个问题。\n    - **领域类型**：事实寻求型问题。\n    - **评测问题类型**：测试事实准确性。\n4.  **Humanity's Last Exam (HLE)** (Phan et al., 2025):\n    - **规模**：2,500个问题。\n    - **领域类型**：跨多样化学科（如科学、人文）。\n    - **评测问题类型**：评估广域推理的极限。\n\n**§2 评估指标体系（全量列出）**\n- **准确性指标**：\n    1.  **精确匹配（Exact Match, EM）**：用于GAIA。预测答案经过标准化后与标准答案完全一致则计为正确。报告百分比。\n    2.  **宏平均F1分数（Macro-F1）**：用于DeepResearcher、SimpleQA和HLE数据集。衡量答案与标准答案在词元重叠上的精度和召回率。\n    3.  **部分匹配（Partial Match, PM）**：用于DeepResearcher、SimpleQA等。使用**GPT-4o-mini作为评判员**，评估生成答案与标准答案之间的语义相似度。提示词与DeepResearcher工作相同。输出一个评分。\n- **效率/部署指标**：**论文未提供**任何关于延迟、Token消耗、API调用次数或显存占用的定量数据。\n- **其他自定义指标**：**Pass@3**：在GAIA验证集上，允许智能体尝试3次，取最佳结果作为最终得分。\n\n**§3 对比基线（完整枚举）**\n1.  **提示工程方法（Prompt Based）**：\n    - **CoT**：思维链提示。\n    - **CoT + RAG**：思维链结合检索增强生成。\n    - **Search-o1 (Web)** (Li et al., 2025c)：使用o1模型进行网络搜索的方法。\n2.  **基于训练的方法（Training Based）**：均使用Qwen2.5 (7B) 作为底座模型。\n    - **Search-r1-base** (Jin et al., 2025)。\n    - **Search-r1-instruct** (Jin et al., 2025)。\n    - **R1-Searcher** (Song et al., 2025)。\n    - **DeepResearcher** (Zheng et al., 2025)：当前最先进的基于训练的系统，作为主要对比基线。\n3.  **GAIA排行榜上的其他智能体**（见表2）：\n    - **Alita** (使用Claude 4 Sonnet, GPT-4o)。\n    - **Skywork Super Agents v1.1**。\n    - **Langfun Agent** (使用Gemini 2.5 Pro)。\n    - **AWorld**。\n    - **Manus**。\n    - **OWL-Workforce** (使用Claude 3.7 Sonnet)。\n    - **OpenAI DeepResearch** (使用o3)。\n    - **OWL-Roleplaying**。\n    - **Open Deep Research** (使用o1)。\n    - **Su Zero Ultra**。\n    - **h2oGPTe Agent** 多个版本。\n\n**§4 实验控制变量与消融设计**\n- **主要消融实验**：对比 **Memento** (完整系统) 与 **Memento w/o CBR** (移除案例推理组件)。该实验旨在量化案例记忆带来的性能增益。结果显示，在分布外（OOD）任务上，案例记忆带来了 **4.7% 到 9.6%** 的绝对分数提升（见图1d）。\n- **记忆设计对比**：比较了**非参数化记忆**（基于相似度）和**参数化记忆**（基于学习的Q函数）在持续学习曲线上的表现（见图1c）。展示了不同记忆机制对学习速度和最终性能的影响。\n- **控制变量**：在所有实验中，**规划器LLM（GPT-4.1）和执行器LLM（o4-mini/o3）保持不变**，以确保性能差异仅源于记忆机制和框架设计，而非底层模型能力。与基于训练的方法（使用Qwen2.5 7B）对比时，模型能力不同，但作者旨在展示其框架即使使用更强商业模型，也能超越专门微调过的较小模型。",
    "core_results": "【五、核心实验结果】\n\n**§1 主实验结果全景（表格式呈现）**\n**表1：DeepResearcher Benchmark 结果（F1 / PM）**\n方法名 | NQ-F1 | NQ-PM | TQ-F1 | TQ-PM | HotpotQA-F1 | HotpotQA-PM | 2Wiki-F1 | 2Wiki-PM | Musique-F1 | Musique-PM | Bamboogle-F1 | Bamboogle-PM | PopQA-F1 | PopQA-PM | Avg-F1 | Avg-PM\n--- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\nCoT | 19.8 | 32.0 | 45.6 | 48.2 | 24.4 | 27.9 | 26.4 | 27.3 | 8.5 | 7.4 | 22.1 | 21.6 | 17.0 | 15.0 | 23.6 | 26.1\nCoT+RAG | 42.0 | 59.6 | 68.9 | 75.8 | 37.1 | 43.8 | 24.4 | 24.8 | 10.0 | 10.0 | 25.4 | 27.2 | 46.9 | 48.8 | 37.7 | 43.2\nSearch-o1 (Web) | 32.4 | 55.1 | 58.9 | 69.5 | 33.0 | 42.4 | 30.9 | 37.7 | 14.7 | 19.7 | 46.6 | 53.6 | 38.3 | 43.4 | 35.2 | 45.0\nSearch-r1-base | 45.4 | 60.0 | 71.9 | 76.2 | 55.9 | 63.0 | 44.6 | 47.9 | 26.7 | 27.5 | 56.5 | 57.6 | 43.2 | 47.0 | 48.3 | 53.8\nSearch-r1-instruct | 33.1 | 49.6 | 44.7 | 49.2 | 45.7 | 52.5 | 43.4 | 48.8 | 26.5 | 28.3 | 45.0 | 47.2 | 43.0 | 44.5 | 39.6 | 45.6\nR1-Searcher | 35.4 | 52.3 | 73.1 | 79.1 | 44.8 | 53.1 | 59.4 | 65.8 | 22.8 | 25.6 | 64.8 | 65.6 | 42.7 | 43.4 | 47.1 | 53.7\nDeepResearcher | 39.6 | 61.9 | 78.4 | 85.0 | 52.8 | 64.3 | 59.7 | 66.6 | 27.1 | 29.3 | 71.0 | 72.8 | 48.5 | 52.7 | 51.8 | 60.5\n**Memento (Ours)** | **42.0** | **74.6** | **85.5** | **93.9** | **66.5** | **81.6** | **81.4** | **94.1** | **40.6** | **53.3** | **86.2** | **92.8** | **64.0** | **72.5** | **66.6** | **80.4**\n\n**表2：GAIA排行榜Top结果（截至2025年6月26日）**\n**验证集**：\nMemento (Pass@3) | GPT4.1, o3 | **87.88%** (平均) | 96.23% (L1) | 90.70% (L2) | 61.54% (L3)\nAlita | Claude 4 Sonnet, GPT-4o | 87.27% | 88.68% | 89.53% | 76.92%\n... (其他基线分数见表2)\n**测试集**：\nSu Zero Ultra | - | 80.40% | 93.55% | 77.36% | 65.31%\nh2oGPTe Agent v1.6.33 | Claude 3.7 Sonnet, Gemini 2.5 Pro | 79.73% | 89.25% | 79.87% | 61.22%\n**Memento** | GPT4.1, o3 | **79.40%** | 90.32% | 75.47% | **71.43%**\n\n**其他数据集**：\n- **SimpleQA**：Memento 达到 **95.0%** 的PM分数（原文提及）。\n- **DeepResearcher 整体**：Memento 平均F1为 **66.6%**，平均PM为 **80.4%**。\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n- **在DeepResearcher各数据集上的表现**：Memento 在**所有7个子数据集**的F1和PM指标上均**全面超越**所有基线，包括最强的训练基线DeepResearcher。提升幅度巨大，例如在2Wiki上，F1从DeepResearcher的59.7提升至81.4（绝对提升21.7点，相对提升36.3%），PM从66.6提升至94.1（绝对提升27.5点）。在Bamboogle上，F1从71.0提升至86.2（绝对提升15.2点）。这表明基于案例记忆的规划能极大提升复杂、多跳问答的准确性。\n- **在GAIA上的表现**：Memento 在验证集上达到**第一（87.88% Pass@3）**，在测试集上达到**第三（79.40%）**，且是开源框架中的第一。特别值得注意的是，在**最困难的Level 3任务**上，Memento 在测试集上取得了 **71.43%** 的分数，显著高于许多其他智能体（如AWorld的46.94%，h2oGPTe Agent的61.22%），甚至高于验证集上的自己（61.54%）。这表明其方法对于**超长视野、需要大量工具组合的任务**具有特别强的适应能力。而Alita在验证集Level 3上更高（76.92%），可能在某些极端复杂任务上仍有优势。\n- **分布外（OOD）泛化**：消融实验（图1d）显示，案例记忆（CBR）为OOD任务带来了 **4.7% 到 9.6%** 的绝对准确率提升。这证明记忆机制不是简单的过拟合，而是帮助智能体将过去经验泛化到新任务上。\n\n**§3 效率与开销的定量对比**\n**原文未提供**任何关于延迟、Token消耗、计算成本或内存占用的具体定量数据。这是一个重要的信息缺失。仅能从方法描述中推断，由于避免了LLM微调，其**训练成本远低于**Search-r1、DeepResearcher等需要训练的方法。但推理阶段的API调用成本（使用GPT-4.1, o4-mini等）可能非常高昂。\n\n**§4 消融实验结果详解**\n- **移除案例推理（Memento w/o CBR）**：如图1b和1d所示，移除CBR组件导致性能下降。在OOD数据集上，性能下降范围为 **4.7% 到 9.6%** 的绝对准确率点。这直接证明了案例记忆对于提升泛化性能的核心贡献。\n- **不同记忆设计对比**：图1c展示了持续学习曲线。可以推断，**参数化记忆**（通过学习Q函数）相比**非参数化记忆**（仅基于相似度）可能带来更快的学习速度或更高的渐近性能，但原文未给出具体数值对比。\n\n**§5 案例分析/定性分析（如有）**\n原文未提供具体的成功或失败案例的文字分析。",
    "conclusion_and_future_work": "【六、结论与未来工作】\n\n**§1 本文核心贡献总结**\n1.  **提出新的学习范式**：首次为LLM智能体提出了一个**无需微调底层LLM**的持续学习范式，通过**记忆增强的马尔可夫决策过程（M-MDP）** 和**案例推理（CBR）** 实现。\n2.  **形式化框架与可学习策略**：将CBR智能体形式化为M-MDP，并推导出可优化的**神经案例选择策略**，使用**在线软Q学习**进行更新，实现了策略的持续改进。\n3.  **实现高性能深度研究智能体**：实例化了Memento系统，在GAIA、DeepResearcher等多个基准上达到最先进性能。例如，在GAIA验证集达到87.88% Pass@3（第一），在DeepResearcher平均PM达到80.4%，超越所有训练基线。\n4.  **验证记忆的有效性**：通过消融实验证明，案例记忆为分布外任务带来4.7%至9.6%的绝对性能提升，证实了其泛化价值。\n\n**§2 局限性（作者自述）**\n原文中作者明确承认的局限性包括：\n1.  **简化假设**：在深度研究场景的实例化中，将CBR规划器简化为**单步设置**，避免了多步MDP的bootstrapping复杂性，但这可能限制了在更严格的序列决策问题上的表现。\n2.  **概率性奖励与记忆更新**：当前模型假设奖励函数和记忆更新是确定性的（使用指示函数建模）。作者指出，在某些特定情况下，它们也可以是概率性的，这被留作未来工作。\n3.  **依赖强大商业模型**：Memento的实现严重依赖GPT-4.1、o4-mini等闭源、昂贵的商业API，这影响了其可访问性和复现成本。\n\n**§3 未来研究方向（全量提取）**\n1.  **探索概率性奖励与记忆更新**：将框架扩展以处理概率性的环境奖励和记忆更新操作，使其能应对更不确定的现实世界场景。\n2.  **研究更高效的内存管理**：当前案例库会无限增长，未来需要研究**记忆压缩、提炼或遗忘机制**，以应对经典的“淹没问题”（swamping problem），即检索成本超过效用。例如，将具体案例提炼为抽象规则或技能。\n3.  **应用于更广泛的序列决策问题**：将M-MDP框架和案例推理策略应用于**超越深度研究的其他复杂序列决策领域**，如机器人控制、长期对话代理等。\n4.  **降低对昂贵模型的依赖**：探索使用更小、开源的LLM作为规划器和执行器，并验证在该范式下是否仍能保持有竞争力的性能。",
    "research_contributions": "【七、研究贡献与学术价值】\n\n**§1 核心学术贡献（按重要性排序）**\n1.  **理论新颖性**：提出了**记忆增强的MDP（M-MDP）**这一形式化框架，将案例推理智能体的学习过程与强化学习理论优雅地结合。推导了基于最大熵RL的优化目标和最优检索策略的封闭解，为“无梯度智能体学习”奠定了理论基础。\n2.  **实验验证充分性**：在**4个主流基准（GAIA, DeepResearcher, SimpleQA, HLE）**上进行了全面实验，不仅展示了超越现有方法（包括SOTA训练方法）的顶级性能，还通过严谨的消融实验量化了案例记忆组件（4.7-9.6%提升）和不同记忆设计的影响，验证了核心假设。\n3.  **对领域的影响**：开辟了LLM智能体研究的一条**新路径**：**脱离对LLM参数微调的依赖，转向外部记忆系统的优化**。这为资源受限场景下的持续学习、终身学习智能体提供了可行的技术蓝图，可能推动领域从“模型中心”向“架构与记忆中心”的思维转变。\n\n**§2 工程与实践贡献**\n- **开源代码**：在 https://github.com/Agent-on-the-Fly/Memento 公开了代码，促进了方法的可复现性和后续研究。\n- **系统实现**：提供了一个完整的、模块化的深度研究智能体实现，集成了**规划器-执行器架构**、**多类型记忆模块**（案例、子任务、工具记忆）以及通过**MCP协议**的丰富工具集，具有较高的工程参考价值。\n- **评测基准整合**：在实验中整合并评估了多个现有基准，为后续研究提供了性能参照系。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于一个**交叉与创新的位置**：它并非简单扩展现有RAG或微调方法。\n- 它是在**案例推理（CBR）** 这一经典AI范式上的现代复兴，但用**深度LLM和可学习的神经检索策略**对其进行了升级。\n- 它也是在**强化学习用于智能体**这一路线上的延伸，但创新性地将“动作”定义为**从记忆库中选择案例**，并将LLM作为固定的动态模型，从而大幅降低了学习复杂度。\n- 因此，本文更像是**开辟了一条介于传统CBR、现代RAG和高效RL之间的新路线**，专注于通过优化外部记忆系统来实现智能体的自适应。",
    "professor_critique": "【八、隐性缺陷与教授锐评】\n\n**§1 实验设计与评估体系的缺陷**\n1.  **基线对比不公平性**：主实验（表1）中，将使用**顶级商业API（GPT-4.1, o4-mini）** 的Memento与使用**开源小模型（Qwen2.5 7B）微调**的基线（如DeepResearcher）进行对比。这存在严重的**能力不对等**。性能优势在多大程度上源于更强大的底座模型，而非其提出的记忆架构？尽管作者意图展示其框架的潜力，但缺乏**控制底座模型相同的消融实验**是重大缺陷。\n2.  **效率评估完全缺失**：论文通篇未报告任何关于**推理延迟、Token消耗、API调用成本或内存占用**的数据。对于声称“高效”、“低成本”的方法，这是不可接受的。Memento每步推理都需要调用昂贵的GPT-4.1和o4-mini API，且检索可能涉及大规模向量比较，其实际部署成本可能极高。\n3.  **评估指标潜在偏差**：部分匹配（PM）指标使用**GPT-4o-mini作为评判员**，而Memento本身使用了GPT系列模型，可能存在**评估偏好（evaluation bias）**，即GPT家族模型可能更倾向于给GPT家族生成的答案高分。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **记忆库无限增长的工程瓶颈**：案例库 \\(M_t\\) 随时间线性增长，非参数化检索的复杂度为O(N)。当N达到百万级时，即使使用向量索引，检索延迟和存储成本也将成为严重问题。论文未讨论任何**记忆淘汰、压缩或摘要机制**，这在长期部署中是不可行的。\n2.  **单步规划假设的局限性**：为了简化，在实际实现中采用了单步Q学习。这**严重限制了方法处理真正长视野、多步依赖任务的能力**。在单步设定下，Q函数只学习“哪个案例对当前状态有用”，而无法学习“选择这个案例如何影响多步后的最终回报”，这与GAIA Level 3任务的需求可能存在脱节。\n3.  **对高质量奖励信号的依赖**：Q学习（即使是单步）需要清晰的奖励信号（0/1）。在复杂的现实任务中，这种二元奖励通常难以自动获得，需要人工标注或设计复杂的奖励函数，这削弱了“持续在线学习”的实用性。\n\n**§3 未经验证的边界场景**\n1.  **多语言混合输入**：系统使用SimCSE进行编码，其多语言能力未经测试。当用户查询和案例库中包含混合语言时，语义相似度检索可能崩溃。\n2.  **领域外知识冲突**：当检索到的案例提供与当前任务相关但事实错误的“旧知识”时，LLM是否会被误导？系统缺乏对记忆内容可信度的评估和过滤机制。\n3.  **恶意对抗输入**：如果用户输入旨在污染记忆库的对抗性查询（例如，将失败案例与成功奖励关联），在线学习的Q函数可能会被“毒化”，导致后续检索策略失效。系统没有针对对抗性输入的鲁棒性设计。\n4.  **工具不可用或动态变化**：Memento假设工具集通过MCP稳定可用。当某个关键工具临时失效，或工具接口发生变化时，基于过去成功案例（包含特定工具调用）的规划可能完全失败，系统缺乏动态工具适配能力。\n\n**§4 可复现性与公平性问题**\n1.  **高昂的复现成本**：完全复现Memento需要调用GPT-4.1、o4-mini、GPT-4o、Gemini 2.5 Pro、Assembly AI等多个商业API，费用极其高昂，**普通研究者或学术实验室根本无法承担**，严重影响了工作的可复现性和可及性。\n2.  **超参数调优不透明**：论文未详细说明关键超参数（如检索数量K、熵权重α、学习率η）是如何确定的，也未报告是否为Memento和Baseline进行了同等的调优。可能存在对Memento有利的调优而对基线不公平。\n3.  **代码与数据完整性**：虽然开源代码，但依赖的商业API和可能未公开的内部工具服务器配置，使得完全复现实验环境仍然困难。",
    "zero_compute_opportunity": "【九、零算力研究契机】\n\n#### 蓝图一：轻量开源模型上的记忆增强智能体效能验证\n- **核心假设**：Memento框架的核心优势（记忆增强规划）可以在**更小、开源的LLM（如Llama 3.1 8B, Qwen2.5 7B）** 上复现，并且相比单纯的提示工程基线，仍能带来显著的性能提升，且提升主要源于架构而非模型规模。\n- **与本文的关联**：基于本文未解决的**基线对比不公平**问题。旨在剥离强大商业模型的影响，纯粹验证记忆架构的价值。\n- **所需资源**：\n    1.  **模型**：Hugging Face上免费的Llama 3.1 8B Instruct或Qwen2.5 7B Instruct模型权重。\n    2.  **数据集**：从GAIA验证集中抽取50-100个样本作为低成本测试集。或使用HotpotQA等公开QA数据集的子集。\n    3.  **计算**：Google Colab免费T4 GPU（16GB显存）足以运行7B模型推理和轻量检索。\n    4.  **费用**：接近0成本（仅Colab可能需要的Pro订阅，非必须）。\n- **执行步骤**：\n    1.  **实现简化版Memento**：使用Sentence-Transformers（all-MiniLM-L6-v2）作为免费编码器实现**非参数化案例记忆**。规划器和执行器使用同一个开源7B模型（通过Hugging Face Transformers加载）。\n    2.  **构建工具模拟器**：对于需要网络搜索的任务，使用**预先爬取好的维基百科段落或简单Python函数**作为“工具”模拟环境，避免真实API调用。\n    3.  **设计实验**：对比三个系统：（A）**基线**：仅使用CoT提示的7B模型。（B）**Memento-lite**：我们的简化实现，带有案例记忆。（C）**本文结果（如有）**：在相同子集上运行开源代码（如果可能），或引用其数据点作为上限参照。\n    4.  **评估**：计算精确匹配（EM）或F1分数，并记录每个查询的推理Token数作为效率代理指标。\n- **预期产出**：一篇短论文或技术报告，证明在控制模型变量后，记忆增强架构能为小型开源LLM带来X%的性能提升。可投稿至EMNLP、ACL的Demo或Findings track。\n- **潜在风险**：小模型的理解和规划能力可能太弱，导致任何架构提升都不明显。应对：选择难度适中的任务子集，并精心设计提示模板以激发小模型潜力。\n\n#### 蓝图二：案例记忆的淹没问题与轻量遗忘机制研究\n- **核心假设**：在持续学习过程中，引入简单的、基于效用的**案例遗忘或压缩策略**（如淘汰长期未被成功检索的案例，或将相似案例合并为模板），可以在几乎不损失性能的情况下，显著降低记忆存储和检索开销。\n- **与本文的关联**：针对本文**未讨论记忆库无限增长问题**这一工程局限，提出低成本的解决方案。\n- **所需资源**：\n    1.  **代码**：基于蓝图一的简化版Memento代码。\n    2.  **数据集**：一个能产生大量交互轨迹的模拟环境，如**ALFWorld**（文本游戏）或**WebShop**（在线购物模拟）的简化版。这些环境有明确的任务和奖励，且可本地运行。\n    3.  **计算**：同蓝图一，Colab免费T4 GPU。\n- **执行步骤**：\n    1.  **基线系统**：实现一个会无限增长案例库的Memento-lite。\n    2.  **设计遗忘策略**：实现两种低成本策略：（i）**LRU遗忘**：当案例库超过大小上限N时，淘汰最久未被检索到的案例。（ii）**效用过滤**：定期检查每个案例的历史成功率（成功次数/被检索次数），淘汰成功率低于阈值θ的案例。\n    3.  **设计压缩策略**：使用**文本摘要**（如用T5-small模型）将多个相似（编码向量接近）的成功案例压缩成一个“模板案例”，记录通用步骤和关键变量。\n    4.  **实验**：让智能体在模拟环境中学习固定数量的回合，对比完整记忆库、带遗忘/压缩的记忆库在**最终性能**和**平均检索延迟/内存占用**上的差异。\n- **预期产出**：一篇专注于高效记忆管理的实验性论文，提出并评估了适用于资源受限场景的轻量记忆维护算法。可投稿至ICLR、NeurIPS的机器学习系统或高效学习track。\n- **潜在风险**：设计的遗忘策略可能过于激进，错误淘汰了有价值的“沉睡”案例。应对：引入更保守的混合策略，并结合案例年龄和效用进行加权决策。\n\n#### 蓝图三：基于公开日志数据的离线案例挖掘与智能体引导\n- **核心假设**：从公开的**人类问题解决日志**（如Stack Overflow问答对、Kaggle竞赛解决方案讨论帖）或**其他智能体交互轨迹**中，可以自动化挖掘出高质量的（问题，解决方案，成功）案例三元组，用于初始化或增强Memento的案例库，从而实现“冷启动”加速或性能提升。\n- **与本文的关联**：本文案例库完全通过自身在线交互积累，起步慢。本蓝图探索利用**免费、丰富的公开数据**来预热记忆库。\n- **所需资源**：\n    1.  **数据**：Stack Overflow数据转储（公开）、Kaggle论坛公开内容。\n    2.  **工具**：简单的网页爬虫（如BeautifulSoup，用于特定论坛），本地文本处理管道。\n    3.  **模型**：同蓝图一，使用开源小模型进行文本理解和案例格式化。\n- **执行步骤**：\n    1.  **数据收集与清洗**：从Stack Overflow爬取带有“已接受答案”的编程问题帖子，将（问题标题+正文，已接受答案，奖励=1）构造成案例。从Kaggle讨论区爬取获奖解决方案的总结帖。\n    2.  **案例质量过滤**：使用开源小模型对爬取的案例进行过滤，例如，判断解决方案是否完整、是否与问题高度相关。\n    3.  **案例库构建与索引**：将过滤后的案例编码并存入本地向量数据库（如ChromaDB）。\n    4.  **实验验证**：在**代码生成**（使用HumanEval基准）或**数据科学问题解决**（使用Kaggle风格问题）任务上，对比：（A）无记忆的基线模型。（B）使用在线学习积累记忆的Memento-lite。（C）使用**预加载公开日志案例库**的Memento-lite。评估初始",
    "source_file": "Memento Fine-tuning LLM Agents without Fine-tuning LLMs.md"
}