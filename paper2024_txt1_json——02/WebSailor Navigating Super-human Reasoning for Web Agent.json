{
    "title": "WebSailor: Navigating Super-human Reasoning for Web Agent",
    "background_and_problem": "【一、研究背景与问题核心】\n\n**§1 领域背景与研究动机（150字以上）**\n本研究聚焦于**基于大型语言模型（LLM）的Web智能体（Web Agent）**领域，具体应用场景为**复杂信息检索任务**。近年来，专有智能体系统（如DeepResearch）在极其复杂的基准测试（如BrowseComp）上展现了超越人类的能力，这标志着智能体在**处理高度不确定性、无预设路径的开放式信息探索任务**上达到了新高度。然而，开源智能体在此类任务上的表现近乎为零，形成了巨大的能力鸿沟。该研究的核心动机在于：理解并复现专有系统所展现的**超人推理模式**——即系统性地在广阔、模糊的信息空间中降低极端不确定性的能力，并将其注入到开源模型中，从而弥合这一鸿沟。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法在处理复杂信息检索任务时存在显著短板，具体失败模式如下：\n1.  **直接推理模型（Direct Inference）**：当面对BrowseComp-en/zh这类需要动态网络交互才能获取答案的复杂问题时，模型仅依赖内部参数知识，无法进行有效搜索。例如，GPT-4.1在BrowseComp-en上的准确率仅为1.5，Qwen-2.5-72B仅为0.6，近乎零分，表明其内部知识无法覆盖此类问题的特异性与不确定性。\n2.  **现有开源智能体（如WebDancer, WebThinker）**：其训练范式主要针对**低不确定性（Level 1）**或**有清晰解决路径（Level 2）**的任务。当输入为Level 3任务（如BrowseComp中具有模糊信息、实体复杂耦合的问题）时，这些方法因缺乏应对高难度不确定性降低的训练，导致推理失败。例如，WebDancer-32B在BrowseComp-en上仅得2.5分，远低于专有系统DeepResearch的51.5分。\n3.  **纯监督微调（SFT）方法**：当任务需要**自适应上下文探索**时，纯SFT训练的智能体泛化能力差。其学习到的固定模式无法应对BrowseComp中非线性的、需要动态剪枝和综合的探索路径，导致在长视野任务中性能迅速退化。\n\n**§3 问题的根本难点与挑战（200字以上）**\n问题的根本难点在于如何让模型学会**在高度不确定、无预设路径的信息空间中导航**。这带来了多重挑战：\n1.  **数据挑战**：现有训练数据集（如多跳QA）逻辑结构清晰、不确定性低，无法模拟真实网络搜索中**信息模糊、路径未知、实体关系复杂耦合**的“Level 3”场景。缺乏高质量、大规模、能体现此类复杂推理模式的数据。\n2.  **监督信号挑战**：即使使用强大的开源推理模型（如QwQ、DeepSeek-R1）生成轨迹，其原生推理输出存在**风格污染（Stylistic Contamination）**和**上下文过载（Context Overload）**问题。冗长的思维链会挤占上下文窗口，且其特定风格会限制受训智能体发展自身灵活的探索策略。\n3.  **训练效率挑战**：针对Web智能体的强化学习（RL）训练极其缓慢。由于每轮推理涉及多轮工具调用（搜索、访问网页）与环境交互，rollout速度远慢于标准RL。同时，此类复杂任务的奖励信号极其稀疏，导致训练初期模型难以获得有效反馈，收敛困难。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**从“不确定性降低”的视角重构智能体训练**。核心假设是：专有智能体的超人能力源于其能处理**初始不确定性极高且难以降低（hard-to-reduce）**的任务。因此，要复现这种能力，关键在于生成并利用具有此类特征的数据进行训练。具体技术假设包括：\n1.  **结构化采样与信息模糊化**：通过从真实网络知识图谱中随机游走采样子图，可以构造出**实体关系复杂耦合、缺乏预设路径**的任务结构（即“难以降低的不确定性”）。再通过信息模糊化（如将精确日期改为模糊时期、部分掩盖名称）直接增加任务的初始模糊性，迫使模型进行深度推理与综合。\n2.  **推理重建**：假设可以从专家模型的成功**动作-观察序列**中，反向推断出简洁、目标导向的**思维（Thought）**，从而获得高质量、无风格污染的监督信号，用于冷启动训练。\n3.  **冷启动必要性**：假设在稀疏奖励的复杂RL任务开始前，一个适度的**拒绝采样微调（RFT）冷启动**是必要的，可以为模型提供基础的工具使用能力和长视野推理骨架，避免RL初期因奖励稀疏而无法学习。\n这些假设基于对现有方法失败模式的分析，并旨在从数据生成、监督信号构建和训练流程三个层面系统性解决问题。",
    "core_architecture": "【二、核心架构与技术机制】\n\n**§1 系统整体架构概览（200字以上）**\nWebSailor是一个完整的**智能体后训练（post-training）流水线**，整体架构包含四个核心阶段，数据流如下：\n1.  **复杂QA合成（SailorFog-QA）**：输入为从互联网获取的原始文本和实体特征 → 通过**随机游走构建复杂知识图谱** → **采样多样拓扑结构的子图**并对图中信息进行**模糊化** → 输出为高不确定性的QA对（Level 3任务）。\n2.  **专家轨迹生成与推理重建**：输入为SailorFog-QA中的问题 → 使用**开源大型推理模型（LRM，如QwQ-32B）**生成包含其原生思维的完整动作轨迹 → **剥离并丢弃LRM的原生冗长思维**，仅保留成功的**动作-观察序列** → 使用另一个强大的指令遵循模型，根据历史上下文、当前动作及后续观察，为每一步**重建简洁、逻辑化的新思维** → 输出为高质量、格式干净的完整推理轨迹（Thought-Action-Observation序列）。\n3.  **拒绝采样微调（RFT）冷启动**：输入为重建后的高质量轨迹 → 经过**三层过滤**（答案正确、轨迹长度≤32k tokens、工具调用次数>5） → 在过滤后的数据上进行监督微调，**训练目标仅为生成有效的思维和动作，观测（Observation）部分的token在损失计算中被掩码** → 输出为具备基础工具使用和推理骨架的初始策略模型。\n4.  **复制采样策略优化（DUPO）强化学习**：输入为初始策略模型和SailorFog-QA问题 → 进行多轮（组大小G=8）rollout与环境交互 → 应用**动态采样策略**：训练前过滤掉所有rollout都正确或都错误的简单样本；训练中，将同一批次内**标准差非零的样本进行复制**以填充批次 → 使用**分组相对优势估计**和**token级策略梯度损失**进行优化 → 输出为最终强化学习后的WebSailor智能体。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：SailorFog-QA 数据合成模块\n-   **模块名**：SailorFog-QA Generator\n-   **输入**：从Wikidata SPARQL服务获取的**稀有实体**作为种子。\n-   **核心处理逻辑**：\n    1.  使用搜索（search）和访问（visit）工具获取种子实体的非结构化文本和特征。\n    2.  从中提取相关实体及关系，形成初始图谱节点和边。\n    3.  进行**迭代扩展**：以一定概率选择现有节点或新发现的实体作为下一个扩展节点，通过模拟浏览寻找新的、不同的实体进行连接。此**随机过程**旨在避免生成简单的线性链（Level 2任务特征），而是形成密集互联、具有重叠关系路径的复杂图结构。\n    4.  从复杂图中**采样具有多样拓扑结构的子图**。\n    5.  对子图中的特征和关系进行**信息模糊化**，例如将精确日期改为模糊时期（“in the early 2010s”），将名称部分掩盖（“an institution founded by someone with the initial ‘F’”）。\n    6.  基于模糊化后的子图生成问题和答案。\n-   **输出**：高不确定性、高难度降低的QA对（Level 3任务）。\n-   **设计理由**：现有数据集（如多跳QA）无法模拟真实网络搜索中信息模糊、路径未知的复杂场景。通过随机游走和图采样，能自动生成大量具有**新兴、非线性结构**的任务，迫使模型发展超越简单启发式的推理模式。信息模糊化直接增加了任务的初始不确定性，是模拟BrowseComp等基准测试关键特征的核心手段。\n\n#### 模块二：推理轨迹重建模块\n-   **模块名**：Reasoning Reconstruction Module\n-   **输入**：1) SailorFog-QA中的问题；2) 专家LRM（如QwQ-32B）生成的完整动作轨迹（含其原生冗长思维）。\n-   **核心处理逻辑**：\n    1.  **轨迹净化**：丢弃专家LRM生成的原始冗长思维（\\(\\tau_i\\)），仅保留成功的**动作-观察序列** \\((a_0, o_0, a_1, o_1, ...)\\)。\n    2.  **思维重建**：对于轨迹中的每一步 \\(t\\)，给定截至上一步的历史 \\(\\mathcal{H}_{t-1}\\)、专家选择的动作 \\(a_t\\) 及后续观察 \\(o_t\\)，使用一个独立的、强大的指令遵循模型 \\(\\pi^*\\) 来生成一个新的、简洁的思维 \\(\\hat{\\tau}_t\\)，作为执行动作 \\(a_t\\) 的逻辑理由。公式化表示为：\\(\\hat{\\tau}_t \\sim \\pi^*(\\tau | \\mathcal{H}_{t-1}, a_t, o_t)\\)。\n    3.  **风格强制**：在重建过程中，强制使用 **“短思维链（short-CoT）”** 风格，确保最终推理链足够紧凑，以适应长视野任务，避免上下文过载。\n-   **输出**：高质量、格式干净、思维简洁的完整推理轨迹 \\(\\hat{\\mathcal{H}}_T = (\\hat{\\tau}_0, a_0, o_0, ..., \\hat{\\tau}_T, a_T, o_T)\\)。\n-   **设计理由**：直接使用专家LRM的原始输出进行微调会导致**风格污染**和**上下文过载**。通过重建思维，可以剥离专家模型特有的冗长风格，获得纯净、目标导向的推理逻辑作为监督信号，同时保证轨迹长度可控，适合后续训练。\n\n#### 模块三：复制采样策略优化（DUPO）模块\n-   **模块名**：Duplicating Sampling Policy Optimization (DUPO)\n-   **输入**：经过RFT冷启动的初始策略模型 \\(\\pi_{\\theta_{old}}\\)，以及一批SailorFog-QA问题 \\((q, y)\\)。\n-   **核心处理逻辑**：\n    1.  **批次内Rollout**：对每个问题，使用当前策略并行生成G=8个rollout轨迹，与环境（工具）交互。\n    2.  **动态采样（训练前）**：在训练开始前，过滤掉那些**所有8个rollout答案都正确或都错误**的“过于简单”的样本，因为这些样本无法提供有信息量的梯度。\n    3.  **动态采样（训练中）**：在计算策略梯度时，对于**奖励标准差为零**（即同一问题的所有rollout答案完全一致，无论对错）的样本，将其从当前训练批次中移除。随后，**随机复制同一批次内标准差非零的样本来填充批次**，以保持批次大小恒定。\n    4.  **奖励计算**：采用基于规则的奖励函数：\\(R_i = 0.1 * R_i^{format} + 0.9 * R_i^{answer}\\)。其中，格式得分 \\(R_i^{format}\\) 验证轨迹是否符合预定义格式（如标签使用是否正确）；答案得分 \\(R_i^{answer}\\) 使用LLM作为裁判判断最终预测是否正确。\n    5.  **策略优化**：使用**分组相对优势估计**（GPRO）计算优势 \\(\\hat{A}_{i,t}\\)，并应用**token级策略梯度损失**和**非对称裁剪**技术（来自DAPO）。优化目标函数如公式(3)和(4)所示。\n-   **输出**：优化后的策略模型参数 \\(\\theta\\)。\n-   **设计理由**：Web智能体RL训练缓慢的核心瓶颈在于与环境交互的rollout速度。DUPO通过**复制采样**替代DAPO的顺序采样填充批次，避免了为不同案例顺序执行rollout，将训练速度提升了约2-3倍。同时，过滤无方差样本能集中计算资源于那些策略尚未稳定、能提供有效学习信号的困难样本上，提高了样本效率。\n\n**§3 关键公式与算法（如有）**\n论文中的核心训练目标函数（DUPO）如下：\n\n\\[\\begin{array}{l} \\mathcal {J} (\\theta) = \\mathbb {E} _ {(q, y) \\sim \\mathcal{D}, \\{o _ {i} \\} _ {i = 1} ^ {G} \\sim \\pi_ {\\theta_ {\\mathrm {o l d}}} (\\cdot | c o n t e x t)} \\ \\left[ \\frac {1}{\\sum_ {i = 1} ^ {G} \\left| o _ {i} \\right|} \\sum_ {i = 1} ^ {G} \\sum_ {t = 1} ^ {\\left| o _ {i} \\right|} \\min  \\left(r _ {i, t} (\\theta) \\hat {A} _ {i, t}, \\operatorname {c l i p} \\left(r _ {i, t} (\\theta), 1 - \\varepsilon_ {l o w}, 1 + \\varepsilon_ {h i g h}\\right) \\hat {A} _ {i, t}\\right) \\right] \\tag {3} \\ \\text {s . t .} \\quad 0 <   \\left| \\left\\{o _ {i} \\mid \\text {i s \\_ e q u i v a l e n t} (y, o _ {i}) \\right\\} \\right| <   G, \\ \\end{array}\\]\n\n其中，\\(r_{i,t}(\\theta) = \\frac{\\pi_{\\theta}(o_{i,t}|context)}{\\pi_{\\theta_{old}}(o_{i,t}|context)}\\) 为重要性采样比率，\\(\\hat{A}_{i,t} = \\frac{R_i - \\operatorname{mean}(\\{R_i\\}_{i=1}^G)}{\\operatorname{std}(\\{R_i\\}_{i=1}^G)}\\) 为分组相对优势估计器。\n约束条件确保组内既有正确也有错误的rollout答案，以保证优势估计的有效性。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n论文主要对比了以下训练流程变体：\n1.  **WebSailor (完整流程)**：包含 **SailorFog-QA数据合成 → 推理重建 → RFT冷启动 → DUPO RL** 的完整流程。\n2.  **无RFT冷启动的Direct RL**：直接对基础指令模型（Qwen-2.5-instruct-32B）应用DUPO进行强化学习，跳过RFT冷启动阶段。\n3.  **仅RFT**：仅使用经过过滤和推理重建的数据进行拒绝采样微调，不进行后续的RL训练。\n此外，在数据层面，SailorFog-QA与基线方法（如WebDancer）使用的训练数据在复杂度（工具调用次数分布）上存在本质差异，后者更偏向简单任务。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上存在本质区别：\n1.  **与WebDancer、WebThinker等开源智能体的差异**：\n    -   **数据生成**：现有方法多基于现有QA数据集或简单爬取的数据进行训练，任务复杂度低（WebDancer数据集中超过50%的轨迹仅需2次工具调用）。而WebSailor通过**图采样与信息模糊化**，专门合成具有**高初始不确定性且难以降低**的Level 3任务，其工具调用分布与目标基准BrowseComp高度相似，更具挑战性。\n    -   **监督信号**：现有方法通常直接使用开源LRM的原始输出进行SFT，导致风格污染和上下文过载。WebSailor创新性地提出**推理重建**，从专家动作序列中反推简洁思维，获得了更干净、更高效的监督信号。\n2.  **与DAPO等RL方法的差异**：\n    -   **训练效率**：DAPO采用动态采样过滤后，需要为不同案例顺序执行rollout来填充批次，加剧了训练缓慢的问题。WebSailor提出的**DUPO**通过**复制同一批次内标准差非零的样本**来填充批次，实现了并行化，将训练速度提升了2-3倍。\n    -   **训练流程**：现有RL方法常直接从零开始或仅经过简单SFT后开始RL。WebSailor强调了**RFT冷启动**在复杂、稀疏奖励任务中的必要性，实验证明其能提供稳定的工具使用骨架，避免RL初期探索失败，最终收敛性能显著优于直接RL。\n3.  **与专有系统（如DeepResearch）的差异**：\n    -   **可复现性与透明度**：专有系统架构和训练细节不公开。WebSailor提供了一个完整、开源、可复现的后训练流水线，从数据合成到训练算法均有详细描述，旨在将“超人推理”能力 democratize。",
    "methodology_and_formulas": "【三、方法论细节与关键公式】\n\n**§1 完整算法流程（伪代码级描述）**\nWebSailor完整训练流程如下：\n**Step 1: 合成SailorFog-QA数据**\n1.  从Wikidata SPARQL服务获取稀有实体作为种子。\n2.  使用search和visit工具获取种子实体特征，构建初始图谱节点和边。\n3.  迭代扩展图谱：以概率p选择现有节点或新实体作为扩展点，寻找新实体连接，避免线性链，形成复杂互联图。\n4.  从复杂图中采样具有多样拓扑的子图。\n5.  对子图信息进行模糊化处理（如模糊时间、部分隐藏名称）。\n6.  基于模糊化后的子图生成QA对。\n\n**Step 2: 生成并重建专家推理轨迹**\n1.  对于每个SailorFog-QA问题，使用专家LRM（如QwQ-32B）生成完整的ReAct格式轨迹（含其原生思维）。\n2.  丢弃专家LRM的原生冗长思维，仅保留正确的动作-观察序列 \\((a_0",
    "3": "RFT冷启动训练**\n1.  **轨迹过滤**：对重建的轨迹进行三层过滤：a) 最终答案正确；b) 轨迹总长度 ≤ 32k tokens；c) 工具调用次数 > 5。\n2.  **监督微调**：使用过滤后的轨迹进行SFT。训练时，**掩码（mask）掉所有环境观察（Observation）对应的token**，仅对思维（Thought）和动作（Action）token计算损失。\n3.  **超参数**：批量大小32，学习率5e-6（带warmup和cosine decay，最小1e-10），权重衰减0.1。\n\n**Step 4: DUPO强化学习训练**\n1.  **初始化**：使用RFT后的模型作为初始策略 \\(\\pi_{\\theta_{old}}\\)。\n2.  **批次构建**：采样一批问题 \\((q, y)\\)。\n3.  **预过滤**：对每个问题执行G=8次rollout。移除该批次中**所有8个rollout答案都正确或都错误**的问题。\n4.  **训练循环**：\n    a. **Rollout**：对批次中剩余的问题，使用当前策略并行执行G=8次rollout，获得轨迹和奖励 \\(R_i\\)（计算公式(5)）。\n    b. **动态采样**：计算每个问题下G个rollout奖励的标准差。移除**标准差为零**的问题样本。\n    c. **批次填充**：**随机复制同一批次内标准差非零的样本**，直至填满原始批次大小（如128）。\n    d. **优势估计**：对于批次中每个样本的每个token，使用分组相对优势估计器计算优势 \\(\\hat{A}_{i,t}\\)（公式(4)）。\n    e. **策略更新**：根据公式(3)计算策略梯度损失，使用PPO风格的裁剪（\\(\\varepsilon_{low}\\)和\\(\\varepsilon_{high}\\)，具体值原文未提供）进行优化。学习率为1e-6。\n    f. 重复步骤a-e，共训练50步（由于同步RL框架效率限制）。\n\n**§2 关键超参数与配置**\n-   **数据生成**：\n    -   图扩展停止条件：边数达到**预定义值**（具体数值原文未提供）。\n    -   信息模糊化策略：包括将精确日期改为模糊时期、部分掩盖名称、定量属性定性描述等。\n-   **轨迹过滤**：\n    -   轨迹最大长度：**32k tokens**（因专家模型上下文能力优于策略模型）。\n    -   最小工具调用次数：**>5次**（以保留体现复杂推理模式的轨迹）。\n-   **RFT训练**：\n    -   批量大小：**32**\n    -   学习率：**5e-6**，采用warmup + cosine decay调度，最小值为**1e-10**。\n    -   权重衰减：**0.1**\n-   **DUPO RL训练**：\n    -   组大小（G）：**8**（每个问题并行rollout次数）。\n    -   Rollout超参数：温度 **1.0**，top-p **1.0**。\n    -   批次大小：**128**，小批次大小：**32**。\n    -   学习率：**1e-6**。\n    -   训练步数：**50步**（因同步RL框架效率限制）。\n    -   奖励函数权重：格式奖励 \\(R_i^{format}\\) 权重 **0.1**，答案奖励 \\(R_i^{answer}\\) 权重 **0.9**。\n    -   裁剪系数 \\(\\varepsilon_{low}\\) 和 \\(\\varepsilon_{high}\\)：原文未提供具体数值。\n-   **推理评估**：\n    -   评估温度：**0.6**\n    -   评估top-p：**0.95**\n    -   最大工具调用次数：**30次**（通过Qwen-Agent框架限制）。\n\n**§3 训练/微调设置（如有）**\n-   **训练数据**：SailorFog-QA，通过图采样与信息模糊化自动生成，规模未明确给出，但强调其高度可扩展（子图数量随图大小非线性增长）。\n-   **专家模型**：用于生成初始轨迹的LRM为**QwQ-32B**。用于重建思维的指令模型 \\(\\pi^*\\) 未具体说明型号。\n-   **基础模型**：在**Qwen-2.5**系列的3B、7B、32B、72B模型上进行后训练。\n-   **训练框架**：SFT使用**Megatron**，RL训练使用**verl**。\n-   **优化器与调度**：SFT使用AdamW优化器，学习率调度为warmup后接cosine decay。RL具体优化器未说明。\n-   **硬件与耗时**：原文未提供。\n\n**§4 推理阶段的工程细节**\n-   **智能体框架**：采用**ReAct**框架，实现Thought-Action-Observation循环。\n-   **工具集**：\n    -   **Search工具**：调用Google搜索引擎，支持同时搜索多个查询，每个查询返回前10个结果（包含标题、摘要、URL）。\n    -   **Visit工具**：访问特定网页。输入为网页URL和访问目标（goal）。首先使用**Jina**检索网页完整内容，然后使用**Qwen-2.5-72B**作为摘要模型，根据目标提取相关信息。\n-   **上下文管理**：轨迹中不同部分（Thought, Action, Observation）使用特殊标签（如`",
    "source_file": "WebSailor Navigating Super-human Reasoning for Web Agent.md"
}