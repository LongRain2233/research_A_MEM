{
    "title": "Scaling Agents via Continual Pre-training",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n大型语言模型（LLMs）已演化为能够自主使用工具并进行多步推理以解决复杂问题的智能体系统。当前研究的核心应用场景是**深度研究智能体（Deep Research Agents）**，即能够通过搜索、浏览、代码执行等工具自主编排复杂工作流，以完成知识密集型任务并提供可信答案的智能体（如OpenAI Deep Research）。然而，基于通用基础模型的后训练方法（如监督微调SFT、强化学习微调RL）在智能体任务上表现不佳，特别是在开源实现中。本文的研究动机在于：识别出通用基础模型缺乏**智能体归纳偏置（agentic inductive biases）**，导致后训练阶段模型必须同时学习多样化的智能体行为并使其与专家演示对齐，从而产生根本性的优化冲突。因此，本文旨在重新定义智能体对齐的训练流程，引入**智能体持续预训练（Agentic Continual Pre-training, Agentic CPT）**作为中间层，以构建强大的智能体基础模型。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法在深度研究任务上存在显著性能差距，具体失败模式如下：\n1.  **基于通用基础模型的后训练方法（SFT/RL）**：当面对需要遍历广阔策略空间的复杂任务时，SFT依赖完整、高质量轨迹数据，导致覆盖范围不足；同时，智能体轨迹本质上是长且复杂的，使得“正确行为”的精确定义变得困难。这导致模型被锁定在复制特定行为模式，而非发展灵活的决策能力。例如，在BrowseComp-en基准测试中，WebSailor-72B仅得12.0分，GLM-4.5得26.4分，DeepSeek-V3.1得30.0分，均远低于OpenAI Deep Research的51.5分。\n2.  **轨迹数据利用不充分**：在后训练阶段，拒绝采样微调和RL会产生大量轨迹数据，但它们严重依赖轨迹级别的延迟反馈进行质量评估。这导致许多轨迹因未达到严格的质量阈值而被完全丢弃或仅使用一次，造成了学习信号的巨大浪费。\n3.  **缺乏对决策过程的显式建模**：现有方法主要学习模仿完整的轨迹序列，而非在关键步骤上进行决策。当智能体在单一推理-行动轮次中生成多个候选方案（如替代查询或探索方向）时，这些候选方案仍然是同一路径的内部分支，监督主要奖励的是完整轨迹的再现。这限制了模型在动态环境中应对工具故障、误导信息等不可预测变化的能力。\n\n**§3 问题的根本难点与挑战（200字以上）**\n问题的根本难点源于智能体对齐的复杂性和数据可扩展性瓶颈：\n1.  **优化冲突**：通用基础模型缺乏智能体行为的先验知识，迫使后训练过程必须同时学习**能力（capabilities）**和**对齐（alignment）**，这产生了内在的优化张力。模型需要在模仿特定行为模式与保持探索灵活性之间取得平衡。\n2.  **数据生成的可扩展性与成本**：为智能体训练生成高质量的轨迹数据通常需要调用昂贵的商业API（如Google Search API、Jina Reader API），并且完整的轨迹生成效率低下，无法满足持续预训练所需的海量数据需求。\n3.  **决策空间探索不足**：智能体任务涉及高维、连续的动作空间（如选择哪个搜索词、何时停止搜索、如何整合信息）。现有方法难以有效探索这个空间，导致模型容易过拟合到有限的演示轨迹上，泛化能力弱。\n4.  **长上下文与多步推理**：深度研究任务通常涉及长篇幅的文档处理和跨多轮工具调用的复杂推理，这对模型的上下文理解能力和长期规划能力提出了极高要求。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**在预训练和后训练之间引入智能体持续预训练（Agentic CPT）阶段**，旨在提供一个**预对齐的智能体基础模型**，使其天然支持智能体行为，从而便于下游微调。其核心假设是：通过大规模、多样化的**合成智能体数据**进行持续预训练，可以在模型内部植入智能体行为的归纳偏置，从而缓解后训练阶段的优化冲突。\n\n本文的核心技术假设基于两个原则：\n1.  **数据收集的广度原则**：种子数据源必须广泛，不局限于单一领域，以确保习得的智能体能力具有广泛的迁移性和适用性。\n2.  **训练数据的多样性原则**：训练数据必须全面包含各种类型的智能体行为（如规划、推理、工具调用），防止模型模仿和记忆特定的行为模式，从而损害其行为探索能力。\n\n为实现此，本文提出了系统化、可扩展的数据合成方法，包括**一阶动作合成（FAS）**和**高阶动作合成（HAS）**，以及**两阶段训练策略**。FAS在**零监督信号**下运作，通过知识到问题的转化和多样化的动作合成来生成数据；HAS则利用**有监督信号**（轨迹数据），通过步骤级扩展将轨迹转化为多决策过程，从而更充分地利用轨迹数据并促进决策学习。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\nAgentFounder的训练流程是对标准LLM开发范式的根本性重新设计。整体架构包含三个阶段，数据流向如下：\n1.  **输入**：Qwen系列预训练基础模型（如Qwen3-30B-A3B-Base）。\n2.  **Agentic CPT Stage 1（初步能力获取）**：模型处理约200B Token的智能体数据和知识推理语料，上下文长度为32K。遵循标准的下一Token预测范式（交叉熵损失）。此阶段使模型初步获得工具调用模式和多步推理链等智能体行为。\n3.  **Agentic CPT Stage 2（能力精炼）**：模型使用100B Token精心策划的高质量智能体数据进行训练，上下文长度扩展至128K。此阶段使LLM发展出对复杂动作空间和长期规划策略的深入理解。\n4.  **下游Post-training（能力解锁与对齐）**：使用不同的SFT配置（SFT-A/B/C）对经过Agentic CPT得到的`AgentFounder-30B-Base`进行微调，以解锁其潜力并完成最终的智能体对齐。\n5.  **最终输出**：`AgentFounder-30B`模型，具备强大的深度研究智能体能力。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：一阶动作合成（First-order Action Synthesis, FAS）\n- **模块名**：First-order Action Synthesis (FAS)\n- **输入**：多样化的静态知识源，包括废弃的轨迹数据、历史工具调用结果（搜索查询和响应）、公开语料（如CommonCrawl）。\n- **核心处理逻辑**：FAS包含两个子模块：\n  1.  **知识到问题转化（Knowledge-to-Question Transformation）**：\n     - **阶段1（实体锚定的开放世界知识记忆）**：将非结构化文本转化为以实体为索引键、关联陈述为值的开放世界记忆。不关注实体间关系，而是通过重新表述来增强知识陈述的密度，保留时间标记、来源等关键信息。\n     - **阶段2（多风格问题合成）**：从实体锚定的记忆中采样实体簇及其关联知识陈述，合成涵盖事实检索、数值计算、多跳推理和综合任务的不同风格问题。\n  2.  **动作合成**：基于合成的问题，进一步生成两种类型的数据：\n     - **规划动作合成（Planning Action Synthesis）**：对于每个问题Q，使用LLM生成K个不同的问题分析及其对应的第一步动作预测（工具调用或直接答案）。为提高多样性，改为为K个不同风格但共享相同知识记忆的问题生成推理-动作数据。采用**基于知识对齐验证的拒绝采样**，使用LLM-as-Judge评估当前推理和动作是否有高概率获取所需知识，以过滤低质量数据。\n     - **推理动作合成（Reasoning Action Synthesis）**：基于QA对生成逻辑推理数据。采用两步法：\n       - **步骤1**：要求LLM将问题Q分解为多个子问题，利用其内部知识为每个子问题生成合理的推测和答案，产生初步答案A1。\n       - **步骤2**：给定问题Q及其映射的必备知识，要求模型精炼答案A1，纠正逻辑错误，生成最终答案A2。\n       同样采用LLM-as-Judge进行拒绝采样，评估生成的答案A2与真实答案的对齐情况。\n- **输出**：高质量的（问题，规划，动作）数据元组和逻辑推理链数据。\n- **设计理由**：传统方法依赖真实API调用来生成轨迹数据，成本高昂且不可扩展。FAS通过合成方法在**离线环境**中大规模生成数据，无需API成本。通过知识到问题的转化，将静态知识转化为动态的问题解决上下文，模拟真实的智能体场景。通过生成多个推理-动作选项，有效扩展了每个问题的动作空间探索。\n\n#### 模块二：高阶动作合成（High-order Action Synthesis, HAS）\n- **模块名**：High-order Action Synthesis (HAS)\n- **输入**：后训练阶段产生的轨迹数据T = {(S1, R1), ..., (SK, RK)}，其中Sk代表第k步的“推理和工具调用”，Rk代表相应的工具/环境响应。整个轨迹带有一个二元判断J∈{0,1}表示失败/成功。\n- **核心处理逻辑**：HAS包含两个组件：\n  1.  **步骤级扩展（Step-level Scaling）**：对于任何步骤Sk，其条件上下文为Ck = (Q, S1, R1, ..., Sk-1, Rk-1)。在不实际执行工具的情况下，使用LLM为上下文Ck生成N个替代的“思考和调用”候选：Ak = {Sk^(1), ..., Sk^(N)}。将原始步骤Sk^(0) ≡ Sk与这些候选合并，得到N+1个可行步骤，然后随机打乱它们形成序列Ãk，同时记录原始步骤在序列中的位置nk。\n  2.  **对比决策-动作合成（Contrastive Decision-Action Synthesis）**：将带有扩展选项的轨迹转化为渐进式决策过程。对于第k步，枚举Ãk中的每个选项，并插入一个本地动作决策声明：“我将选择选项nk'。”，紧接着是相应的真实响应Rk。最后，附加判断文本：“我的决定是{正确/错误}”（对应于J）。完整的合成训练样本通过拼接问题、每一步的选择-决策过程以及最终的判断文本来获得。\n- **输出**：转化为多决策处理文本的合成训练数据。\n- **设计理由**：轨迹数据通常只被使用一次，学习信号浪费严重。HAS将目标从**轨迹模仿**转变为**步骤级决策**，显式地利用每一步的选择空间。它避免了直接使用不确定的步骤级奖励所带来的风险，同时使模型能够从多样化的推理路径中学习，防止过拟合到特定的轨迹模式。\n\n#### 模块三：两阶段渐进训练策略（Progressive Two-stage Training Strategy）\n- **模块名**：Progressive Two-stage Training Strategy\n- **输入**：FAS数据和HAS数据。\n- **核心处理逻辑**：\n  - **第一阶段**：主要使用FAS数据和**短**HAS数据（在32K上下文窗口内）。目标是让模型初步掌握智能体行为，包括工具调用模式和基本的推理链。\n  - **第二阶段**：专注于使用**高质量**的HAS数据，并扩展上下文长度至128K。目标是精炼模型能力，使其能够处理复杂的动作空间和长视野规划策略。\n- **输出**：经过Agentic CPT的预对齐智能体基础模型`AgentFounder-30B-Base`。\n- **设计理由**：分阶段训练可以更高效地吸收两种类型的数据。第一阶段使用较短上下文的数据建立基础能力；第二阶段引入长上下文的高质量数据，进一步提升模型处理复杂、长序列任务的能力，这对于深度研究任务至关重要。\n\n**§3 关键公式与算法（如有）**\n本文遵循标准的预训练范式，使用交叉熵损失进行下一Token预测：\n\\[ \\mathcal{L} = - \\sum_{t=1}^{T} \\log P\\left(x_{t+1} \\mid x_{1}, x_{2}, \\dots, x_{t}\\right) \\]\n其中 \\( P(x_{t+1} | x_1, x_2, ..., x_t) = \\mathrm{softmax}(W_o h_t) \\)，\\( h_t \\) 表示位置t的隐藏状态，\\( W_o \\) 是输出投影矩阵。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n本文在Post-training阶段对比了三种不同的SFT配置：\n- **SFT-A**：采用两阶段训练范式，首先在通用对话数据上训练，然后在具有显式推理链的专用React风格智能体轨迹上训练。\n- **SFT-B**：SFT-A的增强版，保持两阶段训练范式，但在每个阶段都融入通用对话数据和React风格轨迹的平衡混合。\n- **SFT-C**：采用两阶段训练范式，使用通用对话SFT数据和带有总结推理的React轨迹。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n1.  **与标准后训练方法（SFT/RL）的本质区别**：现有方法（如WebSailor、GLM-4.5）直接在通用基础模型（如Qwen2.5-72B）上进行SFT或RL，试图同时学习智能体能力和对齐。本文则引入了一个**额外的预训练阶段（Agentic CPT）**，专门用于构建**智能体基础模型**。这解决了通用基础模型缺乏智能体归纳偏置的核心瓶颈，将“能力学习”与“对齐学习”解耦。\n2.  **与依赖真实API调用的数据生成方法的区别**：许多智能体训练方法需要昂贵的真实工具调用来生成轨迹数据。本文的FAS和HAS方法完全在**离线环境下**合成数据，无需调用任何外部API，从而实现了大规模、低成本的智能体数据生成。\n3.  **与轨迹级模仿学习的区别**：传统方法让模型学习模仿完整的成功轨迹。本文的HAS方法创新性地将轨迹数据重构为**步骤级的对比决策过程**，让模型在每一步面对多个选项并做出选择。这更侧重于**决策能力**的培养，而非简单的序列复制，有助于模型发展更灵活的探索和决策策略。\n4.  **与单一阶段训练的区别**：本文采用**两阶段渐进训练**，先使用短上下文数据建立基础，再使用长上下文数据精炼能力。这与许多直接将所有数据混合训练的方法不同，能更有效地处理不同长度和复杂度的数据，优化训练效率。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n论文未提供完整的算法伪代码框，但根据描述，AgentFounder的训练流程可概括为以下步骤：\n**Step 1: 基础模型初始化**：从Qwen系列预训练基础模型（如Qwen3-30B-A3B-Base）开始。\n**Step 2: Agentic CPT 数据合成**：\n  - **2.1 一阶动作合成（FAS）**：\n    - a. **知识到问题转化**：从多样数据源构建实体锚定的开放世界知识记忆。采样实体簇及关联知识陈述，合成多风格问题Q。\n    - b. **规划动作合成**：对于每个（或每组）问题Q，使用LLM生成K个不同的分析及第一步动作预测。采用基于知识对齐验证的拒绝采样过滤低质量数据。\n    - c. **推理动作合成**：对于每个QA对，使用LLM执行两步逻辑推理（分解子问题→生成初步答案A1→结合必备知识精炼为最终答案A2）。采用LLM-as-Judge拒绝采样确保答案正确性。\n  - **2.2 高阶动作合成（HAS）**：\n    - a. 输入后训练产生的轨迹数据T = {(S1, R1), ..., (SK, RK)}及成功标签J。\n    - b. 对于轨迹中每一步Sk，给定其上下文Ck，使用LLM生成N个替代的“思考和调用”候选Ak。\n    - c. 将原始步骤Sk与候选合并，随机打乱形成选项序列Ãk，记录原始步骤位置nk。\n    - d. 将轨迹转化为决策文本：对于每一步，枚举Ãk中的每个选项，插入决策声明“我将选择选项nk'。”，后接真实响应Rk。最后附加整体判断“我的决定是{Correct/Incorrect}（对应J）”。\n**Step 3: 两阶段Agentic CPT训练**：\n  - **Stage 1**：使用约200B Token的FAS数据和短HAS数据，上下文长度32K，进行下一Token预测训练。\n  - **Stage 2**：使用约100B Token的高质量HAS数据，上下文长度128K，进行下一Token预测训练。输出预对齐的智能体基础模型`AgentFounder-30B-Base`。\n**Step 4: 下游Post-training**：使用三种不同的SFT数据集配置（SFT-A/B/C）对`AgentFounder-30B-Base`进行监督微调，得到最终的`AgentFounder-30B`模型。\n\n**§2 关键超参数与配置**\n- **Agentic CPT数据量**：第一阶段约200B Tokens，第二阶段约100B Tokens。\n- **上下文长度**：第一阶段为32K Tokens，第二阶段为128K Tokens。\n- **推理超参数**：评估时使用temperature=0.85，repetition penalty=1.1，top-p=0.95。这些设置基于大量经验验证，以优化智能体推理任务中创造性与一致性的平衡。\n- **工具使用限制**：每个任务最大工具调用次数限制为128次。\n- **上下文长度限制**：推理时约束为128K Tokens。\n- **FAS中的K值**：在规划动作合成中，为每个问题生成K个不同的分析及动作预测（原文未明确给出具体K值，但提及了此策略）。\n- **HAS中的N值**：在步骤级扩展中，为每个步骤生成N个替代候选（原文未明确给出具体N值）。\n\n**§3 训练/微调设置（如有）**\n- **Post-training数据构造**：遵循WebSailor-V2、WebResearcher、WebWeaver和AgentScaler的方法，构建具有挑战性的信息寻求问题集。具体使用了三种SFT配置（SFT-A/B/C），混合了通用对话数据和React风格的智能体轨迹（有的带有显式推理链，有的带有总结推理）。\n- **优化器与学习率**：原文未提供具体信息。\n- **批次大小与训练轮数**：原文未提供具体信息。\n\n**§4 推理阶段的工程细节**\n- **工具集**：默认配备五个核心工具：**Search**（带结果排名的网页搜索）、**Visit**（网页内容提取）、**Google Scholar**（学术文献访问）、**Python Interpreter**（代码执行）和**File Parser**（文档处理）。\n- **并行化与缓存**：原文未提供具体信息。\n- **向量数据库**：未使用，因为本文是模型训练方法而非检索增强生成（RAG）系统。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n评估分为两大类基准测试：\n#### 通用网页搜索基准：\n1.  **BrowseComp-en**：英文浏览与搜索评测基准，具体规模未提供，问题类型为复杂的开放域问答，需要多步搜索和推理。\n2.  **BrowseComp-zh**：中文浏览与搜索评测基准，与BrowseComp-en风格相似但针对中文语境。\n3.  **GAIA**：通用AI助手基准，评估模型在需要多步推理和工具使用的真实世界任务上的表现。本文评估限于其文本子集。\n4.  **Xbench-DeepSearch**：深度搜索评测基准，具体规模未提供。\n5.  **WebWalkerQA**：网页导航与问答基准，具体规模未提供。\n#### 场景针对性网页搜索基准：\n6.  **DeepResearch Bench**：评估跨多个学术领域的综合研究报告生成能力。包含RACE Overall指标。\n7.  **SEAL-0**：评估模型在面对冲突或误导性搜索结果时的鲁棒性。\n8.  **Frames**：评估模型进行多视角推理和基于角色的信息综合能力，要求在不同上下文框架中一致地整合证据。\n9.  **HLE (Humanity’s Last Exam)**：评估模型在多样学科专家级问题上的表现。\n10. **Academic Browse**：专注于学术研究能力，如文献导航和知识合成。\n\n**§2 评估指标体系（全量列出）**\n- **准确性指标**：所有基准均报告**Pass@1**（单次尝试通过率）。部分基准（如BrowseComp, GAIA）额外报告了**Pass@3**（三次尝试最佳通过率）。在DeepResearch Bench上报告了**RACE Overall**分数，用于衡量生成报告的综合质量、可读性和深度。\n- **效率/部署指标**：原文未提供延迟、Token消耗、显存占用等效率指标。\n- **其他自定义指标**：无。\n\n**§3 对比基线（完整枚举）**\n#### 通用LLMs配合工具：\n1.  **Qwen3-30B-A3B-2507**：开源通用模型，作为本文的起点模型。\n2.  **Qwen3-235B-A22B-2507**：更大规模的Qwen模型。\n3.  **DeepSeek-R1-0528**：专注于推理的模型。\n4.  **Claude-4-Sonnet**：商业通用模型。\n#### 商业深度研究智能体：\n5.  **Kimi-Researcher**\n6.  **OpenAI-o3**\n7.  **OpenAI Deep Research**\n8.  **Grok Deeper Search**\n9.  **Perplexity Deep Research**\n10. **Gemini Deep Research**\n#### 开源深度研究智能体：\n11. **WebThinker-32B-RL**\n12. **ASearcher-Web-QwQ**\n13. **WebSailor-72B**\n14. **WebShaper-72B**\n15. **AFM-32B-RL**\n16. **MiroThinker-32B-DPOv0.2**\n17. **DeepDiver-V2-38B**\n18. **WebExplorer-8B**\n19. **DeepDive-32B**\n20. **Kimi-K2-Instruct**\n21. **GLM-4.5**\n22. **DeepSeek-V3.1**\n\n**§4 实验控制变量与消融设计**\n1.  **训练策略消融（RQ3）**：比较“仅使用第一阶段训练”与“完整两阶段训练”的效果，控制总训练Token数为50B，使用相同的SFT-A数据。\n2.  **数据类型消融（RQ4）**：比较“无CPT”、“仅FAS数据”、“FAS+HAS数据”三种配置，控制总训练Token数为50B，使用相同的SFT-A数据。\n3.  **后训练数据适应性（RQ2）**：使用相同的`AgentFounder-30B-Base`基础模型，分别使用三种不同的SFT数据集（SFT-A, SFT-B, SFT-C）进行微调，评估其性能差异，以验证基础模型对不同后训练范式的适应性。\n4.  **缩放定律探究（RQ5）**：通过增加训练数据量（Token数）和改变模型规模，观察性能平均增益，以验证缩放定律行为。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n以下为论文表1和表2的完整还原（数值后带†表示来自官方或先前工作报告）：\n`方法名 | BrowseComp-en | BrowseComp-zh | GAIA | Xbench-DeepSearch | WebWalkerQA | HLE Pass@1 | DeepResearch Bench RACE Overall | Frames Pass@1 | SEAL-0 Pass@1 | AcademicBrowse Pass@1`\n`Qwen3-30B-A3B | 0.5 | 13.5 | 35.9 | 32.0 | 46.9 | 13.2 | 40.2 | 56.4 | 9.9 | 41.3`\n`Qwen3-235B-A22B | 2.3 | 29.4 | 45.6 | 46.0 | 59.6 | 20.0 | 44.8 | - | 14.4 | 50.7`\n`DeepSeek-R1 | 8.9† | 35.7† | - | 55.0† | - | 24.8† | - | 82.0† | 29.7† | -`\n`Claude-4-Sonnet | 12.2† | 29.1† | 68.3† | 64.6† | 61.7† | 20.3† | - | 80.7† | - | -`\n`Kimi-Researcher | - | - | - | 69.0† | - | 26.9† | 44.6† | 78.8† | 36.0† | -`\n`OpenAI-o3 | 49.7† | 58.1† | 70.5† | 66.0† | 71.7† | 20.2† | - | 84.0† | - | -`\n`OpenAI Deep Research | 51.5† | - | 67.0† | - | - | 26.6† | 46.5† | - | - | -`\n`Grok Deeper Search | - | - | - | - | - | - | 38.2† | - | - | -`\n`Perplexity Deep Research | - | - | - | - | - | 21.1† | 40.5† | - | - | -`\n`Gemini Deep Research | - | - | - | - | - | 26.9† | 49.7† | - | - | -`\n`WebThinker-32B-RL | 2.8† | 7.3† | 48.5† | 24.0† | 46.5† | - | - | - | - | -`\n`ASearcher-Web-QwQ | 5.2† | 15.6† | 52.8† | 42.1† | 34.3† | 12.5† | - | 70.9† | - | -`\n`WebSailor-72B | 12.0† | 30.1† | 55.4† | 55.0† | - | - | - | - | - | -`\n`WebShaper-72B | - | - | 60.1† | - | 52.2† | - | - | - | - | -`\n`AFM-32B-RL | 11.1† | - | 55.3† | 63.0† | - | - | - | - | - | -`\n`MiroThinker-32B-DPOv0.2 | 17.2† | 29.4† | 64.1† | 56.0† | 53.6† | 17.8† | - | 74.8† | - | -`\n`DeepDiver-V2-38B | 13.4† | 34.6† | - | 53.0† | - | - | - | - | - | -`\n`WebExplorer-8B | 15.7† | 32.0† | 50.0† | 53.7† | 62.7† | 17.3† | - | 75.7† | - | -`\n`DeepDive-32B | 14.8† | 25.6† | - | 50.5† | - | - | - | 76.1† | 29.3† | -`\n`Kimi-K2 | 14.1† | 28.8† | 57.3† | 50.0† | 63.0† | 18.1 | 25.4 | 72.0† | 25.2 | 47.3`\n`GLM-4.5 | 26.4† | 37.5† | 66.0† | 70.0† | 65.6† | 21.2† | 39.2 | 78.9† | 34.2 | 55.6`\n`DeepSeek-V3.1 | 30.0† | 49.2† | 63.1† | 71.0† | 61.2† | 29.8† | 35.4 | 83.7† | 42.6† | 65.0`\n`AgentFounder-30B | 39.9 | 43.3 | 72.8 | 73.0 | 71.9 | 31.5 | 47.9 | 89.6 | 43.9 | 75.3`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n- **BrowseComp-en**：AgentFounder-30B得分为39.9，**超越了所有开源深度研究智能体**。相比最佳开源模型DeepSeek-V3.1（30.0），绝对提升9.9个点，相对提升33.0%。与商业模型OpenAI-o3（49.7）和OpenAI Deep Research（51.5）的差距显著缩小。这表明AgentFounder-30B有效掌握了复杂的搜索策略和推理能力。\n- **BrowseComp-zh**：AgentFounder-30B得分为43.3，虽然超过了GLM-4.5（37.5），但与DeepSeek-V3.1（49.2）和OpenAI-o3（58.1）仍有差距。作者归因于两个潜在原因：1) 训练语料中中文数据比例相对有限；2) 底层搜索工具（Google Search）在中文语境下可能表现不佳或存在偏差。\n- **GAIA**：AgentFounder-30B取得了72.8的最高分，超越了所有对比模型，包括OpenAI-o3（70.5）。尽管仅限于GAIA的文本子集，但这表明AgentFounder的能力超越了检索推理本身，可以迁移到更广泛的任务类别，揭示了其作为通用智能体的潜力。\n- **HLE**：AgentFounder-30B得分为31.5，成为**首个超过30分阈值的开源模型**，显著超过了所有已报告的闭源深度研究产品，包括Gemini-2.5-Pro Deep Research、Kimi-Researcher和OpenAI Deep Research。这表明其在专家级跨学科问题上的强大能力。\n- **Academic Browse**：AgentFounder-30B得分为75.3，大幅超过所有现有开源模型（如DeepSeek-V3.1的65.0），证明了其作为学术助手的价值。\n- **Frames**：AgentFounder-30B得分为89.6，显著优于所有开源和闭源模型，展示了其在多视角推理和一致性信息综合方面的卓越能力。\n- **SEAL-0**：AgentFounder-30B得分为43.9，全面优于开源深度研究智能体，表明其对信息干扰有很强的抵抗力。\n- **DeepResearch Bench (RACE Overall)**：AgentFounder-30B得分为47.9，超过了OpenAI Deep Research（46.5）和所有开源深度研究智能体，证实了其生成报告的综合性、可读性和深度。\n\n**§3 效率与开销的定量对比**\n原文**未提供**关于延迟、Token消耗、显存占用等效率指标的定量对比数据。\n\n**§4 消融实验结果详解**\n1.  **两阶段训练策略的有效性（表4）**：在总Token数为50B的条件下，比较“仅第一阶段”与“两阶段完整训练”。在BrowseComp-en上，Pass@1从31.4提升至35.5（+4.1，相对提升13.1%）；Pass@3从49.9提升至52.0（+2.1，相对提升4.2%）。在BrowseComp-zh上，Pass@1从34.3提升至37.2（+2.9，相对提升8.5%）；Pass@3从50.5提升至58.5（+8.0，相对提升15.8%）。在GAIA上，Pass@1从69.9提升至72.8（+2.9，相对提升4.1%）；Pass@3从81.6提升至82.5（+0.9，相对提升1.1%）。两阶段训练在三个基准上平均提升了Pass@1 3.3%，Pass@3 3.7%。\n2.  **数据类型贡献（表5）**：在50B Token下，比较“无CPT”、“仅FAS”、“FAS+HAS”。\n    - **BrowseComp-en**：FAS相比无CPT，Pass@1从26.9提升至31.4（+4.5，相对提升16.7%）；加入HAS后，Pass@1保持31.4不变，Pass@3从49.9微增至50.1（+0.2）。\n    - **BrowseComp-zh**：FAS相比无CPT，Pass@1从29.8提升至37.0（+7.2，相对提升24.2%）；加入HAS后，Pass@1进一步提升至40.1（+3.1，相对提升8.4%）。\n    - **GAIA**：FAS相比无CPT，Pass@1从67.0提升至72.8（+5.8，相对提升8.7%）；加入HAS后，Pass@1反而下降至69.9（-2.9，相对下降4.0%），但Pass@3从80.6提升至82.5（+1.9，相对提升2.4%）。作者未解释GAIA上Pass@1下降的原因。\n3.  **后训练数据适应性（表3）**：使用AgentFounder-30B-Base相比Qwen3-30B-A3B-Base，在不同SFT配置下均带来性能提升。\n    - **SFT-A**：平均性能提升5.75%。\n    - **SFT-B**：平均性能提升6.13%，其中BrowseComp-en提升最大（从28.6到39.9，+11.3，相对提升39.5%）。\n    - **SFT-C**：平均性能提升6.45%，其中BrowseComp-en提升最大（从24.5到38.8，+14.3，相对提升58.4%）。\n    - 信息检索任务（如BrowseComp）比知识密集型任务（如HLE）从Agentic CPT中获益更明显。\n\n**§5 案例分析/定性分析（如有）**\n原文提供了两个数据合成的示例，但未提供模型成功或失败的案例分析。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出了智能体持续预训练（Agentic CPT）范式**：首次在预训练和后训练之间引入Agentic CPT作为中间层，旨在构建预对齐的智能体基础模型，从根本上解决了通用基础模型缺乏智能体归纳偏置导致的优化冲突问题。\n2.  **设计了可扩展的智能体数据合成方法**：提出了**一阶动作合成（FAS）**和**高阶动作合成（HAS）**，能够在无需昂贵API调用的离线环境中大规模生成多样化的智能体训练数据，显著降低了数据生成成本。\n3.  **开发了高效的渐进两阶段训练策略**：通过第一阶段（32K上下文）学习基础智能体行为，第二阶段（128K上下文）精炼复杂规划能力，优化了长序列智能体任务的学习效率。\n4.  **实现了卓越的性能表现**：基于Qwen3-30B底座训练的AgentFounder-30B在10个基准测试中取得了最先进的性能，在多个任务上超越了包括DeepSeek-V3.1在内的所有开源模型，甚至在某些指标上超越了商业模型（如GAIA上超越OpenAI-o3）。\n5.  **验证了智能体基础模型的通用适应性**：实验表明，经过Agentic CPT得到的`AgentFounder-30B-Base`能够有效适应不同的后训练范式（SFT-A/B/C），均带来显著性能提升，证明了其作为通用智能体基础模型的潜力。\n\n**§2 局限性（作者自述）**\n1.  **中文性能相对较弱**：在BrowseComp-zh上，AgentFounder-30B（43.3）的性能虽然超过了GLM-4.5（37.5），但仍低于DeepSeek-V3.1（49.2）和OpenAI-o3（58.1）。作者承认这可能源于训练语料中中文数据的比例相对有限，以及底层搜索工具（Google Search）在中文语境下的潜在偏差或次优性能。\n2.  **知识密集型任务的提升有限**：与信息检索任务（如BrowseComp）相比，知识密集型任务（如HLE）从Agentic CPT中获得的提升相对较小。作者指出，这类任务不仅需要成功的信息检索，还需要强大的推理能力来正确利用检索到的知识，这提示增强基础模型的知识理解能力是未来的研究方向。\n\n**§3 未来研究方向（全量提取）**\n1.  **增强基础模型的知识理解能力**：当前的Agentic CPT主要侧重于智能体行为（规划、工具调用、推理）的适应。未来需要研究如何将**知识理解能力**的增强整合到持续预训练中，以更好地支持HLE等知识密集型任务。\n2.  **探索更高效的数据合成与训练策略**：虽然FAS和HAS提供了可扩展的数据生成方法，但其质量和多样性仍有优化空间。未来可以研究更智能的数据过滤、增强和混合策略，以进一步提升训练效率。\n3.  **研究智能体能力的缩放定律**：本文初步观察到了随着训练数据量增加，性能呈现稳定的平均提升，表现出有希望的缩放定律行为。未来需要更系统地研究智能体能力随模型规模、数据量和计算量变化的缩放规律。\n4.  **向通用智能体发展**：AgentFounder在GAIA等通用任务上表现出色，显示了其作为通用智能体的潜力。未来的工作可以探索如何将这种能力扩展到更广泛的任务领域和交互模式中。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **范式创新：重新定义智能体对齐训练流程**：\n    - **理论新颖性**：首次提出将**智能体持续预训练（Agentic CPT）**作为LLM智能体开发的关键中间阶段，挑战了传统的“预训练-后训练”两阶段范式。这为解决智能体任务中“能力学习”与“对齐学习”的冲突提供了新的理论框架。\n    - **实验验证充分性**：通过大量实验（10个基准，超过20个基线）证明了Agentic CPT的有效性，模型在多个任务上达到SOTA，并系统进行了消融实验验证了各组件贡献。\n    - **对领域的影响**：为构建强大的开源智能体基础模型开辟了一条新路径，可能引领后续研究关注“智能体预训练”而非仅仅“智能体微调”。\n2.  **方法论创新：低成本、可扩展的智能体数据合成**：\n    - **理论新颖性**：提出了**FAS**和**HAS**两种合成方法，前者在零监督信号下从静态知识生成动态问题解决上下文，后者将有监督的轨迹数据转化为步骤级决策过程。这两种方法都避免了对真实API调用的依赖。\n    - **实验验证充分性**：通过消融实验证明了两种数据类型的有效性（FAS对BrowseComp提升显著，HAS对某些任务有补充作用）。\n    - **对领域的影响**：大幅降低了智能体训练的数据获取成本，使资源有限的研究者也能进行大规模的智能体模型训练，促进了领域的民主化。\n3.  **工程贡献：开源强大的深度研究智能体模型**：\n    - **理论新颖性**：相对较小（30B参数）的模型在多项任务上超越了规模更大的开源模型（如72B的WebSailor）和部分商业模型，证明了方法的高效性。\n    - **实验验证充分性**：在广泛的任务上进行了全面评估，包括通用搜索、学术研究、多视角推理、抗干扰等，证明了模型的综合能力强。\n    - **对领域的影响**：发布了`AgentFounder-30B`模型和代码，为社区提供了一个强大的开源基线，推动了深度研究智能体的开源生态发展。\n\n**§2 工程与实践贡献**\n- **开源代码与模型**：论文提供了项目网站和GitHub仓库（https://github.com/Alibaba-NLP/DeepResearch），开源了`AgentFounder`模型，使其他研究者可以复现和在此基础上进行改进。\n- **系统化的数据合成流程**：详细描述了FAS和HAS的数据合成方法，为后续研究生成智能体训练数据提供了可操作的蓝图。\n- **两阶段训练策略**：提供了具体的训练配置（数据量、上下文长度），为训练长上下文智能体模型提供了实践经验。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于一个**承上启下的关键位置**。它并非对现有后训练方法（如React、RLHF）的简单改进，而是**开辟了一条新的技术路线**：即通过**专门的持续预训练阶段**来构建智能体基础模型。它继承了通用基础模型强大的知识能力，但通过Agentic CPT植入了智能体行为的归纳偏置。这使得下游的后训练可以更专注于对齐和细化，而非从零开始学习智能体能力。因此，本文可以视为从“通用LLM+后训练”范式向“专用智能体基础模型+后训练”范式演进的重要一步。",
    "professor_critique": "**§1 实验设计与评估体系的缺陷**\n1.  **基准覆盖的全面性不足**：虽然评估了10个基准，但主要集中在**信息检索和问答**任务上。对于智能体至关重要的其他能力，如**长期规划**（超过128步）、**工具组合使用**（同时使用多个异构工具）、**动态环境适应**（工具故障、网络错误处理）、**多模态工具使用**（图像理解、代码调试）等，缺乏系统性评估。\n2.  **效率指标完全缺失**：论文未报告任何关于推理速度（延迟）、计算开销（FLOPs）、内存占用或API调用成本的定量数据。对于旨在实际部署的智能体系统而言，效率与准确性同等重要。`AgentFounder-30B`相比`Qwen3-30B-A3B`增加的300B Token训练成本也未量化。\n3.  **基线对比的公平性存疑**：所有对比模型是否都在**完全相同的工具集、调用限制（128次）和推理超参数（temperature=0.85等）**下进行评估？特别是对于商业API模型（如OpenAI Deep Research），其底层工具和能力可能完全不同，直接比较分数可能无法完全反映方法本身的优劣。\n4.  **缺乏人工评估**：所有结果均基于自动指标（Pass@1, Pass@3）。对于深度研究任务，生成答案的**事实准确性、逻辑连贯性、引用完整性**等维度更需要人工评估。自动指标可能无法捕捉到细微的质量差异。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **合成数据的真实性鸿沟**：FAS和HAS完全依赖LLM合成数据，可能存在**幻觉、偏见累积和分布偏移**问题。虽然采用了拒绝采样，但LLM-as-Judge本身也可能出错。这种“模型生成数据训练模型”的循环可能导致模型在合成数据上表现良好，但在真实世界交互中泛化能力下降。\n2.  **HAS中步骤级扩展的合理性假设**：HAS假设对于给定的轨迹上下文Ck，使用LLM生成的N个替代候选是“可行”的。然而，这些候选可能**在逻辑上不连贯**或**导致后续步骤不可行**，但HAS仍然将其作为正面（如果原始轨迹成功）或负面（如果原始轨迹失败）的决策样本。这可能会向模型注入噪声甚至错误的学习信号。\n3.  **对搜索工具的强依赖与潜在偏差**：模型性能严重依赖于底层搜索工具（如Google Search）的质量和覆盖范围。在BrowseComp-zh上的性能差距部分归因于搜索工具在中文语境下的“次优性能或偏差”。这意味着模型的性能上限受限于外部工具，而非其自身的推理能力。\n4.  **长上下文训练的代价**：第二阶段使用128K上下文长度训练100B Tokens，计算成本极其高昂。论文未讨论是否采用了有效的长上下文优化技术（如FlashAttention），也未提供训练所需的GPU小时数，可复现性对普通研究者构成挑战。\n\n**§3 未经验证的边界场景**\n1.  **对抗性输入与误导信息**：当用户提供包含矛盾信息或故意误导的问题时，模型是否具备**事实核查和逻辑矛盾检测**能力？SEAL-0基准测试了抗干扰性，但更复杂的对抗场景（如混合真假信息的复杂叙述）未经验证。\n2.  **工具不可用或故障**：当核心工具（如Search、Python Interpreter）暂时不可用、返回错误或超时时，模型的**应急处理和工作流调整能力**如何？论文未测试工具故障恢复场景。\n3.  **跨语言与跨文化任务**：模型在**非中英文**语言（如西班牙语、阿拉伯语）或需要特定文化背景知识的任务上表现如何？训练数据中“中文数据比例有限”的局限性暗示其多语言能力可能不足。\n4.  **极其开放域与创造性任务**：对于需要高度创造性思维、无标准答案或需要整合多个不相关领域知识的任务（例如，“设计一个解决城市交通拥堵的创新方案，需结合经济学、工程学和心理学”），模型的规划与综合能力未经测试。\n5.  **长期多轮对话中的状态维护**：在超过数十轮、涉及多个子任务切换的复杂对话中，模型能否有效维护对话历史、跟踪未完成目标、避免信息遗忘或混淆？\n\n**§4 可复现性与公平性问题**\n1.  **数据合成的具体细节缺失**：论文未提供FAS和HAS中使用的**具体LLM（是什么模型？多大规模？）**、**拒绝采样中LLM-as-Judge的提示词和阈值**、以及**合成数据的总量（除了200B+100B Token的粗略估计）和具体分布**。这使得独立复现数据合成流程非常困难。\n2.  **训练超参数不透明**：除了数据量和上下文长度，论文未提供**学习率、优化器、批次大小、训练步数、硬件配置**等关键训练细节。两阶段训练的具体切换条件和时间点也未说明。\n3.  **对基线模型的超参数调优不平等**：论文为`AgentFounder-30B`设置了特定的推理超参数（temperature=0.85, repetition penalty=1.1, top-p=0.95）并声称是基于经验验证的。然而，是否对所有对比基线模型都进行了同样细致的超参数调优以达到其最佳性能？如果基线使用的是默认参数或报告结果，则对比可能不公平。\n4.  **依赖未开源的商业模型进行数据合成**：虽然训练过程无需API调用，但FAS和HAS的数据合成本身很可能使用了强大的商业LLM（如GPT-4）。这为没有访问此类API的研究者设置了门槛。论文未开源用于数据合成的代码或模型。",
    "zero_compute_opportunity": "#### 蓝图一：探究轻量级智能体持续预训练对小型模型的有效性\n- **核心假设**：Agentic CPT的核心思想（通过合成数据植入智能体归纳偏置）对于参数量更小（如7B、13B）的开源模型同样有效，并能以极低成本显著提升其智能体能力。\n- **与本文的关联**：基于本文发现Agentic CPT能显著提升30B模型性能，但未验证其在更小模型上的可迁移性。同时，小模型训练成本低，更适合资源有限的研究者。\n- **所需资源**：\n  1.  **模型**：免费的开源小模型（如Qwen2.5-7B, Llama-3.1-8B）。\n  2.  **数据**：使用本文描述的FAS方法，利用公开知识库（如Wikipedia dump, Common Crawl子集）合成智能体数据。无需调用任何付费API。\n  3.  **计算**：单个消费级GPU（如RTX 4090, 24GB显存）即可进行微调。预计合成10B Token数据并进行训练，总成本（电费）可控制在100美元以内。\n- **执行步骤**：\n  1.  **数据合成**：编写脚本，基于Wikipedia等公开文本，实现简化的FAS流程（实体提取→问题生成→规划/推理合成）。使用本地运行的7B模型（如Qwen2.5-7B-Instruct）作为合成LLM，以零成本生成数据。\n  2.  **模型训练**：使用Hugging Face Transformers和PEFT（参数高效微调）库，在合成数据上对基础小模型进行持续预训练（下一Token预测）。采用LoRA等高效微调技术以减少显存占用。\n  3.  **评估**：在轻量级的智能体基准上评估（如WebShop，或自建的小规模BrowseComp子集）。与未经CPT的同一基础模型、以及直接SFT的版本进行对比。\n- **预期产出**：验证小模型通过轻量级Agentic CPT后，在工具使用和简单推理任务上的性能提升幅度（预期提升10-20%绝对指标）。可撰写一篇短论文投稿至EMNLP Findings、AACL等会议。\n- **潜在风险**：合成数据质量可能较低，导致模型学习到错误模式。应对方案：引入简单的规则过滤（如答案格式检查）和交叉验证（用另一个小模型评分过滤）。\n\n#### 蓝图二：分析智能体合成数据中“幻觉”对模型性能的影响及缓解策略\n- **核心假设**：FAS/HAS合成数据中不可避免存在LLM生成的“幻觉”（错误事实、逻辑谬误），这些噪声会损害模型的事实性和可靠性。设计简单的数据清洗或加权训练策略可以有效缓解此问题，且成本极低。\n- **与本文的关联**：本文使用了LLM-as-Judge进行拒绝采样，但其有效性未深入分析。本研究可深入探究合成数据质量的影响，并提供低成本的改进方案。\n- **所需资源**：\n  1.  **数据**：复用蓝图一合成的10B Token数据，或使用本文开源数据（如果提供）的子集。\n  2.  **工具**：使用免费的、基于规则的简单校验器（如正则表达式匹配数字、日期格式）、以及开源的NLI（自然语言推理）模型（如DeBERTa）来检测矛盾。\n  3.  **计算**：仅需CPU进行数据清洗，训练阶段同蓝图一。\n- **执行步骤**：\n  1.  **噪声分析**：从合成数据中随机采样，人工标注其中的事实错误和逻辑错误，量化幻觉率。\n  2.  **清洗策略设计**：设计多级过滤：a) 规则过滤（格式错误）；b) 基于NLI模型的矛盾检测（判断推理步骤是否支持最终答案）；c) 基于一致性投票（使用多个不同小模型对同一问题生成答案，取多数票）。\n  3.  **对比实验**：训练三个版本模型：a) 原始合成数据；b) 经过清洗的数据；c) 仅使用高质量人工标注的少量数据（作为上限参照）。评估其在需要高事实性的任务（如GAIA）上的表现。\n- **预期产出**：明确合成数据中幻觉的类型和比例，提出一种低成本的清洗流程，并证明其能提升模型事实性（预期在事实性指标上提升5-10%）。成果可投稿至LREC、*SEM等注重数据质量的会议。\n- **潜在风险",
    "source_file": "Scaling Agents via Continual Pre-training.md"
}