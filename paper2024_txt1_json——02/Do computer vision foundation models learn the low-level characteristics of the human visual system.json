{
    "title": "Do computer vision foundation models learn the low-level characteristics of the human visual system?",
    "background_and_problem": "【一、研究背景与问题核心】\n\n**§1 领域背景与研究动机（150字以上）**\n计算机视觉基础模型（如 DINO、OpenCLIP）通过自监督方式在大型自然图像数据集上进行训练，展现出强大的泛化能力，成为众多高级视觉任务的基石。与此同时，大量证据表明，人类视觉系统（HVS）的发展也深受自然世界色彩和模式统计分布的影响。一个核心的科学问题随之产生：在相似的自然图像数据驱动下，计算模型是否习得了与人类视觉系统相似的低级感知特性？本研究旨在探究这一根本问题，其动机在于理解计算视觉与生物视觉在从观察世界样本中发展时，是遵循了相似还是不同的路径。该研究对于解释深度神经网络的内在表征、评估其与人类感知的相似性，以及指导未来更“类人”的模型设计具有重要理论价值。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有研究表明，机器视觉模型与人类视觉在许多方面存在显著差异，具体失败模式如下：\n1.  **基于ImageNet训练的CNN模型**：当输入经过纹理-形状冲突处理的图像时（如大象纹理的猫图像），这些模型表现出强烈的纹理偏向，而非像人类一样依赖形状进行识别，这表明其底层表征与人类视觉模式存在根本差异。\n2.  **标准深度神经网络（DNNs）**：当输入包含3D视角变化或图像畸变（如噪声、模糊）时，这些模型在物体分类任务中的鲁棒性远低于人类。例如，人类几乎不受对抗样本影响，而DNNs在面对精心设计的、人眼难以察觉的微小扰动时，分类性能会急剧下降。\n3.  **先前评估模型低级特性的方法**：当试图通过训练一个对比度判别分类器来揭示预训练架构中的对比敏感度函数（CSF）特性时，该方法引入了训练偏差，并且错误地假设CSF能同时解释近阈值和超阈值视觉。实际上，对比度恒常性结果表明，这一假设并不成立。\n\n**§3 问题的根本难点与挑战（200字以上）**\n探究计算模型与人类视觉低级特性的对齐性面临多重理论与工程挑战：\n1.  **表征鸿沟**：计算模型（如ViT、CNN）的内部特征空间与人类视觉系统的神经编码机制（如视网膜、外侧膝状体、初级视皮层的处理）在数学和物理层面完全不同，缺乏直接可比性。\n2.  **评估范式差异**：人类视觉特性主要通过心理物理学实验（如2AFC）测量主观感知阈值，而计算模型的输出是高维特征向量。如何将特征空间的距离映射到感知差异，并建立公平的比较标准，是一个核心方法论挑战。\n3.  **数据与训练的错配**：基础模型在包含复杂场景的sRGB图像上训练，其中极少包含用于心理物理学测试的、在均匀背景上的、接近检测阈值的简单模式（如Gabor斑块）。模型可能从未在训练中见过此类刺激，因此其响应可能无法反映对这类刺激的系统性编码特性。\n4.  **多维度复杂性**：人类视觉特性（如对比度检测、掩蔽、恒常性）依赖于空间频率、亮度、刺激面积、颜色通道等多个维度的交互作用，进行全面、系统的评估需要设计庞大且精细的测试矩阵。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点在于绕过高级任务性能（如分类准确率）的比较，直接在最基本的、已被充分理解的人类低级视觉特性层面进行“黑箱”测试。其核心假设是：**如果计算机视觉基础模型和人类视觉系统都旨在高效编码自然世界的视觉信息，并且都暴露于相似的图像统计分布中，那么它们可能发展出相似的低级感知“瓶颈”（如检测阈值）和不变性特性（如对比度恒常性）**。作者假设，通过将模型暴露于精心生成的心理物理学刺激（如Gabor斑块、带限噪声），并计算其在特征空间中的响应差异，可以量化其与人类感知数据的一致性。该方法不引入额外的任务特定训练，避免了先前方法的偏差，并同时考虑近阈值和超阈值视觉。",
    "core_architecture": "【二、核心架构与技术机制】\n\n**§1 系统整体架构概览（200字以上）**\n本文的核心是一个系统性的评估框架，而非提出新的神经网络架构。其整体数据流向遵循严格的标准化流程：\n1.  **刺激生成**：在物理亮度空间（cd/m²）中生成测试图像对（如测试图像 vs. 参考图像）。参数严格遵循心理物理学实验设置，包括空间频率（0.5-32 cpd）、对比度（0.001-1）、亮度（0.1-200 cd/m²）、刺激面积等。\n2.  **显示编码转换**：使用显示模型（峰值亮度400 cd/m²）将线性亮度图转换到sRGB色彩空间。这是关键一步，因为sRGB是基础模型训练数据的通用格式，且其伽马校正部分补偿了亮度的感知非均匀性。为确保模型能处理低对比度值，修改模型输入以接受浮点值（而非8位整数），避免量化伪影。\n3.  **特征提取**：将sRGB格式的测试图像和参考图像分别输入待评估的**基础模型的图像编码器**（共45个模型）。编码器作为黑箱，输出高维特征图。\n4.  **特征向量化与距离计算**：将特征图重塑为一维向量 \\(F_{\\mathrm{T}}\\)（测试）和 \\(F_{\\mathrm{R}}\\)（参考）。计算两者之间的**角度相似度** \\(S_{\\mathrm{ac}}\\)（公式1）。\n5.  **结果分析与量化**：将模型响应的等值线图与人类心理物理数据（如castleCSF预测的对比敏感度函数）进行定性比较，并使用**模型对齐分数**（如Spearman等级相关系数、RMSE）进行定量评估。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：刺激生成与参数化模块\n-   **输入**：根据表1定义的九种测试类型（如空间频率-Gabor消色差、亮度-Gabor消色差、相位相干掩蔽等）的具体参数范围。\n-   **核心处理逻辑**：使用精确的物理单位生成图像对。例如，对于空间频率测试，在2D网格上系统性地改变Gabor斑块的空间频率（x轴）和对比度（y轴）。所有刺激均以224×224像素分辨率生成，对应3.7×3.7视觉度，大致与人眼中央凹视野范围对齐。\n-   **输出**：线性物理亮度（cd/m²）下的测试图像和参考图像对。\n-   **设计理由**：确保刺激在物理上是准确的，并且与人类心理物理学实验中使用的刺激具有可比性。固定的分辨率和视野范围保证了跨模型和跨人类数据比较的一致性。\n\n#### 模块二：特征空间距离度量模块（\\(S_{\\mathrm{ac}}\\)）\n-   **输入**：测试图像特征向量 \\(F_{\\mathrm{T}}\\) 和参考图像特征向量 \\(F_{\\mathrm{R}}\\)。\n-   **核心处理逻辑**：计算归一化的余弦距离，并映射到角度空间。具体公式为：\n    \\[ S_{\\mathrm{ac}} = \\frac{1}{\\pi} \\arccos \\left(\\frac{F_{\\mathrm{T}} \\cdot F_{\\mathrm{R}}}{\\| F_{\\mathrm{T}} \\| \\| F_{\\mathrm{R}} \\|}\\right) \\tag{1} \\]\n    其中 \\(S_{\\mathrm{ac}} = 0\\) 表示两图像在特征空间中等价，\\(S_{\\mathrm{ac}} = 1\\) 表示差异极大。\n-   **输出**：标量值 \\(S_{\\mathrm{ac}} \\in [0, 1]\\)，表示两图像在模型特征空间中的感知差异。\n-   **设计理由**：作者实验了 \\(L_1\\)、\\(L_2\\) 等多种距离度量，发现 \\(S_{\\mathrm{ac}}\\)（角度相似度）得出的结果与心理物理数据最为一致。它对于特征向量的幅度变化不敏感，更关注方向差异，这可能更好地模拟了人类对“差异”的感知。\n\n#### 模块三：模型对齐分数量化模块\n-   **输入**：对于对比度检测和掩蔽实验，是在人类检测阈值线（虚线）附近采样的一系列 \\(S_{\\mathrm{ac}}\\) 值；对于对比度匹配实验，是模型预测的匹配对比度与人类匹配数据。\n-   **核心处理逻辑**：\n    -   **对比度检测/掩蔽**：沿人类阈值线，在对比度方向以乘数 \\(m\\)（\\(0.5 \\leq m \\leq 2\\)）进行偏移采样。计算乘数 \\(m\\) 与对应 \\(S_{\\mathrm{ac}}\\) 值之间的**Spearman等级相关系数** \\(r_s\\)。对齐良好的模型，\\(S_{\\mathrm{ac}}\\) 应随 \\(m\\) 增加而增加，\\(r_s\\) 应接近1。\n    -   **对比度匹配**：计算模型预测的匹配对比度对数与人类数据之间的**均方根误差（RMSE）**。RMSE越低，对齐越好。\n-   **输出**：Spearman相关系数 \\(r_s\\)（越高越好）或 RMSE（越低越好）。\n-   **设计理由**：由于不同模型的 \\(S_{\\mathrm{ac}}\\) 绝对值范围不同，无法直接比较阈值线上的值。该量化方法关注的是模型响应在阈值线**附近的变化趋势**是否与人类敏感度的变化趋势一致，这是一种更鲁棒的对齐度量。\n\n**§3 关键公式与算法（如有）**\n本文的核心公式是特征空间距离度量公式和对比度匹配的优化公式：\n1.  **角度相似度**：\n    \\[ S_{\\mathrm{ac}} = \\frac{1}{\\pi} \\arccos \\left(\\frac{F_{\\mathrm{T}} \\cdot F_{\\mathrm{R}}}{\\| F_{\\mathrm{T}} \\| \\| F_{\\mathrm{R}} \\|}\\right) \\tag{1} \\]\n2.  **对比度匹配求解**：在对比度恒常性实验中，寻找测试对比度 \\(c_{\\mathrm{t}}\\)，使得测试刺激与均匀场之间的 \\(S_{\\mathrm{ac}}\\) 等于参考刺激与均匀场之间的 \\(S_{\\mathrm{ac}}\\)：\n    \\[ \\underset {c _ {\\mathrm {t}}} {\\operatorname {argmin}} \\left(S _ {\\mathrm {ac}} \\left(F \\left(\\rho_ {\\mathrm {r}}, c _ {\\mathrm {r}}\\right), U\\right) - S _ {\\mathrm {ac}} \\left(F \\left(\\rho_ {\\mathrm {t}}, c _ {\\mathrm {t}}\\right), U\\right)\\right) ^ {2} \\tag{2} \\]\n    其中 \\(\\rho_{\\mathrm{r}} = 5\\) cpd 为参考空间频率，\\(U\\) 为均匀场的特征向量。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n本文未提出方法变体，但测试了同一基础模型家族的不同变体（如不同大小的DINOv2、OpenCLIP）。作者发现，**对齐分数在不同变体间存在显著差异，且与变体的复杂度或其原始高级任务上的性能无关**。例如，OpenCLIP的不同变体在对齐分数上波动很大。这表明，模型与人类视觉的对齐性并非简单地随模型容量增加而改善，可能受到训练细节、架构选择等多种因素影响。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上存在本质区别：\n1.  **与 Akbarinia et al. (2023) 和 Li et al. (2022) 的对比**：这些工作试图通过**训练一个额外的对比度判别分类器头**来从预训练网络中提取对比敏感度函数（CSF）。本文指出，这种方法引入了**训练偏差**，并且错误地假设CSF能统一解释近阈值和超阈值视觉。而本文方法**完全避免任何额外的任务特定训练**，直接将预训练编码器作为黑箱进行测试，评估其固有的表征特性，从而提供了无偏的评估。\n2.  **与高级任务性能比较研究（如 Geirhos et al., 2018, 2021）的对比**：这些研究关注模型在ImageNet分类、对抗鲁棒性、形状纹理偏差等**高级语义任务**上的表现与人类的差异。本文则完全绕过高级任务，聚焦于**低级的、前语义的感知特性**（对比度检测、掩蔽、恒常性），这些特性更接近视觉处理的早期阶段，能更直接地揭示模型底层表征与生物视觉的相似性。\n3.  **与使用对抗样本评估鲁棒性的对比**：对抗攻击研究显示DNNs与HVS在对抗扰动敏感性上截然不同。本文采用了一种**建设性而非破坏性**的评估范式：不是寻找模型的失败案例，而是系统性地测试其是否在正常的心理物理刺激上表现出与人类相似的成功模式（如特定的阈值曲线形状）。",
    "methodology_and_formulas": "【三、方法论细节与关键公式】\n\n**§1 完整算法流程（伪代码级描述）**\n本文评估流程可概括为以下步骤：\n**Step 1：刺激参数定义**。根据九种测试类型（见表1），定义空间频率、对比度、亮度、面积、掩蔽对比度等参数的范围和步长。\n**Step 2：图像对生成**。对于参数网格中的每个点 \\((p, c)\\)（如p为空间频率，c为对比度）：\n    - 生成测试图像 \\(I_{\\mathrm{test}}(p, c)\\)（如Gabor斑块）。\n    - 生成参考图像 \\(I_{\\mathrm{ref}}\\)（如均匀场、掩蔽图案）。\n    - 将 \\(I_{\\mathrm{test}}\\) 和 \\(I_{\\mathrm{ref}}\\) 从线性亮度（cd/m²）转换至sRGB色彩空间（使用峰值亮度400 cd/m²的显示模型）。\n**Step 3：特征提取**。将sRGB格式的 \\(I_{\\mathrm{test}}\\) 和 \\(I_{\\mathrm{ref}}\\) 输入待评估的基础模型编码器，分别获得特征图，并将其重塑为一维特征向量 \\(F_{\\mathrm{T}}\\) 和 \\(F_{\\mathrm{R}}\\)。\n**Step 4：距离计算**。根据公式(1)计算 \\(S_{\\mathrm{ac}}(F_{\\mathrm{T}}, F_{\\mathrm{R}})\\)。\n**Step 5：结果可视化**。将 \\(S_{\\mathrm{ac}}\\) 值绘制为以测试参数（如频率、对比度）为轴的等值线图。在同一图中叠加人类心理物理数据（如castleCSF预测的阈值曲线）。\n**Step 6：量化对齐分数**。\n    - **对于检测/掩蔽**：沿人类阈值曲线，在对比度方向按乘数 \\(m = [0.5, 0.75, 1.0, 1.5, 2.0]\\) 采样，获得一系列 \\((m, S_{\\mathrm{ac}})\\) 数据点。计算这些点的Spearman相关系数 \\(r_s\\)。\n    - **对于匹配**：对于每个参考对比度 \\(c_r\\)，根据公式(2)求解模型预测的测试对比度 \\(c_t\\)。计算所有 \\(c_t\\) 的对数值与人类匹配数据对数值之间的RMSE。\n**Step 7：跨模型比较**。对所有45个模型重复Step 2-6，汇总对齐分数（如图6所示），进行横纵向比较分析。\n\n**§2 关键超参数与配置**\n-   **有效分辨率**：60 像素/度。这是将图像像素频率映射到视网膜空间频率的关键参数。作者指出此选择是任意的，模型相似性分数可能沿频率轴有一个小的乘数偏移，但不影响相对比较。\n-   **图像分辨率**：224 × 224 像素。对应 3.7 × 3.7 视觉度，与人眼中央凹视野大致匹配，确保刺激能覆盖模型和人类视觉的多个感受野/令牌。\n-   **显示峰值亮度**：400 cd/m²。用于线性亮度到sRGB的转换模型。\n-   **对比度匹配参考频率**：\\(\\rho_{\\mathrm{r}} = 5\\) cpd。遵循原始人类实验设置[18]。\n-   **对齐分数计算中的乘数范围**：\\(0.5 \\leq m \\leq 2\\)。用于在人类阈值线附近采样，评估模型响应的单调性。\n\n**§3 训练/微调设置（如有）**\n本文不涉及模型训练或微调。所有测试的45个基础模型（DINO, DINOv2, OpenCLIP, SAM, SAM-2, MAE, SD-VAE）均使用其**公开的、预训练的权重**。作者强调，评估是在**没有任何额外任务特定训练**的情况下进行的，以检验模型通过自监督预训练所获得的固有表征特性。\n\n**§4 推理阶段的工程细节**\n-   **输入预处理**：关键修改是将模型输入从通用的8位整数改为**浮点数**，以避免低对比度刺激因量化产生的伪影，这些伪影可能大于人类视觉的检测阈值。\n-   **特征提取**：直接调用各基础模型库（如timm, transformers）中的图像编码器前向传播函数。\n-   **计算平台**：原文未明确说明，但此类实验通常在高性能GPU服务器上进行，以并行处理大量图像对（9种测试 × 多参数组合 × 45个模型）。\n-   **基线模型**：ColorVideoVDP作为人类低级视觉模型的参考，因其直接在物理亮度单位上工作，无需sRGB转换，直接使用其质量分数而非 \\(S_{\\mathrm{ac}}\\)。",
    "experimental_design": "【四、实验设计】\n\n**§1 数据集详情（每个数据集单独列出）**\n本文未使用传统意义上的大型标注数据集，而是**生成了一套系统的心理物理学测试刺激**，其参数严格基于人类视觉科学文献。测试可分为三大类九小类，具体参数见表1：\n1.  **对比度检测（4类）**：\n    -   **空间频率-Gabor消色差**：空间频率0.5-32 cpd，亮度100 cd/m²，半径1度，对比度0.001-1。\n    -   **空间频率-噪声消色差**：空间频率0.5-32 cpd，亮度100 cd/m²，对比度0.001-1。\n    -   **空间频率-Gabor RG（红绿）**：空间频率0.5-32 cpd，亮度100 cd/m²，半径1度，对比度0.001-0.12。\n    -   **空间频率-Gabor YV（黄蓝）**：空间频率0.5-32 cpd，亮度100 cd/m²，半径1度，对比度0.001-0.8。\n2.  **亮度与面积检测（2类）**：\n    -   **亮度-Gabor消色差**：空间频率2 cpd，亮度0.1-200 cd/m²，半径1度，对比度0.001-1。\n    -   **面积-Gabor消色差**：空间频率8 cpd，亮度100 cd/m²，半径0.1-1度，对比度0.001-1。\n3.  **对比度掩蔽（2类）**：\n    -   **相位相干掩蔽**：掩蔽频率2 cpd，测试频率2 cpd，亮度32 cd/m²，测试半径0.5度，掩蔽对比度0.005-0.5，测试对比度0.01-0.5。\n    -   **相位非相干掩蔽**：掩蔽频率0-12 cpd，测试频率1.2 cpd，亮度37 cd/m²，测试半径0.8度，掩蔽对比度0.005-0.5，测试对比度0.01-0.5。\n4.  **超阈值对比度匹配（1类）**：参考频率5 cpd，测试频率0.25-25 cpd，亮度10 cd/m²，参考对比度0.005-0.629。\n**人类数据来源**：对比度检测数据来自castleCSF模型[3]；对比度掩蔽数据来自Foley[14]和Gegenfurtner & Kiper[15]；对比度匹配数据来自Georgeson & Sullivan[18]。\n\n**§2 评估指标体系（全量列出）**\n-   **定性指标**：模型响应 \\(S_{\\mathrm{ac}}\\) 的等值线图与人类阈值曲线（虚线）的视觉对齐度。理想情况下，最低的 \\(S_{\\mathrm{ac}}\\) 等值线应贴合人类阈值曲线。\n-   **定量指标**：\n    1.  **Spearman等级相关系数（\\(r_s\\)）**：用于对比度检测和对比度掩蔽实验。衡量在人类阈值线附近，模型响应 \\(S_{\\mathrm{ac}}\\) 随对比度乘数 \\(m\\) 变化的单调一致性。值越接近1，对齐越好。\n    2.  **均方根误差（RMSE）**：用于超阈值对比度匹配实验。计算模型预测的匹配对比度（对数）与人类匹配数据（对数）之间的误差。值越接近0，对齐越好。\n-   **效率/部署指标**：本文未涉及。\n\n**§3 对比基线（完整枚举）**\n本文测试了总计**45个模型**，并将其分为几大类进行对比：\n1.  **人类视觉模型（参考基线）**：**ColorVideoVDP**[30]。这是一个显式建模人类低级视觉的图像/视频质量度量指标，在几乎所有对比度检测任务和对比度恒常性上预期与人类数据最佳对齐。\n2.  **视觉基础模型（主要评估对象）**：\n    -   **DINO**[9] 及其变体。\n    -   **DINOv2**[31] 及其变体（如ViT-G/14）。\n    -   **OpenCLIP**[22, 33] 及其变体（如VIT-L/14-Laion）。\n    -   **SAM**[23] 及其变体。\n    -   **SAM-2**[35] 及其变体（如AM2.1-hiera-L）。\n    -   **MAE**[21] 及其变体。\n3.  **生成模型编码器**：**Stable Diffusion的VAE编码器（SD-VAE）**[36] 及其变体（如xl-base-1）。\n**选择理由**：这些模型涵盖了当前最具影响力的、通过自监督或语言-视觉对比学习在大型自然图像数据集上训练得到的基础模型和生成模型，代表了机器视觉的前沿。\n\n**§4 实验控制变量与消融设计**\n本文的核心控制变量是**刺激的物理参数**（频率、对比度、亮度、面积、颜色通道）。通过系统性地改变这些参数，观察模型响应的变化模式。\n**消融设计**体现在对不同模型家族及其变体的广泛测试上。作者通过比较同一家族内不同变体（如不同大小的OpenCLIP）的对齐分数，发现对齐性与模型复杂度无关，这本身就是一个重要的“消融”发现，表明对齐性可能取决于训练目标、数据或架构细节，而非单纯的参数量。此外，通过对比不同任务（检测 vs. 掩蔽 vs. 匹配）的结果，可以“消融”出模型在哪些低级特性上与人类更相似。",
    "core_results": "【五、核心实验结果】\n\n**§1 主实验结果全景（表格式呈现）**\n由于结果以等值线图和汇总条形图（图6）形式呈现，以下以文本形式还原核心定量发现（基于图6的解读）：\n**关键观察**：ColorVideoVDP（人类视觉模型）在几乎所有对比度检测任务和对比度恒常性（匹配）上对齐分数最佳（Spearman相关最高，RMSE最低），这与预期一致。**然而，在对比度掩蔽任务中，某些基础模型变体（如OpenCLIP和DINOv2）达到或超过了ColorVideoVDP的对齐水平。**\n**代表性模型在九类测试中的表现趋势（定性）**：\n-   **DINOv2 ViT-G/14**：在多项测试中表现出与人类数据最高的相似性。在消色差Gabor和噪声检测中显示出带通特性；在面积总和中大致匹配人类；在相位非相干掩蔽中与人类数据斜率良好对齐；在对比度匹配中显示出部分对比度恒常性。\n-   **OpenCLIP VIT-L/14-Laion**：在对比度检测中响应非常不一致，但在**对比度掩蔽**（尤其是相位非相干）中表现出与人类数据的显著对齐，在对比度匹配中也显示出部分恒常性。\n-   **SAM-2 AM2.1-hiera-L**：在空间频率检测中响应不一致，在面积总和中增加较小。\n-   **SD-VAE xl-base-1**：在消色差检测中显示出带通特性，但低频下降比视觉系统更剧烈；在面积总和中 \\(S_{\\mathrm{ac}}\\) 增加比人类数据更快；在对比度匹配中低频衰减非常严重。\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n**对比度检测（近阈值）**：\n-   **结果**：所有测试的基础模型**均未**严格遵循人类视觉系统的对比敏感度函数（CSF）模式。没有模型的 \\(S_{\\mathrm{ac}}\\) 最低等值线贴合castleCSF预测的阈值曲线。\n-   **分析**：这表明计算模型不具备与人类视觉相同的“瓶颈”。人类在特定频率（2-4 cpd）的峰值敏感性、以及由侧抑制和光学限制导致的低/高频敏感性下降，并未被基础模型完全捕获。然而，一些模型（如DINOv2、SD-VAE）对消色差Gabor和噪声显示出**整体带通特性**，且许多模型在低频表现出敏感性下降，这可能意味着它们通过训练获得了对（低频）照明的某种不变性。\n\n**对比度掩蔽（近阈值）**：\n-   **结果**：计算模型与人类数据的对齐度**优于对比度检测**。DINOv2和OpenCLIP预测的差异与人类对比度掩蔽数据惊人地吻合，其响应大致匹配人类数据的斜率，在**相位非相干掩蔽**中对齐更强。\n-   **分析**：一个可能的解释是，自然图像中富含能引起对比度掩蔽的信号（即图案叠加图案），而涉及均匀背景上 barely noticeable patterns 的对比度检测刺激在训练数据中很罕见。因此，计算模型更可能习得训练数据中充分表征的特性。\n\n**超阈值对比度匹配（恒常性）**：\n-   **结果**：仅**DINOv2和OpenCLIP**大致遵循人类的对比度恒常性曲线（虚线）。它们在高空间频率处表现出较少的衰减（更强的恒常性），这可能有利于处理小尺度特征；但在低频（低于1 cpd）处表现出衰减，表明在该频率范围的对比度恒常性较差。其他模型（如DINO、SAM-2）在不同频率间表现出很大的不稳定性。\n-   **分析**：这表明选定的模型通过训练获得了**部分对比度恒常性**，这是一种重要的尺度不变性特性，有助于在不同观看距离（即不同空间频率）下保持物体外观的一致性。\n\n**§3 效率与开销的定量对比**\n原文未提供关于延迟、Token消耗、显存占用等效率指标的定量数据。本文焦点完全集中在感知特性的对齐性上，而非计算效率。\n\n**§4 消融实验结果详解**\n本文的“消融”体现在对不同模型变体的广泛测试上。关键发现是：**模型对齐分数在不同变体间存在显著差异，且与变体的复杂度或其原始高级任务上的性能无关**。例如，OpenCLIP的不同变体在对齐分数上波动很大。这消融了“更大/更强的模型必然更类人”的假设，表明对齐性可能取决于更细微的训练目标、数据构成或架构 inductive biases。\n\n**§5 案例分析/定性分析（如有）**\n论文通过等值线图提供了丰富的定性分析案例：\n-   **成功案例（图5g,h）**：DINOv2和OpenCLIP在对比度掩蔽任务中的等值线图显示，其 \\(S_{\\mathrm{ac}}\\) 等值线的形状和走向与人类阈值曲线（虚线）高度相似，尤其是在掩蔽对比度较高时，斜率匹配良好。\n-   **失败/不一致案例**：\n    1.  SAM-2和OpenCLIP在空间频率对比度检测中（图5a-d），等值线图呈现混乱、不规则的图案，没有清晰的带通形状，与人类CSF曲线毫无相似之处。\n    2.  SD-VAE在对比度匹配中（图5i），其预测曲线（实线）在低频区域严重向下偏离人类恒常性曲线（虚线），表明其严重低估了低频对比度的感知强度。\n    3.  所有模型在亮度检测中（图5e），敏感性随亮度下降的速度都快于人类数据，这可能源于训练数据（sRGB图像）中暗部区域信息不足或sRGB编码本身的影响。",
    "conclusion_and_future_work": "【六、结论与未来工作】\n\n**§1 本文核心贡献总结**\n1.  **提出系统评估协议**：设计并实现了一套包含九类测试的完整协议，用于定量评估计算机视觉基础模型与人类视觉系统在低层感知特性（对比度检测、掩蔽、恒常性）上的对齐性。\n2.  **大规模模型评估**：对45个代表性的基础模型和生成模型进行了系统测试，提供了当前模型与人类视觉相似性的全景图。\n3.  **关键发现**：首次明确指出，某些基础模型（如DINOv2、OpenCLIP）在**超阈值对比度掩蔽和匹配**任务上与人类数据高度对齐，但在**近阈值对比度检测**上对齐度很低。这表明计算模型并未获得与人类相同的感知“瓶颈”，但通过训练获得了类似于视觉系统的**不变性和高效对比度编码**能力。\n4.  **开源工具**：公开了代码，为社区提供了检验未来视觉计算模型低级特性的实用工具。\n\n**§2 局限性（作者自述）**\n1.  **参数选择任意性**：将像素映射到视网膜空间频率的有效分辨率（60像素/度）是任意选择的，这可能导致模型相似性分数沿频率轴有一个小的乘数偏移。\n2.  **刺激范围有限**：测试集中在对比度感知的特定方面，未涵盖所有低级视觉特性（如运动感知、颜色恒常性等）。\n3.  **输入表示依赖**：模型使用sRGB输入，这本身包含伽马校正，可能影响其对绝对亮度的响应特性。\n4.  **模型范围**：尽管测试了45个模型，但仍未涵盖所有现有的基础模型架构和训练范式。\n\n**§3 未来研究方向（全量提取）**\n原文在结论部分未明确列出未来工作，但根据全文内容，可推断出以下方向：\n1.  **扩展测试协议**：将当前协议扩展到其他低级视觉特性，如**运动敏感度（时域CSF）、颜色视觉、双眼视觉（立体深度）**等，以构建更全面的模型-人类视觉对齐性评估体系。\n2.  **探究对齐性根源**：深入研究为何某些模型（如DINOv2）在特定任务（如掩蔽）上表现出更高的对齐性。这需要分析**训练目标（自监督vs.语言-视觉对比）、训练数据统计特性、网络架构（ViT vs. CNN）** 等因素的具体影响。\n3.  **连接低级与高级对齐**：探索模型的低级感知对齐性与其在**高级语义任务**（如物体识别、场景理解）上的性能及人类相似性之间的关联。是否存在因果关系？\n4.  **指导模型设计**：利用本评估协议作为指导原则，设计新的**训练目标或架构**，以 explicitly encourage 模型学习更像人类的低级视觉表征，可能提升其鲁棒性、泛化性和可解释性。",
    "research_contributions": "【七、研究贡献与学术价值】\n\n**§1 核心学术贡献（按重要性排序）**\n1.  **方法论贡献（理论新颖性）**：提出了一种全新的、“黑箱”式的评估范式，绕过高级任务性能，直接在最基础的心理物理学刺激上测试模型与人类低级视觉特性的对齐性。该方法避免了引入额外训练偏差，并同时处理近阈值和超阈值视觉，在方法论上显著优于先前工作。\n2.  **实证发现贡献（实验验证充分性）**：通过对45个前沿模型的大规模系统测试，获得了坚实、可重复的实证结果。核心发现——模型在掩蔽和恒常性（超阈值）上表现出对齐，而在检测（近阈值）上没有——挑战了“模型要么类人要么不类人”的简单二分法，揭示了计算视觉与生物视觉之间复杂而微妙的关系。\n3.  **领域影响**：该研究架起了**计算视觉**、**人类视觉科学**和**心理物理学**之间的桥梁。它为理解深度神经网络内部表征的本质提供了新的视角，也为评估和设计更“类人”的机器视觉模型提供了具体的、可量化的基准。\n\n**§2 工程与实践贡献**\n-   **开源评估框架**：在GitHub上公开了完整的代码实现（https://github.com/caiyancheng/VFM_HVS_CVPR2025），使其他研究者能够轻松复现实验，并以此协议评估他们自己的模型。\n-   **标准化测试集**：虽然没有发布传统意义上的数据集，但论文详细描述了刺激生成的所有参数和流程，实质上定义了一套可复现的、标准化的低级视觉测试集。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于一个独特的交叉位置。它既不是单纯地提升模型在高级任务上的性能，也不是单纯地研究人类视觉机制。它开辟了一条新的技术路线：**系统性、定量化地评估和比较机器视觉与生物视觉在基础感知层面的相似性**。这条路线是对现有“对抗鲁棒性”、“形状纹理偏差”等高级差异研究的重要补充，将比较的层次下沉到了视觉信息处理的最前端。",
    "professor_critique": "【八、隐性缺陷与教授锐评】\n\n**§1 实验设计与评估体系的缺陷**\n1.  **评估指标单一且间接**：仅依赖特征空间中的 \\(S_{\\mathrm{ac}}\\) 距离作为感知差异的代理。虽然作者尝试了多种距离度量并选择了最一致的，但这仍是一个**强假设**：特征空间中的余弦距离能否完美对应人类主观感知差异？缺乏与人类主观评分（如JND实验）的直接相关性验证。可能存在“指标幸运”，即某些模型因架构特性偶然在 \\(S_{\\mathrm{ac}}\\) 上表现好，但并非真正感知对齐。\n2.  **基线模型选择不足**：虽然测试了45个模型，但缺乏与更经典的、非基础模型的CNN架构（如VGG、ResNet）在相同测试下的对比。这让人难以判断观察到的对齐性是“基础模型”特有的，还是某些训练范式（自监督）带来的，亦或是所有深度网络共有的趋势。\n3.  **空间频率映射的任意性**：60像素/度的选择虽合理但任意，且作者承认这会导致结果沿频率轴平移。这削弱了绝对频率点上比较的严谨性，尽管相对趋势可能保持。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **sRGB输入的固有偏差**：所有模型接收sRGB输入，该编码包含伽马校正（~2.2），这是一种非线性亮度压缩。这意味着模型从未见过真正的线性物理亮度世界。实验中发现模型对亮度敏感性下降过快，很可能直接源于sRGB编码和训练数据分布（多为曝光良好的图像），而非学习了类似DeVries-Rose/Weber定律的特性。这混淆了“学习到的特性”与“输入编码强加的特性”。\n2.  **“黑箱”测试的局限性**：虽然避免了训练偏差，但也丧失了解释性。我们只知道模型输出是否与人类数据对齐，但完全不知道**为什么**对齐或不对齐。是某个特定的层？某种特定的神经元激活模式？这种黑箱性限制了从结果中提取指导模型设计的深刻洞见。\n3.  **静态图像测试的局限**：人类视觉是高度动态和主动的。本文所有测试基于静态图像，完全忽略了**眼动、注意力、时域处理**等关键生物视觉特性。一个在静态对比度测试中表现不佳的模型，可能在动态视频任务中表现出惊人的类人行为，反之亦然。\n\n**§3 未经验证的边界场景**\n1.  **极端对比度与亮度**：测试对比度最高为1（100%），亮度最高为200 cd/m²。未测试**HDR（高动态范围）场景**（亮度超过1000 cd/m²，对比度超过1000:1），这在真实世界和现代显示技术中很常见。模型在极端条件下的响应可能完全崩溃。\n2.  **复杂自然场景中的掩蔽**：测试使用简单的正弦波或噪声作为掩蔽物。未测试在**复杂自然纹理背景**（如树叶、沙石）下的目标检测，这是更贴近实际应用的场景，其掩蔽效应可能复杂得多。\n3.  **跨模态干扰**：未测试在存在**强烈颜色信息**或**语义信息**干扰时，模型对亮度对比度的感知是否会发生变化（如著名的“穿蓝黑还是白金裙子”错觉）。这能检验低级特性与高级处理的耦合程度。\n4.  **对抗性心理物理刺激**：可以设计针对模型（而非人类）的“对抗性”Gabor斑块——在人类看来完全相同的两个斑块，却在模型特征空间中产生巨大差异 \\(S_{\\mathrm{ac}}\\)。这能直接检验其低级表征与人类感知的**局部线性**是否一致。\n\n**§4 可复现性与公平性问题**\n-   **可复现性高**：实验细节（参数、公式、代码开源）描述非常清晰，复现门槛较低，主要依赖公开的预训练模型。\n-   **公平性问题**：作者修改了模型输入以接受浮点数，这对于公平测试低对比度刺激是必要且合理的。潜在的不公平在于：所有模型都使用相同的、可能并非最优的 **\\(S_{\\mathrm{ac}}\\) 距离度量**和**对齐分数计算方式**。对于某些架构（如具有特殊归一化层的模型），其他距离度量（如经过白化处理的欧氏距离）可能更能反映其内部差异，但本文未进行模型特定的度量调优。",
    "zero_compute_opportunity": "【九、零算力研究契机】\n\n#### 蓝图一：探究视觉Transformer中“注册令牌”（Register Tokens）对低级视觉对齐性的影响\n-   **核心假设**：DINOv2中引入的“注册令牌”（一种特殊的可学习令牌，用于吸收无关信息）可能是其获得良好对比度恒常性和掩蔽对齐性的关键因素，因为它帮助模型分离了内容与干扰（类似照度不变性）。\n-   **与本文的关联**：基于本文发现DINOv2在多项测试中表现最佳，且其变体DINOv2（带注册令牌）是表现突出的模型之一。\n-   **所需资源**：\n    1.  **模型**：使用Hugging Face或timm库中开源的、**带与不带注册令牌的DINOv2变体**（如ViT-S/16）。这些模型可免费加载。\n    2.  **数据**：复用本文开源的刺激生成代码，生成关键测试（如空间频率-Gabor消色差、相位非相干掩蔽、对比度匹配）。\n    3.  **计算**：在Google Colab免费GPU（T4）上运行，单次前向传播计算量很小，测试数百个图像对的总成本几乎为零。\n-   **执行步骤**：\n    1.  加载有注册令牌（`registers=true`）和无注册令牌（`registers=false`）的DINOv2-ViT-S/16模型。\n    2.  运行本文的评估代码，重点对比两者在**对比度检测低频下降**、**掩蔽曲线斜率**、**对比度匹配恒常性**三个方面的 \\(S_{\\mathrm{ac}}\\) 等值线图和对齐分数。\n    3.  可视化注册令牌在应对不同对比度/频率刺激时的激活模式，分析其是否与背景/干扰信息的编码相关。\n-   **预期产出**：明确验证注册令牌是否显著提升模型低级视觉对齐性。若能证实，可撰写短文投往**ICLR的Tiny Papers赛道、CVPR的Workshop或NeurIPS的Datasets and Benchmarks赛道**。\n-   **潜在风险**：ViT-S/16模型较小，效果可能不如文中的ViT-G/14明显。应对：可尝试获取更大的ViT-B/16变体进行对比，或在更多样化的测试上验证。\n\n#### 蓝图二：构建“模型-人类视觉对齐性”轻量级排行榜与诊断工具\n-   **核心假设**：研究者需要一个快速、低成本的工具来诊断自己训练的轻量级模型（如MobileViT、EfficientNet）是否在低级视觉特性上“跑偏”，这可能是其泛化能力差的早期信号。\n-   **与本文的关联**：本文的协议全面但计算量较大。可以从中提取**最具鉴别力的1-2个测试子集**，作为快速诊断工具。\n-   **所需资源**：\n    1.  **代码**：精简本文代码，保留“空间频率-Gabor消色差”和“相位非相干掩蔽”两类测试，并将参数网格大幅简化（如频率取4个点，对比度取5个点）。\n    2.  **基线**：将ColorVideoVDP和DINOv2-ViT-S/16作为参考基线分数集成到工具中。\n    3.  **平台**：构建一个简单的Gradio或Streamlit网页应用，允许用户上传自己的模型权重（PyTorch格式）或选择常见架构，自动运行诊断并生成报告。\n-   **执行步骤**：\n    1.  开发简化版评估脚本和网页界面。\n    2.  在多个公开的轻量级模型（如ImageNet预训练的MobileNetV3, Swin-Tiny）上运行，建立初步的“正常范围”参考。\n    3.  邀请社区测试，收集反馈，并与模型在标准基准（如ImageNet鲁棒性数据集）上的表现进行相关性分析。\n-   **预期产出**：一个开源的、交互式的模型低级视觉诊断工具和一个小型排行榜。可以撰写系统演示论文，投稿至**ACM Multimedia Systems、或CVPR/ICCV的Demo环节**。\n-   **潜在风险**：简化测试可能丢失重要信息，导致误诊。应对：通过相关性分析验证简化测试分数与完整测试分数、以及与高级鲁棒性指标的相关性。\n\n#### 蓝图三：探索数据增广如何影响低级视觉对齐性——以色彩抖动和灰度化为案例\n-   **核心假设**：在训练数据上应用强烈的色彩抖动（Color Jitter）或随机灰度化（Random Grayscale）增广，可能会破坏模型对色差（Chromatic）对比度通道的学习，从而影响其在RG/YV Gabor测试中的对齐性，但对消色差测试影响较小。\n-   **与本文的关联**：本文发现模型对色差模式的响应不如消色差规则。这可能是训练数据或增广策略导致的。\n-   **所需资源**：\n    1.  **模型**：选择一个小型、易于复现训练的架构，如**MoCo-v3的ViT-Tiny**。使用公开代码和ImageNet-1K子集（如10%）。\n    2.  **训练**：在Google Colab Pro（约10美元/月）上进行3组对比训练：A组（标准增广）、B组（强色彩抖动）、C组（高概率灰度化）。每组训练约需1-2天。\n    3.  **评估**：使用本文协议评估训练好的模型，重点关注**空间频率-Gabor RG/YV**测试的对齐分数变化。\n-   **执行步骤**：\n    1.  准备三份相同的ImageNet子集数据，配置不同的增广管道。\n    2.  分别训练三个ViT-Tiny模型。\n    3.  评估并对比它们在消色差 vs. 色差测试上的表现差异。\n    4.  分析模型第一层卷积核或第一层Transformer patch projection层对颜色通道的权重分布。\n-   **预期产出**：明确数据增广对模型低级颜色视觉特性的塑造作用。结果可形成一篇扎实的小型研究论文，投稿至**BMVC、WACV或ICIP**等会议。\n-   **潜在风险**：ViT-Tiny模型容量小，可能无法清晰展现差异。应对：如果结果趋势明显但数值差异小，可尝试用ViT-Small重复实验，或在更专业的数据集（如COCO）上验证。",
    "source_file": "Do computer vision foundation models learn the low-level characteristics of the human visual system.md"
}