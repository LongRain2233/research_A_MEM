{
    "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization",
    "background_and_problem": "【一、研究背景与问题核心】\n\n**§1 领域背景与研究动机（150字以上）**\n本文研究领域是**基于大语言模型（LLM）的网页智能体（Web Agent）**，其核心应用场景是处理复杂的、知识密集型的**多轮网页搜索与信息整合任务**。当前，以ReAct范式为代表的智能体通过迭代执行Thought（思考）、Action（工具调用）、Observation（环境反馈）来完成信息探索。然而，随着复杂查询（如涉及多实体、关系交织、信息高度模糊的问题）的出现，智能体需要执行大量工具调用（如搜索、浏览网页）来逐步收集证据、降低不确定性。这种**长视野探索（Long-Horizon Exploration）** 对智能体的持续推理能力提出了更高要求，也成为当前提升智能体解决复杂问题能力的关键瓶颈。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有主流方法ReAct（Yao et al., 2023）及其变体存在一个根本性限制：**上下文长度约束**。具体失败模式如下：\n1.  **ReAct范式**：该方法将所有历史交互（Thought、Action、Observation）不断追加到对话上下文中。当输入需要超过10轮甚至20轮工具调用的复杂查询时，累积的Token数量会迅速超过模型（如32K上下文窗口）的限制。如图2所示，在BrowseComp-en基准测试中，失败的轨迹通常消耗超过10次工具调用，Token使用量急剧上升并超过32K限制，导致智能体在找到答案前就被迫终止。\n2.  **Recent History基线（简单截断）**：该方法在上下文接近限制时，仅保留最近22K个Token的对话历史，截断旧信息。这种简单的截断方式会**破坏上下文的连续性**，丢失对后续推理至关重要的早期证据和线索，导致智能体无法有效利用完整的历史信息。\n3.  **基于外部记忆组件的方法（如A-Mem、MemOS）**：这些方法引入检索模块来管理上下文，但会**增加显著的计算开销和系统复杂性**，并且与智能体的集成较为松散，难以实现无缝兼容。\n\n**§3 问题的根本难点与挑战（200字以上）**\n该问题的根本难点源于**LLM固有的上下文窗口限制与长视野探索任务需求之间的矛盾**。从理论角度看，复杂查询的解决过程是一个**信息熵逐步降低**的过程，需要多轮交互来逐步缩小搜索空间、验证假设、整合证据。每一轮交互都会产生新的观察结果（Observation），这些结果必须被保留以供后续推理参考。ReAct范式将全部历史信息线性堆叠，导致Token消耗量随交互轮数线性增长，其计算复杂度为O(n)。当n较大时，必然触及上下文天花板。从工程角度看，直接扩展上下文窗口（如从32K到128K）虽然能缓解问题，但会带来**显存占用剧增、推理延迟上升、成本高昂**等新问题，且对于某些极其复杂的任务，即使128K窗口也可能不够用。因此，需要在**不显著增加计算开销**的前提下，找到一种能够**无限延续探索**的机制。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是**周期性上下文摘要（Periodic Context Summarization）**。其核心假设是：在长视野探索中，并非所有历史交互细节都对后续推理同等重要；可以通过一个**目标导向的摘要模型**，将冗长的交互历史压缩成一个**紧凑的推理状态（Compressed Reasoning State）**，该状态保留了已验证的证据、关键线索以及明确的信息缺口。智能体可以基于这个摘要状态重启探索，从而绕过上下文限制。\n该假设的理论依据类似于**认知科学中的工作记忆（Working Memory）与长时记忆（Long-Term Memory）机制**：人类在解决复杂问题时，并不会将所有过往细节都保持在活跃意识中，而是会提取关键信息形成“要点”，并基于这些要点进行下一步思考。本文将此机制形式化为一个可操作的工程范式（ReSum），并通过强化学习（ReSum-GRPO）训练智能体适应这种“摘要-重启”的推理模式，从而实现**理论上无限制的探索**。",
    "core_architecture": "【二、核心架构与技术机制】\n\n**§1 系统整体架构概览（200字以上）**\nReSum系统由三个核心模块构成：**智能体策略模型（Agent Policy Model）**、**摘要工具（Summary Tool）** 和**强化学习训练框架（ReSum-GRPO）**。整体数据流如下：\n1.  **输入**：用户查询q。\n2.  **初始探索**：智能体基于初始上下文 \\(\\mathcal{H}_0 = (q)\\)，按照ReAct范式迭代生成Thought \\(\\tau_t\\)和Action \\(a_t\\)，执行工具获得Observation \\(o_t\\)，并更新历史：\\(\\mathcal{H}_t = \\mathcal{H}_{t-1} \\circ (\\tau_t, a_t, o_t)\\)。\n3.  **摘要触发**：当上下文长度接近预设限制（如32K Token）时，系统触发摘要工具。\n4.  **摘要生成**：摘要工具 \\(\\pi_{sum}\\) 接收当前历史 \\(\\mathcal{H}_t\\)，生成一个目标导向的摘要s：\\(s \\sim \\pi_{sum}(\\cdot | \\mathcal{H}_t)\\)。摘要格式为`<summary>...</summary>`，内容包含已验证证据和待查信息缺口。\n5.  **状态压缩与重启**：将原始查询q与摘要s组合成新的压缩状态 \\(q' = (q, s)\\)，并重置工作历史为 \\(\\mathcal{H}_t \\leftarrow (q')\\)。智能体基于此新状态继续探索。\n6.  **循环与终止**：重复步骤2-5，直至智能体生成最终答案`<answer>...</answer>`或达到资源预算（如最大工具调用次数60次）后终止。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：智能体策略模型（Agent Policy Model）\n- **模块名**：\\(\\pi_\\theta\\)\n- **输入**：当前对话历史 \\(\\mathcal{H}_{t-1}\\)，格式为包含问题、所有过往Thought-Action-Observation序列的文本。\n- **核心处理逻辑**：基于Transformer架构的LLM（如WebSailor-3B/7B/30B），在给定上下文中自回归生成下一个推理步骤。具体生成分为两部分：1) 在`<thought>`标签内生成内部推理 \\(\\tau_t\\)（限制在512个Token内）；2) 在`<tool_call>`标签内生成可解析的工具调用指令 \\(a_t\\)。\n- **输出**：一对Token序列 \\((\\tau_t, a_t)\\)。\n- **设计理由**：沿用成熟的ReAct范式，保证与现有智能体生态的兼容性。限制Thought长度是为了控制Token消耗，确保推理的聚焦性。\n\n#### 模块二：摘要工具（ReSumTool-30B）\n- **模块名**：\\(\\pi_{sum}\\)\n- **输入**：需要压缩的完整对话历史 \\(\\mathcal{H}_t\\)，包含多轮交互的详细文本。\n- **核心处理逻辑**：基于Qwen3-30B-A3B-Thinking模型，通过**监督微调（Supervised Fine-Tuning）** 专门训练而成。训练数据来自SailorFog-QA数据集中使用ReSum流程rollout收集的`<Conversation, Summary>`配对。模型被训练执行**目标导向的对话摘要**，具体任务包括：从冗长嘈杂的交互历史中进行逻辑推理、提炼可验证的证据、明确列出信息缺口、并提出基于网页上下文的具体下一步行动建议。\n- **输出**：结构化的摘要文本s，包裹在`<summary>`标签内。\n- **设计理由**：通用LLM（即使是大型模型）缺乏针对网页搜索场景的摘要能力，容易遗漏关键证据或无法识别信息缺口。通过专门训练一个30B规模的模型，可以在保持部署轻量化的同时，获得媲美甚至超越更大模型（如Qwen3-235B、DeepSeek-R1-671B）的摘要质量，从而平衡效果与成本。\n\n#### 模块三：ReSum-GRPO训练框架\n- **模块名**：ReSum-GRPO\n- **输入**：初始查询q，智能体策略模型 \\(\\pi_\\theta\\)，摘要工具 \\(\\pi_{sum}\\)，训练数据集（如SailorFog-QA的1K样本）。\n- **核心处理逻辑**：\n  1.  **轨迹分割**：在训练rollout过程中，当触发摘要时，完整的ReSum轨迹被自然分割成K+1个片段（Segments）。每个片段 \\(\\mathcal{H}^{(i)}\\) 作为一个独立的训练episode。\n  2.  **奖励计算**：仅从最终片段的最终答案 \\(a_T\\) 计算**轨迹级奖励** \\(R(a, a_T) \\in \\{0, 1\\}\\)，其中a是真实答案，使用LLM-as-Judge策略进行评估。\n  3.  **优势广播**：对同一批（Group）内的G条轨迹，计算每条轨迹的奖励 \\(R_g\\)，进行组内归一化得到轨迹级优势 \\(\\hat{A}_g = \\frac{R_g - \\text{mean}(\\{R_1,...,R_G\\})}{\\text{std}(\\{R_1,...,R_G\\})}\\)。\n  4.  **策略优化**：将该优势值 \\(\\hat{A}_g\\) **广播**给该轨迹内的所有片段，作为每个片段的优势信号 \\(\\hat{A}_g^{(i)}\\)。然后使用GRPO（Group Relative Policy Optimization）算法更新策略参数 \\(\\theta\\)，目标函数为：\n\\[\\mathcal{J}_{\\mathrm{GRPO}}(\\theta) = \\mathbb{E} \\frac{1}{\\sum_{g=1}^{G} n_g} \\sum_{g=1}^{G} \\sum_{i=1}^{n_g} \\min \\left(r_g^{(i)}(\\theta) \\hat{A}_g^{(i)}, \\operatorname{clip}\\left(r_g^{(i)}(\\theta), 1-\\varepsilon_{\\mathrm{low}}, 1+\\varepsilon_{\\mathrm{high}}\\right) \\hat{A}_g^{(i)}\\right),\\]\n其中 \\(r_g^{(i)}(\\theta)\\) 是片段i在策略更新前后的概率比。\n- **输出**：更新后的智能体策略模型参数 \\(\\theta'\\)。\n- **设计理由**：标准监督微调需要收集昂贵的专家级ReSum轨迹数据，且可能覆盖智能体原有技能。强化学习允许智能体通过**自我进化**来适应ReSum范式，同时保留其固有的推理能力。优势广播机制确保了长轨迹中所有片段都能获得一致的学习信号，鼓励智能体既**有效利用摘要**进行推理，又**策略性地收集信息**以产生高质量的摘要。\n\n**§3 关键公式与算法（如有）**\n1.  **历史更新公式**：\\(\\mathcal{H}_t = \\mathcal{H}_{t-1} \\circ (\\tau_t, a_t, o_t)\\)，其中 \\(\\circ\\) 表示拼接操作。\n2.  **摘要生成公式**：\\(s \\sim \\pi_{\\text{sum}}(\\cdot | \\mathcal{H}_t)\\)。\n3.  **状态压缩公式**：\\(q' = (q, s)\\)。\n4.  **ReSum-GRPO目标函数**（已在上文模块三中列出）。\n5.  **轨迹级优势计算**：\\(\\hat{A}_g = \\frac{R_g - \\mathrm{mean}(\\{R_1,...,R_G\\})}{\\mathrm{std}(\\{R_1,...,R_G\\})}\\)。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n本文提出了两个核心变体/设置：\n1.  **训练无关的ReSum（Training-free ReSum）**：直接将ReSum范式应用于现有的WebSailor智能体（3B/7B/30B），无需额外训练。仅需在推理时，当上下文接近限制（如32K Token）时，调用一个摘要工具（可以是通用LLM或专门的ReSumTool-30B）进行压缩并重启。这是本文的**基础范式**。\n2.  **ReSum-GRPO训练**：对WebSailor智能体使用ReSum-GRPO算法进行微调，使其更好地掌握“基于摘要进行推理”的模式。这是**增强版本**，旨在解决智能体在训练数据分布中未见过“查询+摘要”这种输入模式的问题。\n此外，在实验部分还对比了使用不同摘要工具的变体：Qwen3-30B、GPT-OSS-120B、Qwen3-235B、DeepSeek-R1-671B以及本文训练的ReSumTool-30B。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与已有代表性工作在技术实现上存在本质区别：\n1.  **vs. 标准ReAct（Yao et al., 2023）**：ReAct是**无状态追加**模式，将所有历史线性堆叠，直至上下文耗尽。ReSum是**有状态压缩重启**模式，周期性将历史压缩成摘要并重置上下文，从而突破长度限制。这是**范式级**的根本差异。\n2.  **vs. Recent History（简单截断）**：Recent History采用**被动丢弃**策略，直接截断超出窗口的旧信息，可能导致关键早期证据丢失。ReSum采用**主动提炼**策略，通过摘要模型有选择地保留最关键的信息（证据、缺口），信息保留是智能且目标导向的。\n3.  **vs. 基于外部记忆的方法（如A-Mem, MemOS）**：这些方法需要引入**独立的检索模块和记忆存储**，系统架构复杂，且检索过程可能引入额外延迟和噪声。ReSum将记忆管理**内化**到推理流程中，仅通过调用一个摘要工具（可视为一个“工具”）来实现，保持了ReAct的简洁性和“即插即用”的兼容性。\n4.  **vs. 通过RL内部管理上下文的方法（如MEM1, MemAgent）**：这些方法也使用RL训练智能体管理上下文，但通常需要**专门设计的rollout和昂贵的训练流程**，且与现有智能体集成困难。ReSum-GRPO则通过**轨迹分割和优势广播**这一轻量级修改，使标准GRPO算法能够处理ReSum产生的长轨迹，训练成本仅增加约1.5倍，易于应用到现有已训练的智能体上。",
    "methodology_and_formulas": "【三、方法论细节与关键公式】\n\n**§1 完整算法流程（伪代码级描述）**\n**算法：ReSum推理流程**\n**输入**：用户查询q，智能体策略模型\\(\\pi_\\theta\\)，摘要工具\\(\\pi_{sum}\\)，最大工具调用次数\\(T_{max}\\)，上下文窗口限制\\(L_{max}\\)。\n**输出**：最终答案或失败标记。\n1.  **初始化**：设置当前历史 \\(\\mathcal{H} \\leftarrow (q)\\)，工具调用计数 \\(t \\leftarrow 0\\)。\n2.  **While** \\(t < T_{max}\\) **and** 未生成最终答案 **do**:\n    1.  **生成推理与行动**：\\((\\tau_t, a_t) \\sim \\pi_\\theta(\\cdot | \\mathcal{H})\\)。\n    2.  **执行工具**：\\(o_t = \\mathcal{R}(a_t)\\)，其中\\(\\mathcal{R}\\)是工具环境（Search/Visit）。\n    3.  **更新历史**：\\(\\mathcal{H} \\leftarrow \\mathcal{H} \\circ (\\tau_t, a_t, o_t)\\)。\n    4.  **检查格式**：如果生成的\\(\\tau_t\\)或\\(a_t\\)不包含必需的标签（如`<thought>`, `<tool_call>`），则终止轨迹并返回失败。\n    5.  **检查摘要触发条件**：如果 \\(\\text{len}(\\mathcal{H}) \\ge L_{max}\\)（或达到预设的轮次阈值）：\n        - **生成摘要**：\\(s \\sim \\pi_{sum}(\\cdot | \\mathcal{H})\\)。\n        - **压缩状态**：\\(q' \\leftarrow (q, s)\\)。\n        - **重置历史**：\\(\\mathcal{H} \\leftarrow (q')\\)。\n    6.  \\(t \\leftarrow t + 1\\)。\n3.  **If** 生成了最终答案`<answer>` **then** 返回答案；**Else** 返回失败。\n\n**§2 关键超参数与配置**\n- **上下文窗口限制（\\(L_{max}\\)）**：默认为32K tokens，这是WebSailor系列模型的支持上限。选择理由：与基线ReAct保持一致的对比条件。\n- **最大工具调用次数（\\(T_{max}\\)）**：在大多数实验中设置为60次。选择理由：为复杂查询提供充足的探索预算，同时控制计算资源消耗。\n- **Thought生成长度限制**：512 tokens。选择理由：控制单轮推理的Token消耗，确保聚焦。\n- **摘要触发机制**：采用**系统性触发**，即当累积历史Token数接近\\(L_{max}\\)时自动触发。论文未明确给出精确阈值，但暗示是“接近限制时”。\n- **ReSum-GRPO训练超参数**：\n  - **训练数据规模**：1K样本（来自SailorFog-QA）。选择理由：降低训练成本，同时仍能证明方法的有效性。\n  - **训练轮数（Epochs）**：4轮。\n  - **组大小（Group Size G）**：未明确给出，但遵循GRPO标准设置。\n  - **裁剪参数（Clipping parameters）**：\\(\\varepsilon_{low}\\)和\\(\\varepsilon_{high}\\)，遵循GRPO原论文设置。\n- **推理温度（Temperature）与top_p**：在Tongyi-DeepResearch-30B-A3B的实验中，temperature=0.85，top_p=0.95。选择理由：遵循该模型官方的推理参数。\n\n**§3 训练/微调设置（如有）**\n- **ReSumTool-30B训练**：\n  - **基础模型**：Qwen3-30B-A3B-Thinking。\n  - **训练数据**：从SailorFog-QA数据集中，使用ReSum流程进行rollout收集的`<Conversation, Summary>`配对。具体收集方式未详细说明，但暗示使用了强大的开源模型（如Guo et al., 2025; OpenAI, 2025a）作为数据引擎。\n  - **训练方法**：监督微调（Supervised Fine-Tuning）。\n- **ReSum-GRPO训练**：\n  - **基础智能体**：WebSailor-3B/7B/30B（已通过拒绝采样微调RFT获得工具调用能力）。\n  - **训练数据**：从SailorFog-QA数据集中随机选取的1K样本。\n  - **优化器与学习率**：未明确说明，但应遵循GRPO原论文设置。\n  - **批次大小与训练步数**：未明确说明，但训练了4个epoch。\n  - **奖励模型**：使用**LLM-as-Judge**策略，基于Qwen2.5-72B-Instruct模型评估预测答案与真实答案的一致性，给出0/1奖励。\n\n**§4 推理阶段的工程细节**\n- **工具实现**：\n  - **Search工具**：查询Google搜索引擎，支持同时提交多个查询，每个查询返回top-10结果。\n  - **Visit工具**：通过URL使用Jina（Jina.ai, 2025）浏览特定网页，并使用Qwen2.5-72B-Instruct提取目标相关的证据。\n- **并行化策略**：未明确说明，但大规模实验暗示可能采用了并行rollout以收集训练数据。\n- **缓存机制**：未提及。\n- **向量数据库**：未使用。ReSum不依赖外部检索，其“记忆”通过摘要文本内化在上下文中。\n- **统一推理框架**：所有实验在一个统一的推理框架中进行，该框架支持Search和Visit工具，并集成了ReSum的触发与调用逻辑。",
    "experimental_design": "【四、实验设计】\n\n**§1 数据集详情（每个数据集单独列出）**\n1.  **GAIA**：\n    - **名称**：GAIA (Mialon et al., 2023)\n    - **规模**：103个样本的文本验证子集。\n    - **领域类型**：通用AI助手基准，包含需要多步推理和网页搜索的复杂问题。\n    - **评测问题类型**：多跳推理、事实核查、信息整合。\n    - **特殊处理**：遵循现有工作，使用文本验证子集。\n2.  **BrowseComp-en**：\n    - **名称**：BrowseComp-en (Wei et al., 2025)\n    - **规模**：未在论文中明确给出具体样本数，但从上下文推断是一个具有挑战性的网页浏览竞赛基准。\n    - **领域类型**：英文网页搜索与浏览任务。\n    - **评测问题类型**：高度模糊实体、关系交织、需要大量工具调用（常超过10次）的复杂信息寻求任务。\n    - **特殊处理**：无特殊过滤。\n3.  **BrowseComp-zh**：\n    - **名称**：BrowseComp-zh (Zhou et al., 2025a)\n    - **规模**：未明确给出，是BrowseComp-en的中文对应版本。\n    - **领域类型**：中文网页搜索与浏览任务。\n    - **评测问题类型**：同BrowseComp-en，但针对中文查询和网页内容。\n    - **特殊处理**：无特殊过滤。\n**排除的数据集**：论文明确指出排除了SimpleQA、WebWalkerQA、xBench-DeepSearch等较简单的基准，因为其中大多数案例可以在标准上下文限制内解决，ReAct范式已足够适用。\n\n**§2 评估指标体系（全量列出）**\n- **准确性指标**：\n  1.  **Pass@1**：单次推理（单次rollout）的通过率。评估标准是预测答案是否与真实答案对齐，使用Qwen2.5-72B-Instruct作为评分模型（LLM-as-Judge）。\n  2.  **Pass@3**：每个样本进行3次独立推理（3次rollouts），只要有一次通过即计为通过，然后计算所有样本的平均通过率。同样使用Qwen2.5-72B-Instruct评分。\n- **效率/部署指标**：\n  1.  **平均Token消耗**：在附录图6中进行了对比，展示了不同范式下的资源消耗。\n  2.  **工具调用次数**：在附录图6中进行了对比。\n  3.  **训练成本**：ReSum-GRPO相比标准GRPO增加了约1.5倍的训练时间（附录表5）。\n- **其他自定义指标**：无。\n\n**§3 对比基线（完整枚举）**\n1.  **ReAct**：标准ReAct范式（Yao et al., 2023），作为主要基线。所有交互历史被追加到上下文中。\n2.  **Recent History**：简单截断基线。当上下文超过32K限制时，仅保留最近的22K个Token的历史，丢弃旧信息。\n3.  **MEM1**：一个代表性的上下文管理方法（Zhou et al., 2025b），允许智能体通过RL内部管理上下文。在附录D中进行了详细对比。\n4.  **领先的预训练模型+工具**：为了提供性能背景，报告了以下模型使用Search和Visit工具在ReAct范式下的结果：\n    - Claude-4-Sonnet\n    - OpenAI-o3\n    - Kimi-K2\n    - DeepSeek-v3.1\n5.  **其他开源网页智能体（RL训练）**：作为ReSum-GRPO的对比对象，包括：\n    - Qwen3-ARPO-14B (ARPO算法)\n    - MiroThinker-8B/32B v0.1 (DPO算法)\n    - ASearcher-32B (GRPO算法)\n    - WebExplorer-8B (GRPO算法)\n    *注：这些智能体使用了10K+样本进行训练。*\n6.  **标准GRPO**：在训练设置中，将标准GRPO算法应用于WebSailor智能体，作为ReSum-GRPO的对比基线。\n\n**§4 实验控制变量与消融设计**\n- **控制变量**：\n  1.  **智能体模型**：固定使用WebSailor-3B/7B/30B作为基础智能体，确保能力基线一致。\n  2.  **工具与环境**：所有实验使用统一的推理框架，相同的Search和Visit工具实现。\n  3.  **最大工具调用次数**：统一设置为60次。\n  4.  **上下文窗口限制**：统一设置为32K tokens（WebSailor模型限制）。\n  5.  **评分模型**：统一使用Qwen2.5-72B-Instruct作为LLM-as-Judge。\n- **消融设计**：\n  1.  **摘要工具消融**：比较了不同摘要工具（Qwen3-30B, ReSumTool-30B, GPT-OSS-120B, Qwen3-235B, DeepSeek-R1-671B）在ReSum范式下的性能，以验证专用摘要模型的有效性。\n  2.  **训练算法消融**：比较了无训练（Training-free）、标准GRPO训练、ReSum-GRPO训练三种设置下，智能体在ReAct和ReSum两种推理范式上的性能，以验证ReSum-GRPO的必要性。\n  3.  **上下文长度消融**：在Tongyi-DeepResearch-30B-A3B上测试了64K和128K两种上下文限制下，ReAct与ReSum的性能差异，以验证ReSum在更长上下文下的正交有效性。",
    "core_results": "【五、核心实验结果】\n\n**§1 主实验结果全景（表格式呈现）**\n以下表格综合了论文表1和表2的核心数据，格式为：`智能体 | 训练算法 | 推理范式 | GAIA-Pass@1 | GAIA-Pass@3 | BrowseComp-zh-Pass@1 | BrowseComp-zh-Pass@3 | BrowseComp-en-Pass@1 | BrowseComp-en-Pass@3`。\n\n**表1：训练无关设置下的性能对比（%）**\n| Agent | Paradigm | Summary Tool | GAIA-P@1 | GAIA-P@3 | BC-zh-P@1 | BC-zh-P@3 | BC-en-P@1 | BC-en-P@3 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| WebSailor-3B | ReAct | - | 25.6 | 42.7 | 8.2 | 17.0 | 3.3 | 5.6 |\n| WebSailor-3B | Recent History | - | 27.2 | 44.7 | 13.2 | 24.3 | 3.8 | 8.9 |\n| WebSailor-3B | ReSum | Qwen3-30B | 27.5 | 45.6 | 6.9 | 14.5 | 4.2 | 7.8 |\n| WebSailor-3B | ReSum | **ReSumTool-30B** | **35.3** | **52.4** | **13.7** | **24.6** | **6.8** | **10.8** |\n| WebSailor-3B | ReSum | GPT-OSS-120B | 40.5 | 65.1 | 15.2 | 28.0 | 8.5 | 15.8 |\n| WebSailor-3B | ReSum | Qwen3-235B | 32.4 | 49.5 | 11.1 | 23.9 | 5.7 | 10.3 |\n| WebSailor-3B | ReSum | DeepSeek-R1-671B | 39.2 | 60.2 | 13.0 | 23.5 | 7.5 | 13.4 |\n| WebSailor-7B | ReAct | - | 31.7 | 44.7 | 13.2 | 25.6 | 5.7 | 10.3 |\n| WebSailor-7B | Recent History | - | 33.0 | 48.5 | 15.2 | 28.0 | 5.2 | 9.4 |\n| WebSailor-7B | ReSum | Qwen3-30B | 34.6 | 48.5 | 13.3 | 26.6 | 5.8 | 10.3 |\n| WebSailor-7B | ReSum | **ReSumTool-30B** | **40.5** | **60.2** | **17.2** | **30.8** | **9.0** | **15.2** |\n| WebSailor-7B | ReSum | GPT-OSS-120B | 42.4 | 61.2 | 19.2 | 35.6 | 10.5 | 17.2 |\n| WebSailor-7B | ReSum | Qwen3-235B | 43.4 | 60.2 | 18.1 | 32.9 | 8.7 | 15.2 |\n| WebSailor-7B | ReSum | DeepSeek-R1-671B | 41.1 | 58.3 | 17.1 | 32.2 | 10.3 | 16.6 |\n| WebSailor-30B | ReAct | - | 45.0 | 60.2 | 23.9 | 38.4 | 12.8 | 21.8 |\n| WebSailor-30B | Recent History | - | 40.1 | 56.3 | 24.1 | 40.1 | 10.3 | 16.7 |\n| WebSailor-30B | ReSum | Qwen3-30B | 45.6 | 61.2 | 24.8 | 40.1 | 12.2 | 20.4 |\n| WebSailor-30B | ReSum | **ReSumTool-30B** | **47.3** | **63.1** | **24.1** | **42.6** | **16.0** | **25.4** |\n| WebSailor-30B | ReSum | GPT-OSS-120B | 51.5 | 68.9 | 27.3 | 46.4 | 18.8 | 30.9 |\n| WebSailor-30B | ReSum | Qwen3-235B | 46.9 | 67.0 | 25.7 | 42.2 | 17.2 | 26.7 |\n| WebSailor-30B | ReSum | DeepSeek-R1-671B | 49.2 | 71.8 | 27.1 | 41.5 | 13.7 | 22.6 |\n\n**表2：RL训练后的性能对比（%）**\n| Agent | RL Algorithm | Paradigm | GAIA-P@1 | GAIA-P@3 | BC-zh-P@1 | BC-zh-P@3 | BC-en-P@1 | BC-en-P@3 |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| WebSailor-3B | - | ReAct | 25.6 | 42.7 | 8.2 | 17.0 | 3.3 | 5.6 |\n| WebSailor-3B | GRPO | ReAct | 28.5 | 48.5 | 11.8 | 22.5 | 4.2 | 8.5 |\n| WebSailor-3B | GRPO | ReSum | 38.5 | 53.4 | 17.3 | 29.1 | 8.5 | 13.0 |\n| WebSailor-3B | **ReSum-GRPO** | **ReSum** | **37.9** | **56.3** | **20.5** | **34.3** | **9.2** | **13.0** |\n| WebSailor-7B | - | ReAct | 31.7 | 44.7 | 13.2 | 25.6 | 5.7 | 10.3 |\n| WebSailor-7B | GRPO | ReAct | 34.0 | 47.6 | 18.7 | 31.8 | 5.8 | 10.0 |\n| WebSailor-7B | GRPO | ReSum | 37.2 | 53.4 | 25.4 | 40.8 | 8.5 | 15.0 |\n| WebSailor-7B | **ReSum-GRPO** | **ReSum** | **42.4** | **60.2** | **27.1** | **39.5** | **12.3** | **18.5** |\n| WebSailor-30B | - | ReAct | 45.0 | 60.2 | 23.9 | 38.4 | 12.8 | 21.8 |\n| WebSailor-30B | GRPO | ReAct | 48.2 | 62.1 | 23.3 | 36.7 | 14.3 | 21.5 |\n| WebSailor-30B | GRPO | ReSum | 48.5 | 61.2 | 29.3 | 42.6 | 15.0 | 25.0 |\n| WebSailor-30B | **ReSum-GRPO** | **ReSum** | **48.5** | **68.0** | **33.3** | **48.8** | **18.3** | **26.5** |\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n- **在不同基准上的表现**：ReSum范式在所有三个基准（GAIA, BrowseComp-zh, BrowseComp-en）上均一致优于ReAct基线。**提升幅度最大的是BrowseComp-en**，例如WebSailor-30B使用ReSumTool-30B时，Pass@1从12.8%提升至16.0%（绝对提升3.2个百分点，相对提升25.0%）。这表明ReSum对需要**极长探索周期**的任务收益最大。在GAIA上提升相对较小，因为GAIA部分问题可能不需要超长上下文。\n- **在不同规模智能体上的表现**：ReSum对**小规模模型（3B/7B）的提升尤为显著**。例如WebSailor-3B在BrowseComp-zh上，Pass@1从8.2%提升至13.7%（绝对提升5.5个百分点，相对提升67.1%）。这是因为小模型本身上下文管理能力更弱，更容易受限于窗口，ReSum提供的“重启”机制带来了更大增益。对于30B模型，提升依然存在但幅度相对收窄。\n- **与Recent History基线的对比**：Recent History（简单截断）虽然也能延长探索，但其性能**不稳定且通常低于ReSum**。例如WebSailor-30B在BrowseComp-en上，Recent History（10.3%）甚至低于原始ReAct（12.8%），而ReSumTool-30B达到16.0%。这证明**智能摘要压缩远优于简单截断**。\n- **与大型预训练模型的对比**：配备ReSumTool-30B的WebSailor-30B在BrowseComp-en上达到16.0% Pass@1，**超越了使用ReAct的Claude-4-Sonnet（12.2%）和Kimi-K2（14.1%）**。这表明ReSum范式能有效弥补开源小模型与大型专有模型在长视野任务上的性能差距。\n- **ReSum-GRPO的训练效果**：ReSum-GRPO训练进一步放大了ReSum的优势。例如WebSailor-30B在BrowseComp-zh上，未经训练的ReSum为24.1%，经过ReSum-GRPO训练后达到33.3%（绝对提升9.2个百分点）。相比之下，标准GRPO训练对ReSum范式提升有限（29.3%），证明**专门的范式适应训练是必要的**。\n\n**§3 效率与开销的定量对比**\n论文在附录图6中提供了资源消耗对比（原文未提供具体数字表格，需从描述中推断）：\n- **Token消耗**：ReSum范式由于周期性调用摘要模型并重置上下文，**单次推理的Token消耗预计会高于ReAct**，因为摘要生成本身消耗Token，且压缩后重启需要重新生成部分内容。但ReAct会因上下文过长而提前终止失败案例，ReSum则能继续探索直至成功或达到最大工具调用次数，因此**总体成功率更高**，但可能以更高的平均Token消耗为代价。论文提到ReSum实现了“合理的资源利用”。\n- **训练成本**：ReSum-GRPO相比标准GRPO，训练时间增加约**1.5倍**。这是一个可接受的增长，因为ReSum-GRPO需要处理因摘要而分割的更长的轨迹。\n- **工具调用次数**：ReSum允许智能体进行更多轮次的工具调用（最多60次），从而解决了ReAct中因上下文耗尽而提前终止的问题。这直接转化为**更高的任务解决率**，尤其是对那些需要超过10次工具调用的复杂案例。\n\n**§4 消融实验结果详解**\n1.  **摘要工具消融**：\n    - **ReSumTool-30B vs. 其基础模型Qwen3-30B**：ReSumTool-30B在所有配置下均显著优于基础模型。例如WebSailor-3B在GAIA上，Pass@1从27.5%提升至35.3%（绝对提升7.8个百分点，相对提升28.4%），证明了**针对性训练的有效性**。\n    - **ReSumTool-30B vs. 更大模型**：ReSumTool-30B（30B参数）的性能与Qwen3-235B（235B参数）和DeepSeek-R1-671B（671B参数）相当，甚至在部分场景超越。例如在WebSailor-3B + BrowseComp-zh上，ReSumTool-30B（13.7%）优于Qwen3-235B（11.1%）和DeepSeek-R1-671B（13.0%）。这表明**专用的小模型可以媲美通用的大模型**，具有部署成本优势。\n    - **GPT-OSS-120B表现最佳**：在大多数配置中，GPT-OSS-120B作为摘要工具取得了最优性能，但其API成本和部署开销最高。\n2.  **训练算法消融**：\n    - **ReSum-GRPO vs. 标准GRPO（在ReSum范式下）**：ReSum-GRPO训练后的智能体在ReSum范式下性能显著优于仅用标准GRPO训练的智能体。例如WebSailor-7B在BrowseComp-zh上，ReSum-GRPO达到27.1% Pass@1，而标准GRPO+ReSum为25.4%（绝对提升1.7个百分点）。这证明标准GRPO**无法让智能体有效掌握基于摘要的推理模式**，需要ReSum-GRPO的轨迹分割和优势广播机制。\n    - **ReSum-GRPO vs. 无训练（Training-free）**：ReSum-GRPO训练带来了进一步的性能提升。例如WebSailor-30B在BrowseComp-zh上，从训练无关的24.1%提升到训练后的33.3%（绝对提升9.2个百分点）。\n3.  **上下文长度消融（对长上下文模型）**：在Tongyi-DeepResearch-30B-A3B（支持128K上下文）上的实验表明，即使上下文窗口很大，ReSum依然有效。在64K限制下，ReSum在BrowseComp-zh上Pass@1从43.6%提升至48.6%（绝对提升5.0个百分点）；在128K限制下，从45.7%提升至46.6%（绝对提升0.9个百分点）。提升幅度随上下文增大而减小，因为摘要触发频率降低，但**正交有效性**依然存在。\n\n**§5 案例分析/定性分析（如有）**\n论文在附录E.3中提供了定性分析案例：展示了ReSum-GRPO训练后的WebSailor-30B在三个测试案例上的完整轨迹。\n- **案例一**：智能体**未使用摘要**，在少量工具调用后直接推导出答案。这表明ReSum-GRPO训练**没有损害智能体解决简单问题的能力**。\n- **案例二和案例三**：智能体成功**触发了摘要工具**，基于压缩后的摘要状态继续探索，并最终生成正确答案。这证明了ReSum-GRPO训练使智能体学会了**在需要时利用摘要来管理长上下文**。\n这些案例共同表明，ReSum-GRPO训练使智能体能够**灵活地在“直接推理”和“摘要后推理”两种模式间切换**，适应不同复杂度的任务。",
    "conclusion_and_future_work": "【六、结论与未来工作】\n\n**§1 本文核心贡献总结**\n1.  **提出了ReSum范式**：一种通过**周期性上下文摘要**来实现**无限长视野探索**的新推理范式。它是对ReAct的最小修改，保持了即插即用的兼容性。该贡献直接解决了ReAct范式在复杂查询中因上下文耗尽而提前失败的根本问题，在三个基准上平均绝对性能提升4.5%。\n2.  **开发了ReSumTool-30B**：一个通过针对性训练获得的、专用于**目标导向对话摘要**的30B参数模型。它在摘要质量上媲美甚至超越了参数量大一个数量级的通用模型（如Qwen3-235B），实现了高性能与轻量级部署的平衡。\n3.  **设计了ReSum-GRPO算法**：一种**适应ReSum范式的强化学习算法**。通过**轨迹分割**和**轨迹级优势广播**，使智能体能够通过自我进化掌握基于摘要的推理，而无需昂贵的专家轨迹数据。仅用1K训练样本，就使WebSailor-30B在BrowseComp-zh上达到33.3% Pass@1，超越了使用10K+样本训练的其他开源智能体。\n4.  **证明了ReSum的广泛兼容性与正交有效性**：实验表明ReSum不仅适用于上下文受限的模型，也对支持长上下文（如128K）的先进智能体有效，证明了该范式是一种**普适的上下文管理解决方案**。\n\n**§2 局限性（作者自述）**\n作者明确承认了以下局限性：\n1.  **依赖外部摘要工具**：ReSum需要调用一个独立的摘要模型（ReSumTool-30B），这增加了系统复杂性和潜在延迟。\n2.  **基于规则的摘要触发机制**：当前采用系统性触发（如接近上下文限制时），这可能不是最优的。智能体应该**自主决定何时进行摘要**，以更智能地管理上下文。\n\n**§3 未来研究方向（全量提取）**\n1.  **赋予智能体自我摘要能力**：未来工作旨在将摘要能力**内化**到智能体策略模型中，使其能够自主生成摘要，从而消除对外部工具的依赖。这将进一步简化系统架构。\n2.  **开发智能的摘要触发机制**：研究让智能体**学习在合适的时机触发摘要调用**，而不是依赖预定义的规则（如Token数阈值）。这可以通过强化学习或其他机制来实现，使上下文管理更加动态和高效。\n3.  （隐含方向）**探索更高效的摘要压缩技术**：当前摘要仍是自然语言文本，未来可以探索更紧凑的表示形式（如结构化表示、向量编码），以进一步减少上下文占用。\n4.  （隐含方向）**应用于更广泛的智能体场景**：将ReSum范式推广到**代码生成、多模态交互、机器人规划**等其他需要长序列处理的智能体任务中。",
    "research_contributions": "【七、研究贡献与学术价值】\n\n**§1 核心学术贡献（按重要性排序）**\n1.  **范式创新（理论新颖性）**：首次系统性地提出了通过**周期性上下文摘要**来解决LLM智能体长视野探索中上下文限制问题的**新范式**。这不同于以往通过扩展窗口或引入外部记忆的路径，提供了一种轻量级、可兼容的替代方案。其核心思想——将冗长历史压缩为可重启的推理状态——具有理论上的简洁性和通用性。\n2.  **方法创新与实验验证充分性**：不仅提出了范式，还配套开发了专用的摘要模型（ReSumTool-30B）和训练算法（ReSum-GRPO），形成了完整的技术栈。实验设计全面，涵盖了**训练无关**和**训练增强**两种设置，在**三个具有挑战性的基准**上进行了验证，并对比了**多种基线、不同规模的智能体以及不同摘要工具**，结论坚实。\n3.  **对领域的影响**：这项工作直接击中了当前开源网页智能体发展的一个核心痛点——**受限于ReAct范式的上下文管理**。它为社区提供了一种即插即用的升级方案，可以显著提升现有智能体在复杂任务上的性能，而不需要重新设计整个架构。其代码和模型已开源，有望被广泛采纳。\n4.  **资源效率的示范**：证明了使用**相对较小的专用模型（30B）** 可以取得与超大模型（235B/671B）相媲美的摘要效果，并且仅用**1K训练样本**通过ReSum-GRPO就能获得显著性能提升。这为资源受限的研究者提供了可行的研究路径，展示了**数据质量和算法设计的重要性不亚于模型规模**。\n\n**§2 工程与实践贡献**\n- **开源实现**：论文提供了项目主页和GitHub仓库（https://github.com/Alibaba-NLP/DeepResearch），包含了ReSum范式、ReSumTool-30B模型和ReSum-GRPO训练代码的完整实现，促进了可复现性和后续研究。\n- **统一的推理框架**：构建了一个支持Search、Visit工具以及ReSum逻辑的统一推理框架，为社区提供了可靠的实验平台。\n- **新的评估视角**：强调了在**长视野、高工具调用次数**场景下评估智能体的重要性，而不仅仅是关注简单任务。\n\n**§3 与相关工作的定位**\n本文工作在当前技术路线图中处于**对ReAct范式的革新性扩展**的位置。它并非开辟一个全新的智能体架构，而是在承认ReAct简洁高效的基础上，针对其最致命的短板（上下文限制）进行“外科手术式”的修补。它区别于引入外部记忆组件的复杂方案（如A-Mem, MemOS），也区别于完全依赖RL进行内部记忆管理的方案（如MEM1, MemAgent）。ReSum选择了一条**中间道路**：通过一个可插拔的“摘要工具”和轻量级的训练适配，在保持ReAct核心优点的同时，突破了其核心限制。因此，它可以被看作是ReAct范式在长视野场景下的**自然进化**。",
    "professor_critique": "【八、隐性缺陷与教授锐评】\n\n**§1 实验设计与评估体系的缺陷**\n1.  **评估指标单一**：仅使用Pass@1和Pass@3作为主要指标，这只能反映最终答案的正确性，但**完全忽略了探索过程的效率和质量**。例如，ReSum可能以更多的Token消耗和更长的推理时间为代价换取更高的通过率，但论文未提供详细的延迟、Token消耗分布、平均工具调用次数等效率指标的具体对比数据（仅附录中有简略图示）。\n2.  **Baseline选择可能不全面**：虽然对比了ReAct、Recent History和MEM1，但未与近期一些更先进的上下文管理方法进行对比，例如**MemAgent（Yu et al., 2025a）** 或**Chain-of-Agents（Li et al., 2025c）** 中提到的分层记忆机制。这些方法也可能声称能解决长上下文问题。\n3.  **对“简单任务”的排除可能引入偏差**：作者有意排除了SimpleQA等简单基准，理由是ReAct已足够。但这可能导致**无法评估ReSum在简单任务上是否引入不必要的开销**（如多余的摘要调用）。一个全面的评估应该包含从简单到复杂的任务谱系，以展示方法的普适性和效率权衡。\n4.  **LLM-as-Judge的可靠性问题**：使用Qwen2.5-72B-Instruct作为评判模型可能存在**模型自身偏见和错误**。未进行人工评估或使用多个评判模型交叉验证，结果的可信度存疑。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **摘要的信息损失与错误传播风险**：摘要过程本质上是**有损压缩**。摘要模型可能会**遗漏关键细节**或**错误总结证据**，这些错误会随着每次摘要被固化并传递到后续推理中，导致**错误累积（Error Accumulation）**。论文未对摘要的保真度进行定量评估（如ROUGE、BLEU或事实一致性分数）。\n2.  **摘要触发的启发式规则过于粗糙**：当前基于Token数阈值的触发机制是**非智能的**。它无法区分“信息已饱和可摘要”和“正在关键推理中途不宜打断”的场景。**不合时宜的摘要中断可能破坏连贯的推理链**，反而降低性能。\n3.  **对摘要模型的强依赖**：ReSum的性能高度依赖于摘要模型的质量。虽然ReSumTool-30B表现良好，但**其训练数据来源于“强大开源模型”的rollout，这可能引入了数据偏差**。如果摘要模型在某个领域表现不佳，整个ReSum系统在该领域就会失效。\n4.  **计算开销的转移**：ReSum将上下文管理的计算负担从主模型转移到了摘要模型。虽然摘要调用次数有限，但对于**超长轨迹（如超过100轮工具调用）**，可能需要多次摘要，导致**摘要模型的总调用成本可能变得显著**，甚至超过扩展上下文窗口的成本。论文未进行详细的成本效益分析。\n\n**§3 未经验证的边界场景**\n1.  **多语言混合输入**：当前实验仅在英文和中文数据集上进行。如果查询和网页内容混合了多种语言（如中英混杂、小语种），ReSumTool-30B（基于Qwen3）的摘要能力可能会严重下降，导致信息丢失。\n2.  **领域外知识冲突**：当智能体探索的领域完全超出摘要模型（或主模型）的预训练知识范围时，摘要模型可能无法正确理解专业术语和关系，生成误导性摘要。例如，在高度专业的医学或法律文献搜索中。\n3.  **恶意对抗输入与幻觉**：如果网页内容包含对抗性文本或模型产生幻觉，摘要模型可能会将这些错误信息“总结”并“确认”为事实，从而**放大幻觉**。ReSum缺乏对摘要内容可信度的校验机制。\n4.  **实时性要求极高的任务**：对于需要极低延迟的交互式任务（如实时对话辅助），摘要生成和状态重置引入的额外延迟（可能需要数秒）可能是不可接受的。\n5.  **极其碎片化的信息收集**：当所需信息极度分散在数十个网页中，且每轮收集的信息量都很小时，频繁的",
    "source_file": "ReSum Unlocking Long-Horizon Search Intelligence via Context Summarization.md"
}