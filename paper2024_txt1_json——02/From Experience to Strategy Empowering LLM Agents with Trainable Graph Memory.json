{
    "title": "FROM EXPERIENCE TO STRATEGY: EMPOWERING LLM AGENTS WITH TRAINABLE GRAPH MEMORY",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n本研究位于基于大语言模型（LLM）的智能体（Agent）领域，特别是在需要多步推理、工具调用和从经验中学习的开放式任务执行场景中。近年来，LLM智能体在自主任务分解和解决方面展现出巨大潜力，但其决策过程不稳定，常导致低效的动作序列、重复错误甚至任务失败。一个核心挑战是如何让智能体不仅能行动，还能持续从过去的成功和错误中提取洞察并适应。当前，利用先验经验来提升LLM智能体推理能力是一个极具前景的方向，本研究旨在探索如何通过动态、结构化的显式记忆来主动引导和增强隐式的策略学习，以解决现有方法在适应性和可解释性上的根本缺陷。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法主要分为两类，均存在明确且对立的短板：\n1.  **隐式记忆（Implicit Memory）**：通过强化学习（RL）等训练过程将经验编码到模型参数中。当智能体面对与训练分布有偏移的新任务时，由于**灾难性遗忘（Catastrophic Forgetting）**，已编码的经验知识会迅速退化。例如，在训练后面对新的多跳问答（Multi-Hop QA）任务时，模型可能遗忘在类似任务（如HotpotQA）中学习到的有效工具调用模式，导致推理链断裂，准确率下降超过20%。\n2.  **显式记忆（Explicit Memory）**：通过上下文提示（Prompting）注入历史轨迹或策略指导，不修改模型权重。当任务或上下文超出特定范围时，这种方法**缺乏适应性（Lack of Adaptability）**。例如，方法如Expel（Zhao et al., 2024）提取可重用的推理轨迹来指导后续决策，但当输入查询与记忆库中的查询语义相似度较低（如余弦相似度<0.5）时，检索到的策略相关性差，无法提供有效指导，导致性能与无记忆基线相当甚至更差。\n3.  **静态图记忆方法**：如A-MEM（Xu et al., 2025）构建动态记忆笔记，Zep（Rasmussen et al., 2025）和HopRAG（Liu et al., 2025）构建逻辑感知图以促进检索。这些方法通常以**静态方式（Static Manner）** 应用图结构，缺乏评估或精炼记忆组件效用的机制。当记忆图中积累了大量低质量或过时的策略节点时，无法自动区分其效用，检索时会引入噪声，反而降低决策质量。\n\n**§3 问题的根本难点与挑战（200字以上）**\n从理论和工程角度，上述问题难以解决的原因在于：\n1.  **计算复杂性与可扩展性的矛盾**：理想的记忆系统需要存储和检索海量的历史轨迹。简单的基于向量相似度的检索（如Expel）计算复杂度低，但缺乏对策略效用的评估；而引入复杂的效用学习机制（如强化学习）又会增加训练开销和系统复杂性，难以扩展到超大规模的记忆库（例如百万级节点）。\n2.  **经验表示与泛化的鸿沟**：原始执行轨迹（Raw Execution Traces）包含大量任务特定的噪声和细节，直接作为记忆难以在不同任务间迁移。如何将这些低层轨迹**抽象（Abstract）** 为高层、可泛化的策略（Meta-Cognition），同时保留其核心的决策逻辑，是一个本质上的表示学习挑战。\n3.  **记忆整合与策略学习的耦合**：大多数现有工作仅在推理阶段使用记忆，或在训练阶段使用静态记忆。如何将动态演化的记忆**紧密集成（Tight Integration）** 到策略的强化学习训练循环中，使记忆既能指导策略学习，其本身又能根据策略学习的反馈进行优化，形成一个协同进化的系统，是工程实现和算法设计上的双重挑战。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的突破口在于提出一个**可训练的、多层次的图记忆框架**，其核心假设是：**通过一个异构图（Heterogeneous Graph）将代理轨迹结构化为有限状态机（Finite State Machine, FSM）中的规范路径，并进一步提炼为高层元认知策略，然后利用基于强化学习的权重优化机制来校准这些策略的效用，最后将其作为显式的策略先验动态注入到智能体的训练上下文中，可以同时解决隐式记忆的不可解释性和显式记忆的缺乏适应性问题。**\n该假设的理论依据来源于：\n1.  **认知科学中的层次化记忆模型**：人类记忆系统包含情景记忆（Episodic Memory）和语义记忆（Semantic Memory），本文的查询层（Query Layer）和元认知层（Meta-Cognition Layer）分别对应这两种记忆，通过中间的转换路径层（Transition Path Layer）进行连接和抽象。\n2.  **强化学习中的经验回放（Experience Replay）与课程学习（Curriculum Learning）**：本文的图记忆本质上是一个结构化的、效用加权的经验回放缓冲区。通过优化图中边的权重，系统可以优先回放高回报的经验（策略），为智能体提供一种自适应的课程，加速策略学习。\n3.  **图神经网络（GNN）的信息传播机制**：利用异构图中带权重的邻接矩阵进行信息传播，可以从历史查询中聚合信息，激活与当前任务最相关的子图和元认知节点，实现基于结构的、可解释的策略检索。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\n系统整体分为三个阶段，构成一个完整的训练与推理闭环：\n**输入**：原始智能体轨迹（包括成功与失败的）。\n**阶段一：分层记忆图构建**：\n1.  将原始轨迹通过**有限状态机（FSM）** 映射为规范路径（Canonical Path）。\n2.  构建三层异构图：**查询层（Query Layer, Q）** 节点代表任务实例；**转换路径层（Transition Path Layer, T）** 节点代表FSM中的规范路径；**元认知层（Meta-Cognition Layer, M）** 节点代表从路径对比中提炼出的高层策略。\n3.  通过二分邻接矩阵 \\(A^{q \\rightarrow t}\\) 和 \\(A^{t \\rightarrow m}\\) 定义层间连接。\n**输出**：初始化的记忆图 \\(\\mathcal{G} = (V, E)\\)，其中 \\(V = \\mathcal{Q} \\cup \\mathcal{T} \\cup \\mathcal{M}\\)。\n\n**阶段二：可训练图权重优化**：\n1.  为邻接矩阵的每条边关联可学习的权重 \\(w_{qt}\\) 和 \\(w_{tm}\\)。\n2.  对于新查询，根据与历史查询的相似度激活一个任务特定子图。\n3.  在该子图中，根据基于权重的相关性分数 \\(\\rho(m_k)\\) 采样元认知节点 \\(m_k\\)。\n4.  通过对比实验（有指导 vs. 无指导）计算奖励差距 \\(\\Delta R_k\\) 作为效用信号。\n5.  使用REINFORCE算法优化权重，损失函数为 \\(\\mathcal{L}_{\\mathrm{RL}} = - \\mathbb{E}_{m_k \\sim p} [\\Delta R_k \\cdot \\log p(m_k | q_{\\text{new}})]\\)。\n**输出**：权重经过优化的记忆图，其中高效用策略对应的路径权重被加强。\n\n**阶段三：记忆引导的策略优化**：\n1.  对于每个训练实例 \\(q_{\\mathrm{train}}\\)，从优化后的记忆图中检索top-k个元认知策略 \\(\\{m_1, ..., m_k\\}\\)。\n2.  将这些策略文本化并预置到原始查询前，形成增强提示 \\(\\tilde{q}_{\\mathrm{train}} = [m_1, m_2, ..., m_k; q_{\\mathrm{train}}]\\)。\n3.  智能体策略 \\(\\pi_\\theta\\) 以增强提示为输入，通过广义强化策略优化（GRPO）算法进行训练，最大化期望累积奖励。\n**最终输出**：优化后的智能体策略 \\(\\pi_\\theta\\)，能够在任务中有效利用记忆指导。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：有限状态机（Finite State Machine, FSM）与规范路径映射\n-   **模块名**：Finite State Machine (FSM) Mapping\n-   **输入**：原始执行轨迹 \\(\\tau\\)，包含一系列工具调用或推理步骤。\n-   **核心处理逻辑**：预定义一个FSM \\(\\boldsymbol{S} = (\\boldsymbol{S}, \\boldsymbol{A}, \\boldsymbol{T})\\)，其中 \\(S\\) 是抽象认知状态（如StrategyPlanning, InformationAnalysis），\\(A\\) 是动作空间，\\(T: S \\times A \\to S\\) 是转移函数。将原始轨迹的每个步骤映射到FSM的一个状态，形成一条规范路径 \\(t_j\\)。具体FSM规范见附录B.1（原文未提供细节）。\n-   **输出**：规范路径节点 \\(t_j\\)，代表标准化的行为模式。\n-   **设计理由**：将嘈杂的、任务特定的低层轨迹抽象为标准化的状态序列，过滤执行层面的噪声，确保记忆图只保留语义上有意义的决策点，便于跨查询的结构化比较和策略泛化。\n\n#### 模块二：元认知归纳（Meta-Cognition Induction）\n-   **模块名**：Meta-Cognition Induction\n-   **输入**：针对查询 \\(q_i\\) 采样的一组轨迹 \\(\\{\\bar{\\tau}_1^{(i)}, ..., \\bar{\\tau}_N^{(i)}\\}\\)，包含成功轨迹 \\(\\tau_s\\) 和失败轨迹 \\(\\tau_f\\)。\n-   **核心处理逻辑**：\n    1.  **成功-失败对比**：如果存在成功和失败轨迹，对比它们的FSM路径，归纳出解释结果差异的高置信度元认知 \\(m_k\\)。\n    2.  **仅失败时的推测**：如果只有失败轨迹，则检索top-K个语义相似的查询 \\(\\mathrm{Sim}(q_i, q_j) = \\cos(\\mathbf{e}_{q_i}, \\mathbf{e}_{q_j})\\)，从这些邻居查询的成功路径中推导推测性元认知：\\(\\mathcal{M}^{\\mathrm{spec}}(q_i) = \\bigcup_{q_j \\in \\operatorname{TopK}(q_i)} \\{m_k | t_j \\in \\operatorname{SuccessPaths}(q_j), m_k \\in \\mathcal{M}(t_j)\\}\\)。\n-   **输出**：元认知节点 \\(m_k\\)，编码高层策略原则。\n-   **设计理由**：通过对比成功与失败，或借鉴相似任务的成功经验，自动提炼出可解释的、因果性的策略知识，而非简单地记忆原始轨迹。这提升了记忆的泛化性和可解释性。\n\n#### 模块三：基于强化学习的图权重优化（Reinforcement-Driven Weight Optimization）\n-   **模块名**：Reinforcement-Driven Weight Optimization\n-   **输入**：新查询 \\(q_{\\mathrm{new}}\\)，历史记忆图 \\(\\mathcal{G}\\) 及其当前边权重 \\(w_{qt}, w_{tm}\\)。\n-   **核心处理逻辑**：\n    1.  **子图激活与采样**：根据查询相似度激活子图，计算每个元认知节点 \\(m_k\\) 的相关性分数：\\(\\rho(m_k | q_{\\mathrm{new}}) = \\sum_{q_i, t_j: q_i \\rightarrow t_j \\rightarrow m_k} \\operatorname{Sim}(q_{\\mathrm{new}}, q_i) \\cdot w_{qt}^{(i, j)} \\cdot w_{tm}^{(j, k)}\\)。\n    2.  **效用估计**：采样 \\(m_k\\)，执行两次轨迹：一次以 \\(m_k\\) 为指导，获得奖励 \\(R_{\\mathrm{with}}(m_k)\\)；一次无指导，获得奖励 \\(R_{\\mathrm{w/o}}\\)。计算奖励差距 \\(\\Delta R_k = R_{\\mathrm{with}}(m_k) - R_{\\mathrm{w/o}}\\)。\n    3.  **权重更新**：使用REINFORCE算法，损失函数为 \\(\\mathcal{L}_{\\mathrm{RL}} = - \\mathbb{E}_{m_k \\sim p} [\\Delta R_k \\cdot \\log p(m_k | q_{\\text{new}})]\\)，其中 \\(p(m_k | q_{\\mathrm{new}}) \\propto \\exp(\\rho(m_k | q_{\\mathrm{new}}))\\)。正 \\(\\Delta R_k\\) 增加相关路径权重，负值则降低。\n-   **输出**：更新后的边权重矩阵 \\(W^{xy}\\)。\n-   **设计理由**：使记忆图具备适应性，能够根据下游任务的实际效用（奖励反馈）动态评估和调整其内部存储策略的重要性。这解决了静态记忆无法区分策略优劣的问题，确保检索时优先提供高价值指导。\n\n**§3 关键公式与算法（如有）**\n1.  **异构图信息传播公式**：\n    \\[\n    \\mathbf{H}_{\\mathcal{T}}^{(1)} = \\sigma\\Big((A_{qt} \\odot W_{qt})^{\\top} \\mathbf{H}_{\\mathcal{Q}}^{(0)}\\Big), \\quad \\mathbf{H}_{\\mathcal{M}}^{(2)} = \\sigma\\Big((A_{tm} \\odot W_{tm})^{\\top} \\mathbf{H}_{\\mathcal{T}}^{(1)}\\Big)\n    \\]\n    其中 \\(\\odot\\) 表示逐元素相乘，\\(\\sigma\\) 是激活函数。\n2.  **元认知选择概率（Softmax over Relevance Score）**：\n    \\[\n    p(m_k | q_{\\mathrm{new}}) \\propto \\exp(\\rho(m_k | q_{\\mathrm{new}}))\n    \\]\n3.  **图权重优化的强化学习损失函数**：\n    \\[\n    \\mathcal{L}_{\\mathrm{RL}} = - \\mathbb{E}_{m_k \\sim p} \\left[ \\Delta R_k \\cdot \\log p \\left(m_k | q_{\\text{new}}\\right) \\right]\n    \\]\n4.  **记忆引导的策略优化损失函数（采用GRPO）**：\n    \\[\n    \\mathcal{L}_{\\mathrm{GRPO}} = - \\mathbb{E}_{t} \\left[ \\min \\left(\\frac{\\pi_{\\theta}(a_t | \\tilde{q}_{\\text{train}})}{\\pi_{\\theta_{\\text{old}}}(a_t | \\tilde{q}_{\\text{train}})} \\hat{A}_t, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}(a_t | \\tilde{q}_{\\text{train}})}{\\pi_{\\theta_{\\text{old}}}(a_t | \\tilde{q}_{\\text{train}})}, 1 - \\epsilon, 1 + \\epsilon\\right) \\hat{A}_t\\right) \\right]\n    \\]\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n原文未提出多个命名变体（如Base/Pro），但通过消融实验验证了关键组件，可视为以下变体：\n1.  **完整方法（Ours）**：包含所有三个阶段：图构建、权重优化、记忆引导的RL训练。\n2.  **禁用权重优化变体（Ours w/o Weight Update）**：在阶段二冻结记忆图权重，所有边保持均匀权重，检索策略仅基于结构存在性（即均匀权重下的相关性分数）。\n3.  **不同元认知数量k的变体**：在阶段三，检索并注入提示的元认知策略数量 \\(k\\) 可变（如k=0, 1, 3, 5）。\n4.  **不同LLM后端变体**：在阶段一的记忆构建和后续推理中，使用不同的LLM API（如原文使用GPT-4o，变体使用Gemini-2.5-pro）。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上的本质区别如下：\n1.  **与Expel (Zhao et al., 2024) 的差异**：\n    -   **Expel**：提取可重用的**原始推理轨迹**作为记忆，通过向量相似度检索并直接注入提示。其记忆是**静态的、扁平的**，缺乏对策略效用的评估和结构化抽象。\n    -   **本文方法**：将轨迹抽象为**有限状态机（FSM）中的规范路径**，并进一步提炼为**高层元认知策略**。记忆是**层次化、结构化的图**，并且通过强化学习**动态优化**图中边的权重以反映策略效用。\n2.  **与A-MEM (Xu et al., 2025) 的差异**：\n    -   **A-MEM**：构建动态演化的记忆笔记（Memory Notes）图，节点是记忆笔记，边表示时间或语义关系。其更新主要基于新输入的**同化（Assimilation）**，缺乏基于任务性能反馈的**显式效用校准**机制。\n    -   **本文方法**：图结构是严格的三层异构设计（查询-路径-元认知），并引入了**基于奖励差距的强化学习权重优化**，使记忆能够根据其指导任务的实际效果进行自我精炼，强调策略的**实证有效性**。\n3.  **与隐式记忆/标准RL（如Search-R1）的差异**：\n    -   **标准RL**：经验通过策略梯度**隐式地**编码到模型参数中，是黑盒的，易遗忘，且难以解释特定经验的作用。\n    -   **本文方法**：经验被**显式地、结构化地**存储在外部图记忆中，并作为**显式策略先验**通过提示注入训练循环。既保持了可解释性，又通过权重优化和训练循环整合获得了适应性。本质上是**显式记忆与隐式策略学习的协同优化**。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n**整体流程分为离线构建、在线优化与训练三个阶段，但论文未提供完整的单算法框。以下根据论文描述整合出核心步骤：**\n**阶段一：分层记忆图构建（离线/在线）**\nStep 1: 收集智能体在任务上执行产生的轨迹集合 \\(\\{\\tau_i\\}\\)，包含成功与失败轨迹。\nStep 2: 对于每条轨迹 \\(\\tau_i\\)，利用预定义的有限状态机（FSM）将其映射为规范路径 \\(t_j\\)。\nStep 3: 对于每个查询 \\(q_i\\)，分析其相关轨迹：\n    - 若同时存在成功轨迹 \\(\\tau_s\\) 和失败轨迹 \\(\\tau_f\\)，对比它们的FSM路径，归纳出元认知 \\(m_k\\)，并在图中创建边 \\(q_i \\rightarrow t_j \\rightarrow m_k\\)。\n    - 若只有失败轨迹，则计算 \\(q_i\\) 与图中所有 \\(q\\) 的语义相似度，检索Top-K个相似的成功查询，从它们的成功路径关联的元认知中推导出推测性元认知，并建立连接。\nStep 4: 初始化邻接矩阵 \\(A^{q\\rightarrow t}, A^{t\\rightarrow m}\\) 和对应的可学习权重矩阵 \\(W^{qt}, W^{tm}\\)（初始值可能为均匀分布或基于某种启发式）。\n\n**阶段二：可训练图权重优化（在线）**\nStep 5: 对于新到达的查询 \\(q_{\\text{new}}\\)，计算其与图中所有查询节点 \\(q_i\\) 的相似度 \\(\\operatorname{Sim}(q_{\\text{new}}, q_i)\\)。\nStep 6: 根据相似度激活一个子图 \\(\\mathcal{G}(q_{\\text{new}}) = (\\mathcal{Q}', \\mathcal{T}', \\mathcal{M}')\\)，通常包含top-k个相似查询及其关联的路径和元认知。\nStep 7: 对于子图中的每个元认知节点 \\(m_k \\in \\mathcal{M}'\\)，计算其相关性分数：\n\\(\\rho(m_k | q_{\\text{new}}) = \\sum_{q_i, t_j: q_i \\rightarrow t_j \\rightarrow m_k} \\operatorname{Sim}(q_{\\text{new}}, q_i) \\cdot w_{qt}^{(i, j)} \\cdot w_{tm}^{(j, k)}\\)。\nStep 8: 根据softmax概率 \\(p(m_k | q_{\\text{new}}) \\propto \\exp(\\rho(m_k | q_{\\text{new}}))\\) 采样一个元认知 \\(m_k\\)。\nStep 9: 执行两次智能体推理：\n    - 轨迹A：以 \\(m_k\\) 作为指导提示的一部分。\n    - 轨迹B：无 \\(m_k\\) 指导。\n    分别获得奖励 \\(R_{\\text{with}}(m_k)\\) 和 \\(R_{\\text{w/o}}\\)。\nStep 10: 计算奖励差距 \\(\\Delta R_k = R_{\\text{with}}(m_k) - R_{\\text{w/o}}\\)。\nStep 11: 使用REINFORCE算法更新权重：计算损失 \\(\\mathcal{L}_{\\mathrm{RL}} = -\\Delta R_k \\cdot \\log p(m_k | q_{\\text{new}})\\)，并反向传播更新涉及 \\(m_k\\) 的所有路径上的边权重 \\(w_{qt}, w_{tm}\\)。\n\n**阶段三：记忆引导的策略优化（训练循环中）**\nStep 12: 对于训练批次中的每个查询 \\(q_{\\text{train}}\\)，重复Step 5-7，计算所有元认知的相关性分数。\nStep 13: 选择top-k个（如k=3）分数最高的元认知 \\(\\{m_1, ..., m_k\\}\\)。\nStep 14: 将 \\(\\{m_1, ..., m_k\\}\\) 文本化，预置到 \\(q_{\\text{train}}\\) 前，形成增强提示 \\(\\tilde{q}_{\\text{train}}\\)。\nStep 15: 将 \\(\\tilde{q}_{\\text{train}}\\) 输入策略网络 \\(\\pi_\\theta\\)，采样动作，与环境/工具交互，获得轨迹和奖励。\nStep 16: 使用广义强化策略优化（GRPO）算法，基于优势估计 \\(\\hat{A}_t\\) 和裁剪参数 \\(\\epsilon\\)，计算损失 \\(\\mathcal{L}_{\\mathrm{GRPO}}\\)，更新策略参数 \\(\\theta\\)。\nStep 17: 将新产生的轨迹（成功/失败）作为新的经验，返回Step 2，用于更新记忆图的结构和节点（元认知更新）。\n\n**§2 关键超参数与配置**\n1.  **最大工具调用次数（K）**：在智能体交互协议中，设置为6次。理由：平衡任务完成可能性与计算开销，防止无限循环。\n2.  **检索相似查询的Top-K值（用于元认知推测）**：论文未明确给出，记为 \\(K_{\\text{spec}}\\)。理由：在只有失败轨迹时，需要从相似成功查询中借鉴经验。\n3.  **注入提示的元认知数量（k）**：消融实验表明最佳值为3。理由：k=0到3时性能稳步提升，k>3时引入噪声导致收益递减，k=3在策略多样性和提示清晰度间取得最佳平衡。\n4.  **GRPO算法的裁剪参数（\\(\\epsilon\\)）**：论文未明确给出具体数值，是PPO类算法的标准超参数，通常设为0.1或0.2。理由：限制策略更新步长，保证训练稳定性。\n5.  **FSM的状态和转移定义**：具体规范在附录B.1中，但正文未提供，是关键的结构化抽象超参数。\n6.  **奖励差距计算中的基线**：无指导的奖励 \\(R_{\\text{w/o}}\\)，作为评估元认知边际贡献的基准。\n\n**§3 训练/微调设置（如有）**\n1.  **训练数据构造**：使用七个QA数据集（NQ, TriviaQA, PopQA, HotpotQA, 2wiki, Musique, Bamboogle）中的查询。**关键点**：记忆图仅使用**HotpotQA（单领域）**的数据构建，但在所有七个数据集上评估泛化能力。\n2.  **优化器与学习率**：论文未明确说明策略网络和记忆图权重的具体优化器（如Adam）和学习率。\n3.  **批次大小与训练轮数**：论文未提供具体数值。从训练曲线图（Figure 3）推断，训练进行了约10-15个epoch（或等效的步数）。\n4.  **策略网络底座**：使用Qwen3-4B和Qwen3-8B作为基础模型，并进行强化学习训练。\n5.  **奖励设计**：基于任务准确性（如QA的EM或F1分数）设计奖励信号，具体细节未在正文展开。\n\n**§4 推理阶段的工程细节**\n1.  **检索后端**：使用**E5检索器**从2018年维基百科转储中检索文档。检索过程被封装为一个模块化的MCP工具，供LLM智能体调用。\n2.  **交互协议**：智能体遵循结构化协议：在` 和` 标签间进行推理规划；工具调用命令嵌入在 `<tool call>` 和 `</tool call>` 之间；工具输出在 `<tool response>` 和 `</tool response>` 之间；最终答案在 `<answer>` 和 `</answer>` 之间。\n3.  **记忆检索**：在推理时，对于新查询，会从训练好的记忆图中检索top-k个元认知策略，并注入提示。检索基于优化后的边权重和查询相似度。\n4.  **并行化与缓存**：论文未提及具体的并行化策略或缓存机制。\n5.  **向量数据库**：虽然使用了E5进行文档检索，但记忆图本身的存储和检索是自定义的图结构，可能使用内存数据结构或图数据库实现，论文未说明具体选型。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n1.  **Natural Questions (NQ)***：\n    -   **规模**：原文未提供具体样本数，是广泛使用的开放域事实型问答基准。\n    -   **领域类型**：开放域，事实型问答。\n    -   **评测问题类型**：单跳/直接事实检索。\n    -   **特殊处理**：标记为星号(*)，表示**领域外（Out-of-Domain）**数据集，即未用于构建记忆图。\n2.  **TriviaQA***：\n    -   **规模**：原文未提供具体样本数，大型远程监督阅读理解数据集。\n    -   **领域类型**：开放域，琐事问答。\n    -   **评测问题类型**：单跳事实检索。\n    -   **特殊处理**：标记为星号(*)，**领域外**数据集。\n3.  **PopQA***：\n    -   **规模**：原文未提供具体样本数，包含流行问题的开放域QA数据集。\n    -   **领域类型**：开放域，流行知识问答。\n    -   **评测问题类型**：单跳事实检索。\n    -   **特殊处理**：标记为星号(*)，**领域外**数据集。\n4.  **HotpotQA†**：\n    -   **规模**：原文未提供具体样本数，需要多文档推理的QA数据集。\n    -   **领域类型**：多跳推理，需要结合多个文档信息。\n    -   **评测问题类型**：多跳问答。\n    -   **特殊处理**：标记为匕首(†)，表示**领域内（In-Domain）**数据集，是**唯一用于构建记忆图**的数据源。\n5.  **2WikiMultiHopQA (2wiki)***：\n    -   **规模**：原文未提供具体样本数。\n    -   **领域类型**：多跳推理，基于维基百科。\n    -   **评测问题类型**：多跳问答。\n    -   **特殊处理**：标记为星号(*)，**领域外**数据集。\n6.  **Musique***：\n    -   **规模**：原文未提供具体样本数，通过组合单跳问题构成的多跳QA数据集。\n    -   **领域类型**：多跳推理。\n    -   **评测问题类型**：多跳问答。\n    -   **特殊处理**：标记为星号(*)，**领域外**数据集。\n7.  **Bamboogle***：\n    -   **规模**：原文未提供具体样本数，用于测量语言模型组合性差距的数据集。\n    -   **领域类型**：组合性推理。\n    -   **评测问题类型**：多跳/组合性问答。\n    -   **特殊处理**：标记为星号(*)，**领域外**数据集。\n\n**§2 评估指标体系（全量列出）**\n-   **准确性指标**：\n    -   **平均准确率（Avg.）**：在七个数据集上计算的平均性能指标（具体是F1还是Exact Match未明确说明，表格中数值在0-1之间，推测为准确率或F1）。这是主报告指标。\n    -   **各数据集单独准确率**：报告每个数据集上的具体数值（见表1, 2）。\n-   **效率/部署指标**：\n    -   **训练收敛速度**：通过训练曲线图（Figure 3）定性展示，本文方法（Ours）相比基线（如Search-R1）收敛更快（曲线更早达到平台期）。\n    -   **延迟、Token消耗、显存占用**：**原文未提供任何定量数据**。\n-   **其他自定义指标**：\n    -   **相对改进百分比（vs. ITR/Search-R1）**：计算本文方法相对于指定基线的平均性能提升百分比，用于衡量改进幅度。\n    -   **跨领域泛化能力**：通过仅在HotpotQA上构建记忆，但在所有七个数据集（包括六个领域外数据集）上测试来定性评估。\n\n**§3 对比基线（完整枚举）**\n**A. 直接推理（Direct Inference）基线（表1）：**\n1.  **ITR (Tool-Integrated Reasoning)**：使用集成搜索工具进行文档检索的推理方法（Chai et al., 2025）。作为主要比较基准。**类型**：工具增强的提示工程。**底座模型**：与本文相同（Qwen3-4B/8B）。**代表性**：代表了不使用外部记忆、仅依靠工具调用的标准推理能力。\n2.  **Direct Inference**：直接生成答案，无特殊推理或工具调用提示。**类型**：纯生成。**底座模型**：相同。**代表性**：模型的基础能力下限。\n3.  **Chain-of-Thought (CoT)**：使用思维链提示进行内部推理（Wei et al., 2022）。**类型**：提示工程。**底座模型**：相同。**代表性**：经典的复杂推理提示方法。\n4.  **Direct Trajectory**：使用过去的**原始轨迹**直接作为记忆注入提示。**类型**：显式记忆（原始轨迹）。**底座模型**：相同。**代表性**：最朴素的基于经验的记忆方法，对比突出本文结构化抽象的价值。\n5.  **A-MEM**：动态记忆图方法（Xu et al., 2025）。**类型**：显式记忆（动态图）。**底座模型**：相同。**代表性**：最新的动态图记忆工作，对比突出本文权重优化机制的价值。\n6.  **EXPEL**：提取可重用推理轨迹作为记忆的方法（Zhao et al., 2024）。**类型**：显式记忆（轨迹提取）。**底座模型**：相同。**代表性**：基于经验提炼记忆的代表性工作，对比突出本文层次化结构和效用学习的价值。\n\n**B. 强化学习（RL）训练基线（表2）：**\n1.  **Search-R1**：纯强化学习框架，训练LLM进行交替推理和搜索，无记忆支持（Jin et al., 2025）。作为RL训练的主要比较基准。**类型**：RL微调方法。**底座模型**：相同。**代表性**：先进的、无记忆的RL训练框架。\n2.  **Direct Trajectory (RL)**：在RL训练循环中注入原始轨迹作为记忆。\n3.  **A-MEM (RL)**：在RL训练循环中集成A-MEM记忆。\n4.  **EXPEL (RL)**：在RL训练循环中集成EXPEL记忆。\n（后三者用于评估不同记忆设计对RL训练的影响）。\n\n**§4 实验控制变量与消融设计**\n1.  **记忆图构建数据控制**：**关键控制变量**：所有方法（包括基线）在相同的数据集上训练和评估。但本文方法的记忆图**仅使用HotpotQA（单个领域内数据集）构建**，以此测试其跨领域泛化能力。这是实验设计的核心控制之一。\n2.  **消融实验设计**：\n    -   **组件消融**：**禁用权重优化**。保持图结构不变，但冻结边权重（设为均匀），检索策略仅基于结构存在性。用于验证权重优化机制的必要性。\n    -   **超参数消融**：**改变注入的元认知数量k**。测试k=0, 1, 3, 5等不同值对平均性能的影响，以确定最佳k值。\n    -   **系统组件消融**：**改变记忆组成粒度**。论文提到“altering the granularity of memory composition (i.e., API call structure)”，但未在正文展示具体结果，可能指对比原始轨迹记忆（Direct Trajectory）和本文的抽象路径记忆。\n3.  **模型无关性测试**：将记忆构建和推理中使用的LLM API从OpenAI GPT-4o替换为Gemini-2.5-pro，使用相同的记忆图进行下游评估，测试方法对底层模型的依赖性。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n**表1：直接推理（Direct Inference）性能对比（指标为准确率，数值越高越好）**\n`方法名 | Avg. (vs. ITR) | NQ* | TriviaQA* | PopQA* | HotpotQA† | 2wiki* | Musique* | Bamboogle*`\n`--- | --- | --- | --- | --- | --- | --- | --- | ---`\n`Qwen3-8B ITR | 0.334 (-) | 0.275 | 0.593 | 0.358 | 0.325 | 0.324 | 0.094 | 0.365`\n`Qwen3-8B Direct Inference | 0.269 (↓19.5%) | 0.200 | 0.519 | 0.191 | 0.230 | 0.275 | 0.058 | 0.410`\n`Qwen3-8B CoT | 0.252 (↓24.6%) | 0.209 | 0.512 | 0.182 | 0.223 | 0.271 | 0.055 | 0.308`\n`Qwen3-8B Direct Trajectory | 0.352 (↑5.4%) | 0.317 | 0.604 | 0.380 | 0.329 | 0.363 | 0.105 | 0.364`\n`Qwen3-8B A-MEM | 0.334 (0.0%) | 0.286 | 0.590 | 0.366 | 0.339 | 0.332 | 0.112 | 0.313`\n`Qwen3-8B EXPEL | 0.329 (↓1.5%) | 0.306 | 0.594 | 0.379 | 0.317 | 0.327 | 0.092 | 0.287`\n`Qwen3-8B Ours | 0.365 (↑9.3%) | 0.316 | 0.622 | 0.382 | 0.358 | 0.354 | 0.128 | 0.392`\n`Qwen3-4B ITR | 0.279 (-) | 0.298 | 0.581 | 0.157 | 0.268 | 0.281 | 0.077 | 0.290`\n`Qwen3-4B Direct Inference | 0.211 (↓24.4%) | 0.158 | 0.413 | 0.157 | 0.183 | 0.240 | 0.033 | 0.290`\n`Qwen3-4B CoT | 0.181 (↓35.1%) | 0.149 | 0.375 | 0.146 | 0.156 | 0.190 | 0.022 | 0.228`\n`Qwen3-4B Direct Trajectory | 0.325 (↑16.5%) | 0.310 | 0.558 | 0.379 | 0.282 | 0.344 | 0.076 | 0.327`\n`Qwen3-4B A-MEM | 0.319 (↑14.3%) | 0.310 | 0.586 | 0.381 | 0.272 | 0.269 | 0.091 | 0.325`\n`Qwen3-4B EXPEL | 0.321 (↑15.1%) | 0.312 | 0.570 | 0.388 | 0.294 | 0.347 | 0.075 | 0.263`\n`Qwen3-4B Ours | 0.351 (↑25.8%) | 0.335 | 0.596 | 0.393 | 0.299 | 0.347 | 0.099 | 0.391`\n\n**表2：强化学习（RL）训练后性能对比（指标为准确率）**\n`方法名 | Avg. (vs. Search-R1) | NQ* | TriviaQA* | PopQA* | HotpotQA† | 2wiki* | Musique* | Bamboogle*`\n`--- | --- | --- | --- | --- | --- | --- | --- | ---`\n`Qwen3-8B Search-R1 | 0.395 (-) | 0.384 | 0.651 | 0.429 | 0.391 | 0.386 | 0.143 | 0.380`\n`Qwen3-8B Direct Trajectory (RL) | 0.400 (↑1.27%) | 0.406 | 0.657 | 0.433 | 0.376 | 0.367 | 0.139 | 0.423`\n`Qwen3-8B A-MEM (RL) | 0.403 (↑2.03%) | 0.398 | 0.656 | 0.436 | 0.389 | 0.409 | 0.138 | 0.398`\n`Qwen3-8B EXPEL (RL) | 0.371 (↓6.08%) | 0.362 | 0.621 | 0.407 | 0.354 | 0.375 | 0.121 | 0.357`\n`Qwen3-8B Ours (RL) | 0.408 (↑3.29%) | 0.386 | 0.662 | 0.434 | 0.387 | 0.403 | 0.152 | 0.435`\n`Qwen3-4B Search-R1 | 0.375 (-) | 0.357 | 0.625 | 0.426 | 0.354 | 0.402 | 0.115 | 0.348`\n`Qwen3-4B Direct Trajectory (RL) | 0.415 (↑10.67%) | 0.403 | 0.624 | 0.434 | 0.420 | 0.428 | 0.186 | 0.412`\n`Qwen3-4B A-MEM (RL) | 0.388 (↑3.47%) | 0.393 | 0.603 | 0.439 | 0.385 | 0.322 | 0.157 | 0.418`\n`Qwen3-4B EXPEL (RL) | 0.337 (↓10.13%) | 0.322 | 0.577 | 0.399 | 0.311 | 0.363 | 0.081 | 0.305`\n`Qwen3-4B Ours (RL) | 0.426 (↑13.60%) | 0.408 | 0.646 | 0.462 | 0.410 | 0.407 | 0.189 | 0.463`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n**直接推理场景（表1）**：\n-   **总体趋势**：本文方法（Ours）在Qwen3-8B和4B模型上均取得了最佳平均性能（0.365和0.351），相对ITR基线分别提升9.3%和25.8%。**对小模型（4B）的提升尤为显著**，说明本文的结构化记忆有效弥补了小模型内在的推理能力不足。\n-   **领域内 vs. 领域外**：记忆图仅用HotpotQA构建，但本文方法在HotpotQA（领域内）上表现优异（8B: 0.358 vs. ITR 0.325；4B: 0.299 vs. ITR 0.268）。**更重要的是，在全部六个领域外数据集上，本文方法也取得了最优或极具竞争力的性能**，例如在TriviaQA（8B: 0.622）、Bamboogle（8B: 0.392；4B: 0.391）上表现突出。这强有力地证明了从HotpotQA学习到的推理结构具有**卓越的跨领域泛化能力**。\n-   **对比其他记忆方法**：Direct Trajectory（原始轨迹记忆）在部分数据集（如PopQA）上也有不错表现，但不如本文方法稳定。A-MEM和EXPEL在8B模型上甚至未能超越ITR基线，说明静态或非效用加权的记忆在强大底座上可能带来噪声或收益有限。本文方法通过权重优化克服了这一问题。\n\n**强化学习训练场景（表2）**：\n-   **总体趋势**：集成本文记忆的RL训练在两种规模的模型上均取得了最佳平均性能（8B: 0.408, +3.29%；4B: 0.426, +13.60%）。**训练后的Qwen3-4B模型（0.426）甚至超越了未使用记忆的基线Qwen3-8B模型（0.395）**，展示了“小模型+优质记忆”超越“大模型”的潜力，对效率提升意义重大。\n-   **收敛速度**：从训练曲线图（Figure 3）可见，本文方法（Ours）相比Search-R1基线收敛更快，在训练早期就达到更高性能平台，说明记忆提供的策略先验有效**引导和加速了策略学习**。\n-   **不同记忆对RL的影响**：EXPEL记忆在RL训练中表现不佳（8B: -6.08%, 4B: -10.13%），可能其提取的策略与RL目标不兼容或引入冲突。Direct Trajectory和A-MEM在4B模型上带来显著提升（+10.67%, +3.47%），但本文方法提升最大（+13.60%），进一步证明了可训练权重和层次化结构在RL环境中的优势。\n\n**§3 效率与开销的定量对比**\n**原文未提供任何关于延迟（ms）、Token消耗量、显存占用（GB）或API调用次数的具体定量数据。** 仅从训练曲线（Figure 3）定性推断本文方法能加速RL训练收敛，可能间接减少达到特定性能所需的训练步数，从而节省计算开销。\n\n**§4 消融实验结果详解**\n1.  **禁用权重优化**：如图4(a)(b)所示，冻结记忆图权重（均匀权重）导致性能显著下降，尤其在2WikiMultiHopQA数据集上。**具体数值未在正文给出，但从图中曲线位置可清晰看出性能低于完整方法**。这验证了基于强化学习的权重优化机制对于区分高效用策略、实现有效策略重用的**关键作用**。\n2.  **改变注入的元认知数量k**：如图4(c)所示，对于4B模型在七个基准上的平均准确率，k从0增加到3时性能稳步提升，在k=3时达到峰值。继续增加k（如到5）则性能持平或略有下降，呈现收益递减。**结论**：k=3在提供足够策略指导和避免提示噪声之间达到最佳平衡。\n3.  **改变LLM后端**：如表4（在论文中提及但未在提供的文本中复制）所示，将记忆构建和推理的LLM从GPT-4o替换为Gemini-2.5-pro后，本文的记忆增强方法**仍然一致地优于其非记忆对应版本**，尽管绝对数值因模型能力差异而略有不同。这证明了方法的**模型无关性（Model-Agnostic）**。\n\n**§5 案例分析/定性分析（如有）**\n原文在附录E.3和案例研究（Case Study）中提供了具体示例，但提供的文本片段仅包含一个不完整的工具调用示例（Case 1）。从该片段和论文描述可知：\n-   **成功案例**：智能体在回答关于美国历史政治主导州的问题时，首先进行内部推理，意识到需要外部信息（关于州议员George D. Maziarz），然后自主调用搜索工具（`search-query_rag`）获取信息，最终得出答案“New York”。这展示了智能体在记忆（可能包含“当遇到具体人物信息缺失时，应调用搜索工具”）指导下，进行规划、工具调用和推理的完整流程。\n-   **失败案例与元认知归纳**：论文提到通过对比成功和失败轨迹的FSM路径来归纳元认知。例如，失败轨迹可能显示智能体在“信息分析”状态后错误地跳转到“最终答案”状态，而成功轨迹则显示在“信息分析”后应进入“工具调用”状态。由此归纳出的元认知可能是：“在信息不足时，不应过早下结论，而应寻求外部工具验证”。这种对比提炼出了可重用的策略原则。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出了一个以智能体为中心、可训练的多层图记忆框架**：该框架将低层智能体轨迹抽象为有限状态机（FSM）上的规范路径，并进一步提炼为高层、可泛化的元认知策略。这一贡献**实现了从原始经验到结构化策略知识的转化**，为经验复用提供了可解释的载体。\n2.  **设计了一种基于强化学习的权重优化机制**：该机制根据下游任务奖励反馈动态校准记忆连接的效用，使记忆图能够选择性强调经验证有效的高价值策略。这一贡献**解决了静态记忆缺乏适应性的问题**，通过实验验证，禁用该机制会导致性能显著下降（如图4）。\n3.  **将结构化记忆作为显式策略先验整合到RL训练循环中**：通过检索top-k元认知策略并注入提示来引导策略优化，实现了显式记忆与隐式策略学习的协同进化。这一贡献**不仅提升了推理准确率（如4B模型推理平均提升25.8%）**，还**加速了RL训练的收敛速度**（如图3），并使得训练后的小模型性能可超越无记忆的大模型基线（4B的0.426 vs 8B基线的0.395）。\n\n**§2 局限性（作者自述）**\n**原文在结论部分未明确列出作者自述的局限性。** 通常此类论文的局限性可能包括：实验仅在问答（QA）任务上进行验证；记忆图构建依赖于特定定义的有限状态机（FSM），其普适性需要进一步检验；未对超大规模记忆库（例如百万级节点）下的可扩展性进行测试；未详细分析计算开销和延迟。但需注意，这些是基于领域常识的推测，并非原文直接陈述。\n\n**§3 未来研究方向（全量提取）**\n**原文在结论部分未明确列出未来工作方向。** 根据论文结尾的总结性陈述，可以推断出一些潜在方向：\n1.  **探索更复杂的图结构与记忆更新机制**：当前是三层的异构图，未来可以研究更深的层次或更复杂的节点/边类型，以捕获更丰富的经验模式。同时，元认知的更新和淘汰机制可以进一步优化，以保持记忆库的紧凑和高效。\n2.  **将框架扩展到更广泛的任务和领域**：目前主要在工具增强的QA任务上验证。未来可以应用于更复杂的自主智能体场景，如长期对话、机器人任务规划、代码生成等，测试其通用性。\n3.  **研究记忆与不同训练范式的更深度集成**：除了强化学习（RL），可以探索如何将本记忆框架与监督微调（SFT）、从人类反馈中强化学习（RLHF）等其他训练范式结合，以进一步提升智能体的能力和对齐性。\n4.  **进行大规模、真实世界的部署评估**：在拥有海量交互数据的生产环境中测试该框架的可扩展性、鲁棒性和长期学习效果，评估其在实际应用中的价值。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **理论新颖性**：提出并实现了**“可训练的显式图记忆”** 这一新范式，弥合了隐式记忆（适应性强但不可解释）与显式记忆（可解释但缺乏适应性）之间的鸿沟。其核心创新在于将强化学习信号用于优化外部记忆结构的内部权重，使记忆本身成为一个可学习的、效用感知的组件。\n2.  **实验验证充分性**：在七个多样的QA数据集上进行了全面实验，涵盖直接推理和RL训练两种场景，并使用Qwen3-4B/8B两个模型规模。实验设计巧妙（仅用单领域数据构建记忆，测试跨领域泛化），结果显著（小模型推理提升25.8%，训练后小模型超越无记忆大模型），并通过消融实验扎实地验证了每个核心组件（权重优化、元认知数量k）的必要性。\n3.  **对领域的影响**：为LLM智能体的长期学习和经验复用提供了一个强有力的新工具。其“记忆即策略先验”的思想和可训练的设计，可能启发后续工作开发更复杂、更自适应的记忆系统，推动构建真正能够从历史中持续学习和进化的智能体。\n\n**§2 工程与实践贡献**\n1.  **系统设计贡献**：提供了一个完整的三阶段框架（构建-优化-训练）的详细蓝图，包括具体的图结构定义、信息传播公式、权重更新算法和与RL训练循环的集成方式，具有较高的工程参考价值。\n2.  **评测基准**：虽然没有发布新的数据集，但提供了一套在多个现有QA基准上评估记忆增强型智能体泛化能力的实验方案，",
    "source_file": "From Experience to Strategy Empowering LLM Agents with Trainable Graph Memory.md"
}