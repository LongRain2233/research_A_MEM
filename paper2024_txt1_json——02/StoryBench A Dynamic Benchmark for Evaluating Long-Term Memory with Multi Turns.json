{
    "title": "StoryBench: A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n随着大型语言模型（LLM）在**多轮对话**、**任务规划**和**终身学习**等复杂、动态场景中的应用日益深入，模型需要具备**长期记忆**能力来维持上下文连贯性、整合历史知识并进行时序推理。然而，现有的评测基准大多基于静态的、单轮的任务，无法充分模拟现实世界中信息需要被**长期保留、动态更新并用于多步推理**的挑战。因此，在**交互式叙事**（如文字冒险游戏）这一具体场景下，构建一个能够系统评估LLM长期记忆能力的动态基准，对于推动模型向自主智能演进至关重要。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有基准在评估长期记忆的两个关键维度上存在明显短板：\n1.  **知识保留能力评估不足**：现有基准如**NeedleInAHaystack**和**LongBench**，主要评估模型在长上下文中**定位孤立事实**的能力。**当输入是跨越数十轮对话、信息点分散且相互关联的叙事文本时**，这些基准无法评估模型整合和维持**叙事连贯性**（如角色动机、事件因果关系）的能力，导致模型可能出现**上下文矛盾**的失败模式。\n2.  **时序推理能力评估缺失**：现有基准如**RULER**和**LTM Benchmark**虽然包含推理任务，但任务通常是**静态且非交互的**。**当输入是一个动态分支的叙事，其中早期选择会触发后续一系列连锁依赖时**，这些基准无法评估模型进行**多步因果推理**和**回溯纠错**的能力。模型可能表现出**短视行为**，仅能回溯1-2步，而无法修复由多个早期错误共同导致的长期失败结局。\n3.  **灵活性不足**：许多基准如**BABILong**和**LooGLE**的任务具有**单一固定解**。**当输入允许存在多个有效路径（多解分支）时**，这些基准无法评估模型在不确定性环境中的**适应性决策**能力，限制了其在真实、开放场景中的应用价值。\n\n**§3 问题的根本难点与挑战（200字以上）**\n构建有效的长期记忆评估基准面临以下根本挑战：\n- **动态性与连续性难以兼顾**：真实世界的决策是**序列化**且**上下文依赖**的。设计一个基准，既要模拟这种动态分支（每个选择影响未来），又要维持整个故事的语义连续性，在工程和逻辑上非常复杂。\n- **评估指标的定义与量化困难**：长期记忆不仅是“记住”，更是“**在正确的时间运用正确的记忆**”。如何设计指标来量化模型在**知识保留**（如叙事一致性）和**时序推理**（如因果链追踪）两方面的能力，并区分短期反馈辅助与长期自主推理，是一个核心挑战。\n- **数据污染风险**：使用公开的合成数据或真实世界数据构建基准，存在被LLM预训练数据包含的风险，导致评估结果失真。如何构建一个**新颖、可控且富含复杂依赖关系**的数据集，同时避免数据泄露，是另一个主要难点。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是利用**交互式小说游戏**作为评估框架的基础。其核心假设是：**基于分支叙事的多轮交互游戏，能够自然地封装长期记忆评估所需的关键属性（如长程依赖、连续性、动态性、多解性），从而提供一个比静态问答更全面、更真实的压力测试环境。** 该假设受到**认知科学**中关于叙事理解和情景记忆研究的启发，即人类通过构建和更新**心智模型**来理解和参与复杂故事。本文认为，LLM在类似环境中的表现，可以更准确地反映其长期记忆能力的强弱。此外，本文设计了**双模式评估**（即时反馈与自我恢复），其假设是：**移除即时反馈能更纯粹地暴露模型在长程因果推理和自主错误修正方面的内在局限性**，而这正是现有基准所忽略的。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\nStoryBench 是一个**基于交互式小说游戏的动态多轮评估框架**，其核心架构由**数据集构造模块**和**双模式评估引擎**构成。整体数据流为：**输入**一个预构建的、以有向无环图（DAG）形式组织的叙事数据集 → **评估引擎**根据所选模式（Immediate Feedback 或 Self Recovery）加载故事，并逐步向被测LLM呈现场景节点（Scene Node）和选择节点（Choice Node）→ **LLM** 根据当前及历史上下文做出选择（输出选项ID）→ **评估引擎** 根据预定义的“正确”路径判断选择是否正确，并依据模式决定是否提供反馈或允许故事继续 → **最终输出** 一系列评估指标（如准确率、重试次数等）。该框架的核心是模拟一个**决策序列** \\(\\{c_1, c_2, ..., c_T\\}\\)，其中 \\(c_t \\in \\{0, 1\\}\\) 表示第t步的选择正确与否。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：叙事数据集（Narrative Dataset）\n- **输入**：原始交互式小说游戏《The Invisible Guardian》（截至第5章）的文本内容。\n- **核心处理逻辑**：通过**人工标注**，将游戏内容转化为结构化的JSON格式。标注过程严格保留游戏的分支逻辑和因果关系，并确保时间顺序。数据集被组织成一个**有向无环图**，包含两种节点：**场景节点**（311个）和**选择节点**（86个）。边表示节点之间的转移，形成树状结构。\n- **输出**：一个包含**397个节点**的JSON数据集，每个节点包含唯一标识符、类型、内容（场景描述、对话、选项文本等）以及元数据（如是否为结局节点）。\n- **设计理由**：选择游戏而非合成或真实数据，是为了避免数据污染（合成数据可能过于简单，真实数据难以控制），同时游戏能提供**连贯的叙事、明确的分支和因果依赖**，这是评估长期记忆的理想载体。\n\n#### 模块二：即时反馈模式（Immediate Feedback Mode）\n- **输入**：当前故事状态（历史场景和选择序列）、当前的选择节点及其选项。\n- **核心处理逻辑**：在模型每次做出选择后，**立即判断其正确性**。如果选择错误，引擎会**告知模型结果并提示重试**，直到模型选择正确选项为止。该过程模拟了有即时纠正信号的短期记忆和交互学习场景。\n- **输出**：记录每一步的**首次尝试正确性** \\(f_t\\) 和**总重试次数** \\(r_t\\)，用于计算后续指标。\n- **设计理由**：该模式旨在评估模型在**获得错误信号后的快速调整能力**，反映了模型利用短期反馈进行学习的行为，是许多实际应用场景（如对话系统）的简化模拟。\n\n#### 模块三：自我恢复模式（Self Recovery Mode）\n- **输入**：与即时反馈模式相同。\n- **核心处理逻辑**：当模型做出错误选择时，**不提供任何反馈**，故事会沿着错误路径继续，可能最终导向失败结局。当故事进入失败结局或模型在同一个选择节点上连续犯错达到预设阈值（论文中设为**9次**）时，评估引擎会**要求模型回溯到它认为最早出错的选择点，并尝试从该点恢复**。该过程模拟了无外部指导下的长期因果推理和自主错误修正。\n- **输出**：记录模型是否成功完成故事（Success Count），以及**最大连续正确序列长度**、**错误计数超过阈值**等指标。\n- **设计理由**：该模式旨在**剥离短期反馈的辅助**，更纯粹地评估模型内在的**长期记忆保留**和**多步因果推理**能力，这对于评估模型在开放域、无监督环境中的稳健性至关重要。\n\n**§3 关键公式与算法（如有）**\n论文定义了用于评估的**决策序列** \\(\\{c_1, c_2, ..., c_T\\}\\)，其中 \\(c_t \\in \\{0, 1\\}\\)。关键指标公式如下：\n- **整体准确率**：\\(\\text{Accuracy}_{\\text{overall}} = \\frac{1}{T} \\sum_{t=1}^{T} c_{t}\\)\n- **首次尝试准确率**：令 \\(f_t \\in \\{0, 1\\}\\) 表示第t步首次尝试是否正确，则 \\(\\text{Accuracy}_{\\text{first-try}} = \\frac{1}{T} \\sum_{t=1}^{T} f_{t}\\)\n- **最长连续正确序列**：\\(\\text{Longest Corr} = \\max_{1 \\leq i \\leq j \\leq T} \\left(j - i + 1 \\mid c_k = 1 \\forall k \\in [i, j]\\right)\\)\n- **按难度划分的准确率**：将决策分为简单（Easy）和困难（Hard）两类。设 \\(\\mathcal{E}_t\\) 和 \\(\\mathcal{H}_t\\) 为到第t步为止的简单和困难决策集合，则：\n  \\(\\operatorname{Accuracy}_{\\text{easy}}^{(t)} = \\frac{1}{| \\mathcal{E}_t |} \\sum_{i \\in \\mathcal{E}_t} c_i, \\quad \\operatorname{Accuracy}_{\\text{hard}}^{(t)} = \\frac{1}{| \\mathcal{H}_t |} \\sum_{i \\in \\mathcal{H}_t} c_i\\)\n- **总重试次数**：\\(\\operatorname{Retry}_{\\text{total}} = \\sum_{t=1}^{T} r_t\\)，其中 \\(r_t\\) 是第t步达到正确选择所需的重试次数。\n- **最大错误/选择**：\\(\\operatorname{Max Error} = \\max_{1 \\leq t \\leq T} r_t\\)\n- **阈值错误计数**：\\(\\operatorname{Error Count}_{\\geq r_{\\text{thres}}} = \\sum_{t=1}^{T} \\mathbb{I}(r_t \\geq r_{\\text{thres}})\\)，其中 \\(\\mathbb{I}(\\cdot)\\) 是指示函数，\\(r_{\\mathrm{thres}}\\) 是预设阈值（实验中为9）。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n本文提出了评估框架的**两种核心变体/模式**，而非方法本身的变体：\n1.  **Immediate Feedback (IF) 模式**：作为**Base**版本，在每次错误后提供即时纠正。\n2.  **Self Recovery (SR) 模式**：作为**Enhanced**版本，移除即时反馈，引入**自主回溯纠错**机制，并设置了**软干预阈值**（连续错误9次则提示正确答案）。SR模式又进一步分为两个实验阶段：\n    - **Original Self Recovery**：使用原始未过滤数据集，无单轮token限制，模拟高压环境。\n    - **Improved Self Recovery**：过滤敏感词汇，并限制单轮输入为5000个token，以保障评估流程的稳定性。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上存在本质区别：\n1.  **与 NeedleInAHaystack / LongBench 的区别**：这些基准是**静态的、单轮的检索任务**，评估模型在长文本中**定位孤立事实**的能力。而StoryBench是**动态的、多轮的、交互式的决策任务**，评估模型在**叙事连贯性维护**和**多步因果推理**方面的能力。前者关注“**能否找到**”，后者关注“**能否在正确的叙事节点运用**”。\n2.  **与 LTM Benchmark / Multi-Session Chats 的区别**：这些基准虽然涉及多轮对话，但对话通常是**线性的、目标导向的**，缺乏**分支叙事**和**动态状态变化**。StoryBench通过**有向无环图**模拟了**非线性的故事进展**，其中早期选择会永久改变后续可用的路径和结局，这对模型的**状态追踪**和**长期规划**能力提出了更高要求。\n3.  **与 AgentBench / WebArena 的区别**：这些智能体基准侧重于在**模拟真实环境（如网页）中完成任务**，评估的是**工具使用**和**任务完成**能力。StoryBench则剥离了环境交互的复杂性，专注于**纯文本叙事环境下的记忆与推理**内核，提供了一个更**可控、更聚焦**于长期记忆核心能力的评估场。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n论文未提供完整的算法伪代码，但根据描述，评估流程可重构如下：\n**Step 1：初始化**。加载StoryBench数据集（DAG结构），选择评估模式（Immediate Feedback 或 Self Recovery），初始化被测LLM，设置最大重试阈值 \\(r_{\\text{thres}} = 9\\)。\n**Step 2：故事开始**。从起始场景节点开始，将节点内容（场景描述、对话等）作为输入提示给LLM。\n**Step 3：决策点**。当遇到选择节点时，将选项文本列表呈现给LLM，要求其输出选择（例如选项ID）。\n**Step 4：判断与反馈（模式相关）**。\n- **IF模式**：如果选择正确，记录 \\(c_t=1, f_t=1, r_t=0\\)，故事沿正确边进入下一个节点。如果选择错误，记录 \\(c_t=0, f_t=0\\)，**立即告知LLM选择错误并提示重试**，重试次数 \\(r_t\\) 增加，重复Step 3直到选择正确。\n- **SR模式**：如果选择正确，记录 \\(c_t=1, f_t=1, r_t=0\\)，故事继续。如果选择错误，记录 \\(c_t=0, f_t=0\\)，**不提供反馈**，故事沿错误边进入下一个节点（可能通往失败结局）。如果LLM在**同一个选择节点上连续错误次数达到 \\(r_{\\text{thres}}\\)**，则**告知正确答案**并强制进入下一节点。\n**Step 5：状态更新与循环**。更新当前节点为所选边指向的节点。如果当前节点是**结局节点**（成功或失败），则进入Step 6；否则，返回Step 2，呈现新的场景/选择节点。\n**Step 6：自我恢复触发（仅SR模式）**。如果故事以失败结局结束，则**要求LLM回溯并指出它认为最早出错的选择点ID**，然后从该点重新开始故事（回到Step 2，从该点状态开始）。此过程可重复，直到成功或达到最大回溯次数限制。\n**Step 7：指标计算**。故事结束后（或达到最大轮数），根据记录的 \\(\\{c_t\\}, \\{f_t\\}, \\{r_t\\}\\) 序列，计算所有预定义的指标（Overall Acc, First-Try Acc, Hard/Easy Acc, Retry Count, Longest Corr等）。\n\n**§2 关键超参数与配置**\n- **重试阈值 \\(r_{\\text{thres}}\\)**：在Self Recovery模式中设置为**9**。理由：为了防止模型在同一个错误上无限循环而导致评估无法进行，设置此软干预阈值。该值是通过初步实验观察确定的，在大多数情况下，连续错误9次表明模型无法自主纠正。\n- **单轮输入Token限制**：针对GPT-4o等模型，在部分实验中限制单轮输入为**5000个token**。理由：防止因内容过长导致API调用失败或服务器过载。\n- **试验次数**：每个模型在每个任务模式下进行**10次**试验（Immediate Feedback）或**5+5次**试验（Self Recovery，分原始和改进两阶段）。理由：通过多次试验取平均值和标准差，以增强结果的统计稳健性。\n- **提示策略**：采用**思维链（Chain-of-Thought, CoT）提示**。理由：鼓励模型进行逐步推理，以更好地激发其推理能力，这是评估复杂决策任务的常见做法。\n\n**§3 训练/微调设置（如有）**\n本文是**评测基准论文**，不涉及模型训练或微调。所有实验均在**预训练的基础模型**上进行零样本（zero-shot）评估。\n\n**§4 推理阶段的工程细节**\n- **模型选择**：评估了四个代表性基础模型：**Doubao 1.5-pro-256k** (ByteDance), **GPT-4o** (OpenAI), **Claude 3.5 Sonnet** (Anthropic), **Deepseek-R1** (DeepSeek-AI)。选择依据是其广泛的实际应用、竞争性能及不同的技术特点（如长上下文支持、纯强化学习）。\n- **输入格式化**：精心设计输入提示格式，以鼓励结构化推理。具体格式原文未详述，但应包含当前场景/选择描述及历史上下文。\n- **内容过滤**：针对GPT-4o等模型的内容安全策略，过滤了可能触发安全机制（如武器相关）的词汇，以确保评估顺利进行。\n- **并行化与缓存**：原文未提及具体的并行化策略或向量数据库使用。评估 likely 是串行进行的，因为涉及多轮交互和状态依赖。\n- **API调用与成本**：实验通过API调用商业模型（GPT-4o, Claude）和开源模型（Doubao, Deepseek-R1）进行。Runtime Cost和Token Consumption被记录为效率指标。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n本文构建了**单一数据集**，基于交互式小说游戏 *The Invisible Guardian*。\n- **名称**：StoryBench Dataset (基于 *The Invisible Guardian*)\n- **规模**：包含**311个场景节点**和**86个选择节点**，总计**397个节点**。故事涵盖从游戏序章到第5章的内容。\n- **领域类型**：**叙事驱动**的交互式小说（文字冒险游戏）。\n- **评测问题类型**：**多轮、分支、动态的决策任务**。每个决策点要求模型基于当前及历史叙事上下文做出选择，任务类型包括信息回忆、因果推理、角色动机理解、多步规划等。\n- **数据构造标准**：通过**人工标注**构建，严格保留原游戏的分支逻辑、因果关系和时间顺序。标注了场景描述、角色对话、选择选项以及元数据（如是否为结局）。旨在避免使用可能已被LLM预训练数据包含的公开基准，以确保评估新颖性。\n\n**§2 评估指标体系（全量列出）**\n评估指标分为两大类，对应长期记忆的两个核心维度：\n**A. 知识保留指标**\n1.  **整体准确率 (Overall Accuracy)**：所有决策的平均正确率。\n2.  **首次尝试准确率 (First-Try Accuracy)**：模型在第一次尝试时就做出正确选择的决策点比例。\n3.  **最长连续正确序列 (Longest Consecutive Correct Sequence)**：最长的连续正确决策序列的长度，反映叙事连贯性。\n**B. 时序推理指标**\n4.  **按难度划分的准确率 (Easy/Hard Accuracy)**：根据决策所需记忆和推理的难度，将决策分为“简单”和“困难”两类，分别计算准确率。**困难决策**定义为需要回忆遥远上下文信息、追踪潜在状态变化或进行多步时序推理的决策。\n5.  **重试次数 (Retry Count)**：在Immediate Feedback模式下，模型在所有决策点上达到正确选择所需的总重试次数。\n6.  **最大错误/选择 (Max Error/Choice)**：单个决策点上所需的最大重试次数。\n7.  **阈值错误计数 (Error Count≥r_thres)**：重试次数达到或超过预设阈值（r_thres=9）的决策点数量。\n**C. 任务完成与效率指标**\n8.  **成功计数 (Success Count)**：模型成功完成整个故事链（到达成功结局）的试验次数。\n9.  **运行成本 (Runtime Cost)**：以秒为单位的总推理时间，反映推理效率。\n10. **Token消耗 (Token Consumption)**：整个评估过程中消耗的总Token数，反映模型对上下文信息的依赖程度。\n\n**§3 对比基线（完整枚举）**\n本文是**基准论文**，主要贡献是提出新的评测框架和数据集。因此，实验部分**没有将StoryBench与现有基准进行横向对比**，而是**使用StoryBench评估了多个主流LLM**，以展示其区分能力。被评估的模型即作为彼此的**相对基线**：\n1.  **Doubao 1.5-pro-256k**：类型为**长上下文专用模型**，支持256k上下文长度。代表性在于其极长的上下文窗口，适合测试长程信息保留。\n2.  **GPT-4o**：类型为**领先的闭源商业模型**。代表性在于其强大的通用语言理解和推理能力，是行业标杆。\n3.  **Claude 3.5 Sonnet**：类型为**闭源商业模型**，以长上下文理解和知识推理见长，支持200k+ Token。代表性在于其在长文本推理和结构分析任务中的稳定表现。\n4.  **Deepseek-R1**：类型为**基于纯强化学习训练的模型**。代表性在于其出色的逻辑推理和结构化思维能力，在多步推理和规划任务中表现强劲。\n**排除的模型**：论文明确排除了**Mem0**、**MemoryScope**等基于RAG或外部记忆缓冲区的记忆增强方法，因为它们的记忆效用侧重于检索孤立事实内容，而StoryBench强调需要支持推理、自我纠正和因果追踪的长期时序推理。\n\n**§4 实验控制变量与消融设计**\n本文的核心消融实验体现在**两种评估模式的对比**上：\n- **控制变量**：使用**相同的数据集**和**相同的模型**，分别在Immediate Feedback (IF) 和 Self Recovery (SR) 模式下进行评估。\n- **设计目的**：通过对比**有反馈**和**无反馈**下的模型表现，可以分离出**短期反馈辅助**的影响，从而更纯粹地评估模型内在的**长期记忆和自主推理**能力。\n此外，在Self Recovery模式内部，还进行了**两阶段实验**：\n1.  **原始阶段**：使用未过滤的原始数据集，无Token限制。\n2.  **改进阶段**：过滤敏感词汇，并施加单轮5000 Token限制。\n- **设计目的**：探究**输入处理策略**（如内容过滤、长度限制）对模型性能（特别是GPT-4o）的影响，并确保评估的稳定性和可重复性。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n以下是论文中三个主要实验结果表的整合与还原（数值后跟“±”表示均值和标准差，无“±”表示单次或少数试验结果）：\n`模型 | 模式 | Overall Acc (%) | First-Try Acc (%) | Hard Acc (%) | Easy Acc (%) | Retry Count | Longest Corr | Max Err/Choice | ErrorCount≥9 | Runtime Cost (s) | Token Cons | Success Count`\n`Doubao1.5-pro | Immediate Feedback | 80.98 ± 1.31 | 79.14 ± 1.33 | 74.47 ± 2.26 | 88.68 ± 0.15 | 14.67 ± 1.25 | 10.00 ± 0.00 | - | - | 0.65k ± 0.02k | 2043k ± 53k | 3.00`\n`GPT-4o | Immediate Feedback | 71.88 ± 1.03 | 63.49 ± 2.59 | 66.94 ± 1.38 | 77.43 ± 0.88 | 24.67 ± 1.25 | 8.00 ± 0.82 | - | - | 0.44k ± 0.08k | 342k ± 5.8k | 3.00`\n`Claude 3.5 Sonnet | Immediate Feedback | 74.86 ± 1.05 | 68.21 ± 1.55 | 69.38 ± 1.26 | 81.35 ± 1.67 | 20.88 ± 1.17 | 8.50 ± 2.12 | - | - | 2.14k ± 0.16k | 3405k ± 150k | 8.00`\n`Deepseek-R1 | Immediate Feedback | 70.45 ± 4.62 | 65.16 ± 2.41 | 60.21 ± 4.61 | 84.94 ± 4.44 | 26.40 ± 5.95 | 10.20 ± 1.47 | - | - | 2.72k ± 0.23k | 2396k ± 264k | 5.00`\n`Doubao1.5-pro | Original Self Recovery | 69.66 | 83.05 | 58.33 | 93.10 | 21.00 | 15.00 | 9.00 | 2.00 | 1.00k | 4158k | 1.00`\n`Claude 3.5 Sonnet | Original Self Recovery | 68.40 ± 2.88 | 68.28 ± 1.07 | 60.35 ± 4.09 | 77.23 ± 0.31 | 21.50 ± 2.50 | 13.50 ± 1.50 | 6.00 ± 2.00 | 0.00 ± 0.00 | 3.24k ± 0.36k | 5532k ± 37k | 2.00`\n`Doubao1.5-pro | Improved Self Recovery | 73.68 | 83.33 | 62.22 | 90.32 | 17.00 | 16.00 | 9.00 | 1.00 | 0.60k | 343k | 1.00`\n`GPT-4o | Improved Self Recovery | 60.76 ± 1.35 | 58.57 ± 1.43 | 52.84 ± 2.06 | 72.72 ± 2.27 | 30.50 ± 4.50 | 8.00 ± 1.00 | 7.00 ± 2.00 | 0.00 ± 0.00 | 0.58k ± 0.05k | 510k ± 40k | 2.00`\n`Deepseek-R1 | Improved Self Recovery | 70.18 | 75.41 | 62.50 | 88.24 | 26.00 | 12.00 | 9.00 | 2.00 | 4.64k | 549k | 1.00`\n（注：表格中“-”表示该模式未报告此指标。Claude 3.5 Sonnet在Improved Self Recovery模式下无数据报告。）\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n- **知识保留能力（Overall Acc, First-Try Acc）**：在Immediate Feedback模式下，**Doubao1.5-pro**在整体准确率（80.98%）和首次尝试准确率（79.14%）上均领先，表明其**知识整合与维持能力最强**。然而，在更具挑战性的Self Recovery模式下，所有模型的Overall Acc均下降，但**First-Try Acc** 对于Doubao和Deepseek-R1反而**有所提升**（Doubao从79.14%升至83.33%）。这表明**移除短期反馈可能迫使模型建立更连贯的叙事理解**，从而在第一次尝试时做出更准确的判断。\n- **时序推理能力（Hard Acc, Retry Count, Success Count）**：**Claude 3.5 Sonnet**在Hard Acc（Immediate Feedback: 69.38%）和Success Count（Immediate Feedback: 8/10; Original Self Recovery: 2/5）上表现突出，尤其是在Self Recovery模式下其**ErrorCount≥9为0**，说明它**从未在同一个错误上卡住超过9次**，展现了强大的**长程因果推理和自主纠错能力**。相反，Doubao虽然知识保留强，但Success Count低（Self Recovery仅1-2次成功），说明它**在复杂的多步推理链中容易“死于细节”**，无法完成整个故事。\n- **效率与性能权衡（Runtime Cost, Token Cons）**：**GPT-4o**和**Doubao1.5-pro**在运行时间和Token消耗上显著低于Claude和Deepseek-R1。例如，在Immediate Feedback下，GPT-4o的Runtime Cost仅为0.44k秒，Token Cons为342k，而Claude分别为2.14k秒和3405k Token。这表明前两者具有**更好的成本效益比**。\n\n**§3 效率与开销的定量对比**\n- **延迟**：在Immediate Feedback模式下，**GPT-4o**的Runtime Cost最低，为**0.44k秒**（约440秒），比最高的**Deepseek-R1**（2.72k秒，约2720秒）**低83.8%**。在Improved Self Recovery模式下，**Doubao1.5-pro**的Runtime Cost为**0.60k秒**，比**Deepseek-R1**的**4.64k秒**低**87.1%**。\n- **Token消耗**：在Immediate Feedback模式下，**GPT-4o**的Token Consumption最低，为**342k**，比最高的**Claude 3.5 Sonnet**（3405k）**低约90%**。这反映了GPT-4o可能使用了更高效的上下文管理或压缩策略。\n- **显存占用**：原文未提供显存占用数据。\n\n**§4 消融实验结果详解**\n本文的核心消融是**两种模式的对比**，而非移除某个组件。从Immediate Feedback切换到Self Recovery模式，相当于“消融”了即时反馈机制，结果如下：\n- **整体性能下降**：所有模型的Overall Acc在Self Recovery模式下均下降。例如，Doubao从IF的80.98%下降到SR（Improved）的73.68%，**绝对下降7.3个百分点**；GPT-4o从71.88%下降到60.76%，**绝对下降11.12个百分点**。这证明了**即时反馈对模型性能有显著辅助作用**，移除后任务难度急剧增加。\n- **首次尝试准确率与最长连续正确序列的反常提升**：对于Doubao和Deepseek-R1，在Self Recovery模式下，First-Try Acc和Longest Corr**不降反升**。例如，Doubao的First-Try Acc从79.14%升至83.33%，Longest Corr从10升至16。这表明**短期反馈虽然有助于局部纠正，但可能干扰长期叙事连贯性**。移除反馈后，模型被迫建立更全局、一致的心智模型，从而在某些指标上表现更好。\n- **成功计数的显著差异**：Claude 3.5 Sonnet在Immediate Feedback下的Success Count为8/10，在更具挑战的Original Self Recovery下仍能达到2/5，且从未触发错误阈值干预。而其他模型在SR模式下的Success Count普遍很低（0-2）。这凸显了Claude在**无辅助长程推理和任务完成**方面的独特优势。\n\n**§5 案例分析/定性分析（如有）**\n论文进行了失败案例分析，识别出两大主要失败类型：\n1.  **知识保留失败**：模型做出的决策**与早期故事事件、角色动机或已建立的世界逻辑相矛盾**。即使相关事实出现在提示中，模型也**难以连贯地应用它们**。这表明模型在整合和维持分散在数十轮交互中的信息方面存在困难，超越了简单的事实回忆。\n2.  **时序推理失败**：在Self Recovery模式下，模型**无法修复长期或多错误决策**。成功完成通常需要模型回溯多步因果链并修正影响下游结果的早期决策（甚至是多个选择的组合）。然而，大多数模型表现出**短视的搜索策略**，通常只回溯一两步，而不是对叙事结构或目标转变进行更深层次的推理。这种短视行为导致当任务成功依赖于理解和纠正长期依赖关系时，模型持续失败。\n其他观察到的失败包括格式不匹配（如返回选项索引而非决策点ID）、内容过滤阻止、服务器超时或罕见的幻觉解释，但这些相对不频繁。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出了一个动态多轮评估框架**：基于交互式小说游戏，设计了包含**分支叙事**和**两种互补评估模式**（Immediate Feedback, Self Recovery）的基准StoryBench。该框架能更全面、真实地评估LLM的**知识保留**和**时序推理**能力，并具有高度的**灵活性**。\n2.  **构建了一个新颖的长期记忆评估数据集**：通过人工标注，基于游戏*The Invisible Guardian*构建了一个包含**397个节点**的结构化数据集。该数据集具有**叙事连贯性、动态分支、复杂依赖和多解机制**，能模拟真实世界的记忆挑战，并避免了公开数据可能带来的污染问题。\n3.  **进行了可靠且稳健的实验分析**：在四个具有代表性的LLM上进行了系统评估，采用了**重复试验**以增强统计稳健性。实验结果揭示了当前模型在长期记忆能力上的**显著差异和局限性**，特别是**在无反馈自主推理方面的普遍薄弱**，验证了基准的有效性和区分度。\n\n**§2 局限性（作者自述）**\n1.  **领域泛化性有限**：场景源自**单一的交互式小说领域**，且交互环境是**纯文本的**，这可能限制基准在其他需要多模态支持的**知识密集型**或**任务导向型**上下文中的泛化能力。\n2.  **叙事长度和轮数有限**：当前数据集仅包含游戏的**6个章节**，可能无法完全捕捉更广泛叙事中所需要的**长期依赖**和**复杂推理**。\n3.  **评估模型范围有限**：由于API限制和成本，主要评估了**少数主流模型**。其他模型在类似条件下的性能尚未探索。\n4.  **评估场景仍具脚本性**：尽管包含了自我恢复设置来模拟真实世界的错误修正，但评估仍然是**脚本化的**，无法捕捉所有形式的自然反馈。\n\n**§3 未来研究方向（全量提取）**\n1.  **扩展数据集**：未来工作可以通过**添加后续章节**来扩展数据集，以提供对长期记忆更全面的评估。**技术层面**：需要设计更复杂的叙事结构和更长程的因果链，可能涉及自动或半自动的故事生成技术。\n2.  **探索多模态评估**：将基准扩展到**需要多模态支持**的上下文。**技术层面**：集成图像、音频或视频输入，评估模型在多模态信息流中的长期记忆整合能力。\n3.  **纳入更多样化的模型**：评估**更广泛范围**的LLM，特别是那些专注于长上下文或记忆增强的模型。**技术层面**：需要解决API成本问题，可能通过开源模型本地部署或设计更高效的评估流水线来实现。\n4.  **增强评估的自然性**：探索更**自然、非脚本化**的反馈形式。**技术层面**：可以引入基于用户模拟或强化学习的环境，让模型接收更连续、更细微的奖励信号，而非二元的正确/错误反馈。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **提出了一个聚焦于长期记忆核心认知维度的评估框架**：\n    - **理论新颖性**：首次将**交互式叙事**的**动态分支**和**多轮决策**特性系统性地引入LLM长期记忆评估，明确区分并量化了**知识保留**和**时序推理**两个关键维度。\n    - **实验验证充分性**：通过设计**双模式评估**（有/无即时反馈）和**多维度指标**（准确率、重试计数、成功计数等），在多个代表性模型上进行了严谨实验，揭示了模型在不同记忆压力下的行为差异。\n    - **对领域的影响**：为长期记忆研究提供了一个比现有静态检索基准更**贴近真实认知过程**的评测工具，可能推动模型架构和训练目标向更好的叙事理解和因果推理方向发展。\n2.  **构建了一个高质量、可控的长期记忆评估数据集**：\n    - **理论新颖性**：采用**人工标注**方式从现有游戏中构建数据集，在保证**叙事复杂性和因果依赖**的同时，有效避免了使用公开合成数据可能导致的数据污染问题。\n    - **实验验证充分性**：数据集具有明确的结构（DAG）和丰富的元数据，支持对复杂记忆模式（如长程依赖、多解分支）的分析。\n    - **对领域的影响**：为社区提供了一个可复用的、专注于叙事连贯性和动态推理的基准数据，弥补了现有数据集在此方面的空白。\n3.  **提供了对主流LLM长期记忆能力的深入洞察**：\n    - **理论新颖性**：通过失败案例分析，明确指出了当前模型在**维持上下文一致性**和**进行深度回溯推理**方面的具体缺陷，超越了简单的性能排名。\n    - **实验验证充分性**：不仅报告了性能数字，还分析了不同模式（IF vs. SR）下指标的反常变化（如First-Try Acc提升），揭示了短期反馈可能干扰长期连贯性的有趣现象。\n    - **对领域的影响**：这些发现为模型改进提供了明确的方向，例如需要增强模型的状态追踪和自主错误修正能力。\n\n**§2 工程与实践贡献**\n- **开源了基准框架与数据集**：论文虽未明确声明代码和数据已开源，但此类工作通常后续会开源，为研究者提供了一个可直接使用的评测工具。\n- **定义了系统的评估指标体系**：提出了一套涵盖知识保留、时序推理、任务完成和效率的**多维度量化指标**，为未来长期记忆研究提供了可参考的评估标准。\n- **提供了实用的工程经验**：报告中提到了处理API内容过滤、Token长度限制等实际工程问题的经验（如对GPT-4o过滤敏感词、限制单轮输入），对后续研究者有借鉴意义。\n\n**§3 与相关工作的定位**\n本文在LLM评估技术路线图中，位于**从静态、单任务评估向动态、多轮、交互式评估演进**的路径上。它并非简单扩展上下文长度（如InfiniteBench），也非专注于工具使用（如AgentBench），而是**开辟了一条以“叙事理解”和“因果推理”为核心的新评估路线**。它是对现有侧重于事实检索（NeedleInAHaystack）或多轮对话（Multi-Session Chats）的基准的重要补充和深化，将评估焦点从“信息存在性”转向了“信息在动态序列中的可用性与连贯性”。",
    "professor_critique": "**§1 实验设计与评估体系的缺陷**\n1.  **基线对比不充分**：本文仅使用StoryBench评估了四个基础模型，**未将其与任何现有的、专门设计用于增强长期记忆的模型或系统（如MemGPT、Mem0、MemoryScope等）进行对比**。作者以这些方法“专注于检索孤立事实”为由将其排除，但并未验证在StoryBench的叙事任务上，这些记忆增强方法是否真的无效。这可能导致**评估范围狭窄**，无法证明StoryBench能有效区分不同记忆架构的优势。\n2.  **“成功计数”指标的定义模糊且可能误导**：Success Count被定义为“成功完成整个故事链”的次数。然而，在分支叙事中，**可能存在多条路径通往“成功”结局**。论文未明确说明是否只有一条预定义的“黄金路径”才算成功，还是任何达到“非失败”结局的路径都算。如果是前者，则低估了模型的适应性；如果是后者，则可能高估了模型的推理能力，因为模型可能误打误撞到达一个结局。\n3.  **“困难”与“简单”决策的分类主观性强**：论文将决策分类为“困难”和“简单”，但**分类标准（如“需要回忆遥远上下文”或“多步推理”）缺乏客观、可量化的定义**，很可能依赖于人工标注者的主观判断。这导致Hard/Easy Acc指标的可复现性和可比性存疑。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **对“自我恢复”机制的理想化假设**：在Self Recovery模式中，当故事失败后，要求模型“指出最早出错的选择点并从此恢复”。这假设模型**具备完美的自我诊断能力**，能准确识别错误根源。然而，论文中的失败案例表明，模型往往只能进行**浅层回溯（1-2步）**。因此，这个设计可能**高估了模型的实际纠错能力**，因为即使模型指出了错误点，也可能不是真正的根本原因。一个更严谨的设计是提供多个回溯候选点让模型选择，或评估其修正后的路径长度。\n2.  **依赖特定的、成本高昂的闭源模型**：实验严重依赖GPT-4o和Claude 3.5 Sonnet等闭源商业API。这不仅带来**高昂的评估成本**，使得资源有限的研究者难以复现，而且**模型内部更新可能导致结果不可复现**。评估结果更多反映了这些特定API版本在特定时间点的行为，而非一般性的模型能力。\n3.  **数据集规模与多样性有限**：仅基于**单个游戏**的6个章节构建数据集，其叙事风格、领域知识和推理模式相对单一。这可能导致评估结果**过拟合**于该特定游戏的结构和逻辑，而无法推广到其他类型的长期记忆任务（如科学实验规划、法律案例推理等）。\n\n**§3 未经验证的边界场景**\n1.  **领域外知识冲突**：当叙事中引入与模型预训练知识相矛盾的信息时（例如，游戏设定“猫会说话”，而常识认为猫不会说话），模型能否**抑制先验知识**，遵循叙事内部逻辑？这种**知识冲突下的记忆更新与一致性维持**能力未被测试。\n2.  **高频主题切换与干扰**：如果故事在多个看似不相关的子情节间快速切换，模型能否在**大量干扰信息**中保持对关键主线的记忆和推理？这模拟了真实对话中话题漂移的场景，是长期记忆的重要压力测试。\n3.  **对抗性输入与误导性线索**：如果叙事中故意植入**误导性或矛盾的信息**，模型能否识别并忽略这些噪声，还是会被其带偏从而导致推理失败？这考验模型的**鲁棒性**和**信息可信度评估**能力。\n\n**§4 可复现性与公平性问题**\n1.  **超参数调优不对等**：论文提到了对GPT-4o进行**敏感词过滤**和**Token长度限制**（5000）以保障评估顺利进行。然而，**是否对其他模型（如Claude, Deepseek）也进行了完全相同的输入预处理？** 如果没有，那么这种差异化的处理可能使GPT-4o处于不利或有利地位，影响公平比较。\n2.  **提示工程细节缺失**：论文提到使用了Chain-of-Thought提示，但**未提供具体的提示词模板**。提示词的微小变化可能对模型性能产生巨大影响，这使得实验结果的**可复现性大打折扣**。\n3.  **随机性控制不足**：虽然进行了多次试验，但**未说明是否控制了随机种子**，以及LLM生成本身固有的随机性如何影响指标（如Success Count）的稳定性。对于基于概率生成的模型，仅5-10次试验的Success Count可能波动很大，结论的可靠性存疑。",
    "zero_compute_opportunity": "**§1 领域背景与研究动机（150字以上）**\n随着大型语言模型（LLM）在**多轮对话**、**任务规划**和**终身学习**等复杂、动态场景中的应用日益深入，模型需要具备**长期记忆**能力来维持上下文连贯性、整合历史知识并进行时序推理。然而，现有的评测基准大多基于静态的、单轮的任务，无法充分模拟现实世界中信息需要被**长期保留、动态更新并用于多步推理**的挑战。因此，在**交互式叙事**（如文字冒险游戏）这一具体场景下，构建一个能够系统评估LLM长期记忆能力的动态基准，对于推动模型向自主智能演进至关重要。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有基准在评估长期记忆的两个关键维度上存在明显短板：\n1.  **知识保留能力评估不足**：现有基准如**NeedleInAHaystack**和**LongBench**，主要评估模型在长上下文中**定位孤立事实**的能力。**当输入是跨越数十轮对话、信息点分散且相互关联的叙事文本时**，这些基准无法评估模型整合和维持**叙事连贯性**（如角色动机、事件因果关系）的能力，导致模型可能出现**上下文矛盾**的失败模式。\n2.  **时序推理能力评估缺失**：现有基准如**RULER**和**LTM Benchmark**虽然包含推理任务，但任务通常是**静态且非交互的**。**当输入是一个动态分支的叙事，其中早期选择会触发后续一系列连锁依赖时**，这些基准无法评估模型进行**多步因果推理**和**回溯纠错**的能力。模型可能表现出**短视行为**，仅能回溯1-2步，而无法修复由多个早期错误共同导致的长期失败结局。\n3.  **灵活性不足**：许多基准如**BABILong**和**LooGLE**的任务具有**单一固定解**。**当输入允许存在多个有效路径（多解分支）时**，这些基准无法评估模型在不确定性环境中的**适应性决策**能力，限制了其在真实、开放场景中的应用价值。\n\n**§3 问题的根本难点与挑战（200字以上）**\n构建有效的长期记忆评估基准面临以下根本挑战：\n- **动态性与连续性难以兼顾**：真实世界的决策是**序列化**且**上下文依赖**的。设计一个基准，既要模拟这种动态分支（每个选择影响未来），又要维持整个故事的语义连续性，在工程和逻辑上非常复杂。\n- **评估指标的定义与量化困难**：长期记忆不仅是“记住”，更是“**在正确的时间运用正确的记忆**”。如何设计指标来量化模型在**知识保留**（如叙事一致性）和**时序推理**（如因果链追踪）两方面的能力，并区分短期反馈辅助与长期自主推理，是一个核心挑战。\n- **数据污染风险**：使用公开的合成数据或真实世界数据构建基准，存在被LLM预训练数据包含的风险，导致评估结果失真。如何构建一个**新颖、可控且富含复杂依赖关系**的数据集，同时避免数据泄露，是另一个主要难点。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是利用**交互式小说游戏**作为评估框架的基础。其核心假设是：**基于分支叙事的多轮交互游戏，能够自然地封装长期记忆评估所需的关键属性（如长程依赖、连续性、动态性、多解性），从而提供一个比静态问答更全面、更真实的压力测试环境。** 该假设受到**认知科学**中关于叙事理解和情景记忆研究的启发，即人类通过构建和更新**心智模型**来理解和参与复杂故事。本文认为，LLM在类似环境中的表现，可以更准确地反映其长期记忆能力的强弱。此外，本文设计了**双模式评估**（即时反馈与自我恢复），其假设是：**移除即时反馈能更纯粹地暴露模型在长程因果推理和自主错误修正方面的内在局限性**，而这正是现有基准所忽略的。\n\n#### 蓝图一：基于开源游戏引擎构建轻量级、可扩展的长期记忆基准\n- **核心假设**：利用成熟的开源交互式小说创作工具（如Twine, Inform 7）可以快速、低成本地生成大量具有复杂分支和依赖关系的叙事评估任务，从而克服StoryBench数据单一、构建成本高的问题。\n- **与本文的关联**：基于本文**利用交互式小说评估长期记忆**的核心思想，但致力于解决其**数据集规模小、构建费时费力**的局限性。\n- **所需资源**：\n  - **工具**：Twine（免费，基于Web）或Inform 7（免费，自然语言编程）用于创作故事。\n  - **模型API**：使用**Ollama**本地运行开源小模型（如Llama 3.2 3B, Qwen2.5 7B）进行初步测试，成本接近于零。后续可使用**Google Colab免费GPU**运行稍大模型。\n  - **数据集**：从开源游戏社区（如itch.io）寻找已有Twine游戏，或自行创作小型故事（预计1-2周工作量）。\n  - **费用**：主要为电力和时间成本，API调用费用为零（本地运行）。\n- **执行步骤**：\n  1.  **故事设计与生成**：使用Twine设计一个包含5-10个关键决策点、具有明确因果链和多个结局的短篇互动故事。导出为JSON格式。\n  2.  **评估框架移植**：用Python复现StoryBench的**双模式评估引擎**（Immediate Feedback / Self Recovery），使其能够加载Twine导出的故事JSON并驱动LLM进行交互。\n  3.  **指标计算与可视化**：实现论文中的核心指标（Overall Acc, First-Try Acc, Hard/Easy Acc, Success Count等），并开发简单的可视化面板对比不同模型/配置的表现。\n  4.  **实验与验证**：在2-3个开源小模型（如Llama, Qwen）上运行基准，验证其区分度。尝试引入简单的**记忆增强技术**（如关键信息摘要）作为对比，看能否提升性能。\n- **预期产出**：一个**开源、轻量级、可扩展**的长期记忆评估工具包，包含至少一个示例故事和评估代码。可以撰写一篇技术报告或短文，投递到**NLP开源工具研讨会**或**arXiv**。\n- **潜在风险**：\n  - **故事质量**：自创故事可能过于简单或逻辑不严谨，影响评估信度。**应对方案**：借鉴经典互动小说设计模式，或直接使用已有高质量开源游戏进行改编。\n  - **小模型能力有限**：小模型可能在复杂推理上完全失败，导致结果缺乏区分度。**应对方案**：聚焦于设计更精细的、区分“记忆”与“推理”的子任务，或使用模型对比（如7B vs 14B）来展示工具的敏感性。\n\n#### 蓝图二：探究“提示工程”对长期记忆任务性能的边际影响\n- **核心假设**：在固定的模型和数据集上，不同的提示策略（如Chain-of-Thought, Self-Reflection, 结构化输出要求）会对模型在StoryBench类任务上的表现产生**系统性且可预测的影响**，并且这种影响在“知识保留”和“时序推理”任务上可能不同。\n- **与本文的关联**：本文使用了CoT提示但未做消融。本蓝图旨在**系统量化提示工程这一低成本干预手段的有效性**，为资源受限的研究者提供实用指南。\n- **所需资源**：\n  - **模型**：使用**Google Colab免费T4 GPU**运行一个中等规模的开源模型（如Qwen2.5 14B）。或使用**OpenRouter/ Together AI**的廉价API调用Llama 3.1 8B。\n  - **数据集**：直接使用**StoryBench开源的数据集**（若已开源），或使用蓝图一自建的小型数据集。\n  - **费用**：预计API调用费用在5-10美元以内（若使用付费API），Colab免费但有时间限制。\n- **执行步骤**：\n  1.  **提示策略设计**：设计4-5种不同的提示策略：\n     - **基础指令**：仅给出任务指令。\n     - **思维链（CoT）**：要求模型“逐步推理”。\n     - **自我反思（Self-Reflection）**：在每步决策后，要求模型简要解释选择理由。\n     - **结构化输出**：严格要求模型以特定JSON格式输出，包含“选择”和“理由”字段。\n     - **记忆摘要**：在每轮交互后，要求模型用一句话总结当前关键故事状态。\n  2.  **控制变量实验**：在同一个",
    "source_file": "StoryBench A Dynamic Benchmark for Evaluating Long-Term Memory with Multi Turns.md"
}