{
    "title": "Reason ingBank: Scaling Agent Self-Evolving with Reasoning Memory",
    "background_and_problem": "**§1 领域背景与研究动机**\n随着大型语言模型智能体在持久性现实世界角色中的广泛应用，它们会持续遇到任务流。当前智能体系统的一个关键局限在于无法从累积的交互历史中学习，导致它们丢弃宝贵的洞察并重复过去的错误。本研究旨在解决在**持续、流式任务到达的测试时学习（test-time learning）**场景下，智能体如何通过记忆实现自我进化的问题。具体应用场景包括**网页浏览（WebArena, Mind2Web）**和**软件工程（SWE-Bench-Verified）**中的任务自动化。该研究的动机在于，随着智能体部署时间的延长，构建能够从过去经验中学习并持续改进的**记忆感知（memory-aware）**智能体系统变得至关重要。\n\n**§2 现有技术的核心短板——具体失败模式**\n现有记忆机制主要分为两类，均在特定场景下存在明显失败模式：\n1.  **原始轨迹记忆（Raw Trajectory Memory）**：如 Synapse 等方法，存储原始的交互轨迹（action-observation序列）。当输入任务需要**跨任务泛化**时，这种记忆过于冗长和嘈杂，无法提供可迁移的策略指导。例如，在 WebArena 的 Multi 子集（需要跨网站记忆迁移）上，Synapse 的记忆机制未能提供增益，甚至在某些情况下性能下降（如 Gemini-2.5-pro 在 Multi 子集上，Synapse 的 SR 为 6.9，与 No Memory 的 6.9 持平，而 Reason ingBank 达到 13.8）。\n2.  **工作流记忆（Workflow Memory）**：如 AWM 等方法，仅从成功经验中提取通用的工作流程或程序。当输入任务涉及**失败场景或需要从错误中学习**时，这类方法因忽略失败轨迹而无法提供预防性指导。例如，在 WebArena-Shopping 子集上，当尝试从失败轨迹中提取记忆时，AWM 的性能从仅使用成功轨迹时的 44.4 SR 下降至 42.2 SR，表明其无法将失败转化为有效信号。\n3.  **无记忆基线（No Memory）**：智能体孤立处理每个任务。当输入是**连续任务流**时，智能体会重复过去的错误，无法利用历史洞察，导致效率低下（平均交互步数更多）和效果不佳（成功率更低）。\n\n**§3 问题的根本难点与挑战**\n从理论或工程角度看，上述问题难以解决的原因在于：\n1.  **信息抽象与泛化的困难**：从具体的、低层次的交互轨迹中，自动提炼出高层次、可迁移的**推理策略（reasoning strategies）**是一个复杂的归纳过程，需要区分任务无关的通用模式和任务特定的噪声细节。\n2.  **失败信号的有效利用**：失败轨迹通常比成功轨迹更复杂、更多样，且缺乏明确的“正确”标签。如何在没有真实标签（ground-truth）的测试时学习环境中，**自我判断（self-judge）**轨迹的成功/失败，并从失败中提取建设性而非破坏性的教训，是一个核心挑战。\n3.  **记忆与计算扩展的协同**：简单地增加测试时计算（如生成更多轨迹）并不总能带来性能提升。如何将额外的计算资源（**测试时扩展，Test-Time Scaling**）与记忆机制协同，使生成的多样化探索轨迹能够被有效提炼为高质量的记忆，而不是引入噪声，这需要设计新的协同机制。\n\n**§4 本文的切入点与核心假设**\n本文的切入点是设计一个能够从**成功与失败**两种经验中共同提炼通用推理策略的记忆框架。其核心技术假设是：**从智能体自我判断的成功和失败经验中抽象出的结构化、策略级的记忆单元，比原始轨迹或仅成功的流程更具可迁移性，能更有效地指导未来决策。** 该假设受到认知科学中“从错误中学习”的启发，并基于一个观察：失败轨迹包含有价值的**反事实信号（counterfactual signals）**和需要规避的陷阱。此外，本文进一步假设，将这种记忆机制与测试时扩展（TTS）结合，可以形成一个**正反馈循环**：高质量的记忆引导扩展探索走向更有希望的路径，而丰富的探索经验则锻造出更强的记忆。这一假设构成了**记忆驱动的经验扩展（memory-driven experience scaling）**这一新扩展维度的理论基础。",
    "core_architecture": "**§1 系统整体架构概览**\nReason ingBank 系统是一个闭环的记忆框架，由三个核心模块构成，整体数据流如下：\n1.  **输入**：新的任务查询（Query）q，以及当前智能体的记忆库 Reason ingBank（初始为空）。\n2.  **记忆检索模块（Memory Retrieval）**：接收查询 q，通过**基于嵌入的相似性搜索**从 Reason ingBank 中检索 top-k 个最相关的记忆项。检索到的记忆项被注入智能体的系统指令中。\n3.  **智能体与环境交互模块（Agent-Environment Interaction）**：智能体（基于 LLM 的策略 π_ℒ）在记忆的指导下与环境交互，生成轨迹 τ = (o_0:t, a_0:t)。\n4.  **记忆构建模块（Memory Construction）**：任务完成后，使用 **LLM-as-a-judge** 对轨迹 τ 进行自我评估，标记为成功或失败。根据评估结果，应用不同的提取策略：从成功轨迹中提取已验证的策略，从失败轨迹中提取反事实信号和陷阱。为每条轨迹提取多个记忆项。\n5.  **记忆整合模块（Memory Consolidation）**：将新构建的记忆项通过**简单的加法操作**整合到 Reason ingBank 中，更新记忆库。\n6.  **输出**：任务的最终结果，以及更新后的记忆库 Reason ingBank，用于后续任务。\n\n**§2 各核心模块深度拆解**\n#### 模块一：记忆项模式（Memory Schema）\n-   **模块名**：Memory Schema\n-   **输入**：原始经验（成功或失败的轨迹）。\n-   **核心处理逻辑**：使用 LLM 提示（prompting）从轨迹中诱导（induce）出结构化的知识单元。每个记忆项包含三个组件：(i) **标题（Title）**：总结核心策略或推理模式的简洁标识符；(ii) **描述（Description）**：一句话概括记忆项；(iii) **内容（Content）**：记录从过去经验中提炼出的推理步骤、决策原理或操作见解。提取过程旨在**抽象掉低层次的执行细节，保留可迁移的推理模式**。\n-   **输出**：结构化的记忆项，格式为 {Title, Description, Content}。\n-   **设计理由**：与存储原始轨迹（冗长、噪声大）或仅成功的工作流（信息不完整）相比，这种结构化模式旨在创建**人类可解释且机器可用**的记忆单元，便于高效检索和使用，并支持跨任务泛化。\n\n#### 模块二：记忆检索（Memory Retrieval）\n-   **模块名**：Memory Retrieval\n-   **输入**：当前查询上下文（query context）。\n-   **核心处理逻辑**：使用**基于嵌入的相似性搜索**（embedding-based similarity search）从 Reason ingBank 中检索 top-k 个相关记忆项。具体实现中，查询和记忆项被编码为向量，计算余弦相似度后返回最相似的 k 项。检索到的项被**注入（inject）**到智能体的系统指令中，作为额外的上下文。\n-   **输出**：top-k 个相关的记忆项，作为系统指令的一部分提供给智能体策略 π_ℒ。\n-   **设计理由**：通过语义相似性检索，确保智能体的决策能够**基于（ground）**有用的过去经验，而不是盲目搜索。这提高了决策的相关性和效率。\n\n#### 模块三：记忆构建与整合（Memory Construction & Consolidation）\n-   **模块名**：Memory Construction & Consolidation\n-   **输入**：已完成任务的轨迹 τ。\n-   **核心处理逻辑**：\n    1.  **自我评估**：使用 LLM-as-a-judge，在**没有真实标签**的情况下，仅根据查询和轨迹判断结果为成功或失败。\n    2.  **差异化提取**：\n        -   成功经验 → 贡献已验证的策略。\n        -   失败经验 → 提供反事实信号和陷阱，帮助强化防护栏（guardrails）。\n    3.  **多项目提取**：为每条轨迹提取**多个**记忆项（具体细节在附录 A.1）。\n    4.  **整合**：将新构建的记忆项通过**简单的加法操作（simple addition operation）** 整合到 Reason ingBank 中（具体细节在附录 A.2）。\n-   **输出**：更新后的 Reason ingBank 记忆库。\n-   **设计理由**：通过自我评估避免了对人工标注的依赖，符合测试时学习范式。差异化提取确保了从失败中学习的能力。多项目提取增加了记忆的覆盖面和多样性。简单的加法整合操作保持了实现的简洁性和可扩展性。\n\n**§3 关键公式与算法**\n论文中未提供具体的损失函数或目标函数公式。核心算法流程围绕上述模块的闭环操作。\n\n**§4 方法变体对比**\n本文提出了两个主要的变体/扩展：\n1.  **Reason ingBank (Base)**：即上述描述的基础记忆框架，包含检索、构建、整合三个步骤。\n2.  **MaTTS (Memory-aware Test-Time Scaling)**：在 Reason ingBank 基础上，结合测试时扩展。它有两个具体实现：\n    -   **Parallel Scaling (并行扩展)**：在检索到的记忆指导下，为同一查询生成 **k 个并行轨迹**。通过**自我对比（self-contrast）** 不同轨迹，识别一致的推理模式，过滤虚假解决方案，从而从单次查询的多次尝试中进行更可靠的记忆提炼。最终指标采用 **Best-of-N (BoN)**。\n    -   **Sequential Scaling (顺序扩展)**：在初始完成任务后，遵循**自我精炼（self-refinement）** 原则，在**单个轨迹内**迭代优化其推理。在此过程中，自我精炼生成的**中间笔记（intermediate notes）** 也被用作记忆的宝贵信号，因为它们捕获了最终解决方案中可能不出现的推理尝试、修正和见解。\n    -   **与 Base 版的差异**：Base 版每个任务只生成一条轨迹用于构建记忆。MaTTS 通过分配更多计算（k > 1），为每个任务生成**更丰富、更多样**的经验，为合成更高质量的记忆提供了**对比信号（contrastive signals）**。\n\n**§5 与已有方法的核心技术差异**\n1.  **与 Synapse (轨迹记忆) 的差异**：Synapse 存储**原始的动作-观察轨迹**，记忆内容是具体、冗长的。Reason ingBank 则从轨迹中**提炼（distill）** 出结构化的、策略级的推理模式（标题、描述、内容），抽象掉执行细节，使其更具可迁移性。此外，Reason ingBank 明确地从**失败轨迹**中学习，而 Synapse 主要依赖成功轨迹。\n2.  **与 AWM (工作流记忆) 的差异**：AWM 仅从**成功经验**中提取通用的工作流程或程序（procedures）。Reason ingBank 不仅从成功中学习，也从失败中学习，生成包含**反事实信号和陷阱**的记忆项。AWM 的记忆是**流程导向（procedure-oriented）** 的，而 Reason ingBank 的记忆是**策略和推理导向（strategy-and-reasoning-oriented）** 的，旨在提供更高层次的指导。\n3.  **与普通测试时扩展（Vanilla TTS）的差异**：普通的测试时扩展（如 Best-of-N）只是独立生成多条轨迹，然后将它们简单地转化为更多记忆项（如图3a所示）。MaTTS 的关键区别在于**记忆感知的协调与聚合**：它利用多条轨迹（并行）或迭代步骤（顺序）之间的**对比信号**，进行更有效的记忆提炼，确保额外的计算资源被转化为更高质量的记忆，而不是噪声。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\nStep 1: **初始化**。给定任务流 Q = {q1, q2, ..., qN}，初始化空记忆库 M = Reason ingBank。\nStep 2: **对于每个任务 qi in Q**：\n    a. **记忆检索**：使用基于嵌入的相似性搜索，从记忆库 M 中检索与当前查询 qi 最相关的 top-k 个记忆项 R。\n    b. **智能体决策**：将检索到的记忆项 R 注入系统指令。智能体策略 π_ℒ 基于当前观察 o0:t 和历史动作 a0:t，在记忆 R 的指导下选择动作 at+1，与环境交互，生成轨迹 τi。\n    c. **记忆构建**：任务完成后，使用 LLM-as-a-judge 对轨迹 τi 进行自我评估，标记为成功或失败。根据评估标签，应用提示工程从 τi 中提取多个结构化的记忆项 {m1, m2, ..., mn}。\n    d. **记忆整合**：将新提取的记忆项 {m1, m2, ..., mn} 通过简单的加法操作添加到记忆库 M 中。\nStep 3: **（仅适用于 MaTTS）测试时扩展**：对于当前任务 qi，根据 scaling factor k：\n    - **并行扩展**：在步骤 2b 中，并行生成 k 条轨迹 {τi1, τi2, ..., τik}。对所有轨迹进行自我评估和记忆构建，最终任务结果采用 Best-of-N (BoN) 策略从 k 条轨迹中选择最佳。\n    - **顺序扩展**：在生成初始轨迹 τi 后，进行 k-1 次自我精炼迭代。每次迭代基于前一次的结果和中间推理笔记生成新的轨迹版本。所有迭代版本（包括中间笔记）都用于记忆构建。\nStep 4: **输出**：任务 qi 的最终结果，并更新记忆库 M。循环处理下一个任务 qi+1。\n\n**§2 关键超参数与配置**\n1.  **Top-k 检索数量 (k)**：在记忆检索步骤中，从 Reason ingBank 中检索的最相关记忆项数量。论文中未明确给出具体数值，但这是一个关键超参数，影响注入到系统指令中的记忆数量。\n2.  **扩展因子 (Scaling Factor k)**：在 MaTTS 中，k 表示并行扩展生成的轨迹数量，或顺序扩展的迭代精炼步数。实验中使用 k ∈ {1, 3, 5}。k=1 即无扩展的基线。作者通过消融实验（图4）展示了随着 k 增加，性能提升的趋势，从而论证了分配更多推理时计算的好处。\n3.  **LLM-as-a-judge 模型**：用于自我评估轨迹成功/失败的 LLM。论文未指定具体模型，但使用了与智能体主干模型可能不同的 LLM 作为评判者。\n4.  **嵌入模型**：用于记忆检索的语义相似性搜索的嵌入模型。论文未指定具体模型，但提到使用“embedding-based similarity search”。\n\n**§3 训练/微调设置（如有）**\n本文工作在**测试时学习（test-time learning）** 范式下进行，**没有进行任何离线训练或微调**。所有学习都发生在任务流处理过程中，智能体仅利用其自身的过去轨迹和任何自我验证，不依赖外部标签。因此，没有传统的训练数据、优化器、学习率等设置。\n\n**§4 推理阶段的工程细节**\n1.  **环境与动作空间**：\n    -   **网页浏览任务**：使用 BrowserGym 环境，动作空间 A 是一组网页导航操作（如点击、输入）。观察 ot 是基于文本的网页可访问性树（accessibility tree）。\n    -   **软件工程任务**：使用 bash-only 环境（SWE-Bench-Verified），动作空间 A 是 bash 命令。观察 ot 是代码片段。\n2.  **智能体策略**：遵循 ReAct 风格，具有默认的解码配置。策略 π_ℒ 由主干 LLM（Gemini-2.5-flash/pro, Claude-3.7-sonnet）参数化。\n3.  **记忆存储与检索**：记忆库 Reason ingBank 存储结构化的记忆项。检索使用基于嵌入的相似性搜索，暗示可能使用了向量数据库（如通过 embedding 计算余弦相似度）。论文未明确说明向量数据库的具体选型。\n4.  **并行化**：在 MaTTS 的并行扩展中，需要为同一查询并行生成多条轨迹，这可能需要并行调用 LLM 或使用批处理。论文未详细说明并行化策略。\n5.  **缓存机制**：未提及特定的缓存机制。",
    "experimental_design": "**§1 数据集详情**\n1.  **WebArena**：\n    -   **名称**：WebArena\n    -   **规模**：总计 684 个任务，分为 5 个子集：Shopping (187), Admin (182), Gitlab (180), Reddit (106), Multi (29)。\n    -   **领域类型**：通用网页导航，涵盖购物、管理、代码托管、社交论坛等多个领域。Multi 子集要求跨多个网站的记忆迁移。\n    -   **评测问题类型**：交互式网页任务，如查找信息、执行操作等。\n    -   **数据过滤**：原文未提及特殊的数据剔除或过滤标准。\n2.  **Mind2Web**：\n    -   **名称**：Mind2Web\n    -   **规模**：未在正文中给出具体样本数，但分为三个泛化测试集：Cross-Task (252), Cross-Website (177), Cross-Domain (912)。数字可能代表任务数量。\n    -   **领域类型**：网页任务自动化，测试智能体在不同操作和环境下的泛化能力。\n    -   **评测问题类型**：跨任务、跨网站、跨领域的泛化测试，难度递增。\n    -   **数据过滤**：原文未提及。\n3.  **SWE-Bench-Verified**：\n    -   **名称**：SWE-Bench-Verified\n    -   **规模**：未在正文中给出具体样本数。\n    -   **领域类型**：软件工程，仓库级别的 issue 解决。\n    -   **评测问题类型**：给定代码仓库和 issue 描述，智能体需要生成正确的 bash 命令序列来解决问题。\n    -   **数据过滤**：原文未提及。\n\n**§2 评估指标体系**\n-   **准确性指标**：\n    1.  **成功率（Success Rate, SR ↑）**：任务级别的成功率，衡量是否所有步骤都正确（WebArena, Mind2Web）。在 SWE-Bench-Verified 中称为解决率（Resolve Rate）。\n    2.  **元素准确率（Element Accuracy, EA ↑）**：Mind2Web 专用，衡量单个交互步骤中目标元素识别的准确性。\n    3.  **动作 F1 分数（Action F1, AF1 ↑）**：Mind2Web 专用，衡量动作预测的 F1 分数。\n    4.  **步骤成功率（Step Success Rate, SSR ↑）**：Mind2Web 专用，衡量每个步骤的成功率。\n-   **效率/部署指标**：\n    1.  **平均交互步数（Average Interaction Steps, Step ↓）**：完成一个任务所需的平均动作步骤数。步数越少，效率越高。\n-   **其他自定义指标**：\n    1.  **Best-of-N (BoN)**：在 MaTTS 并行扩展中，从 N 条轨迹中选择最佳结果作为最终输出，用于衡量扩展的有效性。\n    2.  **Pass@1**：随机选择一条轨迹作为输出，用于衡量记忆提炼后轨迹的平均质量（特别是在评估扩展对记忆质量的反哺时）。\n\n**§3 对比基线（完整枚举）**\n1.  **No Memory**：\n    -   **类型**：无记忆的智能体基线。\n    -   **底座模型**：与本文方法使用相同的主干 LLM（Gemini-2.5-flash/pro, Claude-3.7-sonnet）。\n    -   **代表性**：代表了智能体在孤立处理每个任务时的性能下限，用于评估记忆机制带来的绝对增益。\n2.  **Synapse**：\n    -   **类型**：基于轨迹的记忆（Trajectory-based Memory）。存储原始的交互轨迹以供重用。\n    -   **底座模型**：与本文方法使用相同的主干 LLM。\n    -   **代表性**：代表了直接重用原始经验的主流记忆方法，是本文方法在“存储内容”上的直接对比对象。\n3.  **AWM (Agent Workflow Memory)**：\n    -   **类型**：基于工作流的记忆（Workflow-based Memory）。仅从成功经验中提取通用的工作流程或程序。\n    -   **底座模型**：与本文方法使用相同的主干 LLM。\n    -   **代表性**：代表了仅从成功中学习、提取高层流程的记忆方法，是本文方法在“学习来源”（成功 vs. 成功+失败）上的直接对比对象。\n\n**§4 实验控制变量与消融设计**\n1.  **主干模型消融**：在三个不同的主干 LLM（Gemini-2.5-flash, Gemini-2.5-pro, Claude-3.7-sonnet）上评估所有方法，以证明方法的普适性。\n2.  **记忆内容消融**：在 WebArena-Shopping 子集上，对比了仅使用成功轨迹构建记忆与同时使用成功和失败轨迹构建记忆的效果（图7），以验证从失败中学习的价值。\n3.  **测试时扩展策略消融**：在 MaTTS 实验中，设置了多个对照：\n    -   **MaTTS w/o memory**：无任何记忆机制的测试时扩展基线。\n    -   **MaTTS w/o aggregation (Vanilla TTS)**：普通的测试时扩展，即独立生成多条轨迹并分别转化为记忆，没有记忆感知的协调与聚合（对应图3a）。\n    -   **MaTTS (Full)**：完整的记忆感知测试时扩展，包含并行或顺序策略下的对比信号利用。\n    通过比较这些变体，可以分离出记忆机制和扩展策略各自的贡献，以及它们之间的协同效应。\n4.  **扩展因子 k 的消融**：通过改变 k 值（1, 3, 5），绘制性能曲线（图4），展示了分配更多计算资源（更大的 k）对性能的影响，并比较了并行扩展和顺序扩展在不同 k 下的表现。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n**表1 WebArena 总体结果 (Success Rate / Steps)**\n`方法名 | Shopping-SR | Shopping-Step | Admin-SR | Admin-Step | Gitlab-SR | Gitlab-Step | Reddit-SR | Reddit-Step | Multi-SR | Multi-Step | Overall-SR | Overall-Step`\n`Gemini-2.5-flash No Memory | 39.0 | 8.2 | 44.5 | 9.5 | 33.9 | 13.3 | 55.7 | 6.7 | 10.3 | 10.0 | 40.5 | 9.7`\n`Gemini-2.5-flash Synapse | 40.6 | 7.0 | 45.1 | 9.1 | 35.6 | 13.0 | 59.4 | 6.5 | 10.3 | 10.5 | 42.1 | 9.2`\n`Gemini-2.5-flash AWM | 44.4 | 7.0 | 46.7 | 8.8 | 37.2 | 13.2 | 62.3 | 6.1 | 3.4 | 7.7 | 44.1 | 9.0`\n`Gemini-2.5-flash REASONINGBANK | 49.7 | 6.1 | 51.1 | 8.2 | 40.6 | 12.3 | 67.0 | 5.6 | 13.8 | 8.8 | 48.8 | 8.3`\n`Gemini-2.5-pro No Memory | 45.5 | 7.6 | 51.1 | 8.7 | 35.0 | 11.6 | 71.7 | 6.0 | 6.9 | 8.8 | 46.7 | 8.8`\n`Gemini-2.5-pro Synapse | 46.5 | 6.6 | 52.2 | 8.9 | 38.3 | 11.3 | 68.9 | 5.9 | 6.9 | 9.0 | 47.7 | 8.5`\n`Gemini-2.5-pro AWM | 48.1 | 6.4 | 49.3 | 9.8 | 40.0 | 11.2 | 68.9 | 6.4 | 3.4 | 9.3 | 47.6 | 8.7`\n`Gemini-2.5-pro REASONINGBANK | 51.9 | 6.0 | 56.6 | 7.7 | 44.4 | 9.8 | 80.2 | 5.1 | 13.8 | 8.2 | 53.9 | 7.4`\n`Claude-3.7-sonnet No Memory | 38.5 | 6.1 | 49.5 | 8.4 | 36.7 | 10.6 | 53.8 | 5.5 | 0.0 | 11.6 | 41.7 | 8.0`\n`Claude-3.7-sonnet Synapse | 39.6 | 5.8 | 50.5 | 8.5 | 38.0 | 10.0 | 53.8 | 6.1 | 0.0 | 11.8 | 42.6 | 7.9`\n`Claude-3.7-sonnet AWM | 39.6 | 7.2 | 47.8 | 9.3 | 34.6 | 10.9 | 52.8 | 7.0 | 0.0 | 12.4 | 40.8 | 8.9`\n`Claude-3.7-sonnet REASONINGBANK | 44.9 | 5.6 | 53.3 | 7.6 | 41.1 | 9.5 | 57.5 | 5.2 | 3.4 | 10.5 | 46.3 | 7.3`\n\n**表2 SWE-Bench-Verified 结果 (Resolve Rate / Steps)**\n`方法名 | Resolve Rate | Step`\n`Gemini-2.5-flash No Memory | 34.2 | 30.3`\n`Gemini-2.5-flash Synapse | 35.4 | 30.7`\n`Gemini-2.5-flash REASONINGBANK | 38.8 | 27.5`\n`Gemini-2.5-pro No Memory | 54.0 | 21.1`\n`Gemini-2.5-pro Synapse | 53.4 | 21.0`\n`Gemini-2.5-pro REASONINGBANK | 57.4 | 19.8`\n\n**表3 Mind2Web 结果 (EA / AF1 / SSR / SR)**\n`方法名 | Cross-Task-EA | Cross-Task-AF1 | Cross-Task-SSR | Cross-Task-SR | Cross-Website-EA | Cross-Website-AF1 | Cross-Website-SSR | Cross-Website-SR | Cross-Domain-EA | Cross-Domain-AF1 | Cross-Domain-SSR | Cross-Domain-SR`\n`Gemini-2.5-flash No Memory | 46.0 | 59.1 | 40.3 | 3.3 | 39.8 | 45.1 | 31.7 | 1.7 | 35.8 | 37.9 | 31.9 | 1.0`\n`Gemini-2.5-flash Synapse | 47.0 | 59.5 | 41.2 | 3.5 | 40.3 | 46.0 | 32.1 | 1.9 | 36.3 | 38.5 | 32.4 | 1.1`\n`Gemini-2.5-flash AWM | 46.3 | 56.1 | 41.0 | 3.5 | 39.1 | 42.2 | 31.7 | 2.1 | 33.3 | 36.5 | 30.1 | 0.7`\n`Gemini-2.5-flash REASONINGBANK | 52.1 | 60.4 | 44.9 | 4.8 | 44.3 | 52.6 | 33.9 | 2.3 | 40.6 | 41.3 | 36.6 | 1.6`\n`Gemini-2.5-pro No Memory | 49.3 | 60.2 | 44.4 | 3.5 | 41.2 | 49.8 | 34.8 | 3.4 | 37.9 | 37.7 | 35.0 | 1.4`\n`Gemini-2.5-pro Synapse | 50.1 | 61.0 | 44.7 | 3.6 | 41.8 | 51.2 | 35.0 | 3.2 | 38.5 | 39.8 | 35.6 | 1.5`\n`Gemini-2.5-pro AWM | 48.6 | 61.2 | 44.4 | 3.7 | 41.9 | 47.9 | 34.8 | 2.3 | 37.3 | 38.1 | 34.4 | 1.2`\n`Gemini-2.5-pro REASONINGBANK | 53.6 | 62.7 | 45.6 | 5.1 | 46.1 | 54.8 | 36.9 | 3.8 | 42.8 | 45.2 | 38.1 | 1.7`\n\n**§2 分任务/分场景深度分析**\n-   **跨模型一致性**：Reason ingBank 在 Gemini-2.5-flash、Gemini-2.5-pro 和 Claude-3.7-sonnet 三个不同主干模型上均一致地优于所有基线，证明了其方法的鲁棒性和模型无关性。\n-   **跨领域泛化**：在 WebArena 的 **Multi** 子集（需要跨网站记忆迁移）上，Reason ingBank 表现尤为突出。例如，使用 Gemini-2.5-pro 时，Reason ingBank 的 SR 为 13.8，而 No Memory 为 6.9，Synapse 为 6.9，AWM 甚至下降至 3.4。这表明 Reason ingBank 提炼的记忆具有更强的**跨任务可迁移性**，而基线方法（尤其是 AWM）在此类泛化要求高的场景下失效。\n-   **Mind2Web 泛化测试**：在 Cross-Task、Cross-Website、Cross-Domain 三个难度递增的设置下，Reason ingBank 在所有指标（EA, AF1, SSR, SR）上均取得最佳性能。特别是在最难的 **Cross-Domain** 设置下，提升最为明显（例如 Gemini-2.5-pro 上 EA 从 37.9 提升至 42.8，绝对提升 4.9 个点），进一步证实了其记忆的泛化能力。\n-   **软件工程任务**：在 SWE-Bench-Verified 上，Reason ingBank 在解决率（Resolve Rate）和效率（Steps）上均优于基线。使用 Gemini-2.5-pro 时，解决率从 No Memory 的 54.0 提升至 57.4（相对提升 6.3%），步数从 21.1 减少至 19.8（减少 6.2%）。\n\n**§3 效率与开销的定量对比**\n-   **交互步数减少**：Reason ingBank 在所有数据集和模型上均减少了平均交互步数，表明其提高了任务解决效率。具体来说：\n    -   在 WebArena 总体（Overall）上，相比 No Memory 基线，Gemini-2.5-flash 版本步数从 9.7 降至 8.3（减少 1.4 步，相对减少 14.4%），Gemini-2.5-pro 版本从 8.8 降至 7.4（减少 1.4 步，相对减少 15.9%），Claude-3.7-sonnet 版本从 8.0 降至 7.3（减少 0.7 步，相对减少 8.8%）。\n    -   在 SWE-Bench-Verified 上，Gemini-2.5-flash 版本步数从 30.3 降至 27.5（减少 2.8 步，相对减少 9.2%），Gemini-2.5-pro 版本从 21.1 降至 19.8（减少 1.3 步，相对减少 6.2%）。\n-   **成功与失败案例的步数分析**：表4显示，Reason ingBank 在**成功案例**上的步数减少更为显著。例如在 WebArena-Shopping 子集，成功案例步数从 6.8 降至 4.7（减少 2.1 步，相对减少 30.9%），而失败案例从 8.7 降至 7.3（减少 1.4 步，相对减少 16.1%）。这表明 Reason ingBank 主要通过**强化有效推理路径**来减少不必要的探索，而非简单地提前终止失败尝试。\n\n**§4 消融实验结果详解**\n1.  **失败轨迹纳入的影响（图7）**：在 WebArena-Shopping (Gemini-2.5-flash) 上，对比仅使用成功轨迹与同时使用成功和失败轨迹构建记忆：\n    -   **Synapse**：SR 从 40.6（仅成功）提升至 41.7（含失败），仅提升 1.1 个点。\n    -   **AWM**：SR 从 44.4 下降至 42.2，下降 2.2 个点（性能退化）。\n    -   **Reason ingBank**：SR 从 46.5（仅成功）提升至 49.7（含失败），提升 3.2 个点。\n    **结论**：Reason ingBank 是唯一能有效利用失败轨迹提升性能的方法，而基线方法要么增益微弱，要么性能下降。\n2.  **MaTTS 扩展因子 k 的影响（图4）**：在 WebArena-Shopping (Gemini-2.5-flash) 上，k 从 1 增加到 5：\n    -   **MaTTS (Parallel)**：SR 从 49.7 (k=1) 提升至 55.1 (k=5)，绝对提升 5.4 个点。\n    -   **MaTTS (Sequential)**：SR 从 49.7 (k=1) 提升至 54.5 (k=5)，绝对提升 4.8 个点。\n    -   **MaTTS w/o memory (Parallel)**：SR 在 39.0 到 42.2 之间波动，提升不稳定且有限。\n    -   **MaTTS w/o memory (Sequential)**：SR 在 37.4 到 40.6 之间波动。\n    **结论**：增加计算资源（k）能提升性能，但**必须与高质量记忆（Reason ingBank）结合**才能实现稳定且显著的提升。\n3.  **记忆与扩展的协同效应（图5，k=3）**：\n    -   **BoN（衡量扩展有效性）**：No Memory 从 39.0 提升至 40.6；Synapse 从 40.6 提升至 42.8；AWM 从 44.4 提升至 45.5；Reason ingBank 从 49.7 提升至 52.4。**Reason ingBank 的提升幅度最大（+2.7）**。\n    -   **Pass@1（衡量记忆质量）**：Synapse 从 40.6 降至 40.1；AWM 从 44.4 降至 41.2；而 Reason ingBank 从 49.7 提升至 50.8。**只有 Reason ingBank 能利用扩展提升记忆质量**，其他方法因扩展引入噪声而导致性能下降。\n\n**§5 案例分析/定性分析（如有）**\n论文通过图6展示了一个记忆项随时间演化的案例研究，说明了**涌现行为（emergent behaviors）**：\n-   **初始阶段**：记忆项包含面向执行的策略（如“查找导航链接”），智能体遵循直接的动作规则。\n-   **进化阶段**：随着经验积累，同一记忆项演化为自适应自反思（如“重新验证标识符以减少简单错误”）。\n-   **成熟阶段**：最终发展为组合策略（如“交叉参考任务需求并重新评估选项”）。\n这个案例表明，Reason ingBank 使智能体能够在测试时学习过程中，将策略从低层次动作**精炼（refine）**为高层次推理，体现了其自我进化的能力。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出了 Reason ingBank**：一个新颖的记忆框架，能够从智能体自我判断的**成功和失败经验**中提炼通用的、策略级的推理信号，超越了仅限于原始轨迹或仅成功流程的现有工作。实验证明，该方法在 WebArena、Mind2Web 和 SWE-Bench-Verified 上 consistently 提升了效果（SR 提升最高达 +8.3 个点）和效率（步数减少最多达 1.6 步）。\n2.  **引入了 MaTTS (Memory-aware Test-Time Scaling)**：将记忆与测试时扩展相结合，建立了二者之间的强大协同效应。实验表明，高质量的记忆引导扩展走向更有希望的路径，而丰富的扩展经验则锻造出更强的记忆，从而将**记忆驱动的经验扩展**确立为智能体的一个新扩展维度。\n3.  **实证了从失败中学习的价值**：通过消融实验证明，Reason ingBank 是唯一能有效利用失败轨迹提升性能的记忆机制（在 WebArena-Shopping 上，纳入失败轨迹后 SR 从 46.5 提升至 49.7），而基线方法则表现不佳甚至性能下降。\n4.  **展示了涌现的自我进化行为**：案例分析表明，Reason ingBank 中的记忆项能够随时间演化，从低层动作策略发展为高层组合推理策略，类似于强化学习中的学习动态，使智能体具备持续进化的能力。\n\n**§2 局限性（作者自述）**\n原文在正文结论部分未明确列出局限性，但在摘要和引言中暗示了以下潜在限制，需参考附录 D 和 E（原文未提供）：\n1.  **依赖 LLM-as-a-judge**：记忆构建依赖于 LLM 对轨迹成功/失败的自我评估，这可能引入评估错误，特别是在模糊或复杂的失败案例中。\n2.  **测试时学习范式的限制**：所有学习发生在测试时，没有离线训练，这可能限制了从大规模历史数据中批量学习的能力。\n3.  **计算开销**：MaTTS 需要为每个任务生成多条轨迹或进行多次迭代精炼，增加了推理时的计算成本（虽然提升了性能）。\n4.  **记忆库规模管理**：随着时间推移，记忆库可能无限增长，论文中使用的“简单加法”整合策略可能需要更复杂的管理机制（如压缩、遗忘）来处理大规模部署。\n\n**§3 未来研究方向（全量提取）**\n原文在结论中提及“additional future directions and limitations in Appendix D and E”，但附录未提供。根据正文内容，可推断出以下方向：\n1.  **构建自适应和终身学习智能体**：将 Reason ingBank 和 MaTTS 应用于更长期、更复杂的现实世界智能体部署场景，研究其持续学习能力和稳定性。\n2.  **探索记忆与扩展的其他协同形式**：研究除了并行和顺序扩展之外，其他形式的测试时计算分配如何与记忆机制产生协同，例如分层扩展或动态资源分配。\n3.  **改进记忆评估与提炼**：研究更可靠、更高效的轨迹自我评估方法，以及更精细的记忆项提炼策略，以减少噪声并提高记忆质量。\n4.  **处理大规模记忆库**：研究记忆项的压缩、聚类、优先级排序和遗忘机制，以应对长期运行中记忆库无限增长的问题。\n5.  **扩展到更多领域和任务**：将框架应用于网页浏览和软件工程之外的其他交互式任务领域，如机器人控制、游戏等，验证其通用性。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **理论新颖性**：首次提出了**从失败中学习**作为智能体记忆构建的核心原则，并通过结构化记忆项（标题、描述、内容）的形式，实现了对成功和失败经验中通用推理策略的提炼。这突破了现有记忆工作仅关注成功经验或原始轨迹的局限。\n2.  **实验验证充分性**：在三个具有挑战性的基准测试（WebArena, Mind2Web, SWE-Bench-Verified）和三个不同的主干 LLM 上进行了全面实验，不仅证明了方法在效果（成功率）和效率（步数）上的显著提升，还通过细致的消融实验（失败轨迹纳入、扩展因子分析、记忆-扩展协同）验证了每个核心组件的有效性。\n3.  **对领域的影响**：提出了**记忆驱动的经验扩展**这一新的智能体扩展维度，揭示了记忆与测试时计算分配之间的协同效应，为未来研究如何更高效地利用计算资源进行持续学习指明了方向。这项工作可能推动智能体从静态工具向动态、自我进化系统的范式转变。\n\n**§2 工程与实践贡献**\n1.  **系统设计**：设计并实现了一个完整的、闭环的智能体记忆框架（Reason ingBank），包含检索、构建、整合三个模块，并与测试时扩展（MaTTS）无缝集成。该系统设计清晰，模块化程度高，易于理解和复现。\n2.  **评测基准**：在多个主流的交互式智能体基准上进行了系统评测，为后续记忆相关研究提供了详细的性能基线和方法对比。\n3.  **开源与复现**：论文未明确声明代码是否开源，但详细的实验设置和模块描述为复现提供了基础。\n\n**§3 与相关工作的定位**\n本文在当前技术路线图中处于**记忆增强智能体**和**测试时学习/扩展**两条研究路线的交叉点。它并非对现有记忆机制（如轨迹记忆、工作流记忆）的简单改进，而是开辟了一条**以推理策略提炼和失败学习为核心**的新记忆范式。同时，它将记忆机制深度融入测试时扩展过程，提出了 MaTTS，这是在智能体领域对测试时扩展概念的重要拓展，强调了**记忆质量与计算资源分配的协同优化**，而非单纯增加计算量。因此，本文是连接记忆学习与计算扩展的关键桥梁工作。",
    "professor_critique": "**§1 实验设计与评估体系的缺陷**\n1.  **数据集覆盖的局限性**：实验主要集中在网页浏览（WebArena, Mind2Web）和软件工程（SWE-Bench）任务。这些任务虽然具有交互性，但**缺乏对更复杂、需要长期规划或世界模型推理的任务的测试**，如战略游戏、物理模拟或需要多轮复杂谈判的对话场景。在这些场景下，从失败中提炼通用策略的假设可能不成立。\n2.  **评估指标的“幸运”可能**：主要评估指标是任务成功率（SR）和平均步数（Step）。然而，**未评估记忆库本身的质**量，例如记忆项的准确性、冗余度、检索相关性。可能存在“指标幸运”：即使成功率提升，也可能是由于记忆检索注入了无关但幸运的“提示”，而非真正的策略泛化。需要设计针对记忆质量的直接评估（如人工评估记忆项的有用性）。\n3.  **基线对比的完整性**：虽然对比了 Synapse 和 AWM，但**未与更近期的、可能更强的记忆方法（如基于强化学习的记忆管理、基于图的记忆组织）进行对比**。这削弱了其声称的“一致优于现有方法”的结论。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **记忆项提炼的可靠性依赖 LLM 评判**：记忆构建的核心步骤是使用 LLM-as-a-judge 来标记轨迹的成功/失败。这引入了**评判幻觉（judgment hallucination）** 的风险。如果评判错误，可能会将错误的策略作为成功经验存储，或将本可成功的轨迹误判为失败而提炼出错误的“陷阱”，导致记忆污染和错误传播。\n2.  **记忆库无限增长的工程挑战**：论文采用“简单的加法操作”整合新记忆项，**缺乏记忆管理机制**（如重要性评分、压缩、遗忘）。在长期部署中，记忆库会无限膨胀，导致检索效率下降（检索 top-k 可能返回大量陈旧或冗余项）和存储成本增加。当记忆库超过百万条时，基于嵌入的相似性检索的精度和速度可能会严重退化。\n3.  **对任务分布偏移的脆弱性**：该方法在**流式任务**设定下学习。如果任务分布发生剧烈偏移（domain shift），例如从购物网站突然切换到完全不同的金融交易界面，之前提炼的“导航策略”可能失效甚至产生误导。系统缺乏检测和适应这种分布偏移的机制。\n\n**§3 未经验证的边界场景**\n1.  **多语言/混合语言输入**：所有实验均在英语环境下进行。当用户查询或界面元素包含**混合语言（如中英混杂）或非英语**时，基于嵌入的检索和 LLM 的自我评估性能可能显著下降，记忆机制可能失效。\n2.  **对抗性/恶意输入**：如果用户提供**对抗性查询**故意诱导智能体进入失败循环，系统可能会不断从这些“恶意失败”中提炼出有害的记忆项，污染整个记忆库，甚至导致智能体行为被“毒化”。论文未测试此类场景的鲁棒性。\n3.  **极端稀疏奖励/延迟奖励任务**：在需要多步探索才能获得奖励（或成功信号）的任务中，**短期失败是必然的**。Reason ingBank 从每次“失败”的中间步骤中提炼记忆的能力未经测试。它可能过早地将探索性失败标记为负面经验，从而抑制了必要的探索。\n\n**§4 可复现性与公平性问题**\n1.  **依赖昂贵的商业 LLM**：实验使用了 Gemini-2.5 和 Claude-3.7 等强大的商业 LLM 作为主干和评判者。这让**资源有限的研究者难以复现**结果，因为 API 调用成本高昂，且模型行为可能随时间变化。未提供使用较小开源模型（如 Llama 3.1 8B）的可行性验证。\n2.  **超参数调优的公平性**：论文未详细说明 **top-k 检索数量、LLM-as-a-judge 的具体提示词、记忆项提取的提示词** 等关键超参数。如果这些参数是针对本文方法进行过精心调优的，而基线方法（Synapse, AWM）使用了默认或次优参数，则对比可能不公平。需要公开所有提示词和超参数设置以确保公平比较。\n3.  **计算成本对比不完整**：虽然报告了步数减少（效率提升），但**未报告 MaTTS 带来的额外计算开销**（例如，k=5 时并行扩展需要 5 倍的 LLM 调用）。对于预算有限的应用，这种 trade-off 至关重要，但论文未进行详细的成本-效益分析。",
    "zero_compute_opportunity": "#### 蓝图一：探究轻量级记忆提炼：在小型开源模型上复现 Reason ingBank 的核心增益\n-   **核心假设**：Reason ingBank 的核心优势（从失败中学习、策略级抽象）不依赖于超大商业 LLM，可以在参数量小得多的开源模型（如 Llama 3.2 3B）上通过精心设计的提示工程和轻量级微调实现，从而大幅降低部署成本。\n-   **与本文的关联**：基于本文发现 Reason ingBank 在多个模型上有效，但未验证其在小型模型上的可行性。资源受限的研究者无法承担 Gemini/Claude 的 API 费用。\n-   **所需资源**：\n    1.  使用 Hugging Face 上免费的 **Llama 3.2 3B Instruct** 模型作为智能体主干和评判者。\n    2.  使用 **WebArena 或 Mind2Web 的公开子集**（部分任务）进行实验。\n    3.  使用 **SentenceTransformers** 库中的免费轻量级嵌入模型（如 all-MiniLM-L6-v2）进行记忆检索。\n    4.  预计成本：零 API 费用，仅需个人电脑 GPU（如 RTX 4090）或 Google Colab 免费额度。\n-   **执行步骤**：\n    1.  **复现基线**：在选定的开源模型上实现无记忆（No Memory）和简单的轨迹记忆（Synapse-lite）基线。\n    2.  **实现 Reason ingBank-lite**：\n        -   简化记忆项模式：仅保留“标题”和“关键教训”两个字段。\n        -   使用相同的开源模型进行自我评估（成功/失败），设计简洁的提示词。\n        -   实现基于轻量级嵌入模型的检索。\n    3.  **对比实验**：在选定数据集上比较 No Memory、Synapse-lite 和 Reason ingBank-lite 的性能（成功率、步数）。\n    4.  **消融研究**：测试仅使用成功轨迹 vs. 使用成功+失败轨迹对小型模型的影响。\n-   **预期产出**：一篇短论文或技术报告，验证小型模型上记忆提炼的有效性，分析性能差距与模型规模的关系。可投稿到 **EMNLP Findings** 或 **AACL-IJCNLP** 等会议。\n-   **潜在风险**：小型模型的推理和评判能力较弱，可能导致记忆构建质量低下，甚至产生负面效果。应对方案：可以尝试对小型模型在相关任务上进行**轻量级的指令微调（LoRA）**，以提升其轨迹分析和策略提炼能力。\n\n#### 蓝图二：设计基于重要性采样的记忆库压缩策略\n-   **核心假设**：Reason ingBank 中简单的“加法”整合会导致记忆库无限膨胀。可以设计一种基于**记忆项使用频率、成功贡献度、时间新鲜度**的重要性评分机制，动态淘汰低重要性记忆，从而在保持性能的同时控制记忆库规模，适用于边缘设备部署。\n-   **与本文的关联**：本文指出了记忆库随任务流增长，但未解决管理问题。这是工程部署的关键瓶颈。\n-   **所需资源**：\n    1.  使用本文已发布的实验日志（如果开源）或自行在小型数据集（如 WebArena-Shopping）上运行 Reason ingBank 收集记忆项使用数据。\n    2.  需要记录每个记忆项的**检索次数、被检索后任务的成功率变化、创建时间**等元数据。\n    3.  计算资源：个人电脑即可完成模拟和分析。\n-   **执行步骤**：\n    1.  **数据收集**：运行 Reason ingBank 实验，记录每个记忆项的元数据。\n    2.  **设计评分函数**：例如，Score(m",
    "source_file": "ReasoningBank Scaling Agent Self-Evolving with Reasoning Memory.md"
}