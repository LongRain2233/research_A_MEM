{
    "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n数学推理是大语言模型（LLM）面临的核心挑战之一，因其需要复杂的逻辑推导和结构化思维。近年来，以GPT-4和Gemini Ultra为代表的闭源模型在MATH等竞赛级数学基准上取得了突破性进展，但其技术细节不公开。开源模型（如Mistral、Llemma）在数学能力上显著落后。本研究旨在探索：**如何在资源受限（7B参数量）的开源模型上，通过高质量数据工程和高效的强化学习算法，逼近顶级闭源模型的数学推理性能**。该研究动机在于，为研究社区提供一个可复现、可分析的数学专用语言模型案例，以证明通过精心设计的数据和训练方法，小模型也能在复杂推理任务上取得卓越表现。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法在数学推理任务上存在明显短板，具体表现为：\n1.  **数据质量与规模不足**：现有开源数学预训练语料（如OpenWebMath 13.6B tokens, Proof-Pile-2 51.9B tokens）规模有限且多为英文。当模型在这些语料上训练时，在中文数学基准（如CMATH）上表现不佳，甚至可能产生负面影响（如表1所示，在MathPile上训练的模型，CMATH准确率从12.3%降至1.2%）。\n2.  **模型规模依赖性强**：先前工作（如Minerva）认为需要极大模型（540B参数）才能达到高数学能力。当使用较小模型（如7B）时，现有方法（如仅使用arXiv论文训练）在MATH等竞赛级数据集上准确率远低于30%，无法与闭源模型竞争。\n3.  **强化学习效率低下**：标准的近端策略优化（PPO）方法需要训练一个与策略模型规模相当的评论家（critic）模型，带来了巨大的内存和计算负担。当在资源受限的环境下部署PPO时，训练开销成为瓶颈，限制了其在开源社区的应用。\n4.  **指令微调数据覆盖不全**：许多开源数学模型（如WizardMath、MetaMath）的指令微调数据主要来源于GSM8K和MATH。当面对领域外（out-of-domain）的中文数学任务或需要工具使用的场景时，这些模型的泛化能力有限，性能提升不明显。\n\n**§3 问题的根本难点与挑战（200字以上）**\n数学推理能力提升的根本难点在于：\n1.  **高质量数学数据的稀缺性与高噪声**：互联网Common Crawl数据中蕴含大量数学相关内容，但与非数学文本混杂，且质量参差不齐。构建一个大规模、高质量、多语言的数学语料库需要极高的数据清洗和标注成本。\n2.  **数学知识的抽象性与结构化**：数学问题涉及从算术到微积分等多个抽象层级，需要模型理解符号、公式和逻辑关系。单纯的文本预测训练难以让模型掌握严格的推导链条。\n3.  **强化学习中的信用分配难题**：在数学推理的多步生成中，仅对最终答案进行奖励（结果监督）难以有效指导中间推理步骤的优化。而过程监督需要精细的逐步骤奖励标注，成本高昂且难以扩展。\n4.  **计算资源与性能的权衡**：为了达到高性能，传统方法倾向于使用更大的模型（如540B）和更复杂的RL算法（如PPO带评论家模型），这与开源社区和资源有限研究者的算力条件相悖。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的核心切入点是**数据质量优先于模型规模**，并假设**高效的、无评论家模型的强化学习算法可以显著提升小模型的数学推理能力**。具体而言：\n1.  **数据工程假设**：作者假设Common Crawl中存在大量未被充分利用的高质量数学网页，通过一个迭代的、基于分类器（fastText）和人工标注的流水线，可以将其有效提取出来，构建远超现有规模的数学语料库（DeepSeekMath Corpus，120B tokens）。\n2.  **代码训练有益假设**：作者假设在数学训练之前进行代码预训练，能够提升模型的逻辑和结构化思维能力，从而有益于数学推理（无论是否使用工具）。这通过使用DeepSeek-Coder-Base-v1.5 7B作为初始化模型来验证。\n3.  **强化学习简化假设**：作者假设在LLM的RL中，由于奖励通常只分配给序列的最后一个token，训练一个精确的逐token价值函数是复杂且不必要的。因此，可以摒弃PPO中的评论家模型，转而使用**组内相对优势估计**（Group Relative Advantage Estimation），利用同一问题下多个采样输出的平均奖励作为基线，大幅降低训练资源消耗。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\nDeepSeekMath 7B的构建是一个多阶段流程，并非单一架构。整体数据流如下：\n1.  **数据收集与过滤**：输入为Common Crawl原始网页→经过URL去重和近似去重→使用基于OpenWebMath训练的fastText分类器进行初步筛选→人工标注新增数学域名下的URL以扩充正例→迭代更新分类器并收集更多数据→最终输出120B token的DeepSeekMath语料库。\n2.  **继续预训练**：输入为DeepSeek-Coder-Base-v1.5 7B模型权重和混合数据（56% DeepSeekMath语料，4% AlgebraicStack代码，10% arXiv论文，20% Github代码，10%中英文自然语言）→在HAI-LLM框架下进行500B token的继续预训练→输出DeepSeekMath-Base 7B基座模型。\n3.  **监督指令微调（SFT）**：输入DeepSeekMath-Base 7B和776K条数学指令数据（包含CoT、PoT、工具集成格式）→以5e-5恒定学习率训练500步，批次大小256，上下文长度4K→输出DeepSeekMath-Instruct 7B指令模型。\n4.  **强化学习（GRPO）**：输入DeepSeekMath-Instruct 7B作为初始策略模型、基于DeepSeekMath-Base训练的奖励模型、以及来自GSM8K和MATH的约144K条CoT格式问题→执行**迭代组相对策略优化（Iterative GRPO）**→输出最终模型DeepSeekMath-RL 7B。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：迭代数据收集管道（Iterative Data Collection Pipeline）\n-   **输入**：Common Crawl的40B个HTML网页（经URL去重和近似去重后）。\n-   **核心处理逻辑**：\n    1.  **种子语料**：使用OpenWebMath作为初始正例，随机从Common Crawl选取负例，训练fastText分类器（向量维度256，学习率0.1，n-gram最大长度3，最小词频3，训练3轮）。\n    2.  **迭代收集**：用分类器对Common Crawl网页打分，保留排名最高的网页（第一轮保留top 40B tokens）。\n    3.  **领域扩展**：将整个Common Crawl按域名划分，计算每个域名中被收集网页的比例。**若超过10%的网页被收集，则该域名被标记为数学相关**。人工标注这些域名下的数学内容URL，将未被收集的对应网页加入种子语料。\n    4.  **迭代更新**：用扩充后的种子语料重新训练fastText分类器，进行下一轮收集。经过4轮迭代后停止（第4轮数据98%已存在于第3轮）。\n-   **输出**：35.5M个数学网页，总计120B tokens的DeepSeekMath语料库。\n-   **设计理由**：该设计解决了从海量噪声数据中高效挖掘高质量数学内容的问题。**10%的阈值**用于识别数学相关域名，平衡了召回率和人工标注成本。迭代方法允许逐步提升分类器性能，避免因初始种子数据单一而导致的数据多样性不足。\n\n#### 模块二：组相对策略优化（Group Relative Policy Optimization, GRPO）\n-   **输入**：策略模型 \\(\\pi_{\\theta_{old}}\\)，奖励模型 \\(r_{\\varphi}\\)，问题数据集 \\(\\mathcal{D}\\)，超参数 \\(\\varepsilon, \\beta, \\mu, G\\)。\n-   **核心处理逻辑**：\n    1.  **组采样**：对每个问题 \\(q\\)，从旧策略中采样 \\(G=64\\) 个输出序列 \\(\\{o_1, ..., o_G\\}\\)。\n    2.  **奖励计算**：使用奖励模型为每个输出计算奖励 \\(r_i\\)。对于过程监督，则为每个推理步骤的结束token计算奖励。\n    3.  **优势估计**：**摒弃PPO中的价值函数**。对于结果监督，计算组内归一化奖励：\\(\\tilde{r}_i = \\frac{r_i - \\text{mean}(\\mathbf{r})}{\\text{std}(\\mathbf{r})}\\)，并将序列中所有token的优势值设为 \\(\\hat{A}_{i,t} = \\tilde{r}_i\\)。对于过程监督，对每个步骤的奖励进行同样的归一化，然后token \\(t\\) 的优势值为后续所有步骤归一化奖励之和：\\(\\hat{A}_{i,t} = \\sum_{index(j) \\geq t} \\tilde{r}_i^{index(j)}\\)。\n    4.  **策略更新**：最大化GRPO目标函数（公式3），其中包含重要性采样比、裁剪以及KL散度正则项。KL散度使用无偏估计器（公式4）计算。\n    5.  **迭代更新**：如算法1所示，策略模型更新后，会基于其采样结果生成新的奖励模型训练集，并采用**重放机制（保留10%历史数据）**持续训练奖励模型。同时，将参考模型更新为当前策略模型，进行下一轮迭代。\n-   **输出**：更新后的策略模型参数 \\(\\pi_{\\theta}\\)。\n-   **设计理由**：GRPO的核心创新在于**用组内统计量（均值、标准差）代替需要单独训练的评论家模型**。这基于一个观察：在LLM的RL中，奖励通常是稀疏的（仅最后一个token）或基于步骤比较的，组内相对比较足以提供有效的策略梯度信号。这大幅减少了训练时的显存占用和计算开销。\n\n#### 模块三：训练配置与超参数设置\n-   **输入**：模型架构、训练数据、优化目标。\n-   **核心处理逻辑**：\n    -   **预训练**：使用AdamW优化器（\\(\\beta_1=0.9, \\beta_2=0.95\\)，权重衰减0.1），多步学习率调度（2000步热身，峰值学习率5.3e-4（1.3B模型）或4.2e-4（7B模型），在80%训练步数后降至峰值的31.6%，90%步数后降至10%），批次大小4M tokens（1.3B）或10M tokens（7B），上下文长度4K。\n    -   **SFT**：恒定学习率5e-5，批次大小256，训练500步，最大上下文长度4K。\n    -   **GRPO**：策略模型学习率1e-6，KL系数 \\(\\beta=0.04\\)，每组采样数 \\(G=64\\)，序列最大长度1024，训练批次大小1024。奖励模型基于DeepSeekMath-Base 7B训练，学习率2e-5。\n-   **输出**：训练好的模型检查点。\n-   **设计理由**：预训练采用大批次和长上下文以提升吞吐和模型容量。SFT使用较小的恒定学习率以避免灾难性遗忘。GRPO使用极低的学习率（1e-6）和较强的KL惩罚（0.04）是为了在优化奖励的同时，严格约束策略模型不要过度偏离初始的SFT模型，防止模式崩溃和退化。\n\n**§3 关键公式与算法（如有）**\n1.  **GRPO目标函数**：\n    \\[\n    \\begin{array}{l}\n    \\mathcal{J}_{GRPO}(\\theta) = \\mathbb{E}[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)] \\n    \\frac{1}{G} \\sum_{i=1}^{G} \\frac{1}{|o_i|} \\sum_{t=1}^{|o_i|} \\left\\{\\min\\left[ \\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})} \\hat{A}_{i,t}, \\operatorname{clip}\\left(\\frac{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta_{old}}(o_{i,t}|q, o_{i,<t})}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i,t} \\right] - \\beta \\mathbb{D}_{KL}[\\pi_{\\theta} || \\pi_{ref}] \\right\\},\n    \\end{array}\n    \\]\n    其中 \\(\\hat{A}_{i,t}\\) 为组内相对优势。\n2.  **KL散度无偏估计**：\n    \\[\n    \\mathbb{D}_{KL}[\\pi_{\\theta} || \\pi_{ref}] = \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})} - \\log \\frac{\\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\\pi_{\\theta}(o_{i,t}|q, o_{i,<t})} - 1\n    \\]\n3.  **组内归一化奖励（结果监督）**：\n    \\[\n    \\tilde{r}_i = \\frac{r_i - \\text{mean}(\\mathbf{r})}{\\text{std}(\\mathbf{r})}, \\quad \\hat{A}_{i,t} = \\tilde{r}_i\n    \\]\n4.  **过程监督优势计算**：\n    \\[\n    \\hat{A}_{i,t} = \\sum_{index(j) \\geq t} \\tilde{r}_i^{index(j)}, \\quad \\tilde{r}_i^{index(j)} = \\frac{r_i^{index(j)} - \\text{mean}(\\mathbf{R})}{\\text{std}(\\mathbf{R})}\n    \\]\n    \n**§4 方法变体对比（如有多个变体/消融组件）**\n本文主要提出了一个统一的RL范式理解，并基于GRPO框架探索了不同变体：\n1.  **结果监督（Outcome Supervision）GRPO**：仅对每个输出序列的最后一个token分配奖励，并将该归一化奖励作为序列中所有token的优势值。\n2.  **过程监督（Process Supervision）GRPO**：对输出序列中每个推理步骤的结束token分配奖励，每个token的优势值为其之后所有步骤的归一化奖励之和。这提供了更细粒度的训练信号。\n3.  **迭代RL（Iterative RL）GRPO**：在标准GRPO基础上，引入迭代机制。每轮训练后，用当前策略模型采样生成新的奖励模型训练数据，并持续训练奖励模型（使用10%历史数据重放）。同时，将参考模型更新为当前策略模型。这解决了策略模型进化后旧奖励模型可能失效的问题。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n本文方法与代表性相关工作在技术实现上的本质区别如下：\n-   **与Minerva (Lewkowycz et al., 2022a) 对比**：Minerva使用540B参数的PaLM模型，并在数学文本上继续训练。**本文的核心差异是模型规模和数据来源**。DeepSeekMath仅使用7B参数模型，但构建了规模大7倍（120B vs 17B tokens）、质量更高、且包含多语言（特别是中文）的DeepSeekMath语料库。实验证明，小模型+高质量数据可以媲美甚至超越超大模型。\n-   **与标准PPO (Schulman et al., 2017; Ouyang et al., 2022) 对比**：标准PPO采用Actor-Critic架构，需要训练一个与策略模型规模相当的评论家（价值函数）模型来估计优势。**GRPO完全摒弃了评论家模型**，转而利用同一问题下多个采样输出的奖励统计量（均值、标准差）作为基线来计算相对优势。这消除了训练价值函数的开销和误差，大幅提升了RL训练的效率，更适合资源受限的场景。\n-   **与WizardMath (Luo et al., 2023) / Math-Shepherd (Wang et al., 2023b) 对比**：这些工作也使用PPO进行数学RL微调，但它们的奖励模型训练和RL流程通常基于特定数据集（如GSM8K/MATH），且未强调领域外泛化。**本文的GRPO在仅使用GSM8K和MATH的CoT数据训练后，在多个未见的领域外中文基准（如CMATH）上也取得了显著提升**，证明了GRPO方法的有效泛化能力。同时，本文提供了对结果监督与过程监督、在线与离线训练等要素的深入实验分析，而WizardMath等并未系统探讨这些维度。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n**算法1：迭代组相对策略优化（Iterative Group Relative Policy Optimization）**\n输入：初始策略模型 \\(\\pi_{\\theta_{\\text{init}}}\\)；奖励模型 \\(r_{\\varphi}\\)；任务提示集合 \\(\\mathcal{D}\\)；超参数 \\(\\varepsilon, \\beta, \\mu\\)\n1: 策略模型 \\(\\pi_{\\theta} \\gets \\pi_{\\theta_{\\text{init}}}\\)\n2: **for** iteration = 1 to I **do**\n3:     参考模型 \\(\\pi_{ref} \\gets \\pi_{\\theta}\\)\n4:     **for** step = 1 to M **do**\n5:         从 \\(\\mathcal{D}\\) 中采样一个批次 \\(\\mathcal{D}_b\\)\n6:         更新旧策略模型 \\(\\pi_{\\theta_{old}} \\gets \\pi_{\\theta}\\)\n7:         对每个问题 \\(q \\in \\mathcal{D}_b\\)，从 \\(\\pi_{\\theta_{old}}\\) 采样 \\(G\\) 个输出 \\(\\{o_i\\}_{i=1}^G\\)\n8:         运行奖励模型 \\(r_{\\varphi}\\)，为每个采样输出 \\(o_i\\) 计算奖励 \\(\\{r_i\\}_{i=1}^G\\)\n9:         通过组相对优势估计，计算 \\(o_i\\) 第t个token的优势 \\(\\hat{A}_{i,t}\\)\n10:         **for** GRPO iteration = 1 to \\(\\mu\\) **do**\n11:             通过最大化GRPO目标函数（公式3）更新策略模型 \\(\\pi_{\\theta}\\)\n12:         使用重放机制持续训练奖励模型 \\(r_{\\varphi}\\)。\n输出：\\(\\pi_{\\theta}\\)\n\n**§2 关键超参数与配置**\n-   **数据收集**：fastText分类器训练：向量维度256，学习率0.1，n-gram最大长度3，最小词频3，训练轮数3。数学域名判定阈值：10%（即一个域名下超过10%的网页被收集，则判定为数学相关）。\n-   **预训练**：\n    -   峰值学习率：DeepSeek-LLM 1.3B为5.3e-4，DeepSeekMath-Base 7B为4.2e-4。\n    -   学习率调度：2000步热身，80%训练步数后降至峰值31.6%，90%步数后降至峰值10%。\n    -   批次大小：1.3B模型为4M tokens，7B模型为10M tokens。\n    -   上下文长度：4K tokens。\n    -   优化器：AdamW (\\(\\beta_1=0.9, \\beta_2=0.95\\)，权重衰减0.1)。\n    -   训练tokens总量：500B。\n-   **SFT**：学习率5e-5（恒定），批次大小256，训练步数500，最大上下文长度4K。\n-   **GRPO**：\n    -   策略模型学习率：1e-6。\n    -   KL惩罚系数 \\(\\beta\\)：0.04。\n    -   每组采样数 \\(G\\)：64。\n    -   裁剪系数 \\(\\varepsilon\\)：原文未明确给出具体数值，为标准PPO超参数。\n    -   序列最大长度：1024。\n    -   训练批次大小：1024。\n    -   奖励模型学习率：2e-5。\n    -   重放机制历史数据保留比例：10%。\n\n**§3 训练/微调设置（如有）**\n-   **预训练数据构成**：DeepSeekMath-Base 7B使用混合数据：56% DeepSeekMath语料库，4% AlgebraicStack（数学代码），10% arXiv论文，20% Github代码，10%中英文自然语言（来自Common Crawl）。总计500B tokens。\n-   **SFT数据构成**：总计776K条数学指令样本，覆盖中英文。包含Chain-of-Thought (CoT)、Program-of-Thought (PoT)、工具集成推理（Tool-integrated）三种格式。具体来源：对GSM8K和MATH问题标注工具集成解决方案；采用MathInstruct子集和Lila-OOD训练集（含CoT/PoT）；收集中国K-12数学问题（覆盖76个子主题），标注CoT和工具集成格式答案。\n-   **RL训练数据**：仅使用SFT数据中来自GSM8K和MATH的约144K条Chain-of-Thought格式问题。旨在研究RL在缺乏数据的领域外任务上的泛化能力。\n-   **优化器与调度**：如§2所述，使用AdamW。预训练采用多步衰减调度，SFT和GRPO使用恒定学习率。\n-   **训练框架**：使用高效的HAI-LLM训练框架进行预训练实验。\n\n**§4 推理阶段的工程细节**\n论文未详细描述推理阶段的工程实现细节（如并行化、缓存等）。评估时主要采用标准Few-shot提示（Chain-of-Thought或Program-of-Thought）进行生成。对于工具集成推理，模型被提示编写Python程序（可使用math、sympy等库），程序的执行结果作为答案。在正式数学证明（miniF2F）任务中，采用Few-shot提示生成证明草图，然后使用现成的自动化证明器Sledgehammer填充缺失细节。所有评估均为Top-1生成（除非特别说明使用了多数投票）。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n1.  **GSM8K**：英文小学数学应用题数据集，包含8.5K个训练样本和1.3K个测试样本。问题涉及多步算术推理。\n2.  **MATH**：英文竞赛级数学数据集（Hendrycks et al., 2021），涵盖代数、几何、微积分等主题，包含12.5K个问题，分为7个难度级别。评测时不使用外部工具和投票技术。\n3.  **OCW Courses**：麻省理工学院开放课程中的数学问题数据集（Lewkowycz et al., 2022a），包含大学水平的数学问题。\n4.  **SAT**：数学SAT考试题目数据集（Azerbayev et al., 2023）。\n5.  **MMLU-STEM**：大规模多任务语言理解基准的STEM子集（Hendrycks et al., 2020），包含57个多项选择题任务中的科学、技术、工程和数学相关题目。\n6.  **CMATH**：中文数学应用题数据集（Wei et al., 2023），包含约9K个问题。\n7.  **Gaokao-MathCloze & Gaokao-MathQA**：中国高考数学填空题和选择题数据集（Zhong et al., 2023）。\n8.  **MGSM-zh**：多语言GSM8K的中文翻译版本（Shi et al., 2023）。\n9.  **miniF2F**：用于正式奥林匹克级数学证明的数据集（Zheng et al., 2021），包含验证集和测试集，使用Isabelle证明助手。\n10. **HumanEval & MBPP**：代码生成基准，分别包含164个和约1000个编程问题。\n11. **BBH**：BIG-Bench Hard基准（Suzgun et al., 2022），包含23个具有挑战性的任务，大多需要多步推理。\n\n**数据去污**：为防止基准污染，使用10-gram精确匹配过滤训练语料中任何与评测基准子字符串匹配的文本段。对于短于10-gram但至少3-gram的基准文本，使用精确匹配过滤。\n\n**§2 评估指标体系（全量列出）**\n-   **准确性指标**：\n    -   **准确率（Accuracy）**：所有数学和代码基准的主要评估指标，报告Top-1准确率（除非特别说明使用多数投票）。\n    -   **Pass@1**：用于代码生成基准HumanEval和MBPP，衡量模型生成代码通过单元测试的比例。\n-   **效率/部署指标**：原文未提供延迟、显存占用等效率指标。\n-   **其他自定义指标**：无。评估主要依赖于标准数据集的准确率。\n\n**§3 对比基线（完整枚举）**\n**闭源基线模型**：\n1.  GPT-4、GPT-4 Code Interpreter、GPT-3.5 (OpenAI)\n2.  Gemini Ultra、Gemini Pro (Google)\n3.  Inflection-2 (Inflection AI)\n4.  Grok-1 (xAI)\n5.  Baichuan-3、GLM-4 (中国公司)\n6.  Minerva 7B/62B/540B (Google，仅Base模型对比)\n\n**开源基线模型**：\n1.  **通用模型**：Mistral 7B、DeepSeek-LLM-Chat 67B、Qwen 72B、SeaLLM-v2 7B、ChatGLM3 6B。\n2.  **数学增强模型**：\n    -   Llemma 7B/34B：在Proof-Pile-2上进行了数学训练的CodeLlama变体。\n    -   InternLM2-Math 20B：基于InternLM2进行数学训练和指令微调。\n    -   Math-Shepherd-Mistral 7B：对Mistral 7B应用带有过程监督奖励模型的PPO训练。\n    -   WizardMath-v1.0/1.1 7B/70B：使用Evol-Instruct和PPO训练Mistral/Llama-2。\n    -   MetaMath 70B：在增强版GSM8K和MATH上微调的Llama-2 70B。\n    -   ToRA 34B：为工具集成数学推理微调的CodeLlama 34B。\n    -   MAmmoTH 70B：在MathInstruct上指令微调的Llama-2 70B。\n    -   CodeLlama 7B/34B：代码预训练模型。\n\n**§4 实验控制变量与消融设计**\n1.  **数据质量消融**：使用1.3B模型对比训练在不同数学语料库（MathPile 8.9B、OpenWebMath 13.6B、Proof-Pile-2 51.9B、DeepSeekMath Corpus 120.2B）上的性能，验证数据规模和质量的影响。\n2.  **代码训练影响消融**：使用1.3B模型，设计不同训练设置（仅通用数据、仅代码、仅数学、代码后数学、数学后代码），评估在有/无工具使用下的数学推理能力，验证代码训练对数学推理的益处。\n3.  **arXiv数据影响**：实验发现，与常见做法相反，加入arXiv论文数据进行训练**并未**对本文采用的数学基准带来显著提升。\n4.  **RL变体对比**：在GRPO框架下，对比了结果监督与过程监督两种奖励分配方式。\n5.  **迭代RL**：对比了单轮GRPO与多轮迭代GRPO（算法1）的效果，并研究了奖励模型持续训练（含10%历史数据重放）的影响。\n6.  **领域内外泛化**：RL阶段仅使用GSM8K和MATH的CoT数据，但评估时包含了多个未见过的中文数学基准（CMATH, MGSM-zh等），以测试RL的泛化能力。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n以下是论文核心结果的完整还原（基于Table 2, 3, 4, 5）：\n\n**表1：DeepSeekMath-Base 7B与强基座模型对比（Chain-of-Thought提示）**\n`模型 | 规模 | GSM8K | MATH | OCW | SAT | MMLU-STEM | CMATH | Gaokao-MathCloze | Gaokao-MathQA`\n`Minerva | 540B | 58.8% | 33.6% | 17.6% | - | 63.9% | - | - | -`\n`Mistral | 7B | 40.3% | 14.3% | 9.2% | 71.9% | 51.1% | 44.9% | 5.1% | 23.4%`\n`Llemma | 34B | 54.0% | 25.3% | 10.3% | 71.9% | 52.9% | 56.1% | 11.9% | 26.2%`\n`DeepSeekMath-Base | 7B | **64.2%** | **36.2%** | **15.4%** | **84.4%** | **56.5%** | **71.7%** | **20.3%** | **35.3%**`\n\n**表2：工具使用与形式数学证明（Few-shot）**\n`模型 | 规模 | GSM8K+Python | MATH+Python | miniF2F-valid | miniF2F-test`\n`Llemma | 34B | 64.6% | 26.3% | 21.0% | 21.3%`\n`DeepSeekMath-Base | 7B | **66.9%** | **31.4%** | **25.8%** | **24.6%**`\n\n**表3：自然语言理解、推理与代码能力**\n`模型 | 规模 | MMLU | BBH | HumanEval (Pass@1) | MBPP (Pass@1)`\n`Mistral | 7B | 62.4% | 55.7% | 28.0% | 41.4%`\n`DeepSeek-Coder-Base-v1.5 | 7B | 49.1% | 55.2% | 43.2% | 60.4%`\n`DeepSeekMath-Base | 7B | **54.9%** | **59.5%** | 40.9% | 52.6%`\n\n**表4：指令微调及RL后模型性能（CoT推理）**\n`模型 | 规模 | GSM8K | MATH | MGSM-zh | CMATH`\n`GPT-4 | - | 92.0% | 52.9% | - | 86.0%`\n`Gemini Ultra | - | 94.4% | 53.2% | - | -`\n`DeepSeek-LLM-Chat | 67B | 84.1% | 32.6% | 74.0% | 80.3%`\n`WizardMath-v1.1 | 7B | 83.2% | 33.0% | - | -`\n`DeepSeekMath-Instruct | 7B | 82.9% | 46.8% | 73.2% | 84.6%`\n`DeepSeekMath-RL | 7B | **88.2%** | **51.7%** | **79.6%** | **88.8%**`\n\n**表5：工具集成推理性能**\n`模型 | 规模 | GSM8K | MATH | MGSM-zh | CMATH`\n`GPT-4 Code Interpreter | - | 97.0% | 69.7% | - | -`\n`DeepSeek-LLM-Chat | 67B | 86.7% | 51.1% | 76.4% | 85.4%`\n`DeepSeekMath-Instruct | 7B | 83.7% | 57.4% | 72.0% | 84.3%`\n`DeepSeekMath-RL | 7B | **86.7%** | **58.8%** | **78.4%** | **87.6%**`\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n-   **竞赛级数学（MATH）**：DeepSeekMath-RL 7B在MATH上达到51.7%（CoT），超越了所有开源模型（包括70B的MetaMath和Qwen）以及大多数闭源模型（如Inflection-2 34.8%， Gemini Pro 32.6%），接近GPT-4（52.9%）和Gemini Ultra（53.2%）。这表明**7B小模型通过高质量数据+高效RL可以挑战顶级闭源大模型**。在工具集成推理下，DeepSeekMath-RL 7B达到58.8%，虽仍落后于GPT-4 Code Interpreter（69.7%），但显著优于其他开源模型。\n-   **小学数学（GSM8K）**：DeepSeekMath-RL 7B达到88.2%（CoT），超越了除GPT-4（92.0%）和Gemini Ultra（94.4%）外的所有对比模型。相比其SFT版本（82.9%），RL带来了5.3个百分点的绝对提升。\n-   **中文数学基准**：在CMATH上，DeepSeekMath-RL 7B达到88.8%（CoT），超越了GPT-4（86.0%）和所有开源模型。在MGSM-zh上达到79.6%，优于67B的DeepSeek-LLM-Chat（74.0%）。这证明了**多语言数学语料库训练和RL的有效泛化**，即使RL数据仅为英文GSM8K/MATH，模型在中文任务上仍有显著提升。\n-   **形式数学证明（miniF2F）**：DeepSeekMath-Base 7B在miniF2F验证集和测试集上分别达到25.8%和24.6%，优于Llemma 34B（21.0%和21.3%），显示了其在将非正式陈述转化为形式证明方面的强大能力。\n-   **通用能力（MMLU, BBH）**：数学训练提升了模型的通用理解和推理能力。DeepSeekMath-Base 7B在MMLU上达到54.9%，BBH上达到59.5%，均显著高于其代码预训练前身DeepSeek-Coder-Base-v1.5（49.1%， 55.2%）和通用模型Mistral 7B（62.4%， 55.7%）。\n\n**§3 效率与开销的定量对比**\n论文未提供详细的延迟、Token消耗或显存占用的定量对比数据。然而，在**算法效率**上，GRPO的核心贡献是**消除了PPO中评论家模型的内存和计算开销**。假设策略模型为7B，标准PPO需要额外一个7B的评论家模型，而GRPO无需此模型，理论上可节省约50%的用于RL训练的显存。训练数据方面，RL阶段仅使用约144K条数据，相比需要大量偏好数据或在线采样的方法，数据效率更高。\n\n**§4 消融实验结果详解**\n1.  **数据质量消融（Table 1）**：使用1.3B模型对比不同语料库。在GSM8K上，DeepSeekMath Corpus（120.2B tokens）达到23.8%，显著高于Proof-Pile-2（51.9B tokens）的14.3%，提升9.5个点（+66.4%）。在CMATH上，DeepSeekMath Corpus达到41.5%，而Proof-Pile-2仅为19.9%，提升21.6个点（+108.5%），证明了其多语言高质量数据的优势。\n2.  **代码训练影响消融（Table 6，部分数据未在提供文本中）**：原文指出，在数学训练**之前**进行代码训练，比在数学训练之后或仅进行数学训练，能带来更好的数学推理性能（无论是否使用工具）。这验证了代码训练对数学推理的促进作用。\n3.  **arXiv数据影响**：作者明确指出，加入arXiv论文数据进行训练**并未**对本文采用的数学基准带来显著提升。这与许多其他数学训练工作形成对比。\n4.  **RL组件有效性**：DeepSeekMath-RL 7B相比DeepSeekMath-Instruct 7B，在GSM8K上从82.9%提升至88.2%（+5.3个点，+6.4%），在MATH上从46.8%提升至51.7%（+4.9个点，+10.5%），在CMATH上从84.6%提升至88.8%（+4.2个点，+5.0%）。**尽管RL训练数据仅限于GSM8K和MATH的CoT格式，但在所有评估基准（包括领域外中文任务）上均观察到一致提升**，证明了GRPO方法的有效泛化。\n\n**§5 案例分析/定性分析（如有）**\n原文未提供具体的成功或失败案例分析。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **构建了大规模高质量多语言数学语料库**：提出了一个迭代数据收集管道，从Common Crawl中提取出120B token的DeepSeekMath语料库，其规模是OpenWebMath的9倍，质量更高且包含中文内容。这直接使得7B基座模型在MATH上达到36.2%，超越了540B的Minerva（33.6%）。\n2.  **验证了代码训练对数学推理的促进作用**：通过实验证明，在数学训练**之前**进行代码预训练，能更有效地提升模型在有/无工具使用场景下的数学解题能力，为“代码训练是否提升推理能力”提供了肯定答案。\n3.  **提出了高效的无评论家强化学习算法GRPO**：GRPO摒弃了PPO中的评论家模型，利用组内相对优势估计，显著降低了RL训练的资源消耗。仅使用GSM8K/MATH的CoT数据微调后，将7B指令模型的MATH准确率从46.8%提升至51.7%（+4.9个点），并实现了领域外泛化。\n4.  **发布了性能领先的开源数学模型**：DeepSeekMath 7B系列模型（Base, Instruct, RL）在多个数学基准上超越了所有同规模开源模型，并与许多70B+模型及闭源模型性能相当，为社区提供了强大的开源替代。\n\n**§2 局限性（作者自述）**\n1.  **arXiv数据无效性**：与许多相关工作不同，本文发现加入arXiv论文数据进行训练并未带来性能提升。作者未深入解释原因，这可能意味着arXiv数据的组织形式或内容与目标基准不匹配。\n2.  **RL训练数据范围有限**：GRPO实验仅使用了GSM8K和MATH的Chain-of-Thought格式数据（约144K条）。虽然显示了良好的泛化能力，但更广泛、多样化的RL训练数据可能带来进一步增益。\n3.  **未探索更大规模模型**：工作集中于7B模型，未将数据收集和GRPO方法扩展到更大规模（如70B或更大）的模型上，其 scalability 有待验证。\n\n**§3 未来研究方向（全量提取）**\n1.  **探索更有效的RL范式**：基于本文提出的统一范式（将RFT、DPO、PPO、GRPO视为直接或简化的RL技术），未来可以探索其他更高效的RL变体，以进一步提升模型性能。\n2.  **深入研究过程监督与结果监督**：本文对比了两种奖励分配方式，但未来可以更系统地研究过程监督在复杂数学推理中的最优应用方式，例如如何设计更合理的逐步骤奖励。\n3.  **扩展数据收集管道到其他领域**：本文的迭代数据收集方法（基于分类器和领域扩展）被证明对数学领域有效，未来可将其应用于其他需要高质量专业数据的领域，如法律、医学、科学文献等。\n4.  **研究代码与数学推理的更深层联系**：本文初步验证了代码训练对数学推理的益处，未来可以深入研究其背后的机制，例如是代码的逻辑结构、函数抽象还是其他特性起到了关键作用。\n5.  **多模态数学推理**：当前工作集中于文本和代码，未来可以探索结合图表、公式图像的多模态数学理解和推理。",
    "research_contributions": "**§1 核心学术贡献（按重要性排序）**\n1.  **方法论贡献：GRPO算法**：\n    -   **理论新颖性**：首次提出在LLM的RL训练中完全摒弃评论家模型，创新性地利用**组内统计量（均值、标准差）**作为基线来计算相对优势。这为资源高效的RL训练提供了新思路。\n    -   **实验验证充分性**：在7B模型上，仅使用有限领域数据即实现了显著的领域内外性能提升（MATH +4.9点，CMATH +4.2点），并与标准PPO进行了概念性对比，证明了其有效性。\n    -   **对领域的影响**：为开源社区和算力有限的研究者提供了一个可行的、高效的RL微调方案，可能降低对齐技术门槛。\n2.  **数据贡献：DeepSeekMath语料库**：\n    -   **理论新颖性**：提出了一套可扩展的、迭代的、结合自动分类与人工标注的互联网数学数据挖掘流程，证明了Common Crawl中高质量数学内容的可获取性。\n    -   **实验验证充分性**：通过严格的消融实验（1.3B模型对比不同语料库），证明了该语料库在规模和质量（尤其是多语言方面）上的优势，其训练出的模型性能超越了使用其他语料库的模型。\n    -   **对领域的影响**：发布了目前已知规模最大的开源数学语料库（120B tokens），为后续数学语言模型研究提供了宝贵的数据资源。\n3.  **实证贡献：代码训练有益于数学推理**：\n    -   **理论新颖性**：通过控制变量实验，明确验证了“代码训练提升推理能力”这一假设在数学领域成立，并指出**训练顺序（代码在先）** 是关键。\n    -   **实验验证充分性**：设计了不同训练设置（仅通用、仅代码、仅数学、代码后数学、数学后代码）的对比实验，在有/无工具使用的数学基准上均给出了定量结果。\n    -   **对领域的影响**：为模型训练流程的设计提供了具体指导，鼓励在构建数学专家模型前先进行代码预训练。\n\n**§2 工程与实践贡献**\n1.  **开源模型与代码**：发布了DeepSeekMath 7B系列的Base、Instruct、RL模型权重，以及相关的训练代码和GRPO实现，促进了研究的可复现性。\n2.  **高质量数据管道**：公开了构建DeepSeekMath语料库的迭代数据收集方法细节，为社区构建其他领域专用语料库提供了可复用的技术蓝图。\n3.  **系统性的评估基准**：在涵盖英文、中文、形式证明、工具使用、通用能力等多个维度的广泛基准上进行了全面评估，为后续研究设立了较高的对比标准。\n\n**§3 与相关工作的定位**\n本文处于**“小模型+高质量数据+高效算法”** 技术路线的前沿。它挑战了“更大参数规模是提升数学能力的唯一途径”的传统观念（如Minerva 540B），并与专注于**改进RL算法效率**（如GRPO vs PPO）和**挖掘互联网高质量数据**（DeepSeekMath Corpus vs OpenWebMath）的研究方向紧密相关。它不是在现有大模型基础上进行微调的增量工作，而是从数据源头和训练算法两个根本环节进行创新，旨在为资源受限的研究者提供一套完整的高性能数学模型构建方案。",
    "professor_critique": "**§1 实验设计与评估体系的缺陷**\n1.  **评估指标单一**：全文仅使用准确率（Accuracy/Pass@1）作为评价标准，缺乏对模型**推理过程可靠性、错误类型、步骤合理性**的细粒度分析。例如，模型可能通过“猜测”或记忆获得正确答案，但其推理链条可能逻辑混乱。需要引入如步骤正确率、逻辑一致性等指标。\n2.  **Baseline选择可能过时或不全**：虽然对比了众多模型，但未包含一些同期或稍晚发布的强劲开源数学模型，如**Qwen-Math**、**DeepSeek-Coder-Math**或**MetaMath-Turbo**。与闭源模型的对比也仅限于GPT-4和Gemini，未包含Claude 3或更专业的数学模型如**AlphaGeometry**。\n3.  **缺乏鲁棒性和对抗性测试**：所有评估均在标准测试集上进行。未测试模型对**问题表述扰动、对抗性示例（如无关信息干扰、逻辑陷阱）或分布外（OOD）复杂问题**的鲁棒性。模型在MATH上的51.7%可能掩盖了对特定问题类型的脆弱性。\n4.  **效率评估缺失**：论文完全未报告模型的**推理速度（延迟/TPS）、显存占用、训练成本（GPU小时）**。GRPO宣称高效，但无任何训练时间或显存占用的定量对比（与标准PPO相比），其“高效”主张缺乏实证支持。\n\n**§2 方法论的理论漏洞或工程局限**\n1.  **GRPO优势的普适性质疑**：GRPO利用组内归一化奖励，其有效性高度依赖于**组内样本的奖励分布**。如果对于某个问题，所有采样输出的奖励都很低或都很高，归一化后的优势值可能很小，导致训练信号微弱。论文未分析这种极端情况或提供组内奖励分布的统计。\n2.  **迭代RL中奖励模型过拟合风险**：算法1中，奖励模型使用当前策略模型采样的新数据和10%历史数据进行持续训练。这可能导致奖励模型快速过拟合到当前策略的输出风格，失去对更优解的判别力，从而陷入局部最优。论文未提供奖励模型在迭代过程中的性能变化分析。\n3.  **数据收集管道的可扩展性**：依赖人工标注URL来扩充种子语料（识别数学相关域名后），这在大规模扩展到其他领域或语言时成本高昂，且可能引入标注者偏差。论文未讨论该过程的自动化替代方案或成本估算。\n4.  **代码训练顺序的因果性未彻底证明**：实验显示了“代码后数学”训练的优势，但未排除**数据混合比例**或**训练总tokens数**的混淆因素。需要更严格的消融实验，控制总训练量不变，仅调整代码和数学数据的顺序和比例。\n\n**§3 未经验证的边界场景**\n1.  **多模态数学问题**：模型仅处理文本和代码。当数学问题包含**图表、几何图形、手写公式图像**时，模型的性能将崩溃，因为其训练数据完全是文本形式的。\n2.  **开放式探索与证明**：评估集中于有明确答案的问题求解。对于**开放式数学探索、猜想生成、或需要创造性构建辅助线/函数的证明题**，模型的能力未知且很可能不足。\n3.  **数学知识冲突与纠错**：当模型内部知识（从预训练语料中学到）与问题中给出的信息或标准答案冲突时（例如，有争议的数学定义或命题），模型能否识别并纠正错误？这需要设计特定的对抗性评测。\n4.  **超长上下文数学推理**：所有实验上下文长度为4K。对于需要引用**长篇数学文献、定理证明或复杂演算过程**的问题，模型在更长上下文（如32K或128K）下的表现未经验证。\n5.  **低资源语言数学问题**：语料库虽包含中文，但其他非英语、非中文的数学内容（如法语、德语数学教材）占比极低。模型在**日语、俄语、阿拉伯语等语言的数学问题**上性能很可能显著下降。\n\n**§4 可复现性与公平性问题**\n1.  **数据管道细节不足**：虽然描述了迭代流程，但未公开fastText分类器的具体训练数据（正负例的详细构成）、人工标注的准则和一致性、以及最终120B语料库的具体语言分布和内容构成。这使得完全复现DeepSeekMath Corpus几乎不可能。\n2.  **依赖内部基础设施**：训练使用了HAI-LLM框架和DeepSeek内部的计算集群。普通研究者缺乏相同规模的算力（500B tokens的预训练）来复现Base模型。GRPO实验虽所需数据量较小，但其奖励模型的训练细节（数据构造、具体架构）也未完全公开。\n3.  **对Baseline的超参数公平性**：论文中对比的Baseline模型（如WizardMath, MetaMath）可能使用了不同的提示模板、采样温度、推理步骤限制。作者是否对所有对比模型使用了**完全相同的评估协议**（例如，CoT提示的Few-shot示例、生成参数）？文中未明确说明，可能存在评估偏差。\n4.  **未开源完整RL训练代码**：论文声称开源代码，但GRPO算法、迭代训练流程、奖励模型构建的完整实现是否包含在内？如果缺少关键组件，复现RL结果将非常困难。",
    "zero_compute_opportunity": "#### 蓝图一：探究小规模多语言数学语料构建的性价比\n-   **核心假设**：对于资源有限的研究者，构建一个远小于120B token（例如10-20B token）但**精心筛选、高度多样化**的多语言数学语料库，其训练出的模型在特定数学子领域（如几何或数论）的性能，可能接近甚至超越在大规模通用数学语料上训练的模型。\n-   **与本文的关联**：基于本文发现arXiv数据无效及高质量网页数据的关键性。质疑“更大即更好”，探索数据质量与规模在极低资源下的权衡。\n-   **所需资源**：\n    -   **数据**：利用公开的数学数据集（如OpenWebMath子集、AoPS论坛公开内容、多语言数学教科书文本）进行组合，目标10B tokens。\n    -   **计算**：使用Colab免费T4 GPU或低成本云服务（如Lambda Labs），对1B以下参数模型（如TinyLlama）进行50-100B tokens的继续预训练。预计费用<100美元。\n    -   **评估**：使用公开的M",
    "source_file": "DeepSeekMath Pushing the Limits of Mathematical Reasoning in Open Language Models.md"
}