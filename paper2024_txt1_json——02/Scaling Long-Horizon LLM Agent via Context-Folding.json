{
    "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
    "background_and_problem": "**§1 领域背景与研究动机（150字以上）**\n该研究聚焦于**长视野（Long-Horizon）LLM智能体**领域，特别是**深度研究（Deep Research）**和**智能体软件工程（Agentic SWE）**等复杂任务场景。随着LLM智能体能力的提升，其所能完成的任务长度被认为正以约7个月翻倍的速度指数增长。然而，智能体向更长视野任务的扩展，从根本上受到其工作上下文（Context）长度的限制。当前，智能体框架在完成需要与环境进行大量交互的复杂任务时，会线性累积整个交互历史（推理、工具调用、观察结果），形成一个不断膨胀的单一上下文，这带来了性能和效率的双重挑战。因此，如何让智能体主动、高效地管理其工作上下文，成为该领域亟待解决的核心问题。\n\n**§2 现有技术的核心短板——具体失败模式（250字以上）**\n现有方法主要分为两类，均存在具体失败模式：\n1.  **基于总结的方法（Summary-based methods）**：当工作上下文已满时，触发事后总结阶段以压缩上下文。**具体失败模式**：当智能体正在进行复杂的多步推理时，这种总结会**突然中断智能体的工作上下文和推理流程**，导致关键中间信息丢失，从而产生次优结果。例如，在深度研究任务中，智能体可能刚通过多次搜索获得关键线索，但上下文已满触发总结，线索被压缩为模糊摘要，导致后续推理方向错误。\n2.  **多智能体系统（Multi-agent systems）**：将任务分配给专门的智能体以管理上下文长度。**具体失败模式**：这类系统**严重依赖手工定制、针对特定问题的工作流**，难以泛化到新任务，且**难以进行端到端优化**。例如，为代码生成任务设计的“规划-执行”多智能体系统，无法直接迁移到需要不同工具和交互模式的深度研究任务中，扩展性差。\n3.  **标准ReAct式智能体（Vanilla ReAct-style agent）**：将所有历史线性追加到上下文中。**具体失败模式**：当交互轨迹$\tau$因大量交互而迅速累积，**超过工作上下文限制（如32K）**时，LLM在极长上下文中的性能会**显著下降**（即“Lost in the Middle”现象），无法有效利用相关信息。同时，注意力机制的**二次方计算复杂度**和不断增长的**KV-cache管理开销**导致效率低下。\n\n**§3 问题的根本难点与挑战（200字以上）**\n该问题的根本难点在于**上下文管理的被动性与任务执行的主动性之间存在本质矛盾**。从理论角度看，LLM的注意力机制具有$\\(O(n^2)\\)$的计算复杂度，其中n是上下文长度，这使得无限制地扩展上下文在计算上是不可行的。从工程角度看，KV-cache随上下文线性增长，对显存和延迟构成巨大压力。更重要的是，智能体在长视野任务中需要动态规划、分解任务并调用工具，其信息需求是**局部化**和**阶段性**的。现有方法要么被动地接受上下文膨胀（ReAct），要么以破坏推理连续性的方式粗暴压缩（总结），要么引入难以优化、不灵活的架构（多智能体），均未能让智能体**将上下文管理作为一种可学习的、与任务解决过程内禀结合的认知技能**。\n\n**§4 本文的切入点与核心假设（200字以上）**\n本文的切入点是让智能体**主动管理其工作上下文**，核心机制是**上下文折叠（Context Folding）**。其核心假设是：**长视野任务可以自然地分解为一系列局部化的子任务，而处理这些子任务所产生的中间步骤（如详细的网页浏览、代码库探索）对于后续的高层推理而言是“短暂”的，只需保留其结果的简洁摘要即可**。这一假设受到人类解决问题方式的启发：我们会将复杂问题拆解，专注于当前子问题，完成后将详细过程“折叠”起来，只记住关键结论。基于此，本文设计了**分支（branch）**和**返回（return）**两个特殊工具，允许智能体动态创建子轨迹处理子任务，完成后折叠中间步骤，仅保留摘要返回主线程。为使该行为可学习，本文进一步提出了**FoldGRPO**强化学习框架，通过**密集的过程奖励（Process Rewards）**直接指导上下文折叠行为。",
    "core_architecture": "**§1 系统整体架构概览（200字以上）**\n系统整体基于**规划-执行（Plan-Execution）框架**实例化上下文折叠机制。数据流如下：\n1.  **输入**：用户问题$q$。\n2.  **规划状态（Planning State）**：智能体在主线程中进行高层推理，分解任务，并决定何时为子任务发起分支。在此状态下，**不鼓励进行令牌密集的工具调用**，以保持主上下文专注于高层策略。\n3.  **执行状态（Execution State）**：智能体在活动分支内操作以完成其分配的子任务。**在此状态下禁用创建新分支**，以防止嵌套复杂性，保持结构清晰。\n4.  **上下文管理器（Context Manager $\\mathcal{F}$）**：根据分支和返回调用，动态折叠交互历史。当调用`return`时，**回滚KV-cache**到对应分支位置，仅将分支结果摘要追加到主线程上下文。\n5.  **输出**：最终的任务解决方案轨迹。\n\n**§2 各核心模块深度拆解（每个模块100字以上，共至少3个模块）**\n#### 模块一：上下文管理器（Context Manager $\\mathcal{F}$）\n-   **输入**：截至步骤$i$的完整交互历史$\\tau_{<i} = (a_1, o_1, \\dots, a_{i-1}, o_{i-1})$。\n-   **核心处理逻辑**：识别`branch`和`return`工具调用对。将每一对之间的所有动作-观察对$(a_k, o_k, ..., a_m, o_m)$**从当前工作上下文中移除（折叠）**，仅保留`branch`调用时的模板消息和`return`调用时携带的结果`message`。在推理时，通过**回滚KV-cache**到分支点来实现高效折叠。\n-   **输出**：折叠后的紧凑上下文序列，用于生成下一个动作$a_i$。\n-   **设计理由**：直接操作KV-cache而非文本拼接，避免了重复计算，显著提升了推理效率。这使折叠成为一种轻量级的、与模型推理过程紧密结合的操作，而非事后的文本处理。\n\n#### 模块二：分支工具（branch tool）\n-   **输入**：`description`（子任务的简要总结）和`prompt`（该分支的详细指令）。\n-   **核心处理逻辑**：当智能体调用`branch(description, prompt)`时，系统**创建一个独立的工作上下文**，其前缀与调用分支前的主线程上下文相同。智能体在此新上下文中开始解决子任务$q'$。工具返回一个**模板消息**，指示分支已创建。\n-   **输出**：一个标志分支开始的模板消息，该消息被追加到主线程上下文中。\n-   **设计理由**：通过共享上下文前缀，确保了子智能体与主智能体在知识状态上的一致性。独立的上下文使得子任务处理不受主线程历史干扰，同时为后续的KV-cache回滚提供了明确的锚点。\n\n#### 模块三：返回工具（return tool）\n-   **输入**：`message`（描述该分支结果的文本）。\n-   **核心处理逻辑**：当智能体在分支中调用`return(message)`时，触发上下文折叠。系统**回滚KV-cache**到对应的`branch`调用点，然后将一个包含分支`description`和结果`message`的**模板化摘要**追加到主线程上下文中。之后，智能体上下文切换回主线程。\n-   **输出**：一个包含分支结果摘要的模板消息，该消息被追加到主线程上下文中，标志着分支结束。\n-   **设计理由**：`return`是折叠动作的触发器。它不仅提供了子任务的结果，其调用时机本身也是智能体需要学习的决策（何时结束子任务）。模板化的摘要格式确保了信息传递的结构化和一致性。\n\n**§3 关键公式与算法（如有）**\n核心建模公式：上下文折叠智能体的轨迹概率建模为\n\\[ p_{\\theta}^{\\text{ContextFold}}(\\tau \\mid q) := \\prod_{i \\in [T]} \\pi_{\\theta} \\left(a_{i} \\mid q, \\mathcal{F}\\left(\\tau_{<i}\\right)\\right). \\tag{1} \\]\n其中$\\mathcal{F}$是上下文管理器。\n\nFoldGRPO学习目标：\n\\[ \\mathcal{J}_{\\mathrm{FoldGRPO}} = \\mathbb{E}_{\\substack{\\{\\tau_{i}\\}_{i=1}^{G} \\sim \\pi_{\\mathrm{old}}(\\cdot | q)}} \\left[ \\frac{1}{\\sum_{i=1}^{G} |\\tau_{i}|} \\sum_{i=1}^{G} \\sum_{t=1}^{|\\tau_{i}|} \\min \\left\\{r_{i, t}(\\theta) \\widehat{A}_{i, t}, \\operatorname{clip}\\big(r_{i, t}(\\theta), 1-\\epsilon_{\\mathrm{low}}, 1+\\epsilon_{\\mathrm{high}} \\big) \\widehat{A}_{i, t} \\right\\} \\right], \\]\n其中重要性采样比和组相对优势估计器为：\n\\[ r_{i, t}(\\theta) = \\frac{\\pi_{\\theta}(\\tau_{i, t} \\mid q, \\mathcal{F}(\\tau_{i, <t}))}{\\pi_{\\theta_{\\mathrm{old}}}(\\tau_{i, t} \\mid q, \\mathcal{F}(\\tau_{i, <t}))} \\cdot \\mathbf{1}_{\\tau_{i, t}}^{\\mathrm{LLM}}, \\quad \\widehat{A}_{i, t} = \\frac{\\operatorname{clip}(R_{i} + Q_{i, t}, 0, 1) - \\operatorname{mean}(\\{R_{i}\\}_{i=1}^{G})}{\\operatorname{std}(\\{R_{i}\\}_{i=1}^{G})}. \\]\n$\\mathbf{1}_{\\tau_{i, t}}^{\\mathrm{LLM}}$确保只优化LLM生成的令牌，工具观察的令牌被掩码。$Q_{i, t}$是过程奖励。\n\n**§4 方法变体对比（如有多个变体/消融组件）**\n论文中对比了以下变体：\n1.  **Folding Agent (Seed-OSS-36B)**：基础上下文折叠智能体，未经RL训练。\n2.  **Folding Agent + RL (GRPO)**：使用标准GRPO算法进行RL训练的上下文折叠智能体。\n3.  **Folding Agent + RL (FoldGRPO)**：使用本文提出的FoldGRPO算法（包含动态折叠上下文和过程奖励）进行RL训练的上下文折叠智能体。这是最终版本。\n\n**§5 与已有方法的核心技术差异（200字以上）**\n1.  **与标准ReAct智能体的差异**：ReAct线性累积所有历史到单一上下文，而本文方法通过`branch`/`return`动态创建和折叠子轨迹，**主动管理上下文长度**，避免了上下文无限膨胀导致的性能下降和效率问题。在建模上，ReAct使用完整历史$\\tau_{<i}$，而本文使用折叠后的历史$\\mathcal{F}(\\tau_{<i})$。\n2.  **与基于总结的方法（如Summary Agent）的差异**：总结方法是在上下文已满时**被动、启发式地触发**总结，可能在任何时间点中断推理。本文的折叠是**主动、与子任务边界对齐的**。智能体在完成一个逻辑子单元后主动调用`return`进行“总结式折叠”，这保留了推理的连续性，折叠时机本身是学习目标的一部分。\n3.  **与多智能体系统（如Chain of Agents）的差异**：典型多智能体系统依赖**预定义的、专门的子智能体**，且智能体间通常并行运作。本文的“子智能体”是**由主智能体动态创建**的，共享相同的模型权重和上下文前缀，并且主线程与子线程是**交错执行**而非完全并行，这使得整个系统更轻量、更易于端到端优化。",
    "methodology_and_formulas": "**§1 完整算法流程（伪代码级描述）**\n论文未提供形式化的伪代码，但根据描述可重构核心流程：\nStep 1: 初始化。给定问题$q$，设置主线程上下文为初始指令和$q$，设置活动分支栈为空。\nStep 2: 循环生成动作，直到任务完成或达到步数限制。\nStep 3: 根据当前状态（规划或执行）和折叠后的上下文$\\mathcal{F}(\\tau_{<i})$，由策略$\\pi_{\\theta}$生成动作$a_i$（可能包含推理和工具调用）。\nStep 4: 如果$a_i$是工具调用，执行工具获得观察$o_i$。\n    - 若工具是`branch(description, prompt)`：将当前主线程上下文状态压入栈，创建新的分支上下文（共享前缀），切换至执行状态。将分支模板消息追加到主线程历史。\n    - 若工具是`return(message)`：从栈中弹出对应分支的上下文状态，回滚KV-cache。将包含分支结果摘要的模板消息追加到主线程上下文。切换回规划状态。\n    - 若是其他工具（如`search`, `execute_bash`）：正常执行，将观察$o_i$追加到当前（主线程或分支）上下文。\nStep 5: 将$(a_i, o_i)$对加入到完整历史$\\tau$中。\nStep 6: 返回Step 2。\n\n**§2 关键超参数与配置**\n-   **LLM最大上下文长度**：32,768 tokens。这是智能体**任一时刻**（主线程或任一分支）的**活动上下文**上限。\n-   **最大分支数**：10。这决定了理论的**最大总令牌消耗**为32,768 × 10 = 327,680 tokens。选择10是基于实验观察，大多数任务实例在此范围内已完成。\n-   **过程奖励阈值**：**未折叠令牌惩罚（Unfolded Token Penalty）**在**主线程总上下文长度超过工作上下文限制的50%**（即超过16,384 tokens）时触发。\n-   **RL训练超参数**：\n    -   基础模型：Seed-OSS-36B-Instruct。\n    -   滚动批次大小（Rollout batch size）：32。\n    -   组大小（Group size）：8（用于计算组相对优势）。\n    -   PPO批次大小：128。\n    -   学习率：$1 \\times 10^{-6}$。\n    -   裁剪范围：clip_high = 0.28, clip_low = 0.2。\n    -   最大离策略步数（Maximum off-policy step）：5。\n    -   训练步数：50步（约2个epoch）。\n-   **推理解码**：温度（Temperature）= 0（贪婪解码）。\n\n**§3 训练/微调设置（如有）**\n-   **训练数据构造**：\n    -   **深度研究（BrowseComp-Plus）**：将数据集分为680个实例用于训练，150个用于评估。使用Qwen3-Embed-8B作为检索器。\n    -   **智能体软件工程（SWE-Bench Verified）**：从开源数据集SWE-Gym和SWE-Rebench的子集中，使用基线智能体进行8次滚动，保留成功率在0%到87.5%之间的实例，得到740个训练实例。\n-   **优化器与调度**：原文未明确指定优化器类型，学习率为固定的$1 \\times 10^{-6}$，未提及学习率调度器。\n-   **奖励设计**：最终奖励$R_i \\in \\{0, 1\\}$，基于任务成功与否（RLVR）。深度研究使用官方LLM-based judger，SWE使用实例特定沙箱环境中的单元测试结果。\n\n**§4 推理阶段的工程细节**\n-   **KV-cache管理**：核心工程实现。当`return`被调用时，系统**回滚（roll back）KV-cache**到对应的`branch`调用点。这意味着分支内产生的所有中间键值对都被丢弃，仅保留分支前的状态，从而极大节省了显存并避免了不必要的重复计算。\n-   **并行化策略**：采用**异步滚动（asynchronous rollout）**进行训练，以提升数据收集效率。推理时未提及特殊并行策略。\n-   **向量数据库**：仅提及在深度研究任务中使用Qwen3-Embed-8B作为检索器，未说明具体向量数据库选型。",
    "experimental_design": "**§1 数据集详情（每个数据集单独列出）**\n1.  **BrowseComp-Plus (BC-Plus)**：\n    -   **名称**：BrowseComp-Plus。\n    -   **规模**：总共830个实例，本文拆分后使用**680个用于训练**，**150个用于评估**。\n    -   **领域类型**：深度研究（Deep Research），涉及开放域网页浏览和信息整合。\n    -   **评测问题类型**：复杂、多跳的查询问题，需要智能体进行搜索、阅读、推理和综合。\n    -   **特殊处理**：补充了原始BrowseComp数据并带有验证语料库。为了解耦数据分布影响，作者进行了自定义的训练/评估分割。\n    -   **工具**：`search(query, topk)`, `open_page(url)`。\n2.  **SWE-Bench Verified (SWEB-V)**：\n    -   **名称**：SWE-Bench Verified。\n    -   **规模**：**500个实例**全部用于评估。训练数据来自其他开源数据集。\n    -   **领域类型**：智能体软件工程（Agentic Software Engineering），涉及解决真实的GitHub问题。\n    -   **评测问题类型**：代码修改任务，需要理解问题描述、定位代码、编写并通过测试。\n    -   **特殊处理**：根据原始数据集的“解决时间”指标将实例分为易、中、难三个等级：简单（≤15分钟，194例）、中等（15分钟–1小时，261例）、困难（≥1小时，45例）。\n    -   **工具**：`execute_bash`, `str_replace_editor`, `think`。\n\n**§2 评估指标体系（全量列出）**\n-   **准确性指标**：\n    -   **Pass@1**：智能体单次尝试解决任务的成功率。这是主指标。\n-   **效率/部署指标**：\n    -   **峰值长度（Peak Length）**：智能体在任何时间点的活动上下文的最大令牌数。\n    -   **最大令牌数（Max #Token）**：整个交互轨迹消耗的理论最大令牌数（上下文长度 × 最大分支数）。\n    -   **工具调用次数（Tool Calls）**：智能体在整个任务中调用工具的总次数。\n    -   **完成率（Finish Rate）**：在给定的上下文限制内成功完成（而非因上下文满而中断）的任务比例。\n    -   **主轨迹长度（Main Len）**：主线程上下文的平均令牌数。\n    -   **范围准确率（Scope）**：分支内行为与指定子任务提示相符的比例（使用GPT-5-nano判断）。\n    -   **分支数量（# Branch）**：平均每个任务创建的分支数。\n-   **其他自定义指标**：无。\n\n**§3 对比基线（完整枚举）**\n1.  **ReAct Agent**：\n    -   **类型**：标准ReAct式智能体，保持所有上下文。\n    -   **变体**：\n        -   **短上下文（32K）**：上下文长度32,768 tokens，与本文活动上下文相同。\n        -   **中上下文（65K/131K）**：上下文长度65,536和131,072 tokens。\n        -   **长上下文（327K）**：上下文长度327,680 tokens，与本文最大总令牌成本相同。\n    -   **代表性**：作为最基础的、广泛使用的智能体范式，是性能比较的基准。\n2.  **Summary Agent**：\n    -   **类型**：基于总结的上下文管理方法，当上下文满时触发总结。\n    -   **配置**：最大上下文长度32,768 tokens，允许最多10次总结会话，以与本文进行公平比较（总令牌预算相同）。\n    -   **代表性**：代表了通过压缩来扩展上下文窗口的主流启发式方法。\n3.  **使用100B+参数LLM的ReAct Agent**（作为性能上限参考）：\n    -   包括：GPT-5, GPT-4.1, DeepSeek-V3.1, GLM-4.5-Air, Qwen3-235B-A22B。\n    -   **代表性**：展示了更大规模模型在相同ReAct范式下的性能，用于定位本文36B模型所达到的相对水平。\n\n**§4 实验控制变量与消融设计**\n-   **控制变量**：所有基线（ReAct, Summary）与本文方法使用**相同的基础模型（Seed-OSS-36B-Instruct）**、**相同的数据集**、**相同的基础设施**和**相同的RL训练超参数**（当应用RL时）。这确保了比较的公平性。\n-   **消融设计**：\n    1.  **RL算法消融**：对比了**无RL**、**标准GRPO**和**本文FoldGRPO**对同一上下文折叠智能体的训练效果，以验证过程奖励和动态折叠上下文的重要性。\n    2.  **上下文长度缩放实验**：在BrowseComp上，通过改变允许的分支数（0到16），评估性能随最大总令牌数（从32K到524K）的变化，并与不同上下文长度的ReAct基线对比。\n    3.  **任务复杂性缩放实验**：通过将多个简单问题组合成单个复合查询（1到50个问题），增加任务所需的动作和上下文长度，评估方法的长度泛化能力。\n    4.  **行为统计消融**：在表2中，对比了不同优化方法下智能体的**完成率（Finish）**、**主轨迹长度（Main Len）**、**范围准确率（Scope）**和**分支数量（# Branch）**，以定量分析FoldGRPO如何塑造智能体行为。",
    "core_results": "**§1 主实验结果全景（表格式呈现）**\n方法名 | BrowseComp-Plus Pass@1 | BrowseComp-Plus Tool Calls | SWE-Bench Verified Pass@1 | SWE-Bench Verified Tool Calls\n--- | --- | --- | --- | ---\n**ReAct Agent with 100B+ LLM** | | | | \nGPT-5 (327K) | 0.793 | 14.2 | 0.718 | 42.6\nGPT-4.1 (327K) | 0.640 | 5.6 | 0.486 | 28.7\nDeepSeek-V3.1 (327K) | 0.613 | 10.6 | 0.610 | 53.2\nGLM-4.5-Air (327K) | 0.566 | 11.1 | 0.576 | 51.2\nQwen3-235B-A22B (327K) | 0.560 | 12.8 | 0.344 | 32.1\n**ReAct Agent (Seed-OSS-36B)** | | | | \n32K Context | 0.286 | 3.8 | 0.436 | 25.8\n32K + RL (GRPO) | 0.446 | 5.5 | 0.480 | 27.8\n327K Context (ψ) | 0.478 | 10.8 | 0.552 | 49.5\n327K + RL (GRPO) | 0.540 | 10.2 | 0.574 | 55.4\n**Summary Agent (Seed-OSS-36B)** | | | | \n32K × 10 | 0.386 | 17.4 | 0.488 | 77.0\n32K × 10 + RL (GRPO) | 0.527 | 18.0 | 0.550 | 74.9\n**Folding Agent (Ours, Seed-OSS-36B)** | | | | \n32K × 10 (No RL) | 0.420 | 12.9 | 0.492 | 72.8\n32K × 10 + RL (GRPO) | 0.567 | 16.0 | 0.564 | 79.5\n**32K × 10 + RL (FoldGRPO)** | **0.620** | **19.2** | **0.580** | **96.5**\n\n**§2 分任务/分场景深度分析（每个维度100字以上）**\n-   **BrowseComp-Plus (深度研究)**：本文的FoldGRPO智能体取得了**0.620的Pass@1**。这**超越了所有使用相同36B模型的基线**：比327K长上下文ReAct+RL（0.540）**绝对提升8.0个百分点（相对提升14.8%）**，比32K总结代理+RL（0.527）**绝对提升9.3个百分点**。其性能甚至**接近或超过了部分100B+参数的模型**（如GLM-4.5-Air的0.566）。提升原因在于智能体学会了将令牌密集的网页搜索和浏览操作**卸载到分支中**，保持主线程专注于高层规划和信息整合，从而更有效地利用有限的32K活动上下文。\n-   **SWE-Bench Verified (智能体软件工程)**：FoldGRPO智能体取得了**0.580的Pass@1**。同样**优于所有36B基线**：比327K长上下文ReAct+RL（0.574）**绝对提升0.6个百分点**，比32K总结代理+RL（0.550）**绝对提升3.0个百分点**。在SWE任务中，智能体学会了创建分支来处理独立的代码模块探索或复杂的bash命令序列，避免了在主要编辑上下文中堆积大量代码和输出，从而更清晰地跟踪代码修改的逻辑主线。\n-   **按任务难度分析**：RL训练（特别是FoldGRPO）在所有难度级别（易、中、难）上都带来了**一致的性能提升**，并且在**中等和困难子集上的提升幅度最大**。这表明本文方法尤其擅长处理那些需要更复杂、更长上下文管理的复杂问题。例如，在困难任务上，智能体通过RL训练后，响应长度从约100K tokens增长到超过160K tokens，说明它学会了为复杂问题分配更多的交互和计算资源。\n\n**§3 效率与开销的定量对比**\n-   **上下文效率**：本文方法仅使用**32K的活动上下文窗口**，通过最多10个分支，理论最大总令牌消耗为327K。而达到相似性能的ReAct基线需要**完整的327K长上下文窗口**。因此，本文方法在**保持高性能的同时，将任一时刻所需的活跃显存（KV-cache）降低了90%**（从327K降至32K）。\n-   **训练速度**：由于采用了动态折叠上下文，FoldGRPO在训练时**每个训练步骤的时间成本低于使用327K上下文的ReAct模型**（原文图8显示327K ReAct模型需要更长的训练时间）。\n\n**§4 消融实验结果详解**\n-   **RL算法消融（FoldGRPO vs. GRPO）**：在BrowseComp-Plus上，使用FoldGRPO训练的智能体Pass@1为0.620，而使用标准GRPO训练的为0.567，**绝对提升5.3个百分点（相对提升9.3%）**。在SWE-Bench上，FoldGRPO（0.580）比GRPO（0.564）**绝对提升1.6个百分点**。这证明了**过程奖励和动态折叠上下文对于学习有效分支行为至关重要**。\n-   **行为统计消融（表2）**：\n    -   **完成率（Finish）**：在BrowseComp上，FoldGRPO将完成率从GRPO的0.738提升至0.935，**绝对提升19.7个百分点**，表明智能体学会了在上下文限制内完成任务，避免因主线程过长而中断。\n    -   **主轨迹长度（Main Len）**：在BrowseComp上，FoldGRPO将主线程长度从GRPO的22,285 tokens大幅压缩至7,752 tokens，**压缩比达到65.2%**，实现了显著的上下文压缩。\n    -   **范围准确率（Scope）**：在BrowseComp上，FoldGRPO将分支内行为与提示相符的比例从GRPO的0.762提升至0.895，**绝对提升13.3个百分点**，表明过程奖励中的“超出范围惩罚”有效引导了智能体专注于子任务。\n\n**§5 案例分析/定性分析（如有）**\n论文图7展示了一个深度研究案例：查询关于寻找符合特定条件的研究出版物。智能体首先在主线程规划，然后创建分支进行初步搜索和验证，获得关键见解但未完全确认。接着，它创建另一个分支扩大搜索范围，最终找到正确答案。在这个过程中，**4个分支将完整的107K令牌上下文压缩到仅6K令牌的主线程上下文中**，压缩率超过94%。这直观展示了上下文折叠如何通过保留关键摘要、丢弃冗余细节来维持一个紧凑且信息丰富的工作记忆。",
    "conclusion_and_future_work": "**§1 本文核心贡献总结**\n1.  **提出上下文折叠（Context Folding）机制**：引入了`branch`和`return`两个特殊工具，使智能体能够**主动管理其工作上下文**，通过创建和折叠子轨迹来处理长视野任务，从根本上解决了上下文线性膨胀的问题。\n2.  **提出FoldGRPO强化学习框架**：设计了**端到端的RL算法**，通过**动态折叠的LLM上下文**和**密集的令牌级过程奖励**（如未折叠令牌惩罚、超出范围惩罚），有效地训练智能体掌握上下文折叠技能，在BrowseComp-Plus和SWE-Bench上分别带来**20.0%和8.8%的绝对性能提升**。\n3.  **实证验证了高效性与可扩展性**：实验表明，仅使用**32K活动上下文**的折叠智能体，其性能**匹配甚至超越了需要327K全长上下文的ReAct基线**，同时**显著优于基于总结的上下文管理方法**，并展现出良好的长度泛化能力。\n\n**§2 局限性（作者自述）**\n原文在结论部分未明确列出局限性。但从实验部分可推断：\n-   方法在**深度优先（depth-first）**性质的任务（如BrowseComp, SWE）上表现良好，但对于**广度优先（breadth-first）**结构的任务（如WideSearch），其并行分支的潜力尚未充分验证（见4.5.3节并行分支实验）。\n-   训练依赖于**特定构造的训练数据集**（从BrowseComp-Plus和SWE相关数据集中筛选），其通用性有待在更广泛的任务上验证。\n\n**§3 未来研究方向（全量提取）**\n1.  **多层上下文折叠（Multi-layer Context Folding）**：开发**分层折叠策略**，其中折叠本身可以进一步被折叠，以实现更深层次的压缩。这类似于构建一个任务解决的抽象层次树",
    "source_file": "Scaling Long-Horizon LLM Agent via Context-Folding.md"
}