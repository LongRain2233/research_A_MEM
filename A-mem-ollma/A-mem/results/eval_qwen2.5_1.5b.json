{
  "model": "qwen2.5:1.5b",
  "dataset": "D:\\research\\A-mem-ollma\\A-mem\\data/locomo10.json",
  "total_questions": 1986,
  "category_distribution": {
    "2": 321,
    "3": 96,
    "1": 282,
    "4": 841,
    "5": 446
  },
  "aggregate_metrics": {
    "overall": {
      "exact_match": {
        "mean": 0.16163141993957703,
        "std": 0.36820506429158834,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 1986
      },
      "f1": {
        "mean": 0.3290572536855731,
        "std": 0.3757560252454569,
        "median": 0.18181818181818182,
        "min": 0.0,
        "max": 1.0,
        "count": 1986
      },
      "rouge1_f": {
        "mean": 0.34153655061536886,
        "std": 0.3753330182469563,
        "median": 0.21525096525096524,
        "min": 0.0,
        "max": 1.0,
        "count": 1986
      },
      "rouge2_f": {
        "mean": 0.18670504715905012,
        "std": 0.3455147102678503,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 1986
      },
      "rougeL_f": {
        "mean": 0.33469753743308794,
        "std": 0.37455445897077544,
        "median": 0.1904761904761905,
        "min": 0.0,
        "max": 1.0,
        "count": 1986
      },
      "bleu1": {
        "mean": 0.2743622954021131,
        "std": 0.3653961105113118,
        "median": 0.08986579282344431,
        "min": 0,
        "max": 1.0,
        "count": 1986
      },
      "bleu2": {
        "mean": 0.20206797704647794,
        "std": 0.32519762695741816,
        "median": 0.028775278567031985,
        "min": 0,
        "max": 1.0,
        "count": 1986
      },
      "bleu3": {
        "mean": 0.16122837023230763,
        "std": 0.2870469111684688,
        "median": 0.020427282860665447,
        "min": 0,
        "max": 1.0,
        "count": 1986
      },
      "bleu4": {
        "mean": 0.13147489110649777,
        "std": 0.2496357006930366,
        "median": 0.016185662876867617,
        "min": 0,
        "max": 1.0,
        "count": 1986
      },
      "bert_precision": {
        "mean": 0.9052895681383628,
        "std": 0.0665021003526496,
        "median": 0.8939506411552429,
        "min": 0.0,
        "max": 1.000000238418579,
        "count": 1985
      },
      "bert_recall": {
        "mean": 0.900060121929015,
        "std": 0.06937614479622,
        "median": 0.8909922242164612,
        "min": 0.0,
        "max": 1.000000238418579,
        "count": 1985
      },
      "bert_f1": {
        "mean": 0.9019560830410633,
        "std": 0.06936997086030565,
        "median": 0.8906096518039703,
        "min": 0.0,
        "max": 1.000000238418579,
        "count": 1986
      },
      "meteor": {
        "mean": 0.2559450028583678,
        "std": 0.33983098520854416,
        "median": 0.09090909090909091,
        "min": 0.0,
        "max": 0.9998177842565598,
        "count": 1986
      },
      "sbert_similarity": {
        "mean": 0.522741524193601,
        "std": 0.3244324670505804,
        "median": 0.5088258385658264,
        "min": -0.0958837941288948,
        "max": 1.000000238418579,
        "count": 1986
      }
    },
    "category_1": {
      "exact_match": {
        "mean": 0.04609929078014184,
        "std": 0.21007293446108075,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 282
      },
      "f1": {
        "mean": 0.20602682003894582,
        "std": 0.2821146071891639,
        "median": 0.08,
        "min": 0.0,
        "max": 1.0,
        "count": 282
      },
      "rouge1_f": {
        "mean": 0.21853959114514127,
        "std": 0.28379770828232986,
        "median": 0.10263157894736841,
        "min": 0.0,
        "max": 1.0,
        "count": 282
      },
      "rouge2_f": {
        "mean": 0.05901502126102576,
        "std": 0.15983751924894124,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 282
      },
      "rougeL_f": {
        "mean": 0.20852540630104272,
        "std": 0.280653323552477,
        "median": 0.09761904761904763,
        "min": 0.0,
        "max": 1.0,
        "count": 282
      },
      "bleu1": {
        "mean": 0.14080283290958942,
        "std": 0.24047223165592962,
        "median": 0.01559132342588665,
        "min": 0,
        "max": 1.0,
        "count": 282
      },
      "bleu2": {
        "mean": 0.07122596029488193,
        "std": 0.14328372883366544,
        "median": 0.007697538253446426,
        "min": 0,
        "max": 1.0,
        "count": 282
      },
      "bleu3": {
        "mean": 0.04770421109743159,
        "std": 0.09952835728043147,
        "median": 0.0042604621896972835,
        "min": 0,
        "max": 0.7788007830714049,
        "count": 282
      },
      "bleu4": {
        "mean": 0.03629336836027575,
        "std": 0.08046031901362174,
        "median": 0.0033037000233624965,
        "min": 0,
        "max": 0.7788007830714049,
        "count": 282
      },
      "bert_precision": {
        "mean": 0.8880564862109245,
        "std": 0.043437839733529146,
        "median": 0.8837191462516785,
        "min": 0.7811535596847534,
        "max": 1.0,
        "count": 282
      },
      "bert_recall": {
        "mean": 0.8767394918076535,
        "std": 0.0457004215429361,
        "median": 0.8688885867595673,
        "min": 0.7599582076072693,
        "max": 1.0,
        "count": 282
      },
      "bert_f1": {
        "mean": 0.8820892942289934,
        "std": 0.04194753502862462,
        "median": 0.8749602735042572,
        "min": 0.7867491245269775,
        "max": 1.0,
        "count": 282
      },
      "meteor": {
        "mean": 0.11187945682116492,
        "std": 0.1804199163029663,
        "median": 0.0,
        "min": 0.0,
        "max": 0.9375,
        "count": 282
      },
      "sbert_similarity": {
        "mean": 0.45266846856036297,
        "std": 0.2535379861089999,
        "median": 0.42295853793621063,
        "min": -0.033974725753068924,
        "max": 1.0000001192092896,
        "count": 282
      }
    },
    "category_2": {
      "exact_match": {
        "mean": 0.018691588785046728,
        "std": 0.13564487719881047,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 321
      },
      "f1": {
        "mean": 0.24622505244483975,
        "std": 0.261469081107673,
        "median": 0.25,
        "min": 0.0,
        "max": 1.0,
        "count": 321
      },
      "rouge1_f": {
        "mean": 0.25801762273724893,
        "std": 0.25796614286829267,
        "median": 0.25,
        "min": 0.0,
        "max": 1.0,
        "count": 321
      },
      "rouge2_f": {
        "mean": 0.04977613248641286,
        "std": 0.15606329225627988,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 321
      },
      "rougeL_f": {
        "mean": 0.25058092651550595,
        "std": 0.24986598871018126,
        "median": 0.25,
        "min": 0.0,
        "max": 1.0,
        "count": 321
      },
      "bleu1": {
        "mean": 0.16199900474021225,
        "std": 0.21866114640884757,
        "median": 0.049787068367863944,
        "min": 0,
        "max": 1.0,
        "count": 321
      },
      "bleu2": {
        "mean": 0.08421248120707792,
        "std": 0.12862556097815958,
        "median": 0.018354763685601443,
        "min": 0,
        "max": 0.5773502691896257,
        "count": 321
      },
      "bleu3": {
        "mean": 0.055990128795193175,
        "std": 0.08182636169051058,
        "median": 0.014286435890917338,
        "min": 0,
        "max": 0.4888069165703076,
        "count": 321
      },
      "bleu4": {
        "mean": 0.045245949214810545,
        "std": 0.06146878353976085,
        "median": 0.012274570264879268,
        "min": 0,
        "max": 0.41113361690051975,
        "count": 321
      },
      "bert_precision": {
        "mean": 0.9049570239221567,
        "std": 0.0410046249669234,
        "median": 0.9051557779312134,
        "min": 0.7730153799057007,
        "max": 1.0,
        "count": 321
      },
      "bert_recall": {
        "mean": 0.8946590399445031,
        "std": 0.04404208665246674,
        "median": 0.8927247524261475,
        "min": 0.8013855814933777,
        "max": 1.0,
        "count": 321
      },
      "bert_f1": {
        "mean": 0.8994649709942185,
        "std": 0.03929095081868926,
        "median": 0.8994058966636658,
        "min": 0.7939959168434143,
        "max": 1.0,
        "count": 321
      },
      "meteor": {
        "mean": 0.11679747406746362,
        "std": 0.1631677746335475,
        "median": 0.08771929824561403,
        "min": 0.0,
        "max": 0.9814814814814815,
        "count": 321
      },
      "sbert_similarity": {
        "mean": 0.6236418979329483,
        "std": 0.22020454411860743,
        "median": 0.6698888540267944,
        "min": 0.04326232522726059,
        "max": 1.0000001192092896,
        "count": 321
      }
    },
    "category_3": {
      "exact_match": {
        "mean": 0.041666666666666664,
        "std": 0.20087527770480482,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 96
      },
      "f1": {
        "mean": 0.11657544457572992,
        "std": 0.2440632164753989,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 96
      },
      "rouge1_f": {
        "mean": 0.13820986512176936,
        "std": 0.25366985339907294,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 96
      },
      "rouge2_f": {
        "mean": 0.02559285206876963,
        "std": 0.1071753025339284,
        "median": 0.0,
        "min": 0.0,
        "max": 0.7272727272727273,
        "count": 96
      },
      "rougeL_f": {
        "mean": 0.13364314683553888,
        "std": 0.25071072446960085,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 96
      },
      "bleu1": {
        "mean": 0.08618226906324226,
        "std": 0.21801743586357444,
        "median": 0.0,
        "min": 0,
        "max": 1.0,
        "count": 96
      },
      "bleu2": {
        "mean": 0.03475663256464906,
        "std": 0.08832500742429407,
        "median": 0.0,
        "min": 0,
        "max": 0.5270462766947299,
        "count": 96
      },
      "bleu3": {
        "mean": 0.023381460987595058,
        "std": 0.06861405521721686,
        "median": 0.0,
        "min": 0,
        "max": 0.49543631365378615,
        "count": 96
      },
      "bleu4": {
        "mean": 0.01897633358496117,
        "std": 0.05902890315698628,
        "median": 0.0,
        "min": 0,
        "max": 0.44632361378533286,
        "count": 96
      },
      "bert_precision": {
        "mean": 0.8853918301562468,
        "std": 0.04465730886489256,
        "median": 0.8753678202629089,
        "min": 0.7880029082298279,
        "max": 0.9999999403953552,
        "count": 96
      },
      "bert_recall": {
        "mean": 0.8720138358573118,
        "std": 0.050499162195518046,
        "median": 0.8666876554489136,
        "min": 0.7667020559310913,
        "max": 0.9999999403953552,
        "count": 96
      },
      "bert_f1": {
        "mean": 0.8782903657605251,
        "std": 0.0443259927968007,
        "median": 0.8731498718261719,
        "min": 0.7772065997123718,
        "max": 0.9999999403953552,
        "count": 96
      },
      "meteor": {
        "mean": 0.06978618504750303,
        "std": 0.15441873326464242,
        "median": 0.0,
        "min": 0.0,
        "max": 0.7488207547169812,
        "count": 96
      },
      "sbert_similarity": {
        "mean": 0.40128821942198556,
        "std": 0.25058112486681017,
        "median": 0.34488679468631744,
        "min": -0.07359732687473297,
        "max": 1.0,
        "count": 96
      }
    },
    "category_4": {
      "exact_match": {
        "mean": 0.12604042806183116,
        "std": 0.33209241869410727,
        "median": 0,
        "min": 0,
        "max": 1,
        "count": 841
      },
      "f1": {
        "mean": 0.31346994395640976,
        "std": 0.3568886893464901,
        "median": 0.16666666666666666,
        "min": 0.0,
        "max": 1.0,
        "count": 841
      },
      "rouge1_f": {
        "mean": 0.32834619751068217,
        "std": 0.3588944618484495,
        "median": 0.1904761904761905,
        "min": 0.0,
        "max": 1.0,
        "count": 841
      },
      "rouge2_f": {
        "mean": 0.1692799621355965,
        "std": 0.3144608014887324,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 841
      },
      "rougeL_f": {
        "mean": 0.3192998000235939,
        "std": 0.35757701923217194,
        "median": 0.16666666666666666,
        "min": 0.0,
        "max": 1.0,
        "count": 841
      },
      "bleu1": {
        "mean": 0.2605878213083719,
        "std": 0.3413936065955007,
        "median": 0.10000000000000002,
        "min": 0,
        "max": 1.0,
        "count": 841
      },
      "bleu2": {
        "mean": 0.1832708238275151,
        "std": 0.2928407844070925,
        "median": 0.033149677206589796,
        "min": 0,
        "max": 1.0,
        "count": 841
      },
      "bleu3": {
        "mean": 0.13548553103222882,
        "std": 0.23537848797327665,
        "median": 0.022461584318318094,
        "min": 0,
        "max": 1.0,
        "count": 841
      },
      "bleu4": {
        "mean": 0.10352511579254904,
        "std": 0.18829033319017466,
        "median": 0.018653567503517977,
        "min": 0,
        "max": 1.0,
        "count": 841
      },
      "bert_precision": {
        "mean": 0.8990047819557644,
        "std": 0.05899416485496177,
        "median": 0.8881061375141144,
        "min": 0.0,
        "max": 1.0000001192092896,
        "count": 840
      },
      "bert_recall": {
        "mean": 0.8996460258960723,
        "std": 0.05949675164382637,
        "median": 0.8902994990348816,
        "min": 0.0,
        "max": 1.0000001192092896,
        "count": 840
      },
      "bert_f1": {
        "mean": 0.8980230341768435,
        "std": 0.06533781763020127,
        "median": 0.886437177658081,
        "min": 0.0,
        "max": 1.0000001192092896,
        "count": 841
      },
      "meteor": {
        "mean": 0.254809714047854,
        "std": 0.31569017482518313,
        "median": 0.10869565217391304,
        "min": 0.0,
        "max": 0.9995,
        "count": 841
      },
      "sbert_similarity": {
        "mean": 0.4744564981225271,
        "std": 0.31142809829474616,
        "median": 0.41491031646728516,
        "min": -0.051031116396188736,
        "max": 1.000000238418579,
        "count": 841
      }
    },
    "category_5": {
      "exact_match": {
        "mean": 0.4304932735426009,
        "std": 0.49570127711599493,
        "median": 0.0,
        "min": 0,
        "max": 1,
        "count": 446
      },
      "f1": {
        "mean": 0.5415929040070879,
        "std": 0.4552330208529821,
        "median": 0.6666666666666666,
        "min": 0.0,
        "max": 1.0,
        "count": 446
      },
      "rouge1_f": {
        "mean": 0.5480548626958796,
        "std": 0.4540899324627518,
        "median": 0.6666666666666666,
        "min": 0.0,
        "max": 1.0,
        "count": 446
      },
      "rouge2_f": {
        "mean": 0.4335302403127515,
        "std": 0.4676185748524414,
        "median": 0.0,
        "min": 0.0,
        "max": 1.0,
        "count": 446
      },
      "rougeL_f": {
        "mean": 0.5473268911158901,
        "std": 0.4546544927084218,
        "median": 0.6666666666666666,
        "min": 0.0,
        "max": 1.0,
        "count": 446
      },
      "bleu1": {
        "mean": 0.5061605016055432,
        "std": 0.4607110813859976,
        "median": 0.45454545454545453,
        "min": 0,
        "max": 1.0,
        "count": 446
      },
      "bleu2": {
        "mean": 0.4410799900863901,
        "std": 0.4455323692318867,
        "median": 0.316227766016838,
        "min": 0,
        "max": 1.0,
        "count": 446
      },
      "bleu3": {
        "mean": 0.38696451245676383,
        "std": 0.4222633351510456,
        "median": 0.2187761623949553,
        "min": 0,
        "max": 1.0,
        "count": 446
      },
      "bleu4": {
        "mean": 0.3306370039378085,
        "std": 0.38528220639083754,
        "median": 0.1778279410038923,
        "min": 0,
        "max": 1.0,
        "count": 446
      },
      "bert_precision": {
        "mean": 0.9325449023011554,
        "std": 0.09610776297512581,
        "median": 0.9336114227771759,
        "min": 0.0,
        "max": 1.000000238418579,
        "count": 446
      },
      "bert_recall": {
        "mean": 0.9255095594666999,
        "std": 0.10169404218331517,
        "median": 0.9399987459182739,
        "min": 0.0,
        "max": 1.000000238418579,
        "count": 446
      },
      "bert_f1": {
        "mean": 0.92882084619304,
        "std": 0.09817607534521536,
        "median": 0.9324268102645874,
        "min": 0.0,
        "max": 1.000000238418579,
        "count": 446
      },
      "meteor": {
        "mean": 0.48939537309123016,
        "std": 0.4393274229311328,
        "median": 0.5,
        "min": 0.0,
        "max": 0.9998177842565598,
        "count": 446
      },
      "sbert_similarity": {
        "mean": 0.611617770610845,
        "std": 0.41425974524636044,
        "median": 0.8232377171516418,
        "min": -0.0958837941288948,
        "max": 1.000000238418579,
        "count": 446
      }
    }
  },
  "individual_results": [
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ support group?",
      "prediction": "20 July, 2023",
      "reference": "7 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9400274157524109,
        "bert_recall": 0.9613881707191467,
        "bert_f1": 0.950587809085846,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8017004728317261
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie paint a sunrise?",
      "prediction": "last year",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8761670589447021,
        "bert_recall": 0.8760063648223877,
        "bert_f1": 0.8760867118835449,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371804714203
      }
    },
    {
      "sample_id": 0,
      "question": "What fields would Caroline be likely to pursue in her educaton?",
      "prediction": "counseling or working in mental health",
      "reference": "Psychology, counseling certification",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8539317846298218,
        "bert_recall": 0.8617445826530457,
        "bert_f1": 0.8578203916549683,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.6807615756988525
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline research?",
      "prediction": "Life is about learning and exploring.",
      "reference": "Adoption agencies",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8597315549850464,
        "bert_recall": 0.8571322560310364,
        "bert_f1": 0.8584299087524414,
        "meteor": 0.0,
        "sbert_similarity": 0.07966974377632141
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's identity?",
      "prediction": "Caroline identifies as trans.",
      "reference": "Transgender woman",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8903799057006836,
        "bert_recall": 0.8909751772880554,
        "bert_f1": 0.8906774520874023,
        "meteor": 0.0,
        "sbert_similarity": 0.5271410346031189
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie run a charity race?",
      "prediction": "2023-05-25",
      "reference": "The sunday before 25 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8905057311058044,
        "bert_recall": 0.8533567190170288,
        "bert_f1": 0.8715355396270752,
        "meteor": 0.0,
        "sbert_similarity": 0.5880062580108643
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie planning on going camping?",
      "prediction": "15 July 2023",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9427533745765686,
        "bert_recall": 0.9602571725845337,
        "bert_f1": 0.951424777507782,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8751159906387329
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's relationship status?",
      "prediction": "Single",
      "reference": "Single",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline give a speech at a school?",
      "prediction": "20 July 2023",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9412370920181274,
        "bert_recall": 0.9011907577514648,
        "bert_f1": 0.9207786917686462,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.6533970832824707
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline meet up with her friends, family, and mentors?",
      "prediction": "Last week",
      "reference": "The week before 9 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8895326852798462,
        "bert_recall": 0.8208845853805542,
        "bert_f1": 0.8538310527801514,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5041570663452148
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Caroline had her current group of friends for?",
      "prediction": "2 years",
      "reference": "4 years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9887830018997192,
        "bert_recall": 0.9887830018997192,
        "bert_f1": 0.9887830018997192,
        "meteor": 0.25,
        "sbert_similarity": 0.8421549201011658
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Caroline move from 4 years ago?",
      "prediction": "Adoption process and future of the adopted child",
      "reference": "Sweden",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8344739675521851,
        "bert_recall": 0.8333072662353516,
        "bert_f1": 0.8338901996612549,
        "meteor": 0.0,
        "sbert_similarity": 0.07822366058826447
      }
    },
    {
      "sample_id": 0,
      "question": "How long ago was Caroline's 18th birthday?",
      "prediction": "ten years ago",
      "reference": "10 years ago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.965503990650177,
        "bert_recall": 0.965503990650177,
        "bert_f1": 0.965503990650177,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9706802368164062
      }
    },
    {
      "sample_id": 0,
      "question": "What career path has Caroline decided to persue?",
      "prediction": "counseling or mental health",
      "reference": "counseling or mental health for Transgender people",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727273,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7272727272727273,
        "bleu1": 0.4723665527410147,
        "bleu2": 0.4723665527410147,
        "bleu3": 0.4723665527410147,
        "bleu4": 0.4723665527410147,
        "bert_precision": 0.9656624794006348,
        "bert_recall": 0.9319413304328918,
        "bert_f1": 0.9485023021697998,
        "meteor": 0.5923507462686567,
        "sbert_similarity": 0.676784873008728
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline still want to pursue counseling as a career if she hadn't received support growing up?",
      "prediction": "No",
      "reference": "Likely no",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9279205203056335,
        "bert_recall": 0.8702245950698853,
        "bert_f1": 0.89814692735672,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.6290402412414551
      }
    },
    {
      "sample_id": 0,
      "question": "What activities does Melanie partake in?",
      "prediction": "games, food, hang out together, playing violin",
      "reference": "pottery, camping, painting, swimming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.3,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.03585339536577223,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8737421035766602,
        "bert_recall": 0.8854826092720032,
        "bert_f1": 0.8795732259750366,
        "meteor": 0.0,
        "sbert_similarity": 0.34268274903297424
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie sign up for a pottery class?",
      "prediction": "17 August",
      "reference": "2 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9176473021507263,
        "bert_recall": 0.8814741373062134,
        "bert_f1": 0.8991971015930176,
        "meteor": 0.0,
        "sbert_similarity": 0.5544170141220093
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline going to the transgender conference?",
      "prediction": "3 July",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9164230823516846,
        "bert_recall": 0.8840759992599487,
        "bert_f1": 0.8999589681625366,
        "meteor": 0.25,
        "sbert_similarity": 0.7475788593292236
      }
    },
    {
      "sample_id": 0,
      "question": "Where has Melanie camped?",
      "prediction": "Melanie has camped in the mountains.",
      "reference": "beach, mountains, forest",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.891248345375061,
        "bert_recall": 0.886396050453186,
        "bert_f1": 0.8888155817985535,
        "meteor": 0.0,
        "sbert_similarity": 0.2961304485797882
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's kids like?",
      "prediction": "nature",
      "reference": "dinosaurs, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9019801616668701,
        "bert_recall": 0.8497076034545898,
        "bert_f1": 0.875063955783844,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7095581889152527
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the museum?",
      "prediction": "8 May, 2023",
      "reference": "5 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9577499628067017,
        "bert_recall": 0.9577577114105225,
        "bert_f1": 0.9577538371086121,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8667038679122925
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline have a picnic?",
      "prediction": "8 July, 2023",
      "reference": "The week before 6 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.9392896294593811,
        "bert_recall": 0.9119795560836792,
        "bert_f1": 0.9254330992698669,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7992604970932007
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline likely have Dr. Seuss books on her bookshelf?",
      "prediction": "Yes",
      "reference": "Yes, since she collects classic children's books",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.00033546262790251185,
        "bleu2": 0.0001060825974037491,
        "bleu3": 7.33912263594384e-05,
        "bleu4": 5.965462840365855e-05,
        "bert_precision": 0.9121391177177429,
        "bert_recall": 0.8539786338806152,
        "bert_f1": 0.8821011781692505,
        "meteor": 0.0,
        "sbert_similarity": 0.24328027665615082
      }
    },
    {
      "sample_id": 0,
      "question": "What books has Melanie read?",
      "prediction": "favorite childhood memories",
      "reference": "\"Nothing is Impossible\", \"Charlotte's Web\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8673453330993652,
        "bert_recall": 0.8380610942840576,
        "bert_f1": 0.852451741695404,
        "meteor": 0.0,
        "sbert_similarity": 0.09707914292812347
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to destress?",
      "prediction": "Melanie runs longer since their last chat.",
      "reference": "Running, pottery",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8471943140029907,
        "bert_recall": 0.8330022096633911,
        "bert_f1": 0.8400383591651917,
        "meteor": 0.0,
        "sbert_similarity": 0.24443723261356354
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the LGBTQ conference?",
      "prediction": "2023",
      "reference": "10 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9345014095306396,
        "bert_recall": 0.894832968711853,
        "bert_f1": 0.9142370820045471,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7750519514083862
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie read the book \"nothing is impossible\"?",
      "prediction": "20 July, 2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8752356171607971,
        "bert_recall": 0.9211703538894653,
        "bert_f1": 0.8976156711578369,
        "meteor": 0.0,
        "sbert_similarity": 0.7409049868583679
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline pursue writing as a career option?",
      "prediction": "No",
      "reference": "LIkely no; though she likes reading, she wants to be a counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 2.2603294069810542e-06,
        "bleu2": 7.14778918831783e-07,
        "bleu3": 4.945061934077801e-07,
        "bleu4": 4.0194972443398975e-07,
        "bert_precision": 0.8570750951766968,
        "bert_recall": 0.8017058968544006,
        "bert_f1": 0.8284663558006287,
        "meteor": 0.0,
        "sbert_similarity": 0.1082494780421257
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to the adoption meeting?",
      "prediction": "last Friday",
      "reference": "The friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8760493993759155,
        "bert_recall": 0.8210278749465942,
        "bert_f1": 0.8476467132568359,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5948777794837952
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the pottery workshop?",
      "prediction": "3 July 2023",
      "reference": "The Friday before 15 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.21239529438966132,
        "bleu3": 0.11974448045963429,
        "bleu4": 0.08839449202722564,
        "bert_precision": 0.9414175152778625,
        "bert_recall": 0.8881641030311584,
        "bert_f1": 0.9140157699584961,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.7403748035430908
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered a member of the LGBTQ community?",
      "prediction": "Yes",
      "reference": "Likely no, she does not refer to herself as part of it",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8459942936897278,
        "bert_recall": 0.8135164976119995,
        "bert_f1": 0.8294376134872437,
        "meteor": 0.0,
        "sbert_similarity": 0.17943957448005676
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in June?",
      "prediction": "27 June, 2023",
      "reference": "The week before 27 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9405211210250854,
        "bert_recall": 0.9120429754257202,
        "bert_f1": 0.9260631203651428,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.832425594329834
      }
    },
    {
      "sample_id": 0,
      "question": "What LGBTQ+ events has Caroline participated in?",
      "prediction": "Carolinesays:",
      "reference": "Pride parade, school speech, support group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7956180572509766,
        "bert_recall": 0.813331127166748,
        "bert_f1": 0.8043770790100098,
        "meteor": 0.0,
        "sbert_similarity": 0.1809406280517578
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go to a pride parade during the summer?",
      "prediction": "20 July",
      "reference": "The week before 3 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.9055333137512207,
        "bert_recall": 0.8585836887359619,
        "bert_f1": 0.8814337253570557,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5047455430030823
      }
    },
    {
      "sample_id": 0,
      "question": "What events has Caroline participated in to help children?",
      "prediction": "Caroline has participated in adoption agencies research and attended LGBTQ support groups.",
      "reference": "Mentoring program, school speech",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8487591743469238,
        "bert_recall": 0.8442116379737854,
        "bert_f1": 0.8464792966842651,
        "meteor": 0.0,
        "sbert_similarity": 0.1804981827735901
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go camping in July?",
      "prediction": "25 May 2023",
      "reference": "two weekends before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9375103712081909,
        "bert_recall": 0.8843295574188232,
        "bert_f1": 0.9101437926292419,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.6492732167243958
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a mentorship program?",
      "prediction": "2023",
      "reference": "The weekend before 17 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9061209559440613,
        "bert_recall": 0.8231768608093262,
        "bert_f1": 0.862659752368927,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.627448558807373
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie paint recently?",
      "prediction": "A painting I did recently.",
      "reference": "sunset",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8579288721084595,
        "bert_recall": 0.8462859392166138,
        "bert_f1": 0.8520675897598267,
        "meteor": 0.0,
        "sbert_similarity": 0.15551766753196716
      }
    },
    {
      "sample_id": 0,
      "question": "What activities has Melanie done with her family?",
      "prediction": "played games, ate good food, and just hung out together.",
      "reference": "Pottery, painting, camping, museum, swimming, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8508895635604858,
        "bert_recall": 0.8178927302360535,
        "bert_f1": 0.8340649604797363,
        "meteor": 0.0,
        "sbert_similarity": 0.22723115980625153
      }
    },
    {
      "sample_id": 0,
      "question": "In what ways is Caroline participating in the LGBTQ community?",
      "prediction": "Caroline participates by attending school events on transgender journeys and joining LGBTQ activist groups, as well as attending a conference focused on advocacy.",
      "reference": "Joining activist group, going to pride parades, participating in an art show, mentoring program",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.2162162162162162,
        "rouge2_f": 0.05714285714285715,
        "rougeL_f": 0.16216216216216214,
        "bleu1": 0.12000000000000001,
        "bleu2": 0.022360679774997897,
        "bleu3": 0.013529754476103547,
        "bleu4": 0.009970222994544455,
        "bert_precision": 0.8720399141311646,
        "bert_recall": 0.8730232119560242,
        "bert_f1": 0.8725313544273376,
        "meteor": 0.10067114093959731,
        "sbert_similarity": 0.5148221254348755
      }
    },
    {
      "sample_id": 0,
      "question": "How many times has Melanie gone to the beach in 2023?",
      "prediction": "1",
      "reference": 2,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9691944718360901,
        "bert_recall": 0.9691944718360901,
        "bert_f1": 0.9691944718360901,
        "meteor": 0.0,
        "sbert_similarity": 0.8054155111312866
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline join a new activist group?",
      "prediction": "July 9th",
      "reference": "The Tuesday before 20 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8960304260253906,
        "bert_recall": 0.8432999849319458,
        "bert_f1": 0.8688659071922302,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.6409107446670532
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be more interested in going to a national park or a theme park?",
      "prediction": "Melanie would likely prefer going to a national park.",
      "reference": "National park; she likes the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.4,
        "rouge2_f": 0.15384615384615385,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.2,
        "bleu2": 0.14907119849998599,
        "bleu3": 0.06705330284284157,
        "bleu4": 0.0446323613785333,
        "bert_precision": 0.9020847678184509,
        "bert_recall": 0.9037720561027527,
        "bert_f1": 0.9029276371002197,
        "meteor": 0.15873015873015875,
        "sbert_similarity": 0.638026773929596
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of art does Caroline make?",
      "prediction": "Abstract",
      "reference": "abstract art",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.8076551556587219,
        "bert_recall": 0.8311402797698975,
        "bert_f1": 0.8192294836044312,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7856197357177734
      }
    },
    {
      "sample_id": 0,
      "question": "When is Melanie's daughter's birthday?",
      "prediction": "20 July 2023",
      "reference": "13 August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8824952840805054,
        "bert_recall": 0.928849995136261,
        "bert_f1": 0.9050794243812561,
        "meteor": 0.0,
        "sbert_similarity": 0.5167979001998901
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline attend a pride parade in August?",
      "prediction": "2023",
      "reference": "The Friday before 14 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.904046356678009,
        "bert_recall": 0.8216317892074585,
        "bert_f1": 0.8608710765838623,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.6382638216018677
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie be considered an ally to the transgender community?",
      "prediction": "Yes",
      "reference": "Yes, she is supportive",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.00579191356895553,
        "bleu3": 0.004007025187889067,
        "bleu4": 0.003257032351754417,
        "bert_precision": 0.9382220506668091,
        "bert_recall": 0.87865149974823,
        "bert_f1": 0.9074602127075195,
        "meteor": 0.0,
        "sbert_similarity": 0.28309470415115356
      }
    },
    {
      "sample_id": 0,
      "question": "Who supports Caroline when she has a negative experience?",
      "prediction": "Friends and community",
      "reference": "Her mentors, family, and friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17573142541048448,
        "bleu2": 0.04812603287992944,
        "bleu3": 0.040131988279406595,
        "bleu4": 0.035617249382866124,
        "bert_precision": 0.9071347713470459,
        "bert_recall": 0.8977760076522827,
        "bert_f1": 0.9024311304092407,
        "meteor": 0.20833333333333331,
        "sbert_similarity": 0.3915882110595703
      }
    },
    {
      "sample_id": 0,
      "question": "What types of pottery have Melanie and her kids made?",
      "prediction": "clay sculptures",
      "reference": "bowls, cup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8502025008201599,
        "bert_recall": 0.8612175583839417,
        "bert_f1": 0.8556745648384094,
        "meteor": 0.0,
        "sbert_similarity": 0.23033247888088226
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline and Melanie go to a pride fesetival together?",
      "prediction": "2023",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9776392579078674,
        "bert_recall": 0.9776392579078674,
        "bert_f1": 0.9776392579078674,
        "meteor": 0.0,
        "sbert_similarity": 0.9072221517562866
      }
    },
    {
      "sample_id": 0,
      "question": "What would Caroline's political leaning likely be?",
      "prediction": "Caroline is likely to lean towards progressive politics.",
      "reference": "Liberal",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8477497100830078,
        "bert_recall": 0.8597188591957092,
        "bert_f1": 0.853692352771759,
        "meteor": 0.0,
        "sbert_similarity": 0.3305216431617737
      }
    },
    {
      "sample_id": 0,
      "question": "What has Melanie painted?",
      "prediction": "Melaniesays: 'I painted a painting of a sunset last year.'",
      "reference": "Horse, sunset, sunrise",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.831510066986084,
        "bert_recall": 0.8709594011306763,
        "bert_f1": 0.8507776856422424,
        "meteor": 0.0,
        "sbert_similarity": 0.5072517395019531
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's pets' names?",
      "prediction": "pup and kitty",
      "reference": "Oliver, Luna, Bailey",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8441241979598999,
        "bert_recall": 0.8437861800193787,
        "bert_f1": 0.8439551591873169,
        "meteor": 0.0,
        "sbert_similarity": 0.41748642921447754
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline apply to adoption agencies?",
      "prediction": "This is when you took the first step towards becoming a mom.",
      "reference": "The week of 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.864669919013977,
        "bert_recall": 0.8231716156005859,
        "bert_f1": 0.8434106111526489,
        "meteor": 0.07575757575757576,
        "sbert_similarity": 0.07878866791725159
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline draw a self-portrait?",
      "prediction": "2023",
      "reference": "The week before 23 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9048652648925781,
        "bert_recall": 0.8417279720306396,
        "bert_f1": 0.8721554279327393,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.638247013092041
      }
    },
    {
      "sample_id": 0,
      "question": "What subject have Caroline and Melanie both painted?",
      "prediction": "Abstract art",
      "reference": "Sunsets",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8811995983123779,
        "bert_recall": 0.8336800336837769,
        "bert_f1": 0.8567814230918884,
        "meteor": 0.0,
        "sbert_similarity": 0.21250203251838684
      }
    },
    {
      "sample_id": 0,
      "question": "What symbols are important to Caroline?",
      "prediction": "roots",
      "reference": "Rainbow flag, transgender symbol",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8726975321769714,
        "bert_recall": 0.8154717087745667,
        "bert_f1": 0.8431146740913391,
        "meteor": 0.0,
        "sbert_similarity": -0.02691306546330452
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline encounter people on a hike and have a negative experience?",
      "prediction": "25 August 2023",
      "reference": "The week before 25 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.955288827419281,
        "bert_recall": 0.9140462875366211,
        "bert_f1": 0.9342125654220581,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.838809609413147
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie make a plate in pottery class?",
      "prediction": "25 August",
      "reference": "24 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9559719562530518,
        "bert_recall": 0.8978074789047241,
        "bert_f1": 0.9259772300720215,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.7131437063217163
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline be considered religious?",
      "prediction": "No",
      "reference": "Somewhat, but not extremely religious",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8561220765113831,
        "bert_recall": 0.8083587288856506,
        "bert_f1": 0.8315550684928894,
        "meteor": 0.0,
        "sbert_similarity": 0.2169199436903
      }
    },
    {
      "sample_id": 0,
      "question": "What instruments does Melanie play?",
      "prediction": "guitar",
      "reference": "clarinet and violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8814570903778076,
        "bert_recall": 0.8603768348693848,
        "bert_f1": 0.8707894086837769,
        "meteor": 0.0,
        "sbert_similarity": 0.3830392360687256
      }
    },
    {
      "sample_id": 0,
      "question": "What musical artists/bands has Melanie seen?",
      "prediction": "Bach, Mozart, Ed Sheeran",
      "reference": "Summer Sounds, Matt Patterson",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8165477514266968,
        "bert_recall": 0.8506207466125488,
        "bert_f1": 0.833236038684845,
        "meteor": 0.0,
        "sbert_similarity": 0.1263461410999298
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go to the park?",
      "prediction": "July 9th",
      "reference": "27 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8937286138534546,
        "bert_recall": 0.863886833190918,
        "bert_f1": 0.8785543441772461,
        "meteor": 0.0,
        "sbert_similarity": 0.6855852007865906
      }
    },
    {
      "sample_id": 0,
      "question": "When is Caroline's youth center putting on a talent show?",
      "prediction": "2023-08-14",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8649902939796448,
        "bert_recall": 0.9073399901390076,
        "bert_f1": 0.8856591582298279,
        "meteor": 0.0,
        "sbert_similarity": 0.6657427549362183
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie likely enjoy the song \"The Four Seasons\" by Vivaldi?",
      "prediction": "Yes",
      "reference": "Yes; it's classical music",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.91709965467453,
        "bert_recall": 0.8398867249488831,
        "bert_f1": 0.876796543598175,
        "meteor": 0.0,
        "sbert_similarity": 0.289455771446228
      }
    },
    {
      "sample_id": 0,
      "question": "What are some changes Caroline has faced during her transition journey?",
      "prediction": "Transitioning wasn't easy and acceptance wasn't either, but the help she got from friends, family, and people she looked up to was invaluable. They boosted her through tough times and helped her find out who she really is.",
      "reference": "Changes to her body, losing unsupportive friends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.1276595744680851,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0851063829787234,
        "bleu1": 0.08888888888888888,
        "bleu2": 0.014213381090374034,
        "bleu3": 0.008160855611090737,
        "bleu4": 0.005783214009218943,
        "bert_precision": 0.8493397235870361,
        "bert_recall": 0.8744708895683289,
        "bert_f1": 0.8617221117019653,
        "meteor": 0.09900990099009901,
        "sbert_similarity": 0.34788939356803894
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do with her family on hikes?",
      "prediction": "Melanie goes hiking with her family.",
      "reference": "Roast marshmallows, tell stories",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8857251405715942,
        "bert_recall": 0.8562963008880615,
        "bert_f1": 0.8707621693611145,
        "meteor": 0.0,
        "sbert_similarity": 0.13756534457206726
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline go biking with friends?",
      "prediction": "12:09 am on 13 September, 2023",
      "reference": "The weekend before 13 September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8737818002700806,
        "bert_recall": 0.9020897746086121,
        "bert_f1": 0.8877102136611938,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7477822303771973
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been practicing art?",
      "prediction": "Seven years",
      "reference": "Since 2016",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8994471430778503,
        "bert_recall": 0.8979287147521973,
        "bert_f1": 0.8986872434616089,
        "meteor": 0.0,
        "sbert_similarity": 0.42544734477996826
      }
    },
    {
      "sample_id": 0,
      "question": "What personality traits might Melanie say Caroline has?",
      "prediction": "caring heart",
      "reference": "Thoughtful, authentic, driven",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8387929797172546,
        "bert_recall": 0.8340814113616943,
        "bert_f1": 0.8364304900169373,
        "meteor": 0.0,
        "sbert_similarity": 0.3124227523803711
      }
    },
    {
      "sample_id": 0,
      "question": "What transgender-specific events has Caroline attended?",
      "prediction": "Carolinesays: Hey Melanie! How's it going? I wanted to tell you about my school event last week. It was awesome! I talked about my transgender journey and encouraged students to get involved in the LGBTQ community.",
      "reference": "Poetry reading, conference",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8090323209762573,
        "bert_recall": 0.8498802185058594,
        "bert_f1": 0.8289533853530884,
        "meteor": 0.0,
        "sbert_similarity": 0.23115581274032593
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Melanie read from Caroline's suggestion?",
      "prediction": "The Art of Loving",
      "reference": "\"Becoming Nicole\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8711302280426025,
        "bert_recall": 0.8912160396575928,
        "bert_f1": 0.8810586333274841,
        "meteor": 0.0,
        "sbert_similarity": 0.15092675387859344
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's friend adopt a child?",
      "prediction": "Last year",
      "reference": 2022,
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.882870078086853,
        "bert_recall": 0.8763824105262756,
        "bert_f1": 0.8796142935752869,
        "meteor": 0.0,
        "sbert_similarity": 0.3672371804714203
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie get hurt?",
      "prediction": "2023",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9493842720985413,
        "bert_recall": 0.9251530170440674,
        "bert_f1": 0.937112033367157,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8269596695899963
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie's family go on a roadtrip?",
      "prediction": "8 May, 2023",
      "reference": "The weekend before 20 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.05536842069051654,
        "bleu3": 0.04649251600872798,
        "bleu4": 0.04873039689743777,
        "bert_precision": 0.9310612678527832,
        "bert_recall": 0.8853945136070251,
        "bert_f1": 0.90765380859375,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7787668108940125
      }
    },
    {
      "sample_id": 0,
      "question": "How many children does Melanie have?",
      "prediction": "One",
      "reference": 3,
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9298871755599976,
        "bert_recall": 0.9298871755599976,
        "bert_f1": 0.9298871755599976,
        "meteor": 0.0,
        "sbert_similarity": 0.6139547824859619
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie go on a hike after the roadtrip?",
      "prediction": "15 July, 2023",
      "reference": "19 October 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9433063268661499,
        "bert_recall": 0.963848352432251,
        "bert_f1": 0.9534667134284973,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8680276870727539
      }
    },
    {
      "sample_id": 0,
      "question": "Would Melanie go on another roadtrip soon?",
      "prediction": "no",
      "reference": "Likely no; since this one went badly",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.0009118819655545162,
        "bleu2": 0.0002883623968383479,
        "bleu3": 0.00019949803698118587,
        "bleu4": 0.00016215809237314185,
        "bert_precision": 0.8541873097419739,
        "bert_recall": 0.8115471005439758,
        "bert_f1": 0.8323214054107666,
        "meteor": 0.0,
        "sbert_similarity": 0.35138261318206787
      }
    },
    {
      "sample_id": 0,
      "question": "What items has Melanie bought?",
      "prediction": "necklace",
      "reference": "Figurines, shoes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8702006340026855,
        "bert_recall": 0.8287681341171265,
        "bert_f1": 0.8489791750907898,
        "meteor": 0.0,
        "sbert_similarity": 0.3392271399497986
      }
    },
    {
      "sample_id": 0,
      "question": "When did Caroline pass the adoption interview?",
      "prediction": "9:55 am on October 22, 2023",
      "reference": "The Friday before 22 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.0,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.08451542547285168,
        "bleu3": 0.053841561511499283,
        "bleu4": 0.043472087194499145,
        "bert_precision": 0.8834879994392395,
        "bert_recall": 0.907113254070282,
        "bert_f1": 0.8951447606086731,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7705892324447632
      }
    },
    {
      "sample_id": 0,
      "question": "When did Melanie buy the figurines?",
      "prediction": "3 July, 2023",
      "reference": "21 October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9472948312759399,
        "bert_recall": 0.9630638360977173,
        "bert_f1": 0.9551142454147339,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8525474071502686
      }
    },
    {
      "sample_id": 0,
      "question": "Would Caroline want to move back to her home country soon?",
      "prediction": "No",
      "reference": "No; she's in the process of adopting children.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.19999999999999998,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19999999999999998,
        "bleu1": 4.5399929762484854e-05,
        "bleu2": 1.435671836611194e-05,
        "bleu3": 9.93242240643695e-06,
        "bleu4": 8.07337603138401e-06,
        "bert_precision": 0.8743382096290588,
        "bert_recall": 0.8402692675590515,
        "bert_f1": 0.8569652438163757,
        "meteor": 0.0,
        "sbert_similarity": 0.21591618657112122
      }
    },
    {
      "sample_id": 0,
      "question": "What did the charity race raise awareness for?",
      "prediction": "mental health",
      "reference": "mental health",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie realize after the charity race?",
      "prediction": "Melanie realized how rewarding it was to take care of one's mind, especially considering her own experience and the struggles she had overcome.",
      "reference": "self-care is important",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.07142857142857142,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857142,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8749234080314636,
        "bert_recall": 0.909410834312439,
        "bert_f1": 0.8918339014053345,
        "meteor": 0.0,
        "sbert_similarity": 0.3162122368812561
      }
    },
    {
      "sample_id": 0,
      "question": "How does Melanie prioritize self-care?",
      "prediction": "Melanie prioritizes self-care by starting her day early and carving out time for activities like running, reading, or playing violin.",
      "reference": "by carving out some me-time each day for activities like running, reading, or playing the violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.6842105263157895,
        "rouge2_f": 0.38888888888888884,
        "rougeL_f": 0.6315789473684211,
        "bleu1": 0.6086956521739131,
        "bleu2": 0.49901087934784544,
        "bleu3": 0.4398479733747257,
        "bleu4": 0.3972418603247485,
        "bert_precision": 0.9260475635528564,
        "bert_recall": 0.953697919845581,
        "bert_f1": 0.9396693706512451,
        "meteor": 0.6546059262245517,
        "sbert_similarity": 0.4499396085739136
      }
    },
    {
      "sample_id": 0,
      "question": "What are Caroline's plans for the summer?",
      "prediction": "Caroline is planning to go camping in the mountains.",
      "reference": "researching adoption agencies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8677752614021301,
        "bert_recall": 0.8525650501251221,
        "bert_f1": 0.8601029515266418,
        "meteor": 0.0,
        "sbert_similarity": 0.012994199991226196
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Caroline is considering support?",
      "prediction": "LGBTQ+",
      "reference": "LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9714654684066772,
        "bert_recall": 0.9448345899581909,
        "bert_f1": 0.9579649567604065,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8889155387878418
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose the adoption agency?",
      "prediction": "She was excited to give kids a loving home.",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8750830888748169,
        "bert_recall": 0.8234549164772034,
        "bert_f1": 0.8484843373298645,
        "meteor": 0.0,
        "sbert_similarity": 0.11098581552505493
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline excited about in the adoption process?",
      "prediction": "Taking in kids in need - you're so kind.",
      "reference": "creating a family for kids who need one",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.042640143271122095,
        "bleu3": 0.028234511901075607,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.8845543265342712,
        "bert_recall": 0.8942122459411621,
        "bert_f1": 0.8893570303916931,
        "meteor": 0.1234567901234568,
        "sbert_similarity": 0.45317763090133667
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie think about Caroline's decision to adopt?",
      "prediction": "Melanie thinks Caroline has such a caring heart that will provide love and stability to the child.",
      "reference": "she thinks Caroline is doing something amazing and will be an awesome mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.07142857142857144,
        "rougeL_f": 0.20000000000000004,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.11433239009500591,
        "bleu3": 0.04477464303423438,
        "bleu4": 0.027166383723579154,
        "bert_precision": 0.8929132223129272,
        "bert_recall": 0.8994300365447998,
        "bert_f1": 0.8961597681045532,
        "meteor": 0.23554104477611942,
        "sbert_similarity": 0.6974647045135498
      }
    },
    {
      "sample_id": 0,
      "question": "How long have Mel and her husband been married?",
      "prediction": "14 years",
      "reference": "Mel and her husband have been married for 5 years.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666669,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.005554498269121153,
        "bleu2": 0.0024840471421319643,
        "bleu3": 0.001933458847739007,
        "bleu4": 0.0016611824434051396,
        "bert_precision": 0.8904068470001221,
        "bert_recall": 0.8614765405654907,
        "bert_f1": 0.8757028579711914,
        "meteor": 0.0,
        "sbert_similarity": 0.20599965751171112
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's necklace symbolize?",
      "prediction": "Caroline's necklace symbolizes love, faith, and strength.",
      "reference": "love, faith, and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7272727272727273,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.6,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5454545454545454,
        "bleu2": 0.5222329678670935,
        "bleu3": 0.49839100664976926,
        "bleu4": 0.4617366309441026,
        "bert_precision": 0.9094644784927368,
        "bert_recall": 0.9607594013214111,
        "bert_f1": 0.9344084858894348,
        "meteor": 0.6847545219638244,
        "sbert_similarity": 0.4463343322277069
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Caroline's grandma from?",
      "prediction": "USA",
      "reference": "Sweden",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8925297260284424,
        "bert_recall": 0.866078794002533,
        "bert_f1": 0.8791053295135498,
        "meteor": 0.0,
        "sbert_similarity": 0.6155799031257629
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Caroline?",
      "prediction": "The necklace is a gift from Caroline's grandma, given when she was young and it stands for love, faith, and strength.",
      "reference": "necklace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.08695652173913045,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913045,
        "bleu1": 0.03846153846153846,
        "bleu2": 0.012403473458920851,
        "bleu3": 0.009042091188715478,
        "bleu4": 0.0072658577559704465,
        "bert_precision": 0.8143506646156311,
        "bert_recall": 0.8651564121246338,
        "bert_f1": 0.8389850854873657,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.6634422540664673
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family do while camping?",
      "prediction": "roasted marshmallows around the campfire",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.12047768476488081,
        "bleu2": 0.09524597277019774,
        "bleu3": 0.045856046346674784,
        "bleu4": 0.034222269664256,
        "bert_precision": 0.9047108888626099,
        "bert_recall": 0.8845294713973999,
        "bert_f1": 0.8945063352584839,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.6161686182022095
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Caroline interested in pursuing?",
      "prediction": "Carolinesays she wants to pursue counseling and mental health as a career, specifically focusing on supporting individuals who have similar experiences.",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0625,
        "rougeL_f": 0.17647058823529413,
        "bleu1": 0.21739130434782608,
        "bleu2": 0.09940534656094302,
        "bleu3": 0.03732141177156653,
        "bleu4": 0.022023814946586635,
        "bert_precision": 0.8593567609786987,
        "bert_recall": 0.8804699182510376,
        "bert_f1": 0.8697851896286011,
        "meteor": 0.26956521739130435,
        "sbert_similarity": 0.5044403672218323
      }
    },
    {
      "sample_id": 0,
      "question": "What workshop did Caroline attend recently?",
      "prediction": "Counseling or mental health work",
      "reference": "LGBTQ+ counseling workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8588880300521851,
        "bert_recall": 0.8338919281959534,
        "bert_f1": 0.8462054133415222,
        "meteor": 0.15625,
        "sbert_similarity": 0.48090022802352905
      }
    },
    {
      "sample_id": 0,
      "question": "What was discussed in the LGBTQ+ counseling workshop?",
      "prediction": "The LGBTQ+ counseling workshop focused on therapeutic methods and strategies tailored to working effectively with trans individuals.",
      "reference": "therapeutic methods and how to best work with trans people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.5185185185185185,
        "rouge2_f": 0.24000000000000005,
        "rougeL_f": 0.5185185185185185,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.24253562503633297,
        "bleu3": 0.15725058206571177,
        "bleu4": 0.07036147891341805,
        "bert_precision": 0.8985320925712585,
        "bert_recall": 0.9113467931747437,
        "bert_f1": 0.9048941135406494,
        "meteor": 0.5931718481785236,
        "sbert_similarity": 0.7511304020881653
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Caroline to pursue counseling?",
      "prediction": "It's inspiring to see your dedication to helping others.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8839716911315918,
        "bert_recall": 0.866591215133667,
        "bert_f1": 0.8751952052116394,
        "meteor": 0.0,
        "sbert_similarity": 0.34818220138549805
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Caroline want to create for people?",
      "prediction": "a safe and loving home",
      "reference": "a safe and inviting place for people to grow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.42857142857142855,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.26959737847033294,
        "bleu2": 0.24610760938743165,
        "bleu3": 0.21016694651579904,
        "bleu4": 0.11948321931215808,
        "bert_precision": 0.9446678161621094,
        "bert_recall": 0.912566065788269,
        "bert_f1": 0.9283395409584045,
        "meteor": 0.436046511627907,
        "sbert_similarity": 0.5569763779640198
      }
    },
    {
      "sample_id": 0,
      "question": "Did Melanie make the black and white bowl in the photo?",
      "prediction": "Melaniesays: Thanks, Caroline! Yeah, I made this bowl in my class.",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7927957773208618,
        "bert_recall": 0.8080359101295471,
        "bert_f1": 0.8003432750701904,
        "meteor": 0.0,
        "sbert_similarity": 0.08477479219436646
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of books does Caroline have in her library?",
      "prediction": "kids' books",
      "reference": "kids' books - classics, stories from different cultures, educational books",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.03567399334725241,
        "bleu2": 0.03567399334725241,
        "bleu3": 0.03567399334725241,
        "bleu4": 0.020060960693725614,
        "bert_precision": 0.9489127397537231,
        "bert_recall": 0.8753682374954224,
        "bert_f1": 0.910658061504364,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.7524629831314087
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's favorite book from her childhood?",
      "prediction": "",
      "reference": "\"Charlotte's Web\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.0,
        "bleu2": 0.0,
        "bleu3": 0.0,
        "bleu4": 0.0,
        "bert_f1": 0.0,
        "meteor": 0.0,
        "sbert_similarity": 0.0
      }
    },
    {
      "sample_id": 0,
      "question": "What book did Caroline recommend to Melanie?",
      "prediction": "A book",
      "reference": "\"Becoming Nicole\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9192982912063599,
        "bert_recall": 0.8887425065040588,
        "bert_f1": 0.90376216173172,
        "meteor": 0.0,
        "sbert_similarity": 0.09420599043369293
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline take away from the book \"Becoming Nicole\"?",
      "prediction": "Caroline took inspiration and hope regarding her own path.",
      "reference": "Lessons on self-acceptance and finding support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.876020610332489,
        "bert_recall": 0.8855737447738647,
        "bert_f1": 0.880771279335022,
        "meteor": 0.07936507936507937,
        "sbert_similarity": 0.21350061893463135
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Melanie got used for?",
      "prediction": "new shoes",
      "reference": "Running",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9031277894973755,
        "bert_recall": 0.9146972298622131,
        "bert_f1": 0.9088756442070007,
        "meteor": 0.0,
        "sbert_similarity": 0.35518911480903625
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie's reason for getting into running?",
      "prediction": "Melanie runs to de-stress and clear her mind.",
      "reference": "To de-stress and clear her mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8750000000000001,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.8750000000000001,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6454972243679028,
        "bleu3": 0.6227700724866008,
        "bleu4": 0.5873949094699213,
        "bert_precision": 0.943814754486084,
        "bert_recall": 0.9672422409057617,
        "bert_f1": 0.9553849101066589,
        "meteor": 0.8032258064516131,
        "sbert_similarity": 0.6212152242660522
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie say running has been great for?",
      "prediction": "running has been great for my headspace.",
      "reference": "Her mental health",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8710024952888489,
        "bert_recall": 0.888644814491272,
        "bert_f1": 0.8797351717948914,
        "meteor": 0.0,
        "sbert_similarity": 0.06288055330514908
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids make during the pottery workshop?",
      "prediction": "They all made their own pots.",
      "reference": "pots",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.835502028465271,
        "bert_recall": 0.8303170204162598,
        "bert_f1": 0.8329014182090759,
        "meteor": 0.0,
        "sbert_similarity": 0.6031421422958374
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of pot did Mel and her kids make with clay?",
      "prediction": "a cup",
      "reference": "a cup with a dog face on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.049787068367863944,
        "bleu3": 0.02328716145731824,
        "bleu4": 0.015744053406497194,
        "bert_precision": 0.9445455074310303,
        "bert_recall": 0.8711543083190918,
        "bert_f1": 0.9063666462898254,
        "meteor": 0.13513513513513511,
        "sbert_similarity": 0.6318643093109131
      }
    },
    {
      "sample_id": 0,
      "question": "What creative project do Mel and her kids do together besides pottery?",
      "prediction": "painting",
      "reference": "painting",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Mel and her kids paint in their latest project in July 2023?",
      "prediction": "A painting",
      "reference": "a sunset with a palm tree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.9122894406318665,
        "bert_recall": 0.887075662612915,
        "bert_f1": 0.8995059728622437,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.22985491156578064
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline see at the council meeting for adoption?",
      "prediction": "a photo",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9294551610946655,
        "bert_recall": 0.8050891160964966,
        "bert_f1": 0.8628136515617371,
        "meteor": 0.0,
        "sbert_similarity": 0.006002599373459816
      }
    },
    {
      "sample_id": 0,
      "question": "What do sunflowers represent according to Caroline?",
      "prediction": "warmth and happiness",
      "reference": "warmth and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Why are flowers important to Melanie?",
      "prediction": "Flowers bring joy and represent growth, beauty, and reminding us to appreciate the small moments.",
      "reference": "They remind her to appreciate the small moments and were a part of her wedding decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4137931034482759,
        "rouge1_f": 0.45161290322580644,
        "rouge2_f": 0.2758620689655172,
        "rougeL_f": 0.38709677419354843,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28005601680560194,
        "bleu3": 0.24846891229294918,
        "bleu4": 0.21042990347620458,
        "bert_precision": 0.8919190168380737,
        "bert_recall": 0.8806442618370056,
        "bert_f1": 0.886245846748352,
        "meteor": 0.3537735849056604,
        "sbert_similarity": 0.5083188414573669
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's painting for the art show?",
      "prediction": "Caroline's self-acceptance and love theme.",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.08067401742967989,
        "bleu2": 0.02755542282313655,
        "bleu3": 0.021159205542625806,
        "bleu4": 0.018653567503517977,
        "bert_precision": 0.8797534704208374,
        "bert_recall": 0.8586400747299194,
        "bert_f1": 0.8690685629844666,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.23503156006336212
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Melanie go to the beach with her kids?",
      "prediction": "Once or twice a year",
      "reference": "once or twice a year",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9607255458831787,
        "bert_recall": 0.9513533115386963,
        "bert_f1": 0.956016480922699,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie and her family see during their camping trip last year?",
      "prediction": "A view from the top was amazing!",
      "reference": "Perseid meteor shower",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672182559967041,
        "bert_recall": 0.7854665517807007,
        "bert_f1": 0.8243204355239868,
        "meteor": 0.0,
        "sbert_similarity": 0.1719217598438263
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel while watching the meteor shower?",
      "prediction": "Melanie felt tiny and in awe of the universe.",
      "reference": "in awe of the universe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.7142857142857143,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7142857142857143,
        "bleu1": 0.5,
        "bleu2": 0.4714045207910317,
        "bleu3": 0.44042320075255215,
        "bleu4": 0.392814650900513,
        "bert_precision": 0.9181756973266602,
        "bert_recall": 0.9523166418075562,
        "bert_f1": 0.9349346160888672,
        "meteor": 0.7349537037037037,
        "sbert_similarity": 0.497897207736969
      }
    },
    {
      "sample_id": 0,
      "question": "Whose birthday did Melanie celebrate recently?",
      "prediction": "Melanie's daughter's birthday",
      "reference": "Melanie's daughter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6,
        "bleu2": 0.5477225575051662,
        "bleu3": 0.4677351412871982,
        "bleu4": 0.26591479484724945,
        "bert_precision": 0.9566453695297241,
        "bert_recall": 0.9828019142150879,
        "bert_f1": 0.9695472121238708,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8563249111175537
      }
    },
    {
      "sample_id": 0,
      "question": "Who performed at the concert at Melanie's daughter's birthday?",
      "prediction": "Melaniesays",
      "reference": "Matt Patterson",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8212988972663879,
        "bert_recall": 0.9075658321380615,
        "bert_f1": 0.8622801303863525,
        "meteor": 0.0,
        "sbert_similarity": 0.12857577204704285
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose to use colors and patterns in her pottery project?",
      "prediction": "Melanie chose to use colors and patterns because it is calming.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1904761904761905,
        "rouge1_f": 0.1904761904761905,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1904761904761905,
        "bleu1": 0.25,
        "bleu2": 0.047673129462279626,
        "bleu3": 0.02935354896563489,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.8830522298812866,
        "bert_recall": 0.8972393274307251,
        "bert_f1": 0.890089213848114,
        "meteor": 0.09900990099009903,
        "sbert_similarity": 0.38554078340530396
      }
    },
    {
      "sample_id": 0,
      "question": "What pet does Caroline have?",
      "prediction": "pup and kitty",
      "reference": "guinea pig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8771774768829346,
        "bert_recall": 0.8840682506561279,
        "bert_f1": 0.880609393119812,
        "meteor": 0.0,
        "sbert_similarity": 0.3984302878379822
      }
    },
    {
      "sample_id": 0,
      "question": "What pets does Melanie have?",
      "prediction": "pup and kitty",
      "reference": "Two cats and a dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.8972745537757874,
        "bert_recall": 0.9116873741149902,
        "bert_f1": 0.9044235944747925,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7113078832626343
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oliver hide his bone once?",
      "prediction": "in my slipper",
      "reference": "In Melanie's slipper",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9570469856262207,
        "bert_recall": 0.9307029247283936,
        "bert_f1": 0.9436911344528198,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.6715267896652222
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Caroline used to do with her dad?",
      "prediction": "roasting marshmallows",
      "reference": "Horseback riding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8608863353729248,
        "bert_recall": 0.8367708325386047,
        "bert_f1": 0.8486573100090027,
        "meteor": 0.0,
        "sbert_similarity": -0.051031116396188736
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline make for a local church?",
      "prediction": "a poster",
      "reference": "a stained glass window",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.9483988285064697,
        "bert_recall": 0.897986650466919,
        "bert_f1": 0.9225044846534729,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.24058020114898682
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline find in her neighborhood during her walk?",
      "prediction": "A park where she saw children playing",
      "reference": "a rainbow sidewalk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.895065188407898,
        "bert_recall": 0.9045659303665161,
        "bert_f1": 0.8997904658317566,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.24266254901885986
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Caroline to be courageous?",
      "prediction": "Brave by Sara Bareilles",
      "reference": "Brave by Sara Bareilles",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "Which  classical musicians does Melanie enjoy listening to?",
      "prediction": "Bach and Mozart",
      "reference": "Bach and Mozart",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Melanie a fan of in terms of modern music?",
      "prediction": "Ed Sheeran's \"Perfect\"",
      "reference": "Ed Sheeran",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9072465896606445,
        "bert_recall": 0.9754121899604797,
        "bert_f1": 0.9400953650474548,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7055439949035645
      }
    },
    {
      "sample_id": 0,
      "question": "How long has Melanie been creating art?",
      "prediction": "Melanie has been creating art for seven years.",
      "reference": "7 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8689018487930298,
        "bert_recall": 0.8895487785339355,
        "bert_f1": 0.8791040778160095,
        "meteor": 0.19230769230769235,
        "sbert_similarity": 0.4301570951938629
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Melanie see at the caf\u00e9?",
      "prediction": "Melaniesays that they saw a sign posted on a door stating that someone is not being able to leave.",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7857142857142858,
        "rouge1_f": 0.7333333333333334,
        "rouge2_f": 0.6428571428571429,
        "rougeL_f": 0.7333333333333334,
        "bleu1": 0.55,
        "bleu2": 0.5104177855340404,
        "bleu3": 0.46975720926993464,
        "bleu4": 0.4348553979929487,
        "bert_precision": 0.8974385261535645,
        "bert_recall": 0.9551213979721069,
        "bert_f1": 0.9253818988800049,
        "meteor": 0.8360169491525423,
        "sbert_similarity": 0.7672106027603149
      }
    },
    {
      "sample_id": 0,
      "question": "What advice does Caroline give for getting started with adoption?",
      "prediction": "I got lots of help from this adoption advice/assistance group I attended.",
      "reference": "Do research, find an adoption agency or lawyer, gather necessary documents, and prepare emotionally.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08,
        "rouge1_f": 0.07407407407407408,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407408,
        "bleu1": 0.10472498435744391,
        "bleu2": 0.024373413669121567,
        "bleu3": 0.01602923720716669,
        "bleu4": 0.0125774279703401,
        "bert_precision": 0.8637874126434326,
        "bert_recall": 0.8708359003067017,
        "bert_f1": 0.867297351360321,
        "meteor": 0.07246376811594203,
        "sbert_similarity": 0.6100552082061768
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Melanie face in October 2023?",
      "prediction": "Caroline's son's situation",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8812732696533203,
        "bert_recall": 0.8772444725036621,
        "bert_f1": 0.8792542219161987,
        "meteor": 0.0,
        "sbert_similarity": 0.2732318937778473
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie do to keep herself busy during her pottery break?",
      "prediction": "pottery",
      "reference": "Read a book and paint.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8502522706985474,
        "bert_recall": 0.8841280937194824,
        "bert_f1": 0.8668593764305115,
        "meteor": 0.0,
        "sbert_similarity": 0.298631876707077
      }
    },
    {
      "sample_id": 0,
      "question": "What painting did Melanie show to Caroline on October 13, 2023?",
      "prediction": "A blue vase with a bouquet of sunflowers and roses",
      "reference": "A painting inspired by sunsets with a pink sky.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.11764705882352941,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.3,
        "bleu2": 0.18257418583505533,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.04939382737115371,
        "bert_precision": 0.8594225645065308,
        "bert_recall": 0.8995235562324524,
        "bert_f1": 0.8790158629417419,
        "meteor": 0.2808302808302808,
        "sbert_similarity": 0.4222846031188965
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of painting did Caroline share with Melanie on October 13, 2023?",
      "prediction": "A stunning sunset painting",
      "reference": "An abstract painting with blue streaks on a wall.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.11156508007421491,
        "bleu2": 0.028805979809715422,
        "bleu3": 0.021499505248604424,
        "bleu4": 0.021318810323722334,
        "bert_precision": 0.9148735404014587,
        "bert_recall": 0.9015582799911499,
        "bert_f1": 0.9081671237945557,
        "meteor": 0.11764705882352941,
        "sbert_similarity": 0.5380687713623047
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Caroline attended about?",
      "prediction": "A poetry reading",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.42857142857142855,
        "rouge1_f": 0.4,
        "rouge2_f": 0.15384615384615385,
        "rougeL_f": 0.4,
        "bleu1": 0.03567399334725241,
        "bleu2": 0.025225322607845962,
        "bleu3": 0.01327430611062756,
        "bleu4": 0.009486242622316766,
        "bert_precision": 0.9473236799240112,
        "bert_recall": 0.8877926468849182,
        "bert_f1": 0.9165925979614258,
        "meteor": 0.2302302302302302,
        "sbert_similarity": 0.6092147827148438
      }
    },
    {
      "sample_id": 0,
      "question": "What did the posters at the poetry reading say?",
      "prediction": "trans lives matter",
      "reference": "\"Trans Lives Matter\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.513417119032592,
        "bleu2": 0.513417119032592,
        "bleu3": 0.513417119032592,
        "bleu4": 0.288715663092199,
        "bert_precision": 0.9293972849845886,
        "bert_recall": 0.9074417352676392,
        "bert_f1": 0.9182883501052856,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9525076746940613
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline's drawing symbolize for her?",
      "prediction": "It stands for freedom and being real.",
      "reference": "Freedom and being true to herself.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.5,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.29129144784395894,
        "bleu4": 0.1477219991186121,
        "bert_precision": 0.912691593170166,
        "bert_recall": 0.9272005558013916,
        "bert_f1": 0.9198888540267944,
        "meteor": 0.4826958105646631,
        "sbert_similarity": 0.3979474604129791
      }
    },
    {
      "sample_id": 0,
      "question": "How do Melanie and Caroline describe their journey through life together?",
      "prediction": "Caroline appreciates the moments and family time, while Melanie emphasizes learning and exploration.",
      "reference": "An ongoing adventure of learning and growing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2105263157894737,
        "rouge1_f": 0.2,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.2,
        "bleu1": 0.2,
        "bleu2": 0.11952286093343935,
        "bleu3": 0.04937604958709526,
        "bleu4": 0.030934588294313718,
        "bert_precision": 0.8708993196487427,
        "bert_recall": 0.9034805297851562,
        "bert_f1": 0.8868907690048218,
        "meteor": 0.24671052631578952,
        "sbert_similarity": 0.3043944239616394
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Melanie's son on their road trip?",
      "prediction": "My son was okay after the accident.",
      "reference": "He got into an accident",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.9175523519515991,
        "bert_recall": 0.9160751104354858,
        "bert_f1": 0.9168131351470947,
        "meteor": 0.0,
        "sbert_similarity": 0.6028062701225281
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's son handle the accident?",
      "prediction": "The child experienced an accident but was lucky to have survived.",
      "reference": "He was scared but reassured by his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2105263157894737,
        "rouge1_f": 0.2105263157894737,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03892494720807617,
        "bleu3": 0.025677367760996977,
        "bleu4": 0.020255986027125642,
        "bert_precision": 0.8995947241783142,
        "bert_recall": 0.8932800889015198,
        "bert_f1": 0.8964263200759888,
        "meteor": 0.12048192771084339,
        "sbert_similarity": 0.2741706967353821
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family after the accident?",
      "prediction": "Melanie felt lucky to have her family by her side during the accident and expressed gratitude for their support.",
      "reference": "They are important and mean the world to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2962962962962963,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.2,
        "bleu2": 0.0324442842261525,
        "bleu3": 0.018754750180155714,
        "bleu4": 0.013618796864073041,
        "bert_precision": 0.8773490190505981,
        "bert_recall": 0.8861286640167236,
        "bert_f1": 0.8817170262336731,
        "meteor": 0.2,
        "sbert_similarity": 0.24370108544826508
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie's children handle the accident?",
      "prediction": "They were traumatized and had to cope with the incident.",
      "reference": "They were scared but resilient",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.15384615384615383,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.13483997249264845,
        "bleu3": 0.06036431606009924,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.9146585464477539,
        "bert_recall": 0.9024520516395569,
        "bert_f1": 0.9085143208503723,
        "meteor": 0.34090909090909094,
        "sbert_similarity": 0.5532317161560059
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel after the accident?",
      "prediction": "Melanie felt relieved and grateful.",
      "reference": "Grateful and thankful for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.9170709252357483,
        "bert_recall": 0.9012904167175293,
        "bert_f1": 0.909112274646759,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.5454837083816528
      }
    },
    {
      "sample_id": 0,
      "question": "What was Melanie's reaction to her children enjoying the Grand Canyon?",
      "prediction": "Melanie's reaction was that it was so special having these moments together as a family - I'll never forget it!",
      "reference": "She was happy and thankful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913045,
        "rouge1_f": 0.07692307692307693,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07692307692307693,
        "bleu1": 0.043478260869565216,
        "bleu2": 0.014058038927888336,
        "bleu3": 0.010263558503321437,
        "bleu4": 0.008282282660969604,
        "bert_precision": 0.8507671356201172,
        "bert_recall": 0.8804875612258911,
        "bert_f1": 0.8653722405433655,
        "meteor": 0.07692307692307694,
        "sbert_similarity": 0.34197258949279785
      }
    },
    {
      "sample_id": 0,
      "question": "What do Melanie's family give her?",
      "prediction": "strength and motivation",
      "reference": "Strength and motivation",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9669326543807983,
        "bert_recall": 0.9669326543807983,
        "bert_f1": 0.9669326543807983,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Melanie feel about her family supporting her?",
      "prediction": "Melanie felt supported and appreciated by her family.",
      "reference": "She appreciated them a lot",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.9038572311401367,
        "bert_recall": 0.9072055816650391,
        "bert_f1": 0.9055283069610596,
        "meteor": 0.09433962264150944,
        "sbert_similarity": 0.5170629024505615
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie do after the road trip to relax?",
      "prediction": "Melaniesays it's tough to carve out time for herself, so she's finding ways to refresh and stay present with her family.",
      "reference": "Went on a nature walk or hike",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8252462148666382,
        "bert_recall": 0.8544243574142456,
        "bert_f1": 0.8395819067955017,
        "meteor": 0.0,
        "sbert_similarity": 0.19289889931678772
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline realize after her charity race?",
      "prediction": "self-care is important",
      "reference": "self-care is important",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What are Melanie's plans for the summer with respect to adoption?",
      "prediction": "Melanie is planning to go swimming with her family.",
      "reference": "researching adoption agencies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8503470420837402,
        "bert_recall": 0.8439207077026367,
        "bert_f1": 0.8471217155456543,
        "meteor": 0.0,
        "sbert_similarity": 0.11484918743371964
      }
    },
    {
      "sample_id": 0,
      "question": "What type of individuals does the adoption agency Melanie is considering support?",
      "prediction": "Not mentioned in the conversation or LGBTQ+ individuals",
      "reference": "LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.8773437738418579,
        "bert_recall": 0.9293595552444458,
        "bert_f1": 0.9026029109954834,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.7128360271453857
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Melanie choose the adoption agency?",
      "prediction": "Because of their inclusivity and support for LGBTQ+ individuals",
      "reference": "because of their inclusivity and support for LGBTQ+ individuals",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9895650744438171,
        "bert_recall": 0.9895650744438171,
        "bert_f1": 0.9895650744438171,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What is Melanie excited about in her adoption process?",
      "prediction": "Not mentioned in the conversation",
      "reference": "creating a family for kids who need one",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8936758041381836,
        "bert_recall": 0.8580496311187744,
        "bert_f1": 0.8755003809928894,
        "meteor": 0.0,
        "sbert_similarity": -0.06094197928905487
      }
    },
    {
      "sample_id": 0,
      "question": "What does Melanie's necklace symbolize?",
      "prediction": "love, faith, and strength",
      "reference": "love, faith, and strength",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What country is Melanie's grandma from?",
      "prediction": "Sweden",
      "reference": "Sweden",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandma's gift to Melanie?",
      "prediction": "Not mentioned in the conversation",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8640686273574829,
        "bert_recall": 0.8502070903778076,
        "bert_f1": 0.8570818305015564,
        "meteor": 0.0,
        "sbert_similarity": 0.17644494771957397
      }
    },
    {
      "sample_id": 0,
      "question": "What was grandpa's gift to Caroline?",
      "prediction": "necklace",
      "reference": "necklace",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's hand-painted bowl a reminder of?",
      "prediction": "art and self-expression",
      "reference": "art and self-expression",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family do while camping?",
      "prediction": "roasted marshmallows, went on a hike",
      "reference": "explored nature, roasted marshmallows, and went on a hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6153846153846154,
        "rougeL_f": 0.8,
        "bleu1": 0.5647181220077593,
        "bleu2": 0.5155147567593406,
        "bleu3": 0.44925386909473397,
        "bleu4": 0.335783404331301,
        "bert_precision": 0.9707649350166321,
        "bert_recall": 0.9480518102645874,
        "bert_f1": 0.9592739343643188,
        "meteor": 0.6768837803320561,
        "sbert_similarity": 0.8630470037460327
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling and mental health services is Melanie interested in pursuing?",
      "prediction": "Not mentioned in the conversation or working with trans people, helping them accept themselves and supporting their mental health",
      "reference": "working with trans people, helping them accept themselves and supporting their mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8125000000000001,
        "rouge1_f": 0.8125000000000001,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8125000000000001,
        "bleu1": 0.7,
        "bleu2": 0.6920602346769048,
        "bleu3": 0.6860959499274262,
        "bleu4": 0.6741942962447748,
        "bert_precision": 0.9425563812255859,
        "bert_recall": 0.9763206243515015,
        "bert_f1": 0.9591414332389832,
        "meteor": 0.9556648103028194,
        "sbert_similarity": 0.8913708925247192
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of counseling workshop did Melanie attend recently?",
      "prediction": "LGBTQ+ counseling workshop",
      "reference": "LGBTQ+ counseling workshop",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "What motivated Melanie to pursue counseling?",
      "prediction": "Motivated by her desire to help others who have gone through similar experiences as herself.",
      "reference": "her own journey and the support she received, and how counseling improved her life",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07407407407407407,
        "rouge1_f": 0.0689655172413793,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0689655172413793,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8637495636940002,
        "bert_recall": 0.8707073926925659,
        "bert_f1": 0.8672144412994385,
        "meteor": 0.07092198581560283,
        "sbert_similarity": 0.6872822046279907
      }
    },
    {
      "sample_id": 0,
      "question": "What kind of place does Melanie want to create for people?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a safe and inviting place for people to grow",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8868063688278198,
        "bert_recall": 0.860106348991394,
        "bert_f1": 0.873252272605896,
        "meteor": 0.0,
        "sbert_similarity": 0.10203893482685089
      }
    },
    {
      "sample_id": 0,
      "question": "Did Caroline make the black and white bowl in the photo?",
      "prediction": "No",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9496124982833862,
        "bert_recall": 0.9496124982833862,
        "bert_f1": 0.9496124982833862,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 0,
      "question": "What are the new shoes that Caroline got used for?",
      "prediction": "Running",
      "reference": "Running",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What is Caroline's reason for getting into running?",
      "prediction": "To de-stress and clear her mind",
      "reference": "To de-stress and clear her mind",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline say running has been great for?",
      "prediction": "clear my mind",
      "reference": "Her mental health",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9078664779663086,
        "bert_recall": 0.9103572964668274,
        "bert_f1": 0.9091102480888367,
        "meteor": 0.0,
        "sbert_similarity": 0.250509649515152
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie see at the council meeting for adoption?",
      "prediction": "Many people wanted to create loving homes for children in need.",
      "reference": "many people wanting to create loving homes for children in need",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9090909090909091,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.8333333333333334,
        "bleu2": 0.778498944161523,
        "bleu3": 0.7161767386056302,
        "bleu4": 0.670422683816333,
        "bert_precision": 0.9785996675491333,
        "bert_recall": 0.976657509803772,
        "bert_f1": 0.9776276350021362,
        "meteor": 0.9086363636363638,
        "sbert_similarity": 0.9654260873794556
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Melanie's painting for the art show?",
      "prediction": "Inspired by autumn colors",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8682358264923096,
        "bert_recall": 0.8569586873054504,
        "bert_f1": 0.8625603914260864,
        "meteor": 0.0,
        "sbert_similarity": 0.07769186794757843
      }
    },
    {
      "sample_id": 0,
      "question": "What inspired Caroline's sculpture for the art show?",
      "prediction": "Not mentioned in the conversation or visiting an LGBTQ center and wanting to capture unity and strength",
      "reference": "visiting an LGBTQ center and wanting to capture unity and strength",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7692307692307693,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.6470588235294118,
        "bleu2": 0.6359337738364604,
        "bleu3": 0.626674120796503,
        "bleu4": 0.6102169202557912,
        "bert_precision": 0.930842399597168,
        "bert_recall": 0.9643985033035278,
        "bert_f1": 0.9473233819007874,
        "meteor": 0.9479196352237105,
        "sbert_similarity": 0.8976195454597473
      }
    },
    {
      "sample_id": 0,
      "question": "How often does Caroline go to the beach with her kids?",
      "prediction": "Not mentioned in the conversation",
      "reference": "once or twice a year",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.89439457654953,
        "bert_recall": 0.8584726452827454,
        "bert_f1": 0.8760654926300049,
        "meteor": 0.0,
        "sbert_similarity": 0.05305672064423561
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline and her family see during their camping trip last year?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Perseid meteor shower",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8852364420890808,
        "bert_recall": 0.7874637842178345,
        "bert_f1": 0.833492636680603,
        "meteor": 0.0,
        "sbert_similarity": 0.03334156796336174
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel while watching the meteor shower?",
      "prediction": "Not mentioned in the conversation or in awe of the universe",
      "reference": "in awe of the universe",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.625,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.625,
        "bleu1": 0.45454545454545453,
        "bleu2": 0.4264014327112209,
        "bleu3": 0.3964882289652318,
        "bleu4": 0.3508439695638686,
        "bert_precision": 0.9086282253265381,
        "bert_recall": 0.9530649185180664,
        "bert_f1": 0.9303162693977356,
        "meteor": 0.8892857142857143,
        "sbert_similarity": 0.7008298635482788
      }
    },
    {
      "sample_id": 0,
      "question": "Why did Caroline choose to use colors and patterns in her pottery project?",
      "prediction": "She wanted to catch the eye and make people smile.",
      "reference": "She wanted to catch the eye and make people smile.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9995,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 0,
      "question": "Is Oscar Melanie's pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Yes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8622181415557861,
        "bert_recall": 0.881470799446106,
        "bert_f1": 0.8717381954193115,
        "meteor": 0.0,
        "sbert_similarity": 0.1172415167093277
      }
    },
    {
      "sample_id": 0,
      "question": "Where did Oscar hide his bone once?",
      "prediction": "Not mentioned in the conversation",
      "reference": "In Melanie's slipper",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8998652100563049,
        "bert_recall": 0.8677350282669067,
        "bert_f1": 0.8835080862045288,
        "meteor": 0.15625,
        "sbert_similarity": 0.17263296246528625
      }
    },
    {
      "sample_id": 0,
      "question": "What activity did Melanie used to do with her dad?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Horseback riding",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8857095241546631,
        "bert_recall": 0.8285903930664062,
        "bert_f1": 0.8561983704566956,
        "meteor": 0.0,
        "sbert_similarity": 0.05627930909395218
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie make for a local church?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a stained glass window",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8815366625785828,
        "bert_recall": 0.8578861951828003,
        "bert_f1": 0.8695506453514099,
        "meteor": 0.0,
        "sbert_similarity": 0.03617015480995178
      }
    },
    {
      "sample_id": 0,
      "question": "What did Melanie find in her neighborhood during her walk?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a rainbow sidewalk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8769193887710571,
        "bert_recall": 0.865630030632019,
        "bert_f1": 0.871238112449646,
        "meteor": 0.0,
        "sbert_similarity": -0.04855134338140488
      }
    },
    {
      "sample_id": 0,
      "question": "Which song motivates Melanie to be courageous?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Brave by Sara Bareilles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8690690398216248,
        "bert_recall": 0.8234980702400208,
        "bert_f1": 0.8456700444221497,
        "meteor": 0.0,
        "sbert_similarity": -0.004309762269258499
      }
    },
    {
      "sample_id": 0,
      "question": "What type of instrument does Caroline play?",
      "prediction": "clarinet",
      "reference": "clarinet and violin",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9556031227111816,
        "bert_recall": 0.9187377095222473,
        "bert_f1": 0.9368079304695129,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.8449620604515076
      }
    },
    {
      "sample_id": 0,
      "question": "Which classical musicians does Caroline enjoy listening to?",
      "prediction": "Bach and Mozart",
      "reference": "Bach and Mozart",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "Who is Caroline a fan of in terms of modern music?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ed Sheeran",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8744926452636719,
        "bert_recall": 0.8158300518989563,
        "bert_f1": 0.8441434502601624,
        "meteor": 0.0,
        "sbert_similarity": 0.09608790278434753
      }
    },
    {
      "sample_id": 0,
      "question": "What precautionary sign did Caroline see at the caf\u00e9?",
      "prediction": "A sign stating that someone is not being able to leave",
      "reference": "A sign stating that someone is not being able to leave",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What setback did Caroline face recently?",
      "prediction": "She got hurt and had to take a break from pottery.",
      "reference": "She got hurt and had to take a break from pottery.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline do to keep herself busy during her pottery break?",
      "prediction": "I'm getting creative too, just learning the piano.",
      "reference": "Read a book and paint.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8789584636688232,
        "bert_recall": 0.9169725179672241,
        "bert_f1": 0.8975631594657898,
        "meteor": 0.09433962264150944,
        "sbert_similarity": 0.39585357904434204
      }
    },
    {
      "sample_id": 0,
      "question": "What was the poetry reading that Melanie attended about?",
      "prediction": "It was a transgender poetry reading where transgender people shared their stories.",
      "reference": "It was a transgender poetry reading where transgender people shared their stories.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997106481481481,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 0,
      "question": "What happened to Caroline's son on their road trip?",
      "prediction": "He got into an accident",
      "reference": "He got into an accident",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's son handle the accident?",
      "prediction": "He was scared but reassured by his family",
      "reference": "He was scared but reassured by his family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel about her family after the accident?",
      "prediction": "They are important and mean the world to her",
      "reference": "They are important and mean the world to her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline's children handle the accident?",
      "prediction": "They were scared but resilient",
      "reference": "They were scared but resilient",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "How did Caroline feel after the accident?",
      "prediction": "Grateful and thankful for her family",
      "reference": "Grateful and thankful for her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What was Caroline's reaction to her children enjoying the Grand Canyon?",
      "prediction": "She was happy and thankful",
      "reference": "She was happy and thankful",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 0,
      "question": "What did Caroline do after the road trip to relax?",
      "prediction": "Not mentioned in the conversation or Went on a nature walk or hike",
      "reference": "Went on a nature walk or hike",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7368421052631579,
        "rouge1_f": 0.7000000000000001,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7000000000000001,
        "bleu1": 0.5384615384615384,
        "bleu2": 0.5188745216627708,
        "bleu3": 0.4999673727050369,
        "bleu4": 0.4703709593866897,
        "bert_precision": 0.9064948558807373,
        "bert_recall": 0.9482219219207764,
        "bert_f1": 0.9268890023231506,
        "meteor": 0.9197099892588615,
        "sbert_similarity": 0.7099670171737671
      }
    },
    {
      "sample_id": 0,
      "question": "What does Caroline love most about camping with her family?",
      "prediction": "Being present and bonding with her family",
      "reference": "Being present and bonding with her family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 1,
      "question": "When Jon has lost his job as a banker?",
      "prediction": "16 March 2023",
      "reference": "19 January, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9653885364532471,
        "bert_recall": 0.9611279368400574,
        "bert_f1": 0.9632534980773926,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8507716655731201
      }
    },
    {
      "sample_id": 1,
      "question": "When Gina has lost her job at Door Dash?",
      "prediction": "20 January 2023",
      "reference": "January, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9509673714637756,
        "bert_recall": 0.9509673714637756,
        "bert_f1": 0.9509673714637756,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8646116852760315
      }
    },
    {
      "sample_id": 1,
      "question": "How do Jon and Gina both like to destress?",
      "prediction": "Jon likes dancing as his stress fix, while Gina says dancing helps her de-stress.",
      "reference": "by dancing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333336,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8435804843902588,
        "bert_recall": 0.8847697377204895,
        "bert_f1": 0.863684356212616,
        "meteor": 0.15625000000000003,
        "sbert_similarity": 0.34412744641304016
      }
    },
    {
      "sample_id": 1,
      "question": "What do Jon and Gina both have in common?",
      "prediction": "Both Jon and Gina have strong relationships based on support and belief.",
      "reference": "They lost their jobs and decided to start their own businesses.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8682993650436401,
        "bert_recall": 0.8796705007553101,
        "bert_f1": 0.8739479184150696,
        "meteor": 0.09009009009009009,
        "sbert_similarity": 0.01188746653497219
      }
    },
    {
      "sample_id": 1,
      "question": "Why did Jon decide to start his dance studio?",
      "prediction": "Jon decided to start his dance studio because he is passionate about dancing and wanted to share it with others.",
      "reference": "He lost his job and decided to start his own business to share his passion.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4516129032258065,
        "rouge1_f": 0.5142857142857143,
        "rouge2_f": 0.24242424242424243,
        "rougeL_f": 0.34285714285714286,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.29277002188455997,
        "bleu3": 0.2114748714699507,
        "bleu4": 0.14962848372546667,
        "bert_precision": 0.9127354621887207,
        "bert_recall": 0.9268485903739929,
        "bert_f1": 0.9197378754615784,
        "meteor": 0.453125,
        "sbert_similarity": 0.4236007034778595
      }
    },
    {
      "sample_id": 1,
      "question": "What Jon thinks the ideal dance studio should look like?",
      "prediction": "A room with a view of the ocean and a few yoga mats.",
      "reference": "By the water, with natural light and Marley flooring",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.27272727272727276,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818185,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.040599897147057515,
        "bleu3": 0.024859743585800658,
        "bleu4": 0.018798317647335087,
        "bert_precision": 0.8921095132827759,
        "bert_recall": 0.8736891746520996,
        "bert_f1": 0.8828032612800598,
        "meteor": 0.1595744680851064,
        "sbert_similarity": 0.32028913497924805
      }
    },
    {
      "sample_id": 1,
      "question": "When is Jon's group performing at a festival?",
      "prediction": "4:04 pm on 29 January, 2023",
      "reference": "February, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8563363552093506,
        "bert_recall": 0.9056695699691772,
        "bert_f1": 0.8803123235702515,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6567533016204834
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina launch an ad campaign for her store?",
      "prediction": "2023-01-29",
      "reference": "29 January, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.896928071975708,
        "bert_recall": 0.9209747910499573,
        "bert_f1": 0.9087924361228943,
        "meteor": 0.0,
        "sbert_similarity": 0.7271880507469177
      }
    },
    {
      "sample_id": 1,
      "question": "When was Jon in Paris?",
      "prediction": "2023-01-29",
      "reference": "28 January 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8963268995285034,
        "bert_recall": 0.9031676054000854,
        "bert_f1": 0.8997342586517334,
        "meteor": 0.0,
        "sbert_similarity": 0.7414637207984924
      }
    },
    {
      "sample_id": 1,
      "question": "Which city have both Jean and John visited?",
      "prediction": "Rome",
      "reference": "Rome",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina team up with a local artist for some cool designs?",
      "prediction": "9 July 2023",
      "reference": "February, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9343374967575073,
        "bert_recall": 0.9343374371528625,
        "bert_f1": 0.9343374371528625,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7673101425170898
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina get her tattoo?",
      "prediction": "2023",
      "reference": "A few years ago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8540579676628113,
        "bert_recall": 0.8423479199409485,
        "bert_f1": 0.8481625318527222,
        "meteor": 0.0,
        "sbert_similarity": 0.2525690793991089
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon start to go to the gym?",
      "prediction": "16 June 2023",
      "reference": "March, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9345813989639282,
        "bert_recall": 0.9335323572158813,
        "bert_f1": 0.9340565800666809,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8315902948379517
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina open her online clothing store?",
      "prediction": "23 March 2023",
      "reference": "16 March, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9791317582130432,
        "bert_recall": 0.9567767381668091,
        "bert_f1": 0.967825174331665,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8826572895050049
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon start expanding his studio's social media presence?",
      "prediction": "10:33 am on 9 April, 2023",
      "reference": "April, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.30935528891357367,
        "bleu4": 0.16348126556655487,
        "bert_precision": 0.8543530702590942,
        "bert_recall": 0.9253516793251038,
        "bert_f1": 0.8884361386299133,
        "meteor": 0.78125,
        "sbert_similarity": 0.6557660102844238
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon host a dance competition?",
      "prediction": "20 January, 2023",
      "reference": "May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9193105697631836,
        "bert_recall": 0.9332637786865234,
        "bert_f1": 0.9262346625328064,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8408563137054443
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon go to a fair to get more exposure for his dance studio?",
      "prediction": "19 June, 2023",
      "reference": "24 April, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9790794849395752,
        "bert_recall": 0.9790794849395752,
        "bert_f1": 0.9790794849395752,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9214292168617249
      }
    },
    {
      "sample_id": 1,
      "question": "Why did Gina decide to start her own clothing store?",
      "prediction": "Gina decided to start her own clothing store because she lost her job at DoorDash and wanted something creative that allowed her to showcase her passion.",
      "reference": "She always loved fashion trends and finding unique pieces and she lost her job so decided it was time to start her own business.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4186046511627907,
        "rouge1_f": 0.4,
        "rouge2_f": 0.24999999999999994,
        "rougeL_f": 0.24000000000000002,
        "bleu1": 0.4074074074074074,
        "bleu2": 0.30662207033593325,
        "bleu3": 0.2503328512892703,
        "bleu4": 0.18816393642137244,
        "bert_precision": 0.9104766249656677,
        "bert_recall": 0.9158592820167542,
        "bert_f1": 0.913159966468811,
        "meteor": 0.2626033057851239,
        "sbert_similarity": 0.7044140100479126
      }
    },
    {
      "sample_id": 1,
      "question": "Do Jon and Gina start businesses out of what they love?",
      "prediction": "Yes",
      "reference": "Yes",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina interview for a design internship?",
      "prediction": "2023",
      "reference": "10 May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9340406656265259,
        "bert_recall": 0.8737189173698425,
        "bert_f1": 0.9028733968734741,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7340056300163269
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina get accepted for the design internship?",
      "prediction": "27 May 2023",
      "reference": "27 May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.5066641486392106,
        "bleu3": 0.26662156551470545,
        "bleu4": 0.19053627645285995,
        "bert_precision": 0.9900174140930176,
        "bert_recall": 0.9668341875076294,
        "bert_f1": 0.978288471698761,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9863879084587097
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon start reading \"The Lean Startup\"?",
      "prediction": "27 May 2023",
      "reference": "May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9442589282989502,
        "bert_recall": 0.9442589282989502,
        "bert_f1": 0.9442589282989502,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8950991630554199
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina develop a video presentation to teach how to style her fashion pieces? ",
      "prediction": "8 February",
      "reference": "June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8503498435020447,
        "bert_recall": 0.8364963531494141,
        "bert_f1": 0.8433662056922913,
        "meteor": 0.0,
        "sbert_similarity": 0.5052170157432556
      }
    },
    {
      "sample_id": 1,
      "question": "How did Gina promote her clothes store?",
      "prediction": "Gina launched an ad campaign.",
      "reference": "worked with an artist to make unique fashion pieces, made limited-edition sweatshirts, got some new offers and promotions for online store, developed a video presentation showing how to style her pieces",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0588235294117647,
        "rouge1_f": 0.05405405405405406,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05405405405405406,
        "bleu1": 0.001567260425249201,
        "bleu2": 0.0005429149370447242,
        "bleu3": 0.000423801614244248,
        "bleu4": 0.000383898833591792,
        "bert_precision": 0.8817610144615173,
        "bert_recall": 0.8304600119590759,
        "bert_f1": 0.8553419709205627,
        "meteor": 0.0176056338028169,
        "sbert_similarity": 0.33884963393211365
      }
    },
    {
      "sample_id": 1,
      "question": "Which events has Jon participated in to promote his business venture?",
      "prediction": "Jon has participated in networking events and hosting dance competitions next month.",
      "reference": "fair, networking events, dance competition",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.35294117647058826,
        "rouge1_f": 0.47058823529411764,
        "rouge2_f": 0.26666666666666666,
        "rougeL_f": 0.47058823529411764,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.1386750490563073,
        "bleu3": 0.057551852907951895,
        "bleu4": 0.036362270465000714,
        "bert_precision": 0.8713201284408569,
        "bert_recall": 0.8848786950111389,
        "bert_f1": 0.8780470490455627,
        "meteor": 0.44834307992202727,
        "sbert_similarity": 0.6052389144897461
      }
    },
    {
      "sample_id": 1,
      "question": "What does Jon's dance studio offer?",
      "prediction": "Offering dance classes, workshops, and one-on-one mentoring to help dancers reach their full potential.",
      "reference": "one-on-one metoring and training to dancers,  workshops and classes to local schools and centers",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4799999999999999,
        "rouge1_f": 0.5,
        "rouge2_f": 0.20000000000000004,
        "rougeL_f": 0.3125,
        "bleu1": 0.4117647058823529,
        "bleu2": 0.160422236979937,
        "bleu3": 0.05719584747965367,
        "bleu4": 0.033271889139709514,
        "bert_precision": 0.8874890804290771,
        "bert_recall": 0.868507981300354,
        "bert_f1": 0.8778959512710571,
        "meteor": 0.10714285714285715,
        "sbert_similarity": 0.785266637802124
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon receive mentorship to promote his venture?",
      "prediction": "20 January 2023",
      "reference": "15 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9576760530471802,
        "bert_recall": 0.9419474005699158,
        "bert_f1": 0.9497466683387756,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8258851170539856
      }
    },
    {
      "sample_id": 1,
      "question": "Did Jon and Gina both participate in dance competitions?",
      "prediction": "Speaker Ginasays commented on winning first place in a competition.",
      "reference": "Yes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8204028606414795,
        "bert_recall": 0.8229469656944275,
        "bert_f1": 0.8216729164123535,
        "meteor": 0.0,
        "sbert_similarity": 0.1258157342672348
      }
    },
    {
      "sample_id": 1,
      "question": "When was Jon in Rome?",
      "prediction": "20 January, 2023",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9114115834236145,
        "bert_recall": 0.952965497970581,
        "bert_f1": 0.9317255020141602,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8402194976806641
      }
    },
    {
      "sample_id": 1,
      "question": "Which cities has Jon visited?",
      "prediction": "N/A (Jon is not mentioned to have visited specific cities in the provided information.)",
      "reference": "Paris, Rome",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8125871419906616,
        "bert_recall": 0.8848044872283936,
        "bert_f1": 0.8471595048904419,
        "meteor": 0.0,
        "sbert_similarity": 0.20151054859161377
      }
    },
    {
      "sample_id": 1,
      "question": "When Jon is planning to open his dance studio?",
      "prediction": "19 June 2023",
      "reference": "20 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9791974425315857,
        "bert_recall": 0.9585737586021423,
        "bert_f1": 0.9687758684158325,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.898318886756897
      }
    },
    {
      "sample_id": 1,
      "question": "How long did it take for Jon to open his studio?",
      "prediction": "It took Jon approximately 4 months to open his dance studio.",
      "reference": "six months",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.853533148765564,
        "bert_recall": 0.8807861804962158,
        "bert_f1": 0.8669455647468567,
        "meteor": 0.1724137931034483,
        "sbert_similarity": 0.34381940960884094
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina design a limited collection of hoodies?",
      "prediction": "2023",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9493498802185059,
        "bert_recall": 0.9255779981613159,
        "bert_f1": 0.9373131990432739,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8680327534675598
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon visit networking events for his store?",
      "prediction": "2023",
      "reference": "20 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9378911852836609,
        "bert_recall": 0.8854377269744873,
        "bert_f1": 0.91090989112854,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7710100412368774
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina start being recognized by fashion editors?",
      "prediction": "20 January 2023",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9457698464393616,
        "bert_recall": 0.9663739204406738,
        "bert_f1": 0.9559609293937683,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8386873006820679
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon start learning marketing and analytics tools?",
      "prediction": "2:35 pm on 9 July, 2023",
      "reference": "July, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.30935528891357367,
        "bleu4": 0.16348126556655487,
        "bert_precision": 0.8632218837738037,
        "bert_recall": 0.9182158708572388,
        "bert_f1": 0.8898700475692749,
        "meteor": 0.78125,
        "sbert_similarity": 0.7429641485214233
      }
    },
    {
      "sample_id": 1,
      "question": "When did Jon and Gina decide to collaborate to create dance content?",
      "prediction": "5:44 pm on 21 July, 2023",
      "reference": "21 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8829501867294312,
        "bert_recall": 0.9367016553878784,
        "bert_f1": 0.9090319871902466,
        "meteor": 0.30303030303030304,
        "sbert_similarity": 0.8003746271133423
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina mention Shia Labeouf?",
      "prediction": "23 July",
      "reference": " 23 July, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9424461722373962,
        "bert_recall": 0.8872013092041016,
        "bert_f1": 0.9139897227287292,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.7955402135848999
      }
    },
    {
      "sample_id": 1,
      "question": "When did Gina go to a dance class with a group of friends?",
      "prediction": "February 8",
      "reference": "21 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9066215753555298,
        "bert_recall": 0.8600660562515259,
        "bert_f1": 0.8827304244041443,
        "meteor": 0.0,
        "sbert_similarity": 0.4988899827003479
      }
    },
    {
      "sample_id": 1,
      "question": "What is Gina's favorite style of dance?",
      "prediction": "contemporary",
      "reference": "Contemporary",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9728239178657532,
        "bert_recall": 0.9728239178657532,
        "bert_f1": 0.9728239178657532,
        "meteor": 0.5,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 1,
      "question": "What is Jon's favorite style of dance?",
      "prediction": "Jhonny Cotton",
      "reference": "Contemporary",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.801871657371521,
        "bert_recall": 0.8968710899353027,
        "bert_f1": 0.8467150330543518,
        "meteor": 0.0,
        "sbert_similarity": 0.2022952437400818
      }
    },
    {
      "sample_id": 1,
      "question": "What was Gina's favorite dancing memory?",
      "prediction": "Seeing my students get better at it brings me such joy.",
      "reference": "Winning first place at a regionals dance competition",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8672840595245361,
        "bert_recall": 0.8455347418785095,
        "bert_f1": 0.8562712669372559,
        "meteor": 0.06024096385542169,
        "sbert_similarity": 0.19006505608558655
      }
    },
    {
      "sample_id": 1,
      "question": "What kind of dance piece did Gina's team perform to win first place?",
      "prediction": "contemporary",
      "reference": "\"Finding Freedom\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8625694513320923,
        "bert_recall": 0.8544247150421143,
        "bert_f1": 0.858477771282196,
        "meteor": 0.0,
        "sbert_similarity": 0.1278609037399292
      }
    },
    {
      "sample_id": 1,
      "question": "What do the dancers in the photo represent?",
      "prediction": "passion",
      "reference": "They are performing at the festival",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8622350692749023,
        "bert_recall": 0.8656631708145142,
        "bert_f1": 0.863945722579956,
        "meteor": 0.0,
        "sbert_similarity": 0.040157124400138855
      }
    },
    {
      "sample_id": 1,
      "question": "What does Gina say about the dancers in the photo?",
      "prediction": "Gina says it's awesome that you're doing well and dancing brings you joy.",
      "reference": "They look graceful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8551153540611267,
        "bert_recall": 0.873591423034668,
        "bert_f1": 0.8642545938491821,
        "meteor": 0.0,
        "sbert_similarity": 0.2609146535396576
      }
    },
    {
      "sample_id": 1,
      "question": "What is Jon's attitude towards being part of the dance festival?",
      "prediction": "Jon's attitude towards being part of the dance festival is enthusiastic and positive.",
      "reference": "Glad",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8628787994384766,
        "bert_recall": 0.8733422756195068,
        "bert_f1": 0.8680790066719055,
        "meteor": 0.0,
        "sbert_similarity": 0.145028218626976
      }
    },
    {
      "sample_id": 1,
      "question": "What kind of flooring is Jon looking for in his dance studio?",
      "prediction": "Marley flooring",
      "reference": "Marley flooring",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina find for her clothing store on 1 February, 2023?",
      "prediction": "a cool new fashion piece",
      "reference": "The perfect spot for her store",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705171346664429,
        "bert_recall": 0.8667318820953369,
        "bert_f1": 0.8686203956604004,
        "meteor": 0.0,
        "sbert_similarity": 0.29155421257019043
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina design for her store?",
      "prediction": "Limited edition hoodie collection",
      "reference": "the space, furniture, and decor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8493368625640869,
        "bert_recall": 0.8585646748542786,
        "bert_f1": 0.853925883769989,
        "meteor": 0.0,
        "sbert_similarity": 0.15737928450107574
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina want her customers to feel in her store?",
      "prediction": "Gina wanted her customers to feel appreciated and supported.",
      "reference": "cozy and comfortable",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8732737302780151,
        "bert_recall": 0.8815473318099976,
        "bert_f1": 0.8773910403251648,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.10783257335424423
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon say about Gina's progress with her store?",
      "prediction": "Jon is supportive and encouraging Gina with his feedback on her business.",
      "reference": "hard work's paying off",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8531904816627502,
        "bert_recall": 0.8625836372375488,
        "bert_f1": 0.8578613996505737,
        "meteor": 0.0,
        "sbert_similarity": 0.1038108766078949
      }
    },
    {
      "sample_id": 1,
      "question": "What made Gina choose the furniture and decor for her store?",
      "prediction": "Gina chose the furniture and decor for her store by herself.",
      "reference": "personal style and customer comfort",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8741101026535034,
        "bert_recall": 0.8776915073394775,
        "bert_f1": 0.8758971691131592,
        "meteor": 0.0892857142857143,
        "sbert_similarity": 0.30974531173706055
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon say about creating a special experience for customers?",
      "prediction": "create something awesome",
      "reference": "It's the key to making them feel welcome and coming back",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8581545948982239,
        "bert_recall": 0.8427916765213013,
        "bert_f1": 0.8504037261009216,
        "meteor": 0.0,
        "sbert_similarity": 0.1014561653137207
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina say about creating an experience for her customers?",
      "prediction": "Creating a special shopping experience",
      "reference": "making them want to come back",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9037808775901794,
        "bert_recall": 0.8559473752975464,
        "bert_f1": 0.8792140483856201,
        "meteor": 0.0,
        "sbert_similarity": 0.07501711696386337
      }
    },
    {
      "sample_id": 1,
      "question": "How is Gina's store doing?",
      "prediction": "Gina's store looks great and has been successful as her customers seem to love the new offers and promotions, indicating positive sales.",
      "reference": "The store is doing great.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23076923076923073,
        "rouge1_f": 0.21428571428571427,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.15999999999999998,
        "bleu2": 0.025819888974716113,
        "bleu3": 0.014877148519655915,
        "bleu4": 0.010713701843513142,
        "bert_precision": 0.8887795805931091,
        "bert_recall": 0.9452031254768372,
        "bert_f1": 0.9161233901977539,
        "meteor": 0.14925373134328362,
        "sbert_similarity": 0.5468804836273193
      }
    },
    {
      "sample_id": 1,
      "question": "What does Gina's tattoo symbolize?",
      "prediction": "freedom",
      "reference": "Freedom and expressing herself through dance",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9118189811706543,
        "bert_recall": 0.8418583869934082,
        "bert_f1": 0.8754432201385498,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.5314432382583618
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon and Gina compare their entrepreneurial journeys to?",
      "prediction": "entrepreneurial spirit",
      "reference": "dancing together and supporting each other",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8590664863586426,
        "bert_recall": 0.8594365119934082,
        "bert_f1": 0.8592514395713806,
        "meteor": 0.0,
        "sbert_similarity": 0.08114954829216003
      }
    },
    {
      "sample_id": 1,
      "question": "What advice does Gina give to Jon about running a successful business?",
      "prediction": "Stay determined and focused.",
      "reference": "build relationships with customers, create a strong brand image, stay positive",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.04037930359893108,
        "bleu2": 0.014276239697197267,
        "bleu3": 0.011437705664925165,
        "bleu4": 0.010847596378846365,
        "bert_precision": 0.9250272512435913,
        "bert_recall": 0.8828400373458862,
        "bert_f1": 0.9034414291381836,
        "meteor": 0.04854368932038835,
        "sbert_similarity": 0.20288413763046265
      }
    },
    {
      "sample_id": 1,
      "question": "Why did Jon shut down his bank account?",
      "prediction": "It was tough, but he needed to do it for his business.",
      "reference": "for his business",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.42857142857142855,
        "rouge1_f": 0.4,
        "rouge2_f": 0.3076923076923077,
        "rougeL_f": 0.4,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.1815682598006407,
        "bleu3": 0.1428356262969865,
        "bleu4": 0.07069301148938888,
        "bert_precision": 0.8848710060119629,
        "bert_recall": 0.9154433608055115,
        "bert_f1": 0.899897575378418,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.48820972442626953
      }
    },
    {
      "sample_id": 1,
      "question": "Why did Gina combine her clothing business with dance?",
      "prediction": "Gina combined her clothing business with dance because it allows her to show creativity and share her passion with others, aiming to offer unique fashion items inspired by dance.",
      "reference": "she is passionate about dance and fashion",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.19354838709677416,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666663,
        "bleu1": 0.09677419354838708,
        "bleu2": 0.017960530202677495,
        "bleu3": 0.010845742813191119,
        "bleu4": 0.007939087147543497,
        "bert_precision": 0.8802423477172852,
        "bert_recall": 0.9230772256851196,
        "bert_f1": 0.9011510610580444,
        "meteor": 0.2173913043478261,
        "sbert_similarity": 0.618930459022522
      }
    },
    {
      "sample_id": 1,
      "question": "What does Jon's dance make him?",
      "prediction": "Jon's dance makes him passionate and committed to sharing it.",
      "reference": "happy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8424670696258545,
        "bert_recall": 0.8538684844970703,
        "bert_f1": 0.8481294512748718,
        "meteor": 0.0,
        "sbert_similarity": 0.13162323832511902
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina receive from a dance contest?",
      "prediction": "congrats",
      "reference": "a trophy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9000536799430847,
        "bert_recall": 0.9000536799430847,
        "bert_f1": 0.9000536799430847,
        "meteor": 0.0,
        "sbert_similarity": 0.12211708724498749
      }
    },
    {
      "sample_id": 1,
      "question": "How does Gina stay confident in her business?",
      "prediction": "Gina stays confident by reminding herself of her successes and progress.",
      "reference": "By reminding herself of her successes and progress, having a support system, and focusing on why she started",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285714,
        "rouge1_f": 0.5517241379310345,
        "rouge2_f": 0.5185185185185185,
        "rougeL_f": 0.5517241379310345,
        "bleu1": 0.3422780793550613,
        "bleu2": 0.33440856582566936,
        "bleu3": 0.3268683792605396,
        "bleu4": 0.3148426474727926,
        "bert_precision": 0.9258210062980652,
        "bert_recall": 0.9023672342300415,
        "bert_f1": 0.9139437079429626,
        "meteor": 0.3999056269906807,
        "sbert_similarity": 0.5263329744338989
      }
    },
    {
      "sample_id": 1,
      "question": "What kind of professional experience did Gina get accepted for on May 23, 2023?",
      "prediction": "Fashion Internship",
      "reference": "fashion internship",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9225188493728638,
        "bert_recall": 0.9379208087921143,
        "bert_f1": 0.9301561117172241,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "Where is Gina's fashion internship?",
      "prediction": "international company",
      "reference": "fashion department of an international company",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.8975646495819092,
        "bert_recall": 0.8645281791687012,
        "bert_f1": 0.8807367086410522,
        "meteor": 0.33482142857142855,
        "sbert_similarity": 0.6517025232315063
      }
    },
    {
      "sample_id": 1,
      "question": "What book is Jon currently reading?",
      "prediction": "The context does not provide information about what book Jon is currently reading.",
      "reference": "The Lean Startup",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8386355042457581,
        "bert_recall": 0.9148505330085754,
        "bert_f1": 0.8750867247581482,
        "meteor": 0.125,
        "sbert_similarity": 0.023812085390090942
      }
    },
    {
      "sample_id": 1,
      "question": "What is Jon offering to the dancers at his dance studio?",
      "prediction": "Jon is offering workshops and classes to local schools and centers.",
      "reference": "One-on-one mentoring and training",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.11764705882352942,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352942,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8790848255157471,
        "bert_recall": 0.8814617991447449,
        "bert_f1": 0.8802716732025146,
        "meteor": 0.21276595744680854,
        "sbert_similarity": 0.2525121867656708
      }
    },
    {
      "sample_id": 1,
      "question": "How does Jon use the clipboard with a notepad attached to it?",
      "prediction": "Jon uses the clipboard and notepad attachment to keep track of his goals, organize ideas, and maintain focus on his progress.",
      "reference": "To set goals, track achievements, and find areas for improvement",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.27586206896551724,
        "rouge1_f": 0.25806451612903225,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19354838709677416,
        "bleu1": 0.25,
        "bleu2": 0.14744195615489714,
        "bleu3": 0.04767497026787568,
        "bleu4": 0.0261908774339841,
        "bert_precision": 0.8863584399223328,
        "bert_recall": 0.9174729585647583,
        "bert_f1": 0.9016473889350891,
        "meteor": 0.1801801801801802,
        "sbert_similarity": 0.3363361358642578
      }
    },
    {
      "sample_id": 1,
      "question": "What does Jon tell Gina he won't do?",
      "prediction": "Jon tells Gina he won't give up.",
      "reference": "quit",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8176589608192444,
        "bert_recall": 0.8234117031097412,
        "bert_f1": 0.8205252885818481,
        "meteor": 0.0,
        "sbert_similarity": 0.1645134538412094
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon take a trip to Rome for?",
      "prediction": "To clear my mind.",
      "reference": "To clear his mind",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 0.75,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.75,
        "bleu1": 0.6,
        "bleu2": 0.3872983346207417,
        "bleu3": 0.17404441896107775,
        "bleu4": 0.12574334296829354,
        "bert_precision": 0.9829521179199219,
        "bert_recall": 0.9786978960037231,
        "bert_f1": 0.9808204174041748,
        "meteor": 0.46875,
        "sbert_similarity": 0.7862703204154968
      }
    },
    {
      "sample_id": 1,
      "question": "What is Jon working on opening?",
      "prediction": "Jon's work involves opening a dance studio",
      "reference": "a dance studio",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.375,
        "bleu2": 0.32732683535398854,
        "bleu3": 0.2649097550589377,
        "bleu4": 0.1374708101760565,
        "bert_precision": 0.9042841196060181,
        "bert_recall": 0.9327184557914734,
        "bert_f1": 0.9182812571525574,
        "meteor": 0.866013071895425,
        "sbert_similarity": 0.6905720233917236
      }
    },
    {
      "sample_id": 1,
      "question": "How does Gina describe the studio that Jon has opened?",
      "prediction": "amazing",
      "reference": "amazing",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "How does Jon feel about the opening night of his dance studio?",
      "prediction": "Jon feels determined and passionate about the success of his dance studio.",
      "reference": "excited",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.866918683052063,
        "bert_recall": 0.8656712770462036,
        "bert_f1": 0.8662945032119751,
        "meteor": 0.0,
        "sbert_similarity": 0.18840840458869934
      }
    },
    {
      "sample_id": 1,
      "question": "How does Gina describe the feeling that dance brings?",
      "prediction": "Joy and fulfillment",
      "reference": "magical",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8579031229019165,
        "bert_recall": 0.8741551637649536,
        "bert_f1": 0.8659529089927673,
        "meteor": 0.0,
        "sbert_similarity": 0.26935499906539917
      }
    },
    {
      "sample_id": 1,
      "question": "What does Jon plan to do at the grand opening of his dance studio?",
      "prediction": "Open the dance studio and start offering classes.",
      "reference": "savor all the good vibes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8730111122131348,
        "bert_recall": 0.8758618831634521,
        "bert_f1": 0.8744341731071472,
        "meteor": 0.09433962264150944,
        "sbert_similarity": 0.08624650537967682
      }
    },
    {
      "sample_id": 1,
      "question": "What does Gina say to Jon about the grand opening?",
      "prediction": "Gina says: Can't wait!",
      "reference": "Let's live it up and make some great memories",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.863581120967865,
        "bert_recall": 0.8520947098731995,
        "bert_f1": 0.8577994704246521,
        "meteor": 0.0,
        "sbert_similarity": 0.17191386222839355
      }
    },
    {
      "sample_id": 1,
      "question": "What is the general sentiment about the upcoming grand opening?",
      "prediction": "Excitement and enthusiasm",
      "reference": "excitement",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9241281151771545,
        "bert_recall": 0.9418505430221558,
        "bert_f1": 0.9329051971435547,
        "meteor": 0.4166666666666667,
        "sbert_similarity": 0.8583879470825195
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina make a limited edition line of?",
      "prediction": "Gina made a limited edition hoodie collection.",
      "reference": "Hoodies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8982785940170288,
        "bert_recall": 0.9130754470825195,
        "bert_f1": 0.9056165814399719,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.5702798366546631
      }
    },
    {
      "sample_id": 1,
      "question": "According to Gina, what makes Jon a perfect mentor and guide?",
      "prediction": "Jon's positivity and determination.",
      "reference": "His positivity and determination",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 0.6666666666666665,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666665,
        "bleu1": 0.5,
        "bleu2": 0.447213595499958,
        "bleu3": 0.3721003696282278,
        "bleu4": 0.20205155046766235,
        "bert_precision": 0.9738436341285706,
        "bert_recall": 0.9794800281524658,
        "bert_f1": 0.9766537547111511,
        "meteor": 0.46875,
        "sbert_similarity": 0.6382689476013184
      }
    },
    {
      "sample_id": 1,
      "question": "What temporary job did Jon take to cover expenses?",
      "prediction": "Not mentioned",
      "reference": "Not mentioned",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "What plans does Jon have after receiving advice at the networking event?",
      "prediction": "Jon has just received good news and is looking forward to using his new knowledge effectively.",
      "reference": "Sprucing up his business plan, tweaking his pitch to investors, and working on an online platform.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.19354838709677422,
        "rouge1_f": 0.1875,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0625,
        "bleu1": 0.20917876833006488,
        "bleu2": 0.03409196824036717,
        "bleu3": 0.019772813718890843,
        "bleu4": 0.014461870879312647,
        "bert_precision": 0.8777824640274048,
        "bert_recall": 0.8730961084365845,
        "bert_f1": 0.8754330277442932,
        "meteor": 0.09375,
        "sbert_similarity": 0.2929849624633789
      }
    },
    {
      "sample_id": 1,
      "question": "What offer does Gina make to Jon regarding social media?",
      "prediction": "Gina offers Jon advice on managing his business's social media accounts.",
      "reference": "Helping with making content and managing his social media accounts.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.47619047619047616,
        "rouge1_f": 0.45454545454545453,
        "rouge2_f": 0.3,
        "rougeL_f": 0.45454545454545453,
        "bleu1": 0.46153846153846156,
        "bleu2": 0.3922322702763681,
        "bleu3": 0.3071983684320824,
        "bleu4": 0.22997519112894443,
        "bert_precision": 0.9036165475845337,
        "bert_recall": 0.915854811668396,
        "bert_f1": 0.9096945524215698,
        "meteor": 0.47920792079207924,
        "sbert_similarity": 0.5861000418663025
      }
    },
    {
      "sample_id": 1,
      "question": "What is Jon's favorite style of painting?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Contemporary",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8872315883636475,
        "bert_recall": 0.8783940672874451,
        "bert_f1": 0.8827906847000122,
        "meteor": 0.0,
        "sbert_similarity": 0.23356106877326965
      }
    },
    {
      "sample_id": 1,
      "question": "What was Jon's favorite dancing memory?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Winning first place at a regionals dance competition",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8901429176330566,
        "bert_recall": 0.847588300704956,
        "bert_f1": 0.8683445453643799,
        "meteor": 0.0,
        "sbert_similarity": 0.045117732137441635
      }
    },
    {
      "sample_id": 1,
      "question": "What kind of dance piece did Jon's team perform to win first place?",
      "prediction": "Not mentioned in the conversation",
      "reference": "\"Finding Freedom\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8945667743682861,
        "bert_recall": 0.8947187066078186,
        "bert_f1": 0.8946427702903748,
        "meteor": 0.0,
        "sbert_similarity": 0.04353433474898338
      }
    },
    {
      "sample_id": 1,
      "question": "What is Gina's attitude towards participating in the dance festival?",
      "prediction": "Glad",
      "reference": "Glad",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "What kind of flooring is Gina looking for in her dance studio?",
      "prediction": "Marley flooring",
      "reference": "Marley flooring",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon find for his clothing store on 1 February, 2023?",
      "prediction": "The perfect spot for her store",
      "reference": "The perfect spot for her store",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon design for his store?",
      "prediction": "hoodie",
      "reference": "the space, furniture, and decor",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8604522943496704,
        "bert_recall": 0.8485106229782104,
        "bert_f1": 0.8544397354125977,
        "meteor": 0.0,
        "sbert_similarity": 0.23007193207740784
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon want his customers to feel in her store?",
      "prediction": "cozy and comfortable",
      "reference": "cozy and comfortable",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "What made Jon choose the furniture and decor for his store?",
      "prediction": "personal style and customer comfort",
      "reference": "personal style and customer comfort",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "How is Jon's store doing?",
      "prediction": "The store is doing great.",
      "reference": "The store is doing great.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 1,
      "question": "What does Jon's tattoo symbolize?",
      "prediction": "Freedom and expressing himself through dance",
      "reference": "Freedom and expressing himself through dance",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "Why did Gina shut down her bank account?",
      "prediction": "for her business",
      "reference": "for her business",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 1,
      "question": "Why did Jon combine his clothing business with dance?",
      "prediction": "he is passionate about dance",
      "reference": "he is passionate about dance and fashion",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.6703200460356393,
        "bleu2": 0.6703200460356393,
        "bleu3": 0.6703200460356393,
        "bleu4": 0.6703200460356393,
        "bert_precision": 0.9910009503364563,
        "bert_recall": 0.960396945476532,
        "bert_f1": 0.9754589796066284,
        "meteor": 0.7323529411764707,
        "sbert_similarity": 0.9054123163223267
      }
    },
    {
      "sample_id": 1,
      "question": "What did Gina receive from a dance contest?",
      "prediction": "a trophy",
      "reference": "a trophy",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 1,
      "question": "What kind of professional experience did Jon get accepted for on May 23, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "fashion internship",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8712784647941589,
        "bert_recall": 0.8641520142555237,
        "bert_f1": 0.8677006363868713,
        "meteor": 0.0,
        "sbert_similarity": 0.03995133936405182
      }
    },
    {
      "sample_id": 1,
      "question": "Where is Gina's HR internship?",
      "prediction": "Not mentioned in the conversation",
      "reference": "fashion department of an international company",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672271966934204,
        "bert_recall": 0.8392521142959595,
        "bert_f1": 0.853010356426239,
        "meteor": 0.0,
        "sbert_similarity": 0.08578408509492874
      }
    },
    {
      "sample_id": 1,
      "question": "Where is Jon's fashion internship?",
      "prediction": "Not mentioned in the conversation",
      "reference": "fashion department of an international company",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8672271966934204,
        "bert_recall": 0.8392521142959595,
        "bert_f1": 0.853010356426239,
        "meteor": 0.0,
        "sbert_similarity": 0.08578408509492874
      }
    },
    {
      "sample_id": 1,
      "question": "What book is Gina currently reading?",
      "prediction": "Not mentioned in the conversation",
      "reference": "The Lean Startup",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8883641362190247,
        "bert_recall": 0.9172934293746948,
        "bert_f1": 0.9025970101356506,
        "meteor": 0.15625,
        "sbert_similarity": 0.1566043198108673
      }
    },
    {
      "sample_id": 1,
      "question": "How does Gina use the clipboard with a notepad attached to it?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To set goals, track achievements, and find areas for improvement",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8966887593269348,
        "bert_recall": 0.8530102372169495,
        "bert_f1": 0.8743043541908264,
        "meteor": 0.0,
        "sbert_similarity": 0.0566495805978775
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon take a trip to Barcelona for?",
      "prediction": "Not mentioned in the conversation or To clear his mind",
      "reference": "To clear his mind",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.5,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.9000351428985596,
        "bert_recall": 0.9616580009460449,
        "bert_f1": 0.9298267364501953,
        "meteor": 0.8627717391304348,
        "sbert_similarity": 0.657044529914856
      }
    },
    {
      "sample_id": 1,
      "question": "What did Jon make a limited edition line of?",
      "prediction": "hoodies",
      "reference": "Hoodies",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9396120309829712,
        "bert_recall": 0.9384375810623169,
        "bert_f1": 0.9390243887901306,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 1,
      "question": "What temporary job did Gina take to cover expenses?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Not mentioned",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.929399311542511,
        "bert_recall": 0.9576510190963745,
        "bert_f1": 0.9433136582374573,
        "meteor": 0.8152173913043478,
        "sbert_similarity": 0.7840333580970764
      }
    },
    {
      "sample_id": 1,
      "question": "What plans does Gina have after receiving advice at the networking event?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Sprucing up her business plan, tweaking her pitch to investors, and working on an online platform.",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8892083168029785,
        "bert_recall": 0.8496474027633667,
        "bert_f1": 0.8689778447151184,
        "meteor": 0.0,
        "sbert_similarity": 0.12928283214569092
      }
    },
    {
      "sample_id": 2,
      "question": "Who did Maria have dinner with on May 3, 2023?",
      "prediction": "John",
      "reference": "her mother",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8976172208786011,
        "bert_recall": 0.878473699092865,
        "bert_f1": 0.8879423141479492,
        "meteor": 0.0,
        "sbert_similarity": 0.32391855120658875
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria donate her car?",
      "prediction": "25 May, 2023",
      "reference": "21 December 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9074885845184326,
        "bert_recall": 0.9200844764709473,
        "bert_f1": 0.9137431383132935,
        "meteor": 0.0,
        "sbert_similarity": 0.7582008838653564
      }
    },
    {
      "sample_id": 2,
      "question": "What martial arts has John done?",
      "prediction": "John did kickboxing",
      "reference": "Kickboxing, Taekwondo",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8992139101028442,
        "bert_recall": 0.8752328157424927,
        "bert_f1": 0.8870612978935242,
        "meteor": 0.0,
        "sbert_similarity": 0.5116642713546753
      }
    },
    {
      "sample_id": 2,
      "question": "What type of volunteering have John and Maria both done?",
      "prediction": "volunteering at homeless shelters",
      "reference": "Volunteering at a homeless shelter",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.888888888888889,
        "bleu1": 0.5841005873035536,
        "bleu2": 0.3894003915357024,
        "bleu3": 0.1834031049364524,
        "bleu4": 0.14643803168582153,
        "bert_precision": 0.9729059934616089,
        "bert_recall": 0.970947265625,
        "bert_f1": 0.9719256162643433,
        "meteor": 0.7653061224489797,
        "sbert_similarity": 0.9698159694671631
      }
    },
    {
      "sample_id": 2,
      "question": "When did John join the online support group?",
      "prediction": "3 August",
      "reference": "The week before 1 January 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8727098703384399,
        "bert_recall": 0.8338280916213989,
        "bert_f1": 0.8528259992599487,
        "meteor": 0.0,
        "sbert_similarity": 0.4807729125022888
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria go to the beach?",
      "prediction": "June 16th, 2023",
      "reference": "December 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8903539776802063,
        "bert_recall": 0.9164775013923645,
        "bert_f1": 0.9032268524169922,
        "meteor": 0.0,
        "sbert_similarity": 0.6921895742416382
      }
    },
    {
      "sample_id": 2,
      "question": "Where has Maria made friends?",
      "prediction": "in nature",
      "reference": "homeless shelter, gym, church",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8786786794662476,
        "bert_recall": 0.8544137477874756,
        "bert_f1": 0.8663763403892517,
        "meteor": 0.0,
        "sbert_similarity": 0.13874100148677826
      }
    },
    {
      "sample_id": 2,
      "question": "What items des John mention having as a child?",
      "prediction": "John mentions having pictures of children playing in his memories.",
      "reference": "A doll, a film camera",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8678320646286011,
        "bert_recall": 0.8730064630508423,
        "bert_f1": 0.8704115748405457,
        "meteor": 0.0,
        "sbert_similarity": 0.31925612688064575
      }
    },
    {
      "sample_id": 2,
      "question": "What might John's financial status be?",
      "prediction": "John mentions receiving a promotion at work, which is typically associated with higher income and better job opportunities.",
      "reference": "Middle-class or wealthy",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8521305322647095,
        "bert_recall": 0.8853355050086975,
        "bert_f1": 0.8684157133102417,
        "meteor": 0.0,
        "sbert_similarity": 0.2538158595561981
      }
    },
    {
      "sample_id": 2,
      "question": "Who gave Maria's family money when she was younger and her family was going through tough times?",
      "prediction": "None explicitly mentioned",
      "reference": "Her aunt",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8660399317741394,
        "bert_recall": 0.8740763664245605,
        "bert_f1": 0.8700395226478577,
        "meteor": 0.0,
        "sbert_similarity": 0.17297516763210297
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria meet Jean?",
      "prediction": "6:10 pm on 22 December, 2022",
      "reference": "February 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8386585712432861,
        "bert_recall": 0.8952646255493164,
        "bert_f1": 0.8660376071929932,
        "meteor": 0.0,
        "sbert_similarity": 0.6722593903541565
      }
    },
    {
      "sample_id": 2,
      "question": "What people has Maria met and helped while volunteering?",
      "prediction": "divorce, losing her job, becoming homeless",
      "reference": "David, Jean, Cindy, Laura",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.25,
        "bleu2": 0.059761430466719695,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.03303164318013808,
        "bert_precision": 0.855134129524231,
        "bert_recall": 0.8628648519515991,
        "bert_f1": 0.8589821457862854,
        "meteor": 0.0,
        "sbert_similarity": 0.12402772158384323
      }
    },
    {
      "sample_id": 2,
      "question": "What test has John taken multiple times?",
      "prediction": "aptitude test",
      "reference": "The military aptitude test",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9147357940673828,
        "bert_recall": 0.9008870720863342,
        "bert_f1": 0.9077586531639099,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.7506763339042664
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria's grandmother pass away?",
      "prediction": "6 March, 2023",
      "reference": "The week before 6 March 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.45489799478447507,
        "bleu2": 0.30326532985631666,
        "bleu3": 0.14283448174223617,
        "bleu4": 0.11404605374835301,
        "bert_precision": 0.9494093060493469,
        "bert_recall": 0.9199808835983276,
        "bert_f1": 0.9344634413719177,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8536838889122009
      }
    },
    {
      "sample_id": 2,
      "question": "Would John be considered a patriotic person?",
      "prediction": "Yes",
      "reference": "Yes",
      "category": 3,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 2,
      "question": "What writing classes has Maria taken?",
      "prediction": "A creative writing class",
      "reference": "Poetry, creative writing",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9205028414726257,
        "bert_recall": 0.9197572469711304,
        "bert_f1": 0.9201298952102661,
        "meteor": 0.6048387096774195,
        "sbert_similarity": 0.7203627824783325
      }
    },
    {
      "sample_id": 2,
      "question": "When did John get his degree?",
      "prediction": "2023",
      "reference": "The week before 2 April 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.907595157623291,
        "bert_recall": 0.8385152220726013,
        "bert_f1": 0.8716886639595032,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.643884539604187
      }
    },
    {
      "sample_id": 2,
      "question": "What might John's degree be in?",
      "prediction": "Johnsays:",
      "reference": "Political science, Public administration, Public affairs",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7880029082298279,
        "bert_recall": 0.7667020559310913,
        "bert_f1": 0.7772065997123718,
        "meteor": 0.0,
        "sbert_similarity": 0.08120432496070862
      }
    },
    {
      "sample_id": 2,
      "question": "Who did John go to yoga with?",
      "prediction": "Colleague",
      "reference": "Rob",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8670558333396912,
        "bert_recall": 0.8958966135978699,
        "bert_f1": 0.8812403678894043,
        "meteor": 0.0,
        "sbert_similarity": 0.3745642304420471
      }
    },
    {
      "sample_id": 2,
      "question": "What damages have happened to John's car?",
      "prediction": "John's car has been damaged by a red light and hit incident.",
      "reference": "Broken windshield, Car broke down",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8642479181289673,
        "bert_recall": 0.8805504441261292,
        "bert_f1": 0.8723229765892029,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.4441068172454834
      }
    },
    {
      "sample_id": 2,
      "question": "When did John take a road trip to the Pacific Northwest?",
      "prediction": "10 April, 2023",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8926401734352112,
        "bert_recall": 0.9222603440284729,
        "bert_f1": 0.9072085618972778,
        "meteor": 0.0,
        "sbert_similarity": 0.6445618271827698
      }
    },
    {
      "sample_id": 2,
      "question": "What areas of the U.S. has John been to or is planning to go to?",
      "prediction": "The East Coast",
      "reference": "Pacific northwest, east coast",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3422780793550613,
        "bleu2": 0.29642151188002913,
        "bleu3": 0.16711688476494407,
        "bleu4": 0.12336445138236939,
        "bert_precision": 0.9250351786613464,
        "bert_recall": 0.8898252844810486,
        "bert_f1": 0.9070886969566345,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.8211047649383545
      }
    },
    {
      "sample_id": 2,
      "question": "When did John go to a convention with colleagues?",
      "prediction": "18 April 2023",
      "reference": "March 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9482080936431885,
        "bert_recall": 0.9598850011825562,
        "bert_f1": 0.9540108442306519,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8849347829818726
      }
    },
    {
      "sample_id": 2,
      "question": "What desserts has Maria made?",
      "prediction": "banana split sundae",
      "reference": "Banana split sundae, Peach cobbler",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9683133363723755,
        "bert_recall": 0.9028267860412598,
        "bert_f1": 0.934424102306366,
        "meteor": 0.39062499999999994,
        "sbert_similarity": 0.8433097004890442
      }
    },
    {
      "sample_id": 2,
      "question": "When did John start boot camp with his family?",
      "prediction": "last month",
      "reference": "April.2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8493005037307739,
        "bert_recall": 0.8689366579055786,
        "bert_f1": 0.859006404876709,
        "meteor": 0.0,
        "sbert_similarity": 0.4182291328907013
      }
    },
    {
      "sample_id": 2,
      "question": "What European countries has Maria been to?",
      "prediction": "none",
      "reference": "Spain, England",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8187488317489624,
        "bert_recall": 0.8093240857124329,
        "bert_f1": 0.8140091300010681,
        "meteor": 0.0,
        "sbert_similarity": 0.18778176605701447
      }
    },
    {
      "sample_id": 2,
      "question": "What has Maria done to feel closer to her faith?",
      "prediction": "Maria is highly committed and actively assisting others.",
      "reference": "Join a local church, buy a cross necklace",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.883522629737854,
        "bert_recall": 0.8769000768661499,
        "bert_f1": 0.8801988959312439,
        "meteor": 0.0,
        "sbert_similarity": 0.1269053816795349
      }
    },
    {
      "sample_id": 2,
      "question": "When did John have a party with veterans?",
      "prediction": "20 May 2023",
      "reference": "The Friday before 20 May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9550071358680725,
        "bert_recall": 0.9043822288513184,
        "bert_f1": 0.9290055632591248,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8235771059989929
      }
    },
    {
      "sample_id": 2,
      "question": "What causes does John feel passionate about supporting?",
      "prediction": "John feels passionate about supporting various social and environmental causes.",
      "reference": "Veterans, schools, infrastructure",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8552919626235962,
        "bert_recall": 0.8546711802482605,
        "bert_f1": 0.8549814820289612,
        "meteor": 0.0,
        "sbert_similarity": 0.20576728880405426
      }
    },
    {
      "sample_id": 2,
      "question": "What events is Maria planning for the homeless shelter funraiser?",
      "prediction": "Maria plans to volunteer and possibly organize activities at the homeless shelter fundraiser.",
      "reference": "Chili cook-off, ring-toss tournament",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8640929460525513,
        "bert_recall": 0.8660836219787598,
        "bert_f1": 0.86508709192276,
        "meteor": 0.0,
        "sbert_similarity": 0.11569087207317352
      }
    },
    {
      "sample_id": 2,
      "question": "What shelters does Maria volunteer at?",
      "prediction": "homeless shelter",
      "reference": "The homeless shelter, the dog shelter",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.0820849986238988,
        "bleu2": 0.0820849986238988,
        "bleu3": 0.038394038428908775,
        "bleu4": 0.025957555738330737,
        "bert_precision": 0.9139983654022217,
        "bert_recall": 0.9071416854858398,
        "bert_f1": 0.9105570912361145,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.8430901765823364
      }
    },
    {
      "sample_id": 2,
      "question": "When did John get his dog Max?",
      "prediction": "10 years ago",
      "reference": "In 2013",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7875399589538574,
        "bert_recall": 0.8252474069595337,
        "bert_f1": 0.8059529066085815,
        "meteor": 0.0,
        "sbert_similarity": 0.5437142252922058
      }
    },
    {
      "sample_id": 2,
      "question": "What outdoor activities has John done with his colleagues?",
      "prediction": "John and Max have had a great time on a mountaineering trip with workmates.",
      "reference": "Hiking, mountaineering",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333336,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8709515333175659,
        "bert_recall": 0.9176042079925537,
        "bert_f1": 0.8936694264411926,
        "meteor": 0.15625000000000003,
        "sbert_similarity": 0.45648276805877686
      }
    },
    {
      "sample_id": 2,
      "question": "What types of yoga has Maria practiced?",
      "prediction": "kundalini yoga",
      "reference": "Aerial, kundalini",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9265235662460327,
        "bert_recall": 0.903394877910614,
        "bert_f1": 0.9148131012916565,
        "meteor": 0.25,
        "sbert_similarity": 0.6518124341964722
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria join a gym?",
      "prediction": "4 May 2023",
      "reference": "The week before 16 June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9319300055503845,
        "bert_recall": 0.8833311796188354,
        "bert_f1": 0.9069800972938538,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.7464053630828857
      }
    },
    {
      "sample_id": 2,
      "question": "What states has Maria vacationed at?",
      "prediction": "Florida",
      "reference": "Oregon, Florida",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.8967558741569519,
        "bert_recall": 0.8869853615760803,
        "bert_f1": 0.891843855381012,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7603732943534851
      }
    },
    {
      "sample_id": 2,
      "question": "What music events has John attended?",
      "prediction": "John has attended a violin concert.",
      "reference": "Live music event, violin concert",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.927356481552124,
        "bert_recall": 0.9094351530075073,
        "bert_f1": 0.9183083772659302,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.6711036562919617
      }
    },
    {
      "sample_id": 2,
      "question": "What events for veterans has John participated in?",
      "prediction": "Last Friday's event and this week's gathering to support veterans.",
      "reference": "Petition, march, party, visiting veterans hospital, 5K charity run",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8672975301742554,
        "bert_recall": 0.8591293096542358,
        "bert_f1": 0.8631940484046936,
        "meteor": 0.0,
        "sbert_similarity": 0.5204493999481201
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria get in a car accident?",
      "prediction": "2023-01-09",
      "reference": "July 2, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8815315961837769,
        "bert_recall": 0.9064790606498718,
        "bert_f1": 0.8938313126564026,
        "meteor": 0.0,
        "sbert_similarity": 0.6062711477279663
      }
    },
    {
      "sample_id": 2,
      "question": "Around which US holiday did Maria get into a car accident?",
      "prediction": "Christmas",
      "reference": "Independence Day",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9101325273513794,
        "bert_recall": 0.8815309405326843,
        "bert_f1": 0.8956034779548645,
        "meteor": 0.0,
        "sbert_similarity": 0.28152698278427124
      }
    },
    {
      "sample_id": 2,
      "question": "What are the names of John's children?",
      "prediction": "John doesn't mention any specific names",
      "reference": "Kyle, Sara",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8423433303833008,
        "bert_recall": 0.8537722826004028,
        "bert_f1": 0.8480193018913269,
        "meteor": 0.0,
        "sbert_similarity": 0.14486989378929138
      }
    },
    {
      "sample_id": 2,
      "question": "Does John live close to a beach or the mountains?",
      "prediction": "No",
      "reference": "beach",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8422957062721252,
        "bert_recall": 0.8096333742141724,
        "bert_f1": 0.8256415724754333,
        "meteor": 0.0,
        "sbert_similarity": 0.1658223569393158
      }
    },
    {
      "sample_id": 2,
      "question": "What area was hit by a flood?",
      "prediction": "the old area",
      "reference": "West County",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.867649495601654,
        "bert_recall": 0.8821843862533569,
        "bert_f1": 0.8748565912246704,
        "meteor": 0.0,
        "sbert_similarity": 0.39748579263687134
      }
    },
    {
      "sample_id": 2,
      "question": "When was John's old area hit with a flood?",
      "prediction": "last week",
      "reference": "The week before 7 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8907418251037598,
        "bert_recall": 0.8205779790878296,
        "bert_f1": 0.854221522808075,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.514761209487915
      }
    },
    {
      "sample_id": 2,
      "question": "What activities has Maria done with her church friends?",
      "prediction": "Maria has volunteered at the homeless shelter and taken up community work, including helping with service-focused online groups and participating in a local school mentoring program.",
      "reference": "Hiking, picnic, volunteer work",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06896551724137932,
        "rouge1_f": 0.12903225806451613,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12903225806451613,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.016265001215808875,
        "bleu3": 0.010531409528076477,
        "bleu4": 0.007987276352377327,
        "bert_precision": 0.8455461859703064,
        "bert_recall": 0.8715665936470032,
        "bert_f1": 0.8583592176437378,
        "meteor": 0.08064516129032259,
        "sbert_similarity": 0.40872275829315186
      }
    },
    {
      "sample_id": 2,
      "question": "Would John be open to moving to another country?",
      "prediction": "no",
      "reference": "No, he has goals specifically in the U.S. like joining the military and running for office.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 4.139937718785167e-08,
        "bleu2": 1.3091632562602778e-08,
        "bleu3": 9.057196866699444e-09,
        "bleu4": 7.361966004159171e-09,
        "bert_precision": 0.8360415697097778,
        "bert_recall": 0.8310286998748779,
        "bert_f1": 0.8335276246070862,
        "meteor": 0.0,
        "sbert_similarity": 0.1790430247783661
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria go hiking with her church friends?",
      "prediction": "2023",
      "reference": "The weekend before 22 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9051557779312134,
        "bert_recall": 0.8313997983932495,
        "bert_f1": 0.866711437702179,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.6494185924530029
      }
    },
    {
      "sample_id": 2,
      "question": "What exercises has John done?",
      "prediction": "kickboxing",
      "reference": "Weight training, Circuit training, Kickboxing, yoga",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.00033546262790251185,
        "bleu2": 0.0001060825974037491,
        "bleu3": 7.33912263594384e-05,
        "bleu4": 5.965462840365855e-05,
        "bert_precision": 0.8932955265045166,
        "bert_recall": 0.8489405512809753,
        "bert_f1": 0.8705533742904663,
        "meteor": 0.0,
        "sbert_similarity": 0.5662981271743774
      }
    },
    {
      "sample_id": 2,
      "question": "When did John have his first firefighter call-out?",
      "prediction": "last Sunday",
      "reference": "The sunday before 3` July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.0410424993119494,
        "bleu2": 0.018354763685601443,
        "bleu3": 0.014286435890917338,
        "bleu4": 0.012274570264879268,
        "bert_precision": 0.8765389919281006,
        "bert_recall": 0.8048577904701233,
        "bert_f1": 0.8391704559326172,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5217379927635193
      }
    },
    {
      "sample_id": 2,
      "question": "What food item did Maria drop off at the homeless shelter?",
      "prediction": "food",
      "reference": "Cakes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9274718761444092,
        "bert_recall": 0.8849724531173706,
        "bert_f1": 0.9057238698005676,
        "meteor": 0.0,
        "sbert_similarity": 0.5239286422729492
      }
    },
    {
      "sample_id": 2,
      "question": "What attributes describe John?",
      "prediction": "John is enthusiastic and motivational.",
      "reference": "Selfless, family-oriented, passionate, rational",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9195396900177002,
        "bert_recall": 0.8946646451950073,
        "bert_f1": 0.9069316387176514,
        "meteor": 0.0,
        "sbert_similarity": 0.27197951078414917
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria start volunteering at the homeless shelter?",
      "prediction": "2023",
      "reference": "Around August 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9050372838973999,
        "bert_recall": 0.88333660364151,
        "bert_f1": 0.8940552473068237,
        "meteor": 0.0,
        "sbert_similarity": 0.7116497755050659
      }
    },
    {
      "sample_id": 2,
      "question": "Who have written notes of gratitude to Maria?",
      "prediction": "Laura",
      "reference": "Cindy, Laura",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.919296145439148,
        "bert_recall": 0.8486865758895874,
        "bert_f1": 0.882581353187561,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7756766080856323
      }
    },
    {
      "sample_id": 2,
      "question": "When did John help renovate his hometown community center?",
      "prediction": "5 August 2023",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8927364349365234,
        "bert_recall": 0.9290671944618225,
        "bert_f1": 0.9105396270751953,
        "meteor": 0.0,
        "sbert_similarity": 0.6846304535865784
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria take up community work with her church friends?",
      "prediction": "1 January, 2023",
      "reference": "August 4, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9410261511802673,
        "bert_recall": 0.9515048265457153,
        "bert_f1": 0.9462364912033081,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7653173208236694
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria receive a medal from the homeless shelter?",
      "prediction": "9 August 2023",
      "reference": "The week before 9 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9519796967506409,
        "bert_recall": 0.9062991738319397,
        "bert_f1": 0.9285779595375061,
        "meteor": 0.5165692007797271,
        "sbert_similarity": 0.8726242780685425
      }
    },
    {
      "sample_id": 2,
      "question": "When did John participate in a 5K charity run?",
      "prediction": "9 August, 2023",
      "reference": "first weekend of August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.38940039153570244,
        "bleu2": 0.10054274876162822,
        "bleu3": 0.07504064673338035,
        "bleu4": 0.07440995947486906,
        "bert_precision": 0.9340300559997559,
        "bert_recall": 0.9048007726669312,
        "bert_f1": 0.9191831350326538,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7215131521224976
      }
    },
    {
      "sample_id": 2,
      "question": "What causes has John done events for?",
      "prediction": "Events for any other cause besides those discussed",
      "reference": "Toy drive, Community food drive, veterans, domestic violence",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8234055638313293,
        "bert_recall": 0.8172104358673096,
        "bert_f1": 0.8202963471412659,
        "meteor": 0.0,
        "sbert_similarity": 0.3109297454357147
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria get Coco?",
      "prediction": "12:10 am on 11 August, 2023",
      "reference": "Two weeks before 11 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8936804533004761,
        "bert_recall": 0.9156813621520996,
        "bert_f1": 0.9045471549034119,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.6935675144195557
      }
    },
    {
      "sample_id": 2,
      "question": "When did John go on a camping trip with Max?",
      "prediction": "last weekend",
      "reference": "The summer of 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8881521224975586,
        "bert_recall": 0.8702681660652161,
        "bert_f1": 0.8791192173957825,
        "meteor": 0.0,
        "sbert_similarity": 0.2276880145072937
      }
    },
    {
      "sample_id": 2,
      "question": "What are Maria's dogs' names?",
      "prediction": "Coco",
      "reference": "Coco, Shadow",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9664332866668701,
        "bert_recall": 0.9146385192871094,
        "bert_f1": 0.9398228526115417,
        "meteor": 0.0,
        "sbert_similarity": 0.7857682704925537
      }
    },
    {
      "sample_id": 2,
      "question": "When did Maria adopt Shadow?",
      "prediction": "6:20 pm on 3 August, 2023",
      "reference": "The week before 13 August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8915822505950928,
        "bert_recall": 0.9106395840644836,
        "bert_f1": 0.9010102152824402,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.7057260274887085
      }
    },
    {
      "sample_id": 2,
      "question": "How many dogs has Maria adopted from the dog shelter she volunteers at?",
      "prediction": "1",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9115664958953857,
        "bert_recall": 0.9115664958953857,
        "bert_f1": 0.9115664958953857,
        "meteor": 0.0,
        "sbert_similarity": 0.6732218861579895
      }
    },
    {
      "sample_id": 2,
      "question": "How many weeks passed between Maria adopting Coco and Shadow?",
      "prediction": ":[6]",
      "reference": "two weeks",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.867264986038208,
        "bert_recall": 0.8183783292770386,
        "bert_f1": 0.8421127796173096,
        "meteor": 0.0,
        "sbert_similarity": 0.09329984337091446
      }
    },
    {
      "sample_id": 2,
      "question": "What job might Maria pursue in the future?",
      "prediction": "volunteering",
      "reference": "Shelter coordinator, Counselor",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8666653037071228,
        "bert_recall": 0.8334094882011414,
        "bert_f1": 0.849712073802948,
        "meteor": 0.0,
        "sbert_similarity": 0.4694884717464447
      }
    },
    {
      "sample_id": 2,
      "question": "What is John's main focus in local politics?",
      "prediction": "improving education and infrastructure",
      "reference": "Improving education and infrastructure",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9853813052177429,
        "bert_recall": 0.9853813052177429,
        "bert_f1": 0.9853813052177429,
        "meteor": 0.9921875,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 2,
      "question": "What sparked John's interest in improving education and infrastructure in the community?",
      "prediction": "Growing up, I saw how lack of education and crumbling infrastructure affected my neighborhood.",
      "reference": "Seeing how lack of education and crumbling infrastructure affected his neighborhood while growing up.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7857142857142857,
        "rouge1_f": 0.7857142857142857,
        "rouge2_f": 0.6153846153846154,
        "rougeL_f": 0.6428571428571429,
        "bleu1": 0.75,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.5587884978647216,
        "bleu4": 0.5067309892897293,
        "bert_precision": 0.9731835126876831,
        "bert_recall": 0.9706426858901978,
        "bert_f1": 0.9719114303588867,
        "meteor": 0.7114285714285714,
        "sbert_similarity": 0.8214858174324036
      }
    },
    {
      "sample_id": 2,
      "question": "How did the extra funding help the school shown in the photo shared by John?",
      "prediction": "The extra funding led to improvements and renovations, enhancing the facilities and providing better resources.",
      "reference": "Enabled needed repairs and renovations, making the learning environment safer and more modern for students.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.07142857142857142,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.35294117647058826,
        "bleu2": 0.21004201260420147,
        "bleu3": 0.14608713009398863,
        "bleu4": 0.06770149544242768,
        "bert_precision": 0.9140611886978149,
        "bert_recall": 0.9077308177947998,
        "bert_f1": 0.9108849763870239,
        "meteor": 0.21041666666666667,
        "sbert_similarity": 0.5466601848602295
      }
    },
    {
      "sample_id": 2,
      "question": "What type of workout class did Maria start doing in December 2023?",
      "prediction": "kickboxing",
      "reference": "aerial yoga",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9036062955856323,
        "bert_recall": 0.8612885475158691,
        "bert_f1": 0.8819401264190674,
        "meteor": 0.0,
        "sbert_similarity": 0.2939192056655884
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria donate to a homeless shelter in December 2023?",
      "prediction": "Maria donated a medal",
      "reference": "old car",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8668968677520752,
        "bert_recall": 0.8746364116668701,
        "bert_f1": 0.8707494139671326,
        "meteor": 0.0,
        "sbert_similarity": 0.10310524702072144
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of meal did John and his family make together in the photo shared by John?",
      "prediction": "Pizza",
      "reference": "pizza",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9709845781326294,
        "bert_recall": 0.9709845781326294,
        "bert_f1": 0.9709845781326294,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of online group did John join?",
      "prediction": "virtual support group",
      "reference": "service-focused online group",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9059920310974121,
        "bert_recall": 0.9025075435638428,
        "bert_f1": 0.9042463898658752,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.5550200343132019
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of activities did John and his mates from the online group do as part of their service efforts?",
      "prediction": "playing games, watching movies",
      "reference": "gave out food and supplies at a homeless shelter, organized a toy drive for kids in need",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.014854715642866776,
        "bleu2": 0.0052519350818344916,
        "bleu3": 0.00420769676829611,
        "bleu4": 0.003990607693903362,
        "bert_precision": 0.8693311214447021,
        "bert_recall": 0.8404187560081482,
        "bert_f1": 0.8546304702758789,
        "meteor": 0.03184713375796178,
        "sbert_similarity": 0.19719785451889038
      }
    },
    {
      "sample_id": 2,
      "question": "Who inspired Maria to start volunteering?",
      "prediction": "John",
      "reference": "Her aunt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9170724749565125,
        "bert_recall": 0.8973044157028198,
        "bert_f1": 0.9070808291435242,
        "meteor": 0.0,
        "sbert_similarity": 0.2088337540626526
      }
    },
    {
      "sample_id": 2,
      "question": "Why did Maria sit with the little girl at the shelter event in February 2023?",
      "prediction": "Because she was there to volunteer and make a difference.",
      "reference": "The girl seemed sad and had no other family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8885091543197632,
        "bert_recall": 0.8679796457290649,
        "bert_f1": 0.8781244158744812,
        "meteor": 0.05494505494505494,
        "sbert_similarity": 0.35496431589126587
      }
    },
    {
      "sample_id": 2,
      "question": "What did Jean go through before meeting Maria?",
      "prediction": "Jean had been through a lot.",
      "reference": "divorce, job loss, homelessness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8668482303619385,
        "bert_recall": 0.8496441841125488,
        "bert_f1": 0.8581599593162537,
        "meteor": 0.0,
        "sbert_similarity": 0.08568364381790161
      }
    },
    {
      "sample_id": 2,
      "question": "Why did John decide to run for office again?",
      "prediction": "John decided to run for office again because he was inspired by his previous unsuccessful attempts and believes it is fulfilling to make a difference in his community.",
      "reference": "saw the impact he could make in the community through politics",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2051282051282051,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2051282051282051,
        "bleu1": 0.13793103448275862,
        "bleu2": 0.022194838080923762,
        "bleu3": 0.012769569769300592,
        "bleu4": 0.009152541620698935,
        "bert_precision": 0.8810367584228516,
        "bert_recall": 0.9062838554382324,
        "bert_f1": 0.893481969833374,
        "meteor": 0.11811023622047244,
        "sbert_similarity": 0.46750956773757935
      }
    },
    {
      "sample_id": 2,
      "question": "What activity did John's colleague, Rob, invite him to?",
      "prediction": "John's colleague invited him to participate in a meeting to improve their community.",
      "reference": "beginner's yoga class",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.11111111111111112,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11111111111111112,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8754693269729614,
        "bert_recall": 0.8491024971008301,
        "bert_f1": 0.8620843887329102,
        "meteor": 0.0,
        "sbert_similarity": 0.03911392390727997
      }
    },
    {
      "sample_id": 2,
      "question": "What is the name of John's one-year-old child?",
      "prediction": "Kyle",
      "reference": "Kyle",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "How often does John take his kids to the park?",
      "prediction": "a few times a week",
      "reference": "A few times a week",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9819860458374023,
        "bert_recall": 0.9819860458374023,
        "bert_f1": 0.9819860458374023,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria make for her home to remind her of a trip to England?",
      "prediction": "a painting",
      "reference": "painting of a castle on a hill",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.0820849986238988,
        "bleu2": 0.025957555738330737,
        "bleu3": 0.017958240989131766,
        "bleu4": 0.014597006292595256,
        "bert_precision": 0.9059671759605408,
        "bert_recall": 0.882171630859375,
        "bert_f1": 0.8939110636711121,
        "meteor": 0.15384615384615383,
        "sbert_similarity": 0.5379054546356201
      }
    },
    {
      "sample_id": 2,
      "question": "Where did Maria get the idea for the castle shadow box in her home?",
      "prediction": "England",
      "reference": "England",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What did John receive a certificate for?",
      "prediction": "promotion",
      "reference": "completion of a university degree",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8806921243667603,
        "bert_recall": 0.8520712852478027,
        "bert_f1": 0.8661453127861023,
        "meteor": 0.0,
        "sbert_similarity": 0.14061017334461212
      }
    },
    {
      "sample_id": 2,
      "question": "What areas is John particularly interested in for policymaking?",
      "prediction": "education reform and infrastructure development",
      "reference": "education and infrastructure",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6,
        "bleu2": 0.3872983346207417,
        "bleu3": 0.17404441896107775,
        "bleu4": 0.12574334296829354,
        "bert_precision": 0.9344711899757385,
        "bert_recall": 0.9742155075073242,
        "bert_f1": 0.9539295434951782,
        "meteor": 0.7986111111111112,
        "sbert_similarity": 0.8504996299743652
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria participate in last weekend before April 10, 2023?",
      "prediction": "Maria sought resources and organizations to help with community issues",
      "reference": "a 5K charity run",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.847605288028717,
        "bert_recall": 0.8561315536499023,
        "bert_f1": 0.8518471121788025,
        "meteor": 0.0,
        "sbert_similarity": 0.14995822310447693
      }
    },
    {
      "sample_id": 2,
      "question": "What event did John volunteer at last weekend?",
      "prediction": "last weekend",
      "reference": "career fair at a local school",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.869997501373291,
        "bert_recall": 0.8504289388656616,
        "bert_f1": 0.8601018786430359,
        "meteor": 0.0,
        "sbert_similarity": 0.008903797715902328
      }
    },
    {
      "sample_id": 2,
      "question": "What did John do that put a strain on his wallet?",
      "prediction": "John went to help Maria's kids.",
      "reference": "His car broke down",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8904948830604553,
        "bert_recall": 0.8886713981628418,
        "bert_f1": 0.8895822167396545,
        "meteor": 0.0,
        "sbert_similarity": 0.23707985877990723
      }
    },
    {
      "sample_id": 2,
      "question": "Where did John explore on a road trip last year?",
      "prediction": "Oregon",
      "reference": "Pacific Northwest",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9094816446304321,
        "bert_recall": 0.8966206908226013,
        "bert_f1": 0.9030054211616516,
        "meteor": 0.0,
        "sbert_similarity": 0.6510610580444336
      }
    },
    {
      "sample_id": 2,
      "question": "What topic has John been blogging about recently?",
      "prediction": "politics",
      "reference": "politics and the government",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9661926627159119,
        "bert_recall": 0.9115769267082214,
        "bert_f1": 0.9380905628204346,
        "meteor": 0.13513513513513511,
        "sbert_similarity": 0.7785417437553406
      }
    },
    {
      "sample_id": 2,
      "question": "Why did John start blogging about politics and policies?",
      "prediction": "John started blogging about politics and policies because he wanted to contribute to making a real impact on his community.",
      "reference": "raise awareness and start conversations to create positive change",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.20689655172413793,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13793103448275865,
        "bleu1": 0.09523809523809525,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.0141800886003256,
        "bleu4": 0.010862721615727716,
        "bert_precision": 0.8785865306854248,
        "bert_recall": 0.872067928314209,
        "bert_f1": 0.8753150701522827,
        "meteor": 0.1485148514851485,
        "sbert_similarity": 0.27891552448272705
      }
    },
    {
      "sample_id": 2,
      "question": "What was the focus of John's recent research and writing on his blog?",
      "prediction": "politics and government",
      "reference": "education reform and infrastructure development",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9075474739074707,
        "bert_recall": 0.8814287185668945,
        "bert_f1": 0.8942974209785461,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.32259052991867065
      }
    },
    {
      "sample_id": 2,
      "question": "What did John attend with his colleagues in March 2023?",
      "prediction": "Community meeting",
      "reference": "a tech-for-good convention",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9020812511444092,
        "bert_recall": 0.869875967502594,
        "bert_f1": 0.8856859803199768,
        "meteor": 0.0,
        "sbert_similarity": 0.2719048857688904
      }
    },
    {
      "sample_id": 2,
      "question": "How often does John work out with his family?",
      "prediction": "We've had lots more energy and noticed some gains in strength and endurance.",
      "reference": "Three times a week",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8554925918579102,
        "bert_recall": 0.8941341638565063,
        "bert_f1": 0.874386727809906,
        "meteor": 0.0,
        "sbert_similarity": 0.1256333589553833
      }
    },
    {
      "sample_id": 2,
      "question": "How has John's fitness improved since starting boot camps with his family?",
      "prediction": "John has noticed gains in strength and endurance, which is motivating as they support each other during workouts.",
      "reference": "More energy, gains in strength and endurance",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.3478260869565218,
        "rougeL_f": 0.4,
        "bleu1": 0.3,
        "bleu2": 0.25131234497501725,
        "bleu3": 0.22251085896532885,
        "bleu4": 0.18759202316167214,
        "bert_precision": 0.8824732303619385,
        "bert_recall": 0.938413143157959,
        "bert_f1": 0.9095839262008667,
        "meteor": 0.48996913580246915,
        "sbert_similarity": 0.5924222469329834
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of food did Maria have on her dinner spread iwth her mother?",
      "prediction": "banana split ice cream sundae",
      "reference": "Salads, sandwiches, homemade desserts",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8302475214004517,
        "bert_recall": 0.8524365425109863,
        "bert_f1": 0.8411957621574402,
        "meteor": 0.0,
        "sbert_similarity": 0.40015068650245667
      }
    },
    {
      "sample_id": 2,
      "question": "What activity did Maria and her mom do together in May 2023?",
      "prediction": "playing games",
      "reference": "Made dinner together",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8906488418579102,
        "bert_recall": 0.8749358654022217,
        "bert_f1": 0.8827224373817444,
        "meteor": 0.0,
        "sbert_similarity": 0.1875423640012741
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria do to feel closer to a community and her faith?",
      "prediction": "Maria joined a nearby church.",
      "reference": "joined a nearby church",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.888888888888889,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.8571428571428571,
        "rougeL_f": 0.888888888888889,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.5879493283318652,
        "bleu4": 0.5081327481546147,
        "bert_precision": 0.9367097020149231,
        "bert_recall": 0.9303988814353943,
        "bert_f1": 0.9335436224937439,
        "meteor": 0.7181571815718157,
        "sbert_similarity": 0.6684672832489014
      }
    },
    {
      "sample_id": 2,
      "question": "Why did Maria join a nearby church recently?",
      "prediction": "Maria joined a nearby church to feel closer to a community and strengthen her religious beliefs.",
      "reference": "to feel closer to a community and her faith",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6363636363636364,
        "rouge1_f": 0.64,
        "rouge2_f": 0.5217391304347827,
        "rougeL_f": 0.64,
        "bleu1": 0.47058823529411764,
        "bleu2": 0.42008402520840293,
        "bleu3": 0.392601410850376,
        "bleu4": 0.360056585428503,
        "bert_precision": 0.9288991689682007,
        "bert_recall": 0.9511736035346985,
        "bert_f1": 0.9399044513702393,
        "meteor": 0.8182989690721649,
        "sbert_similarity": 0.6986143589019775
      }
    },
    {
      "sample_id": 2,
      "question": "What did John host for the veterans in May 2023 as part of the project?",
      "prediction": "a photography of a man shaking hands with a soldier",
      "reference": "a small party to share their stories",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8930029273033142,
        "bert_recall": 0.8746085166931152,
        "bert_f1": 0.8837099671363831,
        "meteor": 0.0684931506849315,
        "sbert_similarity": 0.14214277267456055
      }
    },
    {
      "sample_id": 2,
      "question": "What did John and the veterans do during the small party?",
      "prediction": "John and the veterans had a great time throwing a small party and inviting some veterans to share their stories.",
      "reference": "share stories and make connections",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2727272727272727,
        "rouge1_f": 0.24,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16000000000000003,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.026726124191242442,
        "bleu3": 0.016210225632977325,
        "bleu4": 0.012021577610863726,
        "bert_precision": 0.8726940155029297,
        "bert_recall": 0.9027185440063477,
        "bert_f1": 0.8874524235725403,
        "meteor": 0.23076923076923075,
        "sbert_similarity": 0.3420419991016388
      }
    },
    {
      "sample_id": 2,
      "question": "What emotions did John feel during the small party with the veterans?",
      "prediction": "John felt heartwarming and appreciative.",
      "reference": "heartwarming",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8773180246353149,
        "bert_recall": 0.8906521797180176,
        "bert_f1": 0.8839347958564758,
        "meteor": 0.35714285714285715,
        "sbert_similarity": 0.5112243294715881
      }
    },
    {
      "sample_id": 2,
      "question": "What event is Maria getting ready for at the shelter on May 25, 2023?",
      "prediction": "Maria is planning to volunteer at the homeless shelter on May 25, 2023.",
      "reference": "fundraiser",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8306795358657837,
        "bert_recall": 0.8305791616439819,
        "bert_f1": 0.8306293487548828,
        "meteor": 0.0,
        "sbert_similarity": 0.19625422358512878
      }
    },
    {
      "sample_id": 2,
      "question": "What does Maria need to spread the word about for the fundraiser for the volunteer shelter?",
      "prediction": "Maria needs to reach out to businesses and make it a hit.",
      "reference": "chili cook-off",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.862298846244812,
        "bert_recall": 0.8724625706672668,
        "bert_f1": 0.8673508763313293,
        "meteor": 0.0,
        "sbert_similarity": 0.21669992804527283
      }
    },
    {
      "sample_id": 2,
      "question": "What was the name of the pet that John had to say goodbye to on 3 June, 2023?",
      "prediction": "Max",
      "reference": "Max",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "How long was Max a part of John's family?",
      "prediction": "10 years",
      "reference": "10 years",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "How does John plan to honor the memories of his beloved pet?",
      "prediction": "John plans to make sure their loved one's memory is celebrated and honored through continued dedication, supporting similar causes, and sharing the story with others.",
      "reference": "By considering adopting a rescue dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.826993465423584,
        "bert_recall": 0.8620040416717529,
        "bert_f1": 0.8441359400749207,
        "meteor": 0.0,
        "sbert_similarity": 0.1265774518251419
      }
    },
    {
      "sample_id": 2,
      "question": "What important values does John want to teach his kids through adopting a rescue dog?",
      "prediction": "responsibility and compassion",
      "reference": "Responsibility and compassion",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9513729214668274,
        "bert_recall": 0.9387184381484985,
        "bert_f1": 0.9450032711029053,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What new activity did Maria start recently, as mentioned on 3 June, 2023?",
      "prediction": "aerial yoga",
      "reference": "volunteering at a local dog shelter once a month",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8385441899299622,
        "bert_recall": 0.8345639109611511,
        "bert_f1": 0.8365493416786194,
        "meteor": 0.0,
        "sbert_similarity": 0.059487782418727875
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria say it was like being at the waterfall in Oregon?",
      "prediction": "It was awesome.",
      "reference": "Like being in a fairy tale",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9081165194511414,
        "bert_recall": 0.871068000793457,
        "bert_f1": 0.8892064690589905,
        "meteor": 0.0,
        "sbert_similarity": 0.09856733679771423
      }
    },
    {
      "sample_id": 2,
      "question": "What does Maria say she feels when doing upside-down yoga poses?",
      "prediction": "Maria says they make her feel free and light.",
      "reference": "Free and light",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3,
        "bleu2": 0.25819888974716104,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.10445522730720382,
        "bert_precision": 0.8831138014793396,
        "bert_recall": 0.9219617247581482,
        "bert_f1": 0.9021197557449341,
        "meteor": 0.5208333333333335,
        "sbert_similarity": 0.4554651975631714
      }
    },
    {
      "sample_id": 2,
      "question": "What exciting news did Maria share on 16 June, 2023?",
      "prediction": "Maria shared that she was invited to speak at an upcoming local charity event.",
      "reference": "joined a gym",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8345592021942139,
        "bert_recall": 0.8363245725631714,
        "bert_f1": 0.8354409337043762,
        "meteor": 0.0,
        "sbert_similarity": 0.13257448375225067
      }
    },
    {
      "sample_id": 2,
      "question": "What yoga activity has Maria been trying to improve her strength and endurance?",
      "prediction": "kundalini yoga",
      "reference": "kundalini yoga",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "What did John recently get promoted to?",
      "prediction": "assistant manager",
      "reference": "assistant manager",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 2,
      "question": "What was one of the biggest challenges John faced in his journey to becoming assistant manager?",
      "prediction": "John faced self-doubt and fear of making mistakes.",
      "reference": "self-doubt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8942818641662598,
        "bert_recall": 0.9502835273742676,
        "bert_f1": 0.921432614326477,
        "meteor": 0.2941176470588236,
        "sbert_similarity": 0.5093756318092346
      }
    },
    {
      "sample_id": 2,
      "question": "How does John describe the support he received during his journey to becoming assistant manager?",
      "prediction": "John received helpful feedback and mentorship.",
      "reference": "having support at home and his own grit",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.12383969996431167,
        "bleu2": 0.04229931028018368,
        "bleu3": 0.03248071373371268,
        "bleu4": 0.028634401465295497,
        "bert_precision": 0.8702219128608704,
        "bert_recall": 0.8341913223266602,
        "bert_f1": 0.8518258333206177,
        "meteor": 0.06410256410256411,
        "sbert_similarity": 0.28753405809402466
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of event did John and his family attend in June 2023?",
      "prediction": "rescue mission",
      "reference": "live music event",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8944000005722046,
        "bert_recall": 0.9007712602615356,
        "bert_f1": 0.8975743055343628,
        "meteor": 0.0,
        "sbert_similarity": 0.166223406791687
      }
    },
    {
      "sample_id": 2,
      "question": "Why did Maria need to help her cousin find a new place to live?",
      "prediction": "Maria needed to help her cousin find a new place to live.",
      "reference": "Her cousin had to leave and find a new place in a hurry.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6086956521739131,
        "rouge1_f": 0.5599999999999999,
        "rouge2_f": 0.34782608695652173,
        "rougeL_f": 0.48000000000000004,
        "bleu1": 0.5698222022414253,
        "bleu2": 0.41937778554338095,
        "bleu3": 0.3127817606885761,
        "bleu4": 0.22882759955089269,
        "bert_precision": 0.9465579986572266,
        "bert_recall": 0.9429432153701782,
        "bert_f1": 0.9447471499443054,
        "meteor": 0.44375889890840053,
        "sbert_similarity": 0.6728503704071045
      }
    },
    {
      "sample_id": 2,
      "question": "What event did John participate in to show support for veterans' rights?",
      "prediction": "John participated in a marching event for veterans' rights.",
      "reference": "marching event",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.13483997249264845,
        "bleu3": 0.06036431606009924,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.8901114463806152,
        "bert_recall": 0.906390905380249,
        "bert_f1": 0.898177444934845,
        "meteor": 0.6944444444444445,
        "sbert_similarity": 0.6013822555541992
      }
    },
    {
      "sample_id": 2,
      "question": "What inspired John to join the marching event for veterans' rights?",
      "prediction": "It was awesome",
      "reference": "Respect for the military and the desire to show support",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8856530785560608,
        "bert_recall": 0.8697518110275269,
        "bert_f1": 0.8776304125785828,
        "meteor": 0.0,
        "sbert_similarity": 0.0419897697865963
      }
    },
    {
      "sample_id": 2,
      "question": "How often does John get to see sunsets like the one he shared with Maria?",
      "prediction": "How often do you get to see sunsets like that on your walks?",
      "reference": "At least once a week",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8245751857757568,
        "bert_recall": 0.8700623512268066,
        "bert_f1": 0.8467082977294922,
        "meteor": 0.0,
        "sbert_similarity": 0.4290027618408203
      }
    },
    {
      "sample_id": 2,
      "question": "What natural disaster affected John's old area on 7 July, 2023?",
      "prediction": "A flood",
      "reference": "Flood",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9301746487617493,
        "bert_recall": 0.9301746487617493,
        "bert_f1": 0.9301746487617493,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9345195293426514
      }
    },
    {
      "sample_id": 2,
      "question": "How did the flood impact the homes in John's old area?",
      "prediction": "Many homes were ruined.",
      "reference": "Lots of homes were ruined.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.6666666666666665,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666665,
        "bleu1": 0.6549846024623855,
        "bleu2": 0.6341861143397762,
        "bleu3": 0.6050912889434976,
        "bleu4": 0.5475182535069453,
        "bert_precision": 0.9852992296218872,
        "bert_recall": 0.9833086133003235,
        "bert_f1": 0.9843028783798218,
        "meteor": 0.600907029478458,
        "sbert_similarity": 0.9844668507575989
      }
    },
    {
      "sample_id": 2,
      "question": "What motivated Maria and John to discuss potential solutions for their community on 7 July, 2023?",
      "prediction": "The events occurred as part of ongoing discussions between Maria and John aimed at improving infrastructure and education within their community.",
      "reference": "Flood in John's old area",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.07407407407407407,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407407,
        "bleu1": 0.04545454545454546,
        "bleu2": 0.014712247158412498,
        "bleu3": 0.010748014644407517,
        "bleu4": 0.008687475782716616,
        "bert_precision": 0.8568977117538452,
        "bert_recall": 0.8755750060081482,
        "bert_f1": 0.8661357164382935,
        "meteor": 0.0,
        "sbert_similarity": 0.3898124694824219
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria plan to do later on the evening of 7 July, 2023?",
      "prediction": "Maria planned to have dinner with some friends from the gym.",
      "reference": "have dinner with friends from the gym",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7777777777777778,
        "rouge1_f": 0.7777777777777778,
        "rouge2_f": 0.625,
        "rougeL_f": 0.7777777777777778,
        "bleu1": 0.5833333333333334,
        "bleu2": 0.5149286505444373,
        "bleu3": 0.4337136307768789,
        "bleu4": 0.30661487102926754,
        "bert_precision": 0.942131519317627,
        "bert_recall": 0.9663344025611877,
        "bert_f1": 0.9540795087814331,
        "meteor": 0.7957957957957957,
        "sbert_similarity": 0.6340412497520447
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of activities did Maria do at the picnic with her church friends?",
      "prediction": "Maria had a picnic with her church friends under trees, played games, and enjoyed delicious food.",
      "reference": "played games like charades and a scavenger hunt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.09090909090909091,
        "rougeL_f": 0.25,
        "bleu1": 0.21052631578947367,
        "bleu2": 0.108147614087175,
        "bleu3": 0.04230610452681119,
        "bleu4": 0.02560744480557418,
        "bert_precision": 0.8637311458587646,
        "bert_recall": 0.871101975440979,
        "bert_f1": 0.8674008846282959,
        "meteor": 0.17045454545454547,
        "sbert_similarity": 0.3486473560333252
      }
    },
    {
      "sample_id": 2,
      "question": "What does John appreciate about the veteran's hospital visit?",
      "prediction": "John appreciates the veterans' resilience and the impact it has on his perspective.",
      "reference": "the resilience of the veterans and their inspiring stories",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.4545454545454546,
        "rouge2_f": 0.1,
        "rougeL_f": 0.27272727272727276,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.15430334996209186,
        "bleu3": 0.05844218274062023,
        "bleu4": 0.03514847831369806,
        "bert_precision": 0.8960085511207581,
        "bert_recall": 0.9163264036178589,
        "bert_f1": 0.9060535430908203,
        "meteor": 0.2127659574468085,
        "sbert_similarity": 0.7214565277099609
      }
    },
    {
      "sample_id": 2,
      "question": "What did John take away from visiting the veteran's hospital?",
      "prediction": "Appreciation for what we have and the need to give back.",
      "reference": "appreciation for giving back",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.39999999999999997,
        "rouge1_f": 0.5333333333333333,
        "rouge2_f": 0.30769230769230765,
        "rougeL_f": 0.5333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.1507556722888818,
        "bleu3": 0.06275677488087485,
        "bleu4": 0.03986357128268015,
        "bert_precision": 0.9116089344024658,
        "bert_recall": 0.9396238923072815,
        "bert_f1": 0.92540442943573,
        "meteor": 0.5437352245862884,
        "sbert_similarity": 0.8055969476699829
      }
    },
    {
      "sample_id": 2,
      "question": "Why did John feel inspired to join the military after the visit to the hospital?",
      "prediction": "John felt inspired by the veterans' stories and experiences, appreciating what he had and recognizing the importance of giving back.",
      "reference": "seeing the resilience of the veterans",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1739130434782609,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.08333333333333333,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.17391304347826086,
        "bleu2": 0.08891084489487741,
        "bleu3": 0.034671909414527975,
        "bleu4": 0.020828838183973034,
        "bert_precision": 0.8578774929046631,
        "bert_recall": 0.9236500263214111,
        "bert_f1": 0.8895496129989624,
        "meteor": 0.2027027027027027,
        "sbert_similarity": 0.5303962230682373
      }
    },
    {
      "sample_id": 2,
      "question": "In what activity did Maria and her church friends participate in July 2023?",
      "prediction": "community project",
      "reference": "hiking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9074125289916992,
        "bert_recall": 0.8901320099830627,
        "bert_f1": 0.8986892700195312,
        "meteor": 0.0,
        "sbert_similarity": 0.24759365618228912
      }
    },
    {
      "sample_id": 2,
      "question": "What does John think about trying new classes at the yoga studio?",
      "prediction": "John thinks it's a great way to push himself and mix things up, as he feels that trying new stuff is a great way to challenge himself.",
      "reference": "Trying new classes is a fun way to switch up the exercise routine.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.39999999999999997,
        "rouge1_f": 0.3414634146341463,
        "rouge2_f": 0.15384615384615383,
        "rougeL_f": 0.2926829268292683,
        "bleu1": 0.26666666666666666,
        "bleu2": 0.16609095970747997,
        "bleu3": 0.04762842343947981,
        "bleu4": 0.024577774110849788,
        "bert_precision": 0.9027444124221802,
        "bert_recall": 0.9209809899330139,
        "bert_f1": 0.9117715358734131,
        "meteor": 0.390625,
        "sbert_similarity": 0.3430357277393341
      }
    },
    {
      "sample_id": 2,
      "question": "Which activity has John done apart from yoga at the studio?",
      "prediction": "kickboxing",
      "reference": "weight training",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8930211067199707,
        "bert_recall": 0.8930211067199707,
        "bert_f1": 0.8930211067199707,
        "meteor": 0.0,
        "sbert_similarity": 0.29184141755104065
      }
    },
    {
      "sample_id": 2,
      "question": "What community service did Maria mention that she was involved in on 31 July, 2023?",
      "prediction": "a homeless shelter",
      "reference": "volunteered at a homeless shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.513417119032592,
        "bleu2": 0.513417119032592,
        "bleu3": 0.513417119032592,
        "bleu4": 0.288715663092199,
        "bert_precision": 0.9436331987380981,
        "bert_recall": 0.9043905138969421,
        "bert_f1": 0.9235952496528625,
        "meteor": 0.6134259259259259,
        "sbert_similarity": 0.7948549389839172
      }
    },
    {
      "sample_id": 2,
      "question": "How did Maria start volunteering at the homeless shelter?",
      "prediction": "about a year ago",
      "reference": "Witnessed a family struggling on the streets and reached out to the shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.026349806140466083,
        "bleu2": 0.009621588806014277,
        "bleu3": 0.00807918784776961,
        "bleu4": 0.008468073234773096,
        "bert_precision": 0.8597404360771179,
        "bert_recall": 0.8632281422615051,
        "bert_f1": 0.8614807724952698,
        "meteor": 0.04132231404958678,
        "sbert_similarity": 0.1227295845746994
      }
    },
    {
      "sample_id": 2,
      "question": "What did John do the week before August 3, 2023 involving his kids?",
      "prediction": "John went to that community mtg.",
      "reference": "Had a meaningful experience at a military memorial",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8879796862602234,
        "bert_recall": 0.8814451694488525,
        "bert_f1": 0.884700357913971,
        "meteor": 0.0,
        "sbert_similarity": 0.27340584993362427
      }
    },
    {
      "sample_id": 2,
      "question": "How did John describe his kids' reaction at the military memorial?",
      "prediction": "It made me realize how crucial the upgrades are, especially for the kids.",
      "reference": "awestruck and humbled",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8634160161018372,
        "bert_recall": 0.814229428768158,
        "bert_f1": 0.8381016850471497,
        "meteor": 0.0,
        "sbert_similarity": 0.18880468606948853
      }
    },
    {
      "sample_id": 2,
      "question": "Why does Maria think it's important for younger generations to visit military memorials?",
      "prediction": "Maria thinks it's important for younger generations to visit military memorials because it allows them to remember and appreciate those who have served, helping to create a bond with veterans.",
      "reference": "To remember and appreciate those who served",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.3684210526315789,
        "rouge2_f": 0.2777777777777778,
        "rougeL_f": 0.3684210526315789,
        "bleu1": 0.2121212121212121,
        "bleu2": 0.18205477031360476,
        "bleu3": 0.16529655401496457,
        "bleu4": 0.14380553624999495,
        "bert_precision": 0.8766191005706787,
        "bert_recall": 0.9339917898178101,
        "bert_f1": 0.9043964743614197,
        "meteor": 0.6332138590203108,
        "sbert_similarity": 0.5539958477020264
      }
    },
    {
      "sample_id": 2,
      "question": "What does John believe is important for children regarding veterans?",
      "prediction": "teach kids about veterans and what they did for us.",
      "reference": "Teaching them to respect and appreciate those who served",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.2105263157894737,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2105263157894737,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.9169906973838806,
        "bert_recall": 0.9160791039466858,
        "bert_f1": 0.9165347218513489,
        "meteor": 0.10989010989010987,
        "sbert_similarity": 0.6490423679351807
      }
    },
    {
      "sample_id": 2,
      "question": "What happened to John's job in August 2023?",
      "prediction": "John lost his job at the mechanical engineering company.",
      "reference": "John lost his job at the mechanical engineering company.",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "What activity did Maria take up with her friends from church in August 2023?",
      "prediction": "Aerial yoga",
      "reference": "community work",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8510600924491882,
        "bert_recall": 0.9038099050521851,
        "bert_f1": 0.8766422271728516,
        "meteor": 0.0,
        "sbert_similarity": 0.10521769523620605
      }
    },
    {
      "sample_id": 2,
      "question": "What did John do to help his community last year in his hometown?",
      "prediction": "John helped renovate a rundown community center back home.",
      "reference": "Helped renovate a rundown community center.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.8,
        "bleu1": 0.7,
        "bleu2": 0.6236095644623235,
        "bleu3": 0.5825088500130399,
        "bleu4": 0.537284965911771,
        "bert_precision": 0.9495422840118408,
        "bert_recall": 0.9737974405288696,
        "bert_f1": 0.9615169763565063,
        "meteor": 0.7904761904761907,
        "sbert_similarity": 0.7876448035240173
      }
    },
    {
      "sample_id": 2,
      "question": "What cause did the 5K charity run organized by John support?",
      "prediction": "veterans and their families",
      "reference": "veterans and their families",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "Who did John work with to raise awareness and funds for victims of domestic abuse?",
      "prediction": "A local organization",
      "reference": "a local organization",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.978014349937439,
        "bert_recall": 0.978014349937439,
        "bert_f1": 0.978014349937439,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 2,
      "question": "What recognition did Maria receive at the homeless shelter in August 2023?",
      "prediction": "A medal",
      "reference": "a medal for volunteering",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9367085695266724,
        "bert_recall": 0.9155126214027405,
        "bert_f1": 0.9259892702102661,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.7949226498603821
      }
    },
    {
      "sample_id": 2,
      "question": "What is the name of Maria's puppy she got two weeks before August 11, 2023?",
      "prediction": "Coco",
      "reference": "Coco",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What activity did John and Max enjoy together last summer?",
      "prediction": "hiking, swimming",
      "reference": "Camping",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8850679397583008,
        "bert_recall": 0.8976119756698608,
        "bert_f1": 0.891295850276947,
        "meteor": 0.0,
        "sbert_similarity": 0.48958709836006165
      }
    },
    {
      "sample_id": 2,
      "question": "How does John describe the camping trip with Max?",
      "prediction": "awesome",
      "reference": "Peaceful and awesome",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.8988068103790283,
        "bert_recall": 0.8836047649383545,
        "bert_f1": 0.8911409974098206,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.31532901525497437
      }
    },
    {
      "sample_id": 2,
      "question": "Why does John say he feels stuck and questioning his decisions and goals?",
      "prediction": "John says he feels stuck and questioning his decisions and goals because recently, he has been struggling with feeling that his actions are not making a significant difference in this context.",
      "reference": "Not feeling like making much of an impact",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10256410256410256,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.016854996561581053,
        "bleu3": 0.010173995989186396,
        "bleu4": 0.007434360163166208,
        "bert_precision": 0.8745246529579163,
        "bert_recall": 0.8969132304191589,
        "bert_f1": 0.8855774998664856,
        "meteor": 0.14563106796116504,
        "sbert_similarity": 0.4471603035926819
      }
    },
    {
      "sample_id": 2,
      "question": "What is the name of Maria's second puppy?",
      "prediction": "Coco",
      "reference": "Shadow",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8737015724182129,
        "bert_recall": 0.9218830466270447,
        "bert_f1": 0.8971458673477173,
        "meteor": 0.0,
        "sbert_similarity": 0.2880135178565979
      }
    },
    {
      "sample_id": 2,
      "question": "How is Maria's new puppy adjusting to its new home?",
      "prediction": "Maria says:",
      "reference": "doing great - learning commands and house training",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8576231598854065,
        "bert_recall": 0.8490580916404724,
        "bert_f1": 0.8533191084861755,
        "meteor": 0.0,
        "sbert_similarity": 0.025971706956624985
      }
    },
    {
      "sample_id": 2,
      "question": "What is John currently doing as a volunteer in August 2023?",
      "prediction": "John is volunteering at a homeless shelter.",
      "reference": "mentoring students at a local school",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.9157457947731018,
        "bert_recall": 0.890349805355072,
        "bert_f1": 0.9028692245483398,
        "meteor": 0.3073770491803279,
        "sbert_similarity": 0.21507583558559418
      }
    },
    {
      "sample_id": 2,
      "question": "What activities does John's family enjoy doing together?",
      "prediction": "playing games",
      "reference": "going for hikes, hanging out at the park, having picnics, playing board games, having movie nights",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.00012340980408667956,
        "bleu2": 3.902560665090632e-05,
        "bleu3": 2.6999123339997023e-05,
        "bleu4": 2.1945711360427958e-05,
        "bert_precision": 0.907486081123352,
        "bert_recall": 0.8202652931213379,
        "bert_f1": 0.8616741299629211,
        "meteor": 0.03424657534246575,
        "sbert_similarity": 0.3750291168689728
      }
    },
    {
      "sample_id": 2,
      "question": "What did the donations help John's community acquire on 16 August, 2023?",
      "prediction": "funding",
      "reference": "a brand new fire truck",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8923342227935791,
        "bert_recall": 0.8307420015335083,
        "bert_f1": 0.860437273979187,
        "meteor": 0.0,
        "sbert_similarity": 0.08676693588495255
      }
    },
    {
      "sample_id": 2,
      "question": "What is John's main focus in international politics?",
      "prediction": "Improving education and infrastructure",
      "reference": "Improving education and infrastructure",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria donate to a luxury store in December 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "old car",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8866382837295532,
        "bert_recall": 0.8874809145927429,
        "bert_f1": 0.8870593905448914,
        "meteor": 0.0,
        "sbert_similarity": 0.08534561097621918
      }
    },
    {
      "sample_id": 2,
      "question": "Who inspired John to start volunteering?",
      "prediction": "John's passion and dedication",
      "reference": "His aunt",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8937821388244629,
        "bert_recall": 0.9062950611114502,
        "bert_f1": 0.8999950885772705,
        "meteor": 0.0,
        "sbert_similarity": 0.21978545188903809
      }
    },
    {
      "sample_id": 2,
      "question": "Why did Maria decide to run for office again?",
      "prediction": "Not mentioned in the conversation or saw the impact she could make in the community through politics",
      "reference": "saw the impact she could make in the community through politics",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.6470588235294118,
        "bleu2": 0.6359337738364604,
        "bleu3": 0.626674120796503,
        "bleu4": 0.6102169202557912,
        "bert_precision": 0.9263800382614136,
        "bert_recall": 0.9611027240753174,
        "bert_f1": 0.9434220194816589,
        "meteor": 0.9479196352237105,
        "sbert_similarity": 0.8661280870437622
      }
    },
    {
      "sample_id": 2,
      "question": "What activity did Maria's colleague, Rob, invite her to?",
      "prediction": "beginner's yoga class",
      "reference": "beginner's yoga class",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What is the name of Maria's one-year-old child?",
      "prediction": "Kyle",
      "reference": "Kyle",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "How often does John take his kids to the library?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A few times a week",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.902847170829773,
        "bert_recall": 0.9081524610519409,
        "bert_f1": 0.9054920077323914,
        "meteor": 0.0,
        "sbert_similarity": 0.12815096974372864
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria make for her home to remind her of a trip to France?",
      "prediction": "A painting of a castle on a hill",
      "reference": "painting of a castle on a hill",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 0.9333333333333333,
        "rouge2_f": 0.923076923076923,
        "rougeL_f": 0.9333333333333333,
        "bleu1": 0.875,
        "bleu2": 0.8660254037844386,
        "bleu3": 0.8563285146644029,
        "bleu4": 0.8408964152537145,
        "bert_precision": 0.0,
        "bert_recall": 0.0,
        "bert_f1": 0.0,
        "meteor": 0.9844782983615982,
        "sbert_similarity": 0.9855074286460876
      }
    },
    {
      "sample_id": 2,
      "question": "Where did John get the idea for the castle shadow box in his home?",
      "prediction": "England",
      "reference": "England",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria receive a certificate for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "completion of a university degree",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8848448991775513,
        "bert_recall": 0.8603081703186035,
        "bert_f1": 0.8724040389060974,
        "meteor": 0.0,
        "sbert_similarity": -0.002504263073205948
      }
    },
    {
      "sample_id": 2,
      "question": "What areas is John particularly interested in for art appreciation?",
      "prediction": "education and infrastructure",
      "reference": "education and infrastructure",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "Why did Maria start blogging about politics and policies?",
      "prediction": "raise awareness and start conversations to create positive change",
      "reference": "raise awareness and start conversations to create positive change",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What was the focus of John's recent travel and photography blog?",
      "prediction": "travel and photography",
      "reference": "education reform and infrastructure development",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9043297171592712,
        "bert_recall": 0.8689243793487549,
        "bert_f1": 0.8862735629081726,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.10616106539964676
      }
    },
    {
      "sample_id": 2,
      "question": "How often does Maria work out with her family?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Three times a week",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8983799815177917,
        "bert_recall": 0.9112342596054077,
        "bert_f1": 0.9047614932060242,
        "meteor": 0.0,
        "sbert_similarity": 0.07513657212257385
      }
    },
    {
      "sample_id": 2,
      "question": "How has John's artistic skills improved since starting boot camps with his family?",
      "prediction": "More energy, gains in strength and endurance",
      "reference": "More energy, gains in strength and endurance",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What kind of food did Maria have on her dinner spread with her father?",
      "prediction": "homemade desserts",
      "reference": "Salads, sandwiches, homemade desserts",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.9406634569168091,
        "bert_recall": 0.8983628749847412,
        "bert_f1": 0.9190266728401184,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.720646858215332
      }
    },
    {
      "sample_id": 2,
      "question": "What did John do to feel closer to a community and his faith?",
      "prediction": "Joined a nearby church",
      "reference": "joined a nearby church",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9593095183372498,
        "bert_recall": 0.9505109786987305,
        "bert_f1": 0.9548899531364441,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "Why did John join a nearby church recently?",
      "prediction": "to feel closer to a community and her faith",
      "reference": "to feel closer to a community and her faith",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "How long was Max a part of Maria's family?",
      "prediction": "10 years",
      "reference": "10 years",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "How does Maria plan to honor the memories of her beloved pet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "By considering adopting a rescue dog",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.885809063911438,
        "bert_recall": 0.867752194404602,
        "bert_f1": 0.8766876459121704,
        "meteor": 0.0,
        "sbert_similarity": 0.0012524807825684547
      }
    },
    {
      "sample_id": 2,
      "question": "What important values does Maria want to teach her kids through adopting a rescue dog?",
      "prediction": "Responsibility and compassion",
      "reference": "Responsibility and compassion",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria say it was like being at the desert in Oregon?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Like being in a fairy tale",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.1637461506155964,
        "bleu2": 0.05789300674674099,
        "bleu3": 0.046382183633183686,
        "bleu4": 0.0439891724758422,
        "bert_precision": 0.8897460699081421,
        "bert_recall": 0.8714282512664795,
        "bert_f1": 0.8804919123649597,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.1141384020447731
      }
    },
    {
      "sample_id": 2,
      "question": "What does John say she feels when doing upside-down yoga poses?",
      "prediction": "Free and light",
      "reference": "Free and light",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What did Maria recently get promoted to?",
      "prediction": "promotion",
      "reference": "assistant manager",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8819196820259094,
        "bert_recall": 0.8508885502815247,
        "bert_f1": 0.866126298904419,
        "meteor": 0.0,
        "sbert_similarity": 0.36402401328086853
      }
    },
    {
      "sample_id": 2,
      "question": "What was one of the biggest challenges Maria faced in her journey to becoming assistant manager?",
      "prediction": "Not mentioned in the conversation or self-doubt",
      "reference": "self-doubt",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8933588862419128,
        "bert_recall": 0.9551422595977783,
        "bert_f1": 0.9232180714607239,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.6504025459289551
      }
    },
    {
      "sample_id": 2,
      "question": "Why did John need to help his cousin find a new place to live?",
      "prediction": "His cousin had to leave and find a new place in a hurry.",
      "reference": "His cousin had to leave and find a new place in a hurry.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997724169321802,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "What event did Maria participate in to show support for veterans' rights?",
      "prediction": "Not mentioned in the conversation or marching event",
      "reference": "marching event",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.8766100406646729,
        "bert_recall": 0.90091872215271,
        "bert_f1": 0.8885981440544128,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.6670181751251221
      }
    },
    {
      "sample_id": 2,
      "question": "How did the drought impact the homes in John's old area?",
      "prediction": "Lots of homes were ruined.",
      "reference": "Lots of homes were ruined.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "What does John criticize about the veteran's hospital visit?",
      "prediction": "Not mentioned in the conversation",
      "reference": "the resilience of the veterans and their inspiring stories",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.08986579282344431,
        "bleu2": 0.03177235575108143,
        "bleu3": 0.02545508208534111,
        "bleu4": 0.02414176971688927,
        "bert_precision": 0.8899037837982178,
        "bert_recall": 0.8688869476318359,
        "bert_f1": 0.8792697787284851,
        "meteor": 0.05813953488372093,
        "sbert_similarity": -0.023380666971206665
      }
    },
    {
      "sample_id": 2,
      "question": "What did John take away from visiting the orphanage?",
      "prediction": "Not mentioned in the conversation or appreciation for giving back",
      "reference": "appreciation for giving back",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.5,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.9041359424591064,
        "bert_recall": 0.9512563943862915,
        "bert_f1": 0.9270978569984436,
        "meteor": 0.8627717391304348,
        "sbert_similarity": 0.7283632159233093
      }
    },
    {
      "sample_id": 2,
      "question": "Why did Maria feel inspired to join the military after the visit to the hospital?",
      "prediction": "seeing the resilience of the veterans",
      "reference": "seeing the resilience of the veterans",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "How did Maria describe her kids' reaction at the military memorial?",
      "prediction": "Not mentioned in the conversation",
      "reference": "awestruck and humbled",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8783921599388123,
        "bert_recall": 0.8119669556617737,
        "bert_f1": 0.843874454498291,
        "meteor": 0.0,
        "sbert_similarity": 0.032112784683704376
      }
    },
    {
      "sample_id": 2,
      "question": "Why does Maria think it's important for younger generations to visit art galleries?",
      "prediction": "To remember and appreciate those who served",
      "reference": "To remember and appreciate those who served",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 2,
      "question": "What happened to Maria's job in August 2023?",
      "prediction": "John lost his job at the mechanical engineering company.",
      "reference": "John lost his job at the mechanical engineering company.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "What cause did the 5K charity run organized by Maria support?",
      "prediction": "veterans and their families",
      "reference": "veterans and their families",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "Who did John work with to raise awareness and funds for animal welfare?",
      "prediction": "local organization",
      "reference": "a local organization",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.951160192489624,
        "bert_recall": 0.932255744934082,
        "bert_f1": 0.9416130781173706,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.9676264524459839
      }
    },
    {
      "sample_id": 2,
      "question": "What recognition did John receive at the homeless shelter in August 2023?",
      "prediction": "Not mentioned in the conversation or a medal for volunteering",
      "reference": "a medal for volunteering",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.5,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.8900710940361023,
        "bert_recall": 0.9392058849334717,
        "bert_f1": 0.9139785766601562,
        "meteor": 0.8627717391304348,
        "sbert_similarity": 0.7546218633651733
      }
    },
    {
      "sample_id": 2,
      "question": "What is the name of John's puppy he got two weeks before August 11, 2023?",
      "prediction": "Coco",
      "reference": "Coco",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 2,
      "question": "How does Maria describe the camping trip with Max?",
      "prediction": "Peaceful and awesome",
      "reference": "Peaceful and awesome",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "What is the name of Maria's second kitten?",
      "prediction": "Shadow",
      "reference": "Shadow",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 2,
      "question": "How is John's new puppy adjusting to its new home?",
      "prediction": "Not mentioned in the conversation",
      "reference": "doing great - learning commands and house training",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8832921981811523,
        "bert_recall": 0.8631062507629395,
        "bert_f1": 0.8730825185775757,
        "meteor": 0.0,
        "sbert_similarity": 0.028081489726901054
      }
    },
    {
      "sample_id": 3,
      "question": "Is it likely that Nate has friends besides Joanna?",
      "prediction": "No",
      "reference": "Yesteammates on hisvideo game team.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8133167028427124,
        "bert_recall": 0.7956110239028931,
        "bert_f1": 0.8043664693832397,
        "meteor": 0.0,
        "sbert_similarity": 0.21062630414962769
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of interests do Joanna and Nate share?",
      "prediction": "Joanna and Nate both show interest in pet ownership, DIY projects (Nate's) and video gaming (Nate's).",
      "reference": "Watching movies, making desserts",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.041666666666666664,
        "bleu2": 0.013459547551454133,
        "bleu3": 0.009821111002978165,
        "bleu4": 0.007913247271422612,
        "bert_precision": 0.820487380027771,
        "bert_recall": 0.8712751269340515,
        "bert_f1": 0.8451189994812012,
        "meteor": 0.0,
        "sbert_similarity": 0.15882901847362518
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna first watch \"Eternal Sunshine of the Spotless Mind?",
      "prediction": "2022",
      "reference": "2019",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9083740711212158,
        "bert_recall": 0.9107277989387512,
        "bert_f1": 0.9095494747161865,
        "meteor": 0.0,
        "sbert_similarity": 0.5572811365127563
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win his first video game tournament?",
      "prediction": "2022 March 24",
      "reference": "the week before 21Janury, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.8492516279220581,
        "bert_recall": 0.8241806030273438,
        "bert_f1": 0.8365283608436584,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.771993100643158
      }
    },
    {
      "sample_id": 3,
      "question": "What pets wouldn't cause any discomfort to Joanna?",
      "prediction": "Other animals",
      "reference": "Hairless cats or pigs,since they don't have fur, which is one of the main causes of Joanna's allergy.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8739999532699585,
        "bert_recall": 0.8268599510192871,
        "bert_f1": 0.8497766852378845,
        "meteor": 0.0,
        "sbert_similarity": 0.27231526374816895
      }
    },
    {
      "sample_id": 3,
      "question": "What are Joanna's hobbies?",
      "prediction": "creating new items and watching TV shows",
      "reference": "Writing, watchingmovies, exploringnature, hanging withfriends.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8747726678848267,
        "bert_recall": 0.8583706617355347,
        "bert_f1": 0.8664939999580383,
        "meteor": 0.0,
        "sbert_similarity": 0.3643553555011749
      }
    },
    {
      "sample_id": 3,
      "question": "How long has Nate had his first two turtles?",
      "prediction": "Nate likely got his first two turtles sometime between 25 October, 2022 (talk with memory content starting at 8:16 pm) and 9 November, 2022.",
      "reference": "three years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8014999628067017,
        "bert_recall": 0.8462506532669067,
        "bert_f1": 0.8232676386833191,
        "meteor": 0.0,
        "sbert_similarity": 0.21815136075019836
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna finish her first screenplay?",
      "prediction": "7 February, 2022",
      "reference": "The Friday before 23January, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.24761510494160163,
        "bleu3": 0.12494616991426132,
        "bleu4": 0.10305224267080783,
        "bert_precision": 0.9159537553787231,
        "bert_recall": 0.8743847012519836,
        "bert_f1": 0.8946865797042847,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.6902247667312622
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate get his first two turtles?",
      "prediction": "25 May 2022",
      "reference": "2019",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8583633899688721,
        "bert_recall": 0.8931765556335449,
        "bert_f1": 0.8754239678382874,
        "meteor": 0.0,
        "sbert_similarity": 0.5639612674713135
      }
    },
    {
      "sample_id": 3,
      "question": "What major achievement did Joanna accomplish in January 2022?",
      "prediction": "Joanna won a Valorant tournament",
      "reference": "finished her screenplay and printed it",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8490865230560303,
        "bert_recall": 0.8567012548446655,
        "bert_f1": 0.8528769016265869,
        "meteor": 0.0,
        "sbert_similarity": 0.15996643900871277
      }
    },
    {
      "sample_id": 3,
      "question": "What emotions is Joanna feeling about  the screenplay she submitted?",
      "prediction": "Joanna feels relief, excitement, and some anxiety.",
      "reference": "Relief, excitement,worry, hope,anxiety.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.2,
        "rougeL_f": 0.5,
        "bleu1": 0.6,
        "bleu2": 0.5163977794943222,
        "bleu3": 0.4091569047284194,
        "bleu4": 0.31239399369202553,
        "bert_precision": 0.9328494668006897,
        "bert_recall": 0.9263061285018921,
        "bert_f1": 0.9295662045478821,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.6560521721839905
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna allergic to?",
      "prediction": "Joanna is allergic to most reptiles and animals with fur.",
      "reference": "Most reptiles,animals with fur,cockroaches, dairy",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.588235294117647,
        "rouge1_f": 0.588235294117647,
        "rouge2_f": 0.4,
        "rougeL_f": 0.588235294117647,
        "bleu1": 0.45454545454545453,
        "bleu2": 0.3692744729379982,
        "bleu3": 0.2509287951221701,
        "bleu4": 0.11731175160264001,
        "bert_precision": 0.8932861089706421,
        "bert_recall": 0.860755205154419,
        "bert_f1": 0.8767189979553223,
        "meteor": 0.18181818181818182,
        "sbert_similarity": 0.4616609215736389
      }
    },
    {
      "sample_id": 3,
      "question": "What underlying condition might Joanna have based on her allergies?",
      "prediction": "Joanna might have allergies to reptiles and animals with fur.",
      "reference": "asthma",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8158060312271118,
        "bert_recall": 0.8244696259498596,
        "bert_f1": 0.8201149106025696,
        "meteor": 0.0,
        "sbert_similarity": 0.3002912998199463
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna have an audition for a writing gig?",
      "prediction": "24 March 2022",
      "reference": "23 March, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.3422780793550613,
        "bleu2": 0.09373671250115519,
        "bleu3": 0.07816643970700751,
        "bleu4": 0.069372929071742,
        "bert_precision": 0.9722622632980347,
        "bert_recall": 0.9669532775878906,
        "bert_f1": 0.9696004986763,
        "meteor": 0.0,
        "sbert_similarity": 0.9424914121627808
      }
    },
    {
      "sample_id": 3,
      "question": "What nickname does Nate use for Joanna?",
      "prediction": "Joanna",
      "reference": "Jo",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.910824179649353,
        "bert_recall": 0.9533872008323669,
        "bert_f1": 0.9316198229789734,
        "meteor": 0.0,
        "sbert_similarity": 0.505753755569458
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate get purple hair?",
      "prediction": "on 15 April, 2022",
      "reference": "The week before 15April, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.26812801841425576,
        "bleu2": 0.21197381067415422,
        "bleu3": 0.10205450796337225,
        "bleu4": 0.07616306180370792,
        "bert_precision": 0.9390283226966858,
        "bert_recall": 0.9176305532455444,
        "bert_f1": 0.9282061457633972,
        "meteor": 0.0,
        "sbert_similarity": 0.8001176714897156
      }
    },
    {
      "sample_id": 3,
      "question": "What physical transformation did Nate undergo in April 2022?",
      "prediction": "Nate underwent a significant transformation during his road trip adventure.",
      "reference": "dyed his hair purple",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8965353965759277,
        "bert_recall": 0.8673977851867676,
        "bert_f1": 0.8817259073257446,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.1704687923192978
      }
    },
    {
      "sample_id": 3,
      "question": "What movie did Joanna watch on 1 May, 2022?",
      "prediction": "The series 'A Little Haven' with romance themes.",
      "reference": "Lord of the Rings",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.846806526184082,
        "bert_recall": 0.8804497718811035,
        "bert_f1": 0.8633005023002625,
        "meteor": 0.11363636363636363,
        "sbert_similarity": 0.3449209928512573
      }
    },
    {
      "sample_id": 3,
      "question": "Which outdoor spot did Joanna visit in May?",
      "prediction": "nature",
      "reference": "Whispering Falls waterfall",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.871071457862854,
        "bert_recall": 0.8013855814933777,
        "bert_f1": 0.834776759147644,
        "meteor": 0.0,
        "sbert_similarity": 0.2974979877471924
      }
    },
    {
      "sample_id": 3,
      "question": "How many times has Joanna found new hiking trails?",
      "prediction": "Joanna found two new hiking trails:",
      "reference": "twice",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8344313502311707,
        "bert_recall": 0.8566843271255493,
        "bert_f1": 0.845411479473114,
        "meteor": 0.0,
        "sbert_similarity": 0.21021686494350433
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate adopt Max?",
      "prediction": "20 May 2022",
      "reference": "May 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9421707391738892,
        "bert_recall": 0.953156590461731,
        "bert_f1": 0.9476318359375,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.9259423017501831
      }
    },
    {
      "sample_id": 3,
      "question": "Who was the new addition to Nate's family in May 2022?",
      "prediction": "An infant",
      "reference": "Max",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8990877866744995,
        "bert_recall": 0.9133769273757935,
        "bert_f1": 0.9061760306358337,
        "meteor": 0.0,
        "sbert_similarity": 0.07465599477291107
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna start writing her third screenplay?",
      "prediction": "23 January, 2022",
      "reference": "May 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8910573720932007,
        "bert_recall": 0.9309185743331909,
        "bert_f1": 0.9105519652366638,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8418516516685486
      }
    },
    {
      "sample_id": 3,
      "question": "Which of Joanna's screenplay were rejected from production companies?",
      "prediction": "the first full screenplay",
      "reference": "first screenplay on drama and romance, third screenplay on loss identity and connection",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.0410424993119494,
        "bleu2": 0.010597127754793961,
        "bleu3": 0.007909225976319087,
        "bleu4": 0.007842752028330948,
        "bert_precision": 0.9013664722442627,
        "bert_recall": 0.8425477743148804,
        "bert_f1": 0.8709651827812195,
        "meteor": 0.08264462809917356,
        "sbert_similarity": 0.6285973191261292
      }
    },
    {
      "sample_id": 3,
      "question": "When is Nate hosting a gaming party?",
      "prediction": "21 January 2022",
      "reference": "The weekend after 3June, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.08786571270524224,
        "bleu2": 0.03403024420100487,
        "bleu3": 0.031926460841892726,
        "bleu4": 0.029950417327249696,
        "bert_precision": 0.9041212797164917,
        "bert_recall": 0.867006242275238,
        "bert_f1": 0.8851749300956726,
        "meteor": 0.0,
        "sbert_similarity": 0.6061721444129944
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna hike with her buddies?",
      "prediction": "3 June, 2022",
      "reference": "The weekend after 3June, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.23618327637050734,
        "bleu2": 0.19284283762882742,
        "bleu3": 0.09730817497099951,
        "bleu4": 0.08025716728928957,
        "bert_precision": 0.9438190460205078,
        "bert_recall": 0.9009653329849243,
        "bert_f1": 0.9218944311141968,
        "meteor": 0.0,
        "sbert_similarity": 0.730426013469696
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win his third tourney?",
      "prediction": "2022",
      "reference": "The week before 3June, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.887999951839447,
        "bert_recall": 0.8259423971176147,
        "bert_f1": 0.8558477759361267,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5229290127754211
      }
    },
    {
      "sample_id": 3,
      "question": "What places has Joanna submitted her work to?",
      "prediction": "Joanna's creations have been shown at Valorant tournaments and on social media.",
      "reference": "film contest, film festival.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8410456776618958,
        "bert_recall": 0.8732585310935974,
        "bert_f1": 0.8568494915962219,
        "meteor": 0.0,
        "sbert_similarity": 0.24037644267082214
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate make vegan icecream and share it with a vegan diet group?",
      "prediction": "25 February 2022",
      "reference": "The Friday before 24June, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.08786571270524224,
        "bleu2": 0.03403024420100487,
        "bleu3": 0.031926460841892726,
        "bleu4": 0.029950417327249696,
        "bert_precision": 0.9080684185028076,
        "bert_recall": 0.8729841709136963,
        "bert_f1": 0.890180766582489,
        "meteor": 0.0,
        "sbert_similarity": 0.6937575936317444
      }
    },
    {
      "sample_id": 3,
      "question": "When is Joanna going to make Nate's ice cream for her family?",
      "prediction": "14 September, 2022",
      "reference": "The weekend of 24June, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.23618327637050734,
        "bleu2": 0.19284283762882742,
        "bleu3": 0.09730817497099951,
        "bleu4": 0.08025716728928957,
        "bert_precision": 0.9346293210983276,
        "bert_recall": 0.9118723273277283,
        "bert_f1": 0.9231106042861938,
        "meteor": 0.0,
        "sbert_similarity": 0.6312460899353027
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of writings does Joanna do?",
      "prediction": "writing about personal experiences",
      "reference": "Screenplays,books, online blog posts, journal",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8328918814659119,
        "bert_recall": 0.8206520080566406,
        "bert_f1": 0.8267266154289246,
        "meteor": 0.0,
        "sbert_similarity": 0.38387835025787354
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win his fourth video game tournament?",
      "prediction": "5:44 pm on 3 June, 2022",
      "reference": "The Friday before 10July, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.867445170879364,
        "bert_recall": 0.8821547627449036,
        "bert_f1": 0.8747380971908569,
        "meteor": 0.0,
        "sbert_similarity": 0.6223423480987549
      }
    },
    {
      "sample_id": 3,
      "question": "Where did Joanna travel to in July 2022?",
      "prediction": "California",
      "reference": "Woodhaven",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9194069504737854,
        "bert_recall": 0.8894721269607544,
        "bert_f1": 0.9041918516159058,
        "meteor": 0.0,
        "sbert_similarity": 0.26560327410697937
      }
    },
    {
      "sample_id": 3,
      "question": "When did someone write Joanna a touching letter?",
      "prediction": "2022",
      "reference": "The week before 14August, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.8901941776275635,
        "bert_recall": 0.8452227115631104,
        "bert_f1": 0.8671257495880127,
        "meteor": 0.0,
        "sbert_similarity": 0.6329331994056702
      }
    },
    {
      "sample_id": 3,
      "question": "What book recommendations has Joanna given to Nate?",
      "prediction": "fantasy book series",
      "reference": "\"Little Women\",'A Court of Thorns andRoses'.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8533347845077515,
        "bert_recall": 0.801978588104248,
        "bert_f1": 0.8268600106239319,
        "meteor": 0.0,
        "sbert_similarity": 0.29304632544517517
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate take time off to chill with his pets?",
      "prediction": "2 May 2022",
      "reference": "The weekend of 22August, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.08786571270524224,
        "bleu2": 0.03403024420100487,
        "bleu3": 0.031926460841892726,
        "bleu4": 0.029950417327249696,
        "bert_precision": 0.9213039875030518,
        "bert_recall": 0.8917554020881653,
        "bert_f1": 0.9062889218330383,
        "meteor": 0.0,
        "sbert_similarity": 0.6917141675949097
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna share her book with her writers group?",
      "prediction": "June 5th",
      "reference": "The week before 22August, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8881663084030151,
        "bert_recall": 0.8720970153808594,
        "bert_f1": 0.8800582885742188,
        "meteor": 0.0,
        "sbert_similarity": 0.5785601139068604
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win an international tournament?",
      "prediction": "24 June 2022",
      "reference": "21 August, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9550727605819702,
        "bert_recall": 0.9368227124214172,
        "bert_f1": 0.9458596706390381,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8974255919456482
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna make a desert with almond milk?",
      "prediction": "5 September 2022",
      "reference": "The Friday before 14September, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9334335327148438,
        "bert_recall": 0.8705121874809265,
        "bert_f1": 0.9008755087852478,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7110534906387329
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate attend a cooking show?",
      "prediction": "2022",
      "reference": "The Monday before 14September, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.8878107070922852,
        "bert_recall": 0.8035271167755127,
        "bert_f1": 0.8435688614845276,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5690791606903076
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna's laptop crash?",
      "prediction": "14 September",
      "reference": "The week before 14September, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8908041715621948,
        "bert_recall": 0.8596186637878418,
        "bert_f1": 0.8749336004257202,
        "meteor": 0.0,
        "sbert_similarity": 0.5704715251922607
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna make a chocolate tart with raspberries?",
      "prediction": "14 September 2022",
      "reference": "5 October, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9518559575080872,
        "bert_recall": 0.9298299551010132,
        "bert_f1": 0.9407140016555786,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8114752769470215
      }
    },
    {
      "sample_id": 3,
      "question": "What movies have both Joanna and Nate seen?",
      "prediction": "Joanna has watched 'The Lord of the Rings' Trilogy,' while Nate has watched it.",
      "reference": "\"Little Women\", \"Lord of the Rings\"",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.21052631578947367,
        "rouge1_f": 0.4,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.4,
        "bleu1": 0.2777777777777778,
        "bleu2": 0.22140372138502384,
        "bleu3": 0.18612398784440445,
        "bleu4": 0.14216645907653844,
        "bert_precision": 0.8708594441413879,
        "bert_recall": 0.896486222743988,
        "bert_f1": 0.8834870457649231,
        "meteor": 0.2757352941176471,
        "sbert_similarity": 0.3917657136917114
      }
    },
    {
      "sample_id": 3,
      "question": "How long did it take for Joanna to finish writing her book?",
      "prediction": "Last week",
      "reference": "four months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8890553712844849,
        "bert_recall": 0.8890553712844849,
        "bert_f1": 0.8890553712844849,
        "meteor": 0.0,
        "sbert_similarity": 0.3861650824546814
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win a lot of money in a video game tournament?",
      "prediction": "24 June 2022",
      "reference": "September 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9472072720527649,
        "bert_recall": 0.9616732597351074,
        "bert_f1": 0.95438551902771,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7711138725280762
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna finish up the writing for her book?",
      "prediction": "2022-08-14",
      "reference": "The week before 6October, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8259580135345459,
        "bert_recall": 0.8243000507354736,
        "bert_f1": 0.8251281976699829,
        "meteor": 0.0,
        "sbert_similarity": 0.5819759368896484
      }
    },
    {
      "sample_id": 3,
      "question": "What board games has Nate played?",
      "prediction": "Catan",
      "reference": "Chess, Catan.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9378092288970947,
        "bert_recall": 0.910194456577301,
        "bert_f1": 0.9237955212593079,
        "meteor": 0.0,
        "sbert_similarity": 0.6898443698883057
      }
    },
    {
      "sample_id": 3,
      "question": "What places has Nate met new people?",
      "prediction": "Nate has not explicitly mentioned meeting new people.",
      "reference": "A tournament and agaming convention.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8688673973083496,
        "bert_recall": 0.8564947843551636,
        "bert_f1": 0.8626366853713989,
        "meteor": 0.0,
        "sbert_similarity": 0.1831018477678299
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate go to a convention and meet new people?",
      "prediction": "21 January 2022",
      "reference": "The Friday before 9October, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.08786571270524224,
        "bleu2": 0.03403024420100487,
        "bleu3": 0.031926460841892726,
        "bleu4": 0.029950417327249696,
        "bert_precision": 0.9015092849731445,
        "bert_recall": 0.8677663803100586,
        "bert_f1": 0.884316086769104,
        "meteor": 0.0,
        "sbert_similarity": 0.6139209866523743
      }
    },
    {
      "sample_id": 3,
      "question": "How many times has Joanna's scripts been rejected?",
      "prediction": "twice",
      "reference": "Twice",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9704567193984985,
        "bert_recall": 0.9704567193984985,
        "bert_f1": 0.9704567193984985,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What is something Nate gave to Joanna that brings her a lot of joy?",
      "prediction": "Nate's support",
      "reference": "stuffed toy pup",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8667054176330566,
        "bert_recall": 0.8398944139480591,
        "bert_f1": 0.8530893325805664,
        "meteor": 0.0,
        "sbert_similarity": 0.06688378751277924
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate get Tilly for Joanna?",
      "prediction": "August 2014",
      "reference": "25 May, 2022",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8983592987060547,
        "bert_recall": 0.8770801424980164,
        "bert_f1": 0.8875921964645386,
        "meteor": 0.0,
        "sbert_similarity": 0.3861023187637329
      }
    },
    {
      "sample_id": 3,
      "question": "How many of Joanna's writing have made it to the big screen?",
      "prediction": "0",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8302218317985535,
        "bert_recall": 0.8302218317985535,
        "bert_f1": 0.8302218317985535,
        "meteor": 0.0,
        "sbert_similarity": 0.4455368220806122
      }
    },
    {
      "sample_id": 3,
      "question": "How many times has Nate taken his turtles on a walk?",
      "prediction": "4",
      "reference": "Twice.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8553363084793091,
        "bert_recall": 0.8933221101760864,
        "bert_f1": 0.8739166259765625,
        "meteor": 0.0,
        "sbert_similarity": 0.3308301568031311
      }
    },
    {
      "sample_id": 3,
      "question": "When was Joanna's second movie script shown on the big screens?",
      "prediction": "7 November, 2022",
      "reference": "The Sunday before 25October, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.23618327637050734,
        "bleu2": 0.19284283762882742,
        "bleu3": 0.09730817497099951,
        "bleu4": 0.08025716728928957,
        "bert_precision": 0.9265842437744141,
        "bert_recall": 0.8844469785690308,
        "bert_f1": 0.9050254225730896,
        "meteor": 0.0,
        "sbert_similarity": 0.6588612794876099
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna inspired by?",
      "prediction": "Joanna's inspiration comes from creating new items.",
      "reference": "Personal experiences,her own journey ofself discovery, Nate,nature, validation,stories about findingcourage and takingrisks, people she knows, stuff she sees, imagination",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8704851865768433,
        "bert_recall": 0.8050369024276733,
        "bert_f1": 0.83648282289505,
        "meteor": 0.0,
        "sbert_similarity": 0.3533850312232971
      }
    },
    {
      "sample_id": 3,
      "question": "What animal do both Nate and Joanna like?",
      "prediction": "dogs",
      "reference": "Turtles.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8786769509315491,
        "bert_recall": 0.8957287073135376,
        "bert_f1": 0.8871208429336548,
        "meteor": 0.0,
        "sbert_similarity": 0.5217444896697998
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna plan to go over to Nate's and share recipes?",
      "prediction": "2 May, 2022",
      "reference": "5 November, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.38940039153570244,
        "bleu2": 0.3179440883008189,
        "bleu3": 0.16043405788769674,
        "bleu4": 0.13232169883599026,
        "bert_precision": 0.9677196741104126,
        "bert_recall": 0.9741687178611755,
        "bert_f1": 0.9709335565567017,
        "meteor": 0.0,
        "sbert_similarity": 0.7509846687316895
      }
    },
    {
      "sample_id": 3,
      "question": "What things has Nate reccomended to Joanna?",
      "prediction": "Nate's recommended series, movie recommendations, cooking show recipe, and Valorant tournament support.",
      "reference": "A pet,\"The Lord of the Rings\" movies,a dragon book series,coconut flavoring,\"Project Hail Mary\" book,Xenoblade Chronicles, dairy-free margarine, coconut oil",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.060606060606060615,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.052631578947368425,
        "bleu1": 0.08161497535456293,
        "bleu2": 0.04206339550238065,
        "bleu3": 0.016493800117011688,
        "bleu4": 0.010034060886876969,
        "bert_precision": 0.8360459804534912,
        "bert_recall": 0.8019448518753052,
        "bert_f1": 0.8186404705047607,
        "meteor": 0.0,
        "sbert_similarity": 0.4587704539299011
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna do to remember happy memories?",
      "prediction": "Joanna writes down her favorite memories.",
      "reference": "Hangs them on a corkboard, writes themin a notebook.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.16134803485935978,
        "bleu2": 0.03896925267340483,
        "bleu3": 0.026597404361337035,
        "bleu4": 0.022182955195367136,
        "bert_precision": 0.8937921524047852,
        "bert_recall": 0.8585655689239502,
        "bert_f1": 0.8758248090744019,
        "meteor": 0.0574712643678161,
        "sbert_similarity": 0.19475902616977692
      }
    },
    {
      "sample_id": 3,
      "question": "What Console does Nate own?",
      "prediction": "Nate owns a video game console",
      "reference": "A Nintendo Switch; since the game \"Xenoblade 2\" is made for this console.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3157894736842105,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.07993987303984694,
        "bleu2": 0.015987974607969392,
        "bleu3": 0.010354130936843295,
        "bleu4": 0.008590098392241091,
        "bert_precision": 0.8914130330085754,
        "bert_recall": 0.8436266183853149,
        "bert_f1": 0.8668617010116577,
        "meteor": 0.08130081300813008,
        "sbert_similarity": 0.3665906488895416
      }
    },
    {
      "sample_id": 3,
      "question": "What mediums does Nate use to play games?",
      "prediction": "Nate uses video games and watching movies.",
      "reference": "Gamecube, PC,Playstation.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8540430068969727,
        "bert_recall": 0.8466280698776245,
        "bert_f1": 0.8503193855285645,
        "meteor": 0.0,
        "sbert_similarity": 0.23952850699424744
      }
    },
    {
      "sample_id": 3,
      "question": "How many letters has Joanna recieved?",
      "prediction": "context",
      "reference": "Two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9014091491699219,
        "bert_recall": 0.9014091491699219,
        "bert_f1": 0.9014091491699219,
        "meteor": 0.0,
        "sbert_similarity": 0.2785644233226776
      }
    },
    {
      "sample_id": 3,
      "question": "What video games does Nate play?",
      "prediction": "video games",
      "reference": "Valorant, Counter Strike:Global Offensive,Xenoblade Chronicles, StreetFighter, Cyberpunk 2077",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8782330751419067,
        "bert_recall": 0.7599582076072693,
        "bert_f1": 0.8148260712623596,
        "meteor": 0.0,
        "sbert_similarity": 0.39762169122695923
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win a big Valorant tourney?",
      "prediction": "2022",
      "reference": "The Saturday before 7November, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.8891530632972717,
        "bert_recall": 0.8076827526092529,
        "bert_f1": 0.8464621305465698,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5389972925186157
      }
    },
    {
      "sample_id": 3,
      "question": "Which torunament did Nate win in the beginning of November 2022?",
      "prediction": "video game tournament",
      "reference": "Valorant",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8780671954154968,
        "bert_recall": 0.8640540838241577,
        "bert_f1": 0.8710043430328369,
        "meteor": 0.0,
        "sbert_similarity": 0.22196179628372192
      }
    },
    {
      "sample_id": 3,
      "question": "What alternative career might Nate consider after gaming?",
      "prediction": "custom games",
      "reference": "an animalkeeper at a localzoo and workingwith turtles; as heknows a great dealabout turtles andhow to care for them,and he enjoys it.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8576163053512573,
        "bert_recall": 0.8043357133865356,
        "bert_f1": 0.8301219344139099,
        "meteor": 0.0,
        "sbert_similarity": 0.11081277579069138
      }
    },
    {
      "sample_id": 3,
      "question": "What pets does Nate have?",
      "prediction": "Pets include infants and their parents.",
      "reference": "A dog and threeturtles.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.06900655593423542,
        "bleu3": 0.047098549390932394,
        "bleu4": 0.039281465090051315,
        "bert_precision": 0.8670051097869873,
        "bert_recall": 0.8496782779693604,
        "bert_f1": 0.8582541942596436,
        "meteor": 0.11904761904761905,
        "sbert_similarity": 0.4166584312915802
      }
    },
    {
      "sample_id": 3,
      "question": "How many hikes has Joanna been on?",
      "prediction": "2",
      "reference": "Four",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9498066306114197,
        "bert_recall": 0.9498066306114197,
        "bert_f1": 0.9498066306114197,
        "meteor": 0.0,
        "sbert_similarity": 0.6889071464538574
      }
    },
    {
      "sample_id": 3,
      "question": "How many turtles does Nate have?",
      "prediction": "three",
      "reference": "Three",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9647070169448853,
        "bert_recall": 0.9647070169448853,
        "bert_f1": 0.9647070169448853,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What activities does Nate do with his turtles?",
      "prediction": "Nate practices video game tournaments and enjoys exploring nature.",
      "reference": "takes them onwalks, holds them,feeds themstrawberries, givesthem baths.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.07408182206817181,
        "bleu2": 0.024693940689390605,
        "bleu3": 0.018483828554943155,
        "bleu4": 0.015635220893785457,
        "bert_precision": 0.8480187058448792,
        "bert_recall": 0.8037623167037964,
        "bert_f1": 0.8252975940704346,
        "meteor": 0.0,
        "sbert_similarity": 0.1819627434015274
      }
    },
    {
      "sample_id": 3,
      "question": "What do both Joanna and Nate appreciate the beauty of?",
      "prediction": "Joanna appreciates beautiful creations, while Nate loves having someone around who is talented.",
      "reference": "Nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8275332450866699,
        "bert_recall": 0.8509945273399353,
        "bert_f1": 0.8390998840332031,
        "meteor": 0.0,
        "sbert_similarity": 0.1287543922662735
      }
    },
    {
      "sample_id": 3,
      "question": "When did Joanna plan on going to Nate's to watch him play with his turtles?",
      "prediction": "June 5",
      "reference": "10 November, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9121601581573486,
        "bert_recall": 0.8760876655578613,
        "bert_f1": 0.893760085105896,
        "meteor": 0.0,
        "sbert_similarity": 0.49779006838798523
      }
    },
    {
      "sample_id": 3,
      "question": "What state did Joanna visit in summer 2021?",
      "prediction": "Nevada",
      "reference": "Indiana",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8999794721603394,
        "bert_recall": 0.9296318292617798,
        "bert_f1": 0.9145653247833252,
        "meteor": 0.0,
        "sbert_similarity": 0.4922153949737549
      }
    },
    {
      "sample_id": 3,
      "question": "What recommendations has Nate received from Joanna?",
      "prediction": "Joanna recommends trying Nates' cooking creations and sharing her baking recipes.",
      "reference": "\"Eternal Sunshine of the Spotless Mind\" movie, \"A Court of Thorns and Roses\" book, pointers for making living room comfy, starting a cork board for memories, \"Little Women\" movie",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.05405405405405406,
        "rouge1_f": 0.05,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05,
        "bleu1": 0.010410406402816362,
        "bleu2": 0.003426484227228397,
        "bleu3": 0.0025352473219668673,
        "bleu4": 0.0021027213063125744,
        "bert_precision": 0.8415520191192627,
        "bert_recall": 0.7897365093231201,
        "bert_f1": 0.8148213624954224,
        "meteor": 0.03676470588235294,
        "sbert_similarity": 0.11090286821126938
      }
    },
    {
      "sample_id": 3,
      "question": "What are Nate's favorite desserts?",
      "prediction": "Chocolate and vanilla",
      "reference": "coconut milk icecream, dairy-free chocolate cake with berries, chocolate and mixed-berry icecream, dairy-free chocolate mousse",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.1904761904761905,
        "rouge2_f": 0.10526315789473684,
        "rougeL_f": 0.1904761904761905,
        "bleu1": 0.004491964666056978,
        "bleu2": 0.0038901555137074253,
        "bleu3": 0.002193196663017748,
        "bleu4": 0.0016190015957237576,
        "bert_precision": 0.8899470567703247,
        "bert_recall": 0.83742755651474,
        "bert_f1": 0.8628889322280884,
        "meteor": 0.07246376811594203,
        "sbert_similarity": 0.6164671182632446
      }
    },
    {
      "sample_id": 3,
      "question": "How has Nate tried to disburse his vegan ice-cream recipes?",
      "prediction": "Nate taught people vegan ice cream recipes on his own cooking show.",
      "reference": "teaching others, cooking show",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.25,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.11322770341445958,
        "bleu3": 0.05034417113155098,
        "bleu4": 0.03285702044797773,
        "bert_precision": 0.8922191858291626,
        "bert_recall": 0.9147101640701294,
        "bert_f1": 0.9033246636390686,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.5144738554954529
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate win his second tournament?",
      "prediction": "9 November, 2022",
      "reference": "The week before 2 May, 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.1501861529550426,
        "bleu3": 0.0757836828666637,
        "bleu4": 0.06250434473199146,
        "bert_precision": 0.9355828762054443,
        "bert_recall": 0.9146426916122437,
        "bert_f1": 0.9249942898750305,
        "meteor": 0.0,
        "sbert_similarity": 0.7009748816490173
      }
    },
    {
      "sample_id": 3,
      "question": "How many video game tournaments has Nate participated in?",
      "prediction": "2",
      "reference": "nine",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9004371166229248,
        "bert_recall": 0.9004371166229248,
        "bert_f1": 0.9004371166229248,
        "meteor": 0.0,
        "sbert_similarity": 0.536834716796875
      }
    },
    {
      "sample_id": 3,
      "question": "How many screenplays has Joanna written?",
      "prediction": "one",
      "reference": "three",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9172804951667786,
        "bert_recall": 0.9172804951667786,
        "bert_f1": 0.9172804951667786,
        "meteor": 0.0,
        "sbert_similarity": 0.6475731134414673
      }
    },
    {
      "sample_id": 3,
      "question": "How many tournaments has Nate won?",
      "prediction": "two",
      "reference": "seven",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9545558094978333,
        "bert_recall": 0.9545558094978333,
        "bert_f1": 0.9545558094978333,
        "meteor": 0.0,
        "sbert_similarity": 0.5319218635559082
      }
    },
    {
      "sample_id": 3,
      "question": "What recipes has Joanna made?",
      "prediction": "Joanna has been creating a variety of desserts that are suitable for everyone\u2019s dietary restrictions.",
      "reference": "dairy free vanilla cake with strawberry filling and coconut cream frosting, parfait, strawberry chocolate cake, chocolate coconut cupcakes, chocolate raspberry tart, chocolate cake with raspberries, blueberry cheesecake bars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8457202911376953,
        "bert_recall": 0.7980954647064209,
        "bert_f1": 0.821217954158783,
        "meteor": 0.0,
        "sbert_similarity": 0.33554860949516296
      }
    },
    {
      "sample_id": 3,
      "question": "What recipes has Nate made?",
      "prediction": "Nate has shared revisions and perfecting family recipes that turned out very tasty.",
      "reference": "coconut milk icecream, chocolate and vanilla swirl",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8415552377700806,
        "bert_recall": 0.8101800680160522,
        "bert_f1": 0.8255696296691895,
        "meteor": 0.06578947368421054,
        "sbert_similarity": 0.2530604302883148
      }
    },
    {
      "sample_id": 3,
      "question": "What are the skills that Nate has helped others learn?",
      "prediction": "Nate helps others improve their gaming skills.",
      "reference": "coconut milk ice cream recipe, reset high scores, tips to improve gaming skills",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.3,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.3,
        "bleu1": 0.15632325737944064,
        "bleu2": 0.09648480916253598,
        "bleu3": 0.041091347275722945,
        "bleu4": 0.02709850435615371,
        "bert_precision": 0.9115228652954102,
        "bert_recall": 0.8480703830718994,
        "bert_f1": 0.8786525130271912,
        "meteor": 0.08064516129032259,
        "sbert_similarity": 0.42144080996513367
      }
    },
    {
      "sample_id": 3,
      "question": "Was the first half of September 2022 a good month career-wise for Nate and Joanna? Answer yes or no.",
      "prediction": "Yes",
      "reference": "No; because both of them faced setbacks in their career",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8691174983978271,
        "bert_recall": 0.8137537837028503,
        "bert_f1": 0.8405249714851379,
        "meteor": 0.0,
        "sbert_similarity": 0.24034804105758667
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of job is Joanna beginning to preform the duties of because of her movie scripts?",
      "prediction": "acting",
      "reference": "filmmaker.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8655580878257751,
        "bert_recall": 0.8666682243347168,
        "bert_f1": 0.8661127686500549,
        "meteor": 0.0,
        "sbert_similarity": 0.5573679208755493
      }
    },
    {
      "sample_id": 3,
      "question": "When did Nate take his turtles to the beach?",
      "prediction": "7 days ago",
      "reference": "10 November, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7730153799057007,
        "bert_recall": 0.8161470890045166,
        "bert_f1": 0.7939959168434143,
        "meteor": 0.0,
        "sbert_similarity": 0.22654008865356445
      }
    },
    {
      "sample_id": 3,
      "question": "What state did Nate visit?",
      "prediction": "N/A",
      "reference": "Florida",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8322224617004395,
        "bert_recall": 0.8677645921707153,
        "bert_f1": 0.8496220111846924,
        "meteor": 0.0,
        "sbert_similarity": 0.12921057641506195
      }
    },
    {
      "sample_id": 3,
      "question": "What is one of Joanna's favorite movies?",
      "prediction": "That sounds great! What's your favorite game or movie that you've seen recently?",
      "reference": "\"Eternal Sunshineof the Spotless Mind\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8238937854766846,
        "bert_recall": 0.7993955612182617,
        "bert_f1": 0.8114597797393799,
        "meteor": 0.0,
        "sbert_similarity": 0.2186773717403412
      }
    },
    {
      "sample_id": 3,
      "question": "What color did Nate choose for his hair?",
      "prediction": "Nate chose to dye his hair last week.",
      "reference": "purple",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8552917242050171,
        "bert_recall": 0.8643208742141724,
        "bert_f1": 0.8597825765609741,
        "meteor": 0.0,
        "sbert_similarity": 0.306167334318161
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's favorite movie trilogy?",
      "prediction": "Nate's favorite movie trilogy is The Lord of the Rings.",
      "reference": "Lord of the Rings",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.5333333333333333,
        "rouge2_f": 0.4615384615384615,
        "rougeL_f": 0.5333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.30151134457776363,
        "bleu3": 0.26648962915429364,
        "bleu4": 0.21200626759025185,
        "bert_precision": 0.8882473707199097,
        "bert_recall": 0.9447643756866455,
        "bert_f1": 0.9156345725059509,
        "meteor": 0.640096618357488,
        "sbert_similarity": 0.6931428909301758
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's favorite book series about?",
      "prediction": "Joanna expresses enthusiasm and agrees with the speaker's suggestion.",
      "reference": "dragons",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8397314548492432,
        "bert_recall": 0.8435122966766357,
        "bert_f1": 0.8416176438331604,
        "meteor": 0.0,
        "sbert_similarity": -0.028371401131153107
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of lighting does Nate's gaming room have?",
      "prediction": "Nate's gaming room has dark sky and green scenery.",
      "reference": "red and purple lighting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8663849830627441,
        "bert_recall": 0.870972216129303,
        "bert_f1": 0.8686725497245789,
        "meteor": 0.11111111111111112,
        "sbert_similarity": 0.3180186450481415
      }
    },
    {
      "sample_id": 3,
      "question": "What game was the second tournament that Nate won based on?",
      "prediction": "a video game tournament",
      "reference": "Street Fighter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8799509406089783,
        "bert_recall": 0.8752459287643433,
        "bert_f1": 0.877592146396637,
        "meteor": 0.0,
        "sbert_similarity": 0.3444635570049286
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna's third screenplay about?",
      "prediction": "Joanna's third screenplay focuses on a love story.",
      "reference": "loss, identity, and connection",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8701493740081787,
        "bert_recall": 0.8830418586730957,
        "bert_f1": 0.8765482306480408,
        "meteor": 0.0,
        "sbert_similarity": 0.08877234160900116
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's favorite video game?",
      "prediction": "Xeonoblade Chronicles",
      "reference": "Xenoblade Chronicles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9502994418144226,
        "bert_recall": 0.9502994418144226,
        "bert_f1": 0.9502994418144226,
        "meteor": 0.25,
        "sbert_similarity": 0.732913076877594
      }
    },
    {
      "sample_id": 3,
      "question": "What type of movies does Nate enjoy watching the most?",
      "prediction": "movies",
      "reference": "action and sci-fi",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8838737607002258,
        "bert_recall": 0.8826478123664856,
        "bert_f1": 0.8832603096961975,
        "meteor": 0.0,
        "sbert_similarity": 0.5393189191818237
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna just finish last Friday on 23 January, 2022?",
      "prediction": "a second one",
      "reference": "screenplay",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8342238664627075,
        "bert_recall": 0.8691539168357849,
        "bert_f1": 0.8513307571411133,
        "meteor": 0.0,
        "sbert_similarity": 0.3000038266181946
      }
    },
    {
      "sample_id": 3,
      "question": "What genre is Joanna's first screenplay?",
      "prediction": "A Romance",
      "reference": "drama and romance",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9395936727523804,
        "bert_recall": 0.9130799770355225,
        "bert_f1": 0.9261471033096313,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.7031152844429016
      }
    },
    {
      "sample_id": 3,
      "question": "What are Joanna's plans for her finished screenplay in January 2022?",
      "prediction": "Joannasays: I'll be sure to keep you posted.",
      "reference": "submit it to film festivals and get producers and directors to check it out",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.06920912606335215,
        "bleu2": 0.022954070322054584,
        "bleu3": 0.01710001282736,
        "bleu4": 0.014350755161376995,
        "bert_precision": 0.8119024038314819,
        "bert_recall": 0.8362325429916382,
        "bert_f1": 0.8238878846168518,
        "meteor": 0.03731343283582089,
        "sbert_similarity": 0.15029661357402802
      }
    },
    {
      "sample_id": 3,
      "question": "For how long has Nate had his turtles?",
      "prediction": "3 years",
      "reference": "3 years",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate think of the coconut milk ice cream he made?",
      "prediction": "Nate thought it was pretty good",
      "reference": "Super good, rich and creamy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8738992214202881,
        "bert_recall": 0.8806718587875366,
        "bert_f1": 0.8772724866867065,
        "meteor": 0.0,
        "sbert_similarity": 0.30157366394996643
      }
    },
    {
      "sample_id": 3,
      "question": "Which dairy-free dessert flavors does Nate enjoy?",
      "prediction": "Chocolate and Vanilla",
      "reference": "chocolate and mixed berry",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.41368954504257255,
        "bleu3": 0.23323040081963617,
        "bleu4": 0.17216896116316355,
        "bert_precision": 0.9423834085464478,
        "bert_recall": 0.8995319604873657,
        "bert_f1": 0.9204592108726501,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.7220322489738464
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna recently watch and recommend to Nate on February 7, 2022?",
      "prediction": "The Lord of the Rings",
      "reference": "\"Little Women\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8732786774635315,
        "bert_recall": 0.8674817085266113,
        "bert_f1": 0.8703705668449402,
        "meteor": 0.0,
        "sbert_similarity": 0.17579610645771027
      }
    },
    {
      "sample_id": 3,
      "question": "What is \"Little Women\" about according to Joanna?",
      "prediction": "It's a great story about sisterhood, love, and reaching for your dreams.",
      "reference": "Sisterhood, love, and reaching for your dreams",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7368421052631579,
        "rouge1_f": 0.7000000000000001,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7000000000000001,
        "bleu1": 0.5625,
        "bleu2": 0.5477225575051662,
        "bleu3": 0.5346999155254862,
        "bleu4": 0.5129497107827519,
        "bert_precision": 0.9411203265190125,
        "bert_recall": 0.9679880142211914,
        "bert_f1": 0.954365074634552,
        "meteor": 0.7981481481481482,
        "sbert_similarity": 0.6109830141067505
      }
    },
    {
      "sample_id": 3,
      "question": "What flavor of ice cream did Nate make for his friend on 25 February, 2022?",
      "prediction": "chocolate and vanilla",
      "reference": "chocolate and vanilla swirl",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.40293516672844226,
        "bert_precision": 0.9831684827804565,
        "bert_recall": 0.9533915519714355,
        "bert_f1": 0.9680511355400085,
        "meteor": 0.754985754985755,
        "sbert_similarity": 0.7912285327911377
      }
    },
    {
      "sample_id": 3,
      "question": "What inspired Joanna's new screenplay on 25 February, 2022?",
      "prediction": "Joanna expresses excitement and optimism about achieving career goals, specifically mentioning setting up meetings with movie producers.",
      "reference": "personal experiences and her own journey of self-discovery",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07999999999999999,
        "rouge1_f": 0.07692307692307691,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07692307692307691,
        "bleu1": 0.05263157894736841,
        "bleu2": 0.017099639201419235,
        "bleu3": 0.012523428510015045,
        "bleu4": 0.010182425646195498,
        "bert_precision": 0.8655707240104675,
        "bert_recall": 0.8720982074737549,
        "bert_f1": 0.8688221573829651,
        "meteor": 0.05617977528089888,
        "sbert_similarity": 0.3009312152862549
      }
    },
    {
      "sample_id": 3,
      "question": "Why does Nate like turtles as pets?",
      "prediction": "Nate likes turtles because they are unique and slow-paced, making them calming companions that do not require much care.",
      "reference": "Their slow pace and calming nature",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.08333333333333333,
        "rougeL_f": 0.23076923076923075,
        "bleu1": 0.09523809523809525,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.0141800886003256,
        "bleu4": 0.010862721615727716,
        "bert_precision": 0.8652089834213257,
        "bert_recall": 0.8969593644142151,
        "bert_f1": 0.8807981014251709,
        "meteor": 0.136986301369863,
        "sbert_similarity": 0.471476286649704
      }
    },
    {
      "sample_id": 3,
      "question": "How does Nate describe the process of taking care of turtles?",
      "prediction": "low-maintenance",
      "reference": "Not tough; keep their area clean, feed them properly, give them enough light.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8749902248382568,
        "bert_recall": 0.8363729119300842,
        "bert_f1": 0.8552458882331848,
        "meteor": 0.0,
        "sbert_similarity": 0.13454654812812805
      }
    },
    {
      "sample_id": 3,
      "question": "What was Joanna's audition for?",
      "prediction": "Nate's Valorant tournament",
      "reference": "writing gig",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.817306399345398,
        "bert_recall": 0.8593354821205139,
        "bert_f1": 0.8377941250801086,
        "meteor": 0.0,
        "sbert_similarity": 0.11925674974918365
      }
    },
    {
      "sample_id": 3,
      "question": "Why did Nate choose the hair color he did?",
      "prediction": "Nate chose the bright and bold hair color to stand out from usual options.",
      "reference": "Bright and bold - like him",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.23529411764705882,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.2,
        "bleu2": 0.1690308509457033,
        "bleu3": 0.13269551340792207,
        "bleu4": 0.06541868941754436,
        "bert_precision": 0.8563851714134216,
        "bert_recall": 0.8717669248580933,
        "bert_f1": 0.8640075922012329,
        "meteor": 0.4330065359477125,
        "sbert_similarity": 0.4958040416240692
      }
    },
    {
      "sample_id": 3,
      "question": "What are the main ingredients of the ice cream recipe shared by Nate?",
      "prediction": "coconut milk, vanilla extract, sugar, and salt",
      "reference": "Coconut milk, vanilla extract, sugar, salt",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.923076923076923,
        "rouge1_f": 0.923076923076923,
        "rouge2_f": 0.7272727272727272,
        "rougeL_f": 0.923076923076923,
        "bleu1": 0.9,
        "bleu2": 0.8366600265340756,
        "bleu3": 0.8084488919290386,
        "bleu4": 0.7825422900366437,
        "bert_precision": 0.9807156920433044,
        "bert_recall": 0.9858675599098206,
        "bert_f1": 0.9832848310470581,
        "meteor": 0.9653916211293262,
        "sbert_similarity": 0.9922599792480469
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna's project called in the writers group?",
      "prediction": "A suspenseful thriller set in a small Midwestern town",
      "reference": "\"Finding Home\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8562423586845398,
        "bert_recall": 0.8886498212814331,
        "bert_f1": 0.8721451759338379,
        "meteor": 0.0,
        "sbert_similarity": 0.27510157227516174
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's favorite genre of movies?",
      "prediction": "Romance",
      "reference": "Fantasy and sci-fi",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9281836152076721,
        "bert_recall": 0.8839743137359619,
        "bert_f1": 0.9055397510528564,
        "meteor": 0.0,
        "sbert_similarity": 0.38724368810653687
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of books does Nate enjoy?",
      "prediction": "Nate enjoys reading and watching movies.",
      "reference": "Adventures and magic",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8675637245178223,
        "bert_recall": 0.880425214767456,
        "bert_f1": 0.8739472031593323,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.19303344190120697
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of films does Joanna enjoy?",
      "prediction": "dramas and romcoms",
      "reference": "Dramas and emotionally-driven films",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.41368954504257255,
        "bleu3": 0.23323040081963617,
        "bleu4": 0.17216896116316355,
        "bert_precision": 0.927455484867096,
        "bert_recall": 0.926781177520752,
        "bert_f1": 0.9271182417869568,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.5969864130020142
      }
    },
    {
      "sample_id": 3,
      "question": "Which activity helps Nate escape and stimulates his imagination?",
      "prediction": "watching movies",
      "reference": "watching fantasy and sci-fi movies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.22313016014842982,
        "bleu2": 0.07055995207471726,
        "bleu3": 0.04881556015184526,
        "bleu4": 0.03967877695506402,
        "bert_precision": 0.9675403237342834,
        "bert_recall": 0.8991350531578064,
        "bert_f1": 0.932084321975708,
        "meteor": 0.2127659574468085,
        "sbert_similarity": 0.6897871494293213
      }
    },
    {
      "sample_id": 3,
      "question": "What filling did Joanna use in the cake she made recently in May 2022?",
      "prediction": "blueberries",
      "reference": "strawberry",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9495651721954346,
        "bert_recall": 0.9093781113624573,
        "bert_f1": 0.9290372729301453,
        "meteor": 0.0,
        "sbert_similarity": 0.7330352067947388
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of frosting did Joanna use on the cake she made recently in May 2022?",
      "prediction": "white frosting",
      "reference": "coconut cream",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8880242705345154,
        "bert_recall": 0.83896803855896,
        "bert_f1": 0.862799346446991,
        "meteor": 0.0,
        "sbert_similarity": 0.3189767003059387
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate feel he could do when out in cool places like Whispering Falls?",
      "prediction": "Nate feels relaxed and peaceful",
      "reference": "write a whole movie",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8674861192703247,
        "bert_recall": 0.8926730751991272,
        "bert_f1": 0.8798993825912476,
        "meteor": 0.0,
        "sbert_similarity": 0.09749402105808258
      }
    },
    {
      "sample_id": 3,
      "question": "What creative activity does Nate joke about pursuing after being inspired by their hikes with Jo?",
      "prediction": "video games",
      "reference": "Start thinking about a drama and publish a screenplay",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8927192687988281,
        "bert_recall": 0.8508157730102539,
        "bert_f1": 0.8712639808654785,
        "meteor": 0.0,
        "sbert_similarity": 0.16826269030570984
      }
    },
    {
      "sample_id": 3,
      "question": "Who invited Nate to join her on the trails sometime?",
      "prediction": "Joanna",
      "reference": "Joanna",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate do for Joanna on 25 May, 2022?",
      "prediction": "Nate offered support and encouragement to Joanna by expressing confidence in her talent and hard work's potential reward.",
      "reference": "get her a stuffed animal",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0909090909090909,
        "rouge1_f": 0.08333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08333333333333333,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.8477173447608948,
        "bert_recall": 0.8711608648300171,
        "bert_f1": 0.8592791557312012,
        "meteor": 0.07936507936507937,
        "sbert_similarity": 0.06240076571702957
      }
    },
    {
      "sample_id": 3,
      "question": "How does Nate describe the stuffed animal he got for Joanna?",
      "prediction": "a cute and cuddly friend that brightens his days.",
      "reference": "A stuffed animal to remind you of the good vibes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8812043070793152,
        "bert_recall": 0.877656877040863,
        "bert_f1": 0.8794270157814026,
        "meteor": 0.05050505050505051,
        "sbert_similarity": 0.4844591021537781
      }
    },
    {
      "sample_id": 3,
      "question": "What event is Nate organizing in June 2022?",
      "prediction": "road trip",
      "reference": "A gaming party",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8959935903549194,
        "bert_recall": 0.8921465873718262,
        "bert_f1": 0.8940659761428833,
        "meteor": 0.0,
        "sbert_similarity": 0.17755596339702606
      }
    },
    {
      "sample_id": 3,
      "question": "Who did Nate plan to invite to his gaming party in June 2022?",
      "prediction": "Joanna",
      "reference": "Tournament friends, old friends, teammates",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8816978931427002,
        "bert_recall": 0.846015989780426,
        "bert_f1": 0.863488495349884,
        "meteor": 0.0,
        "sbert_similarity": 0.21222242712974548
      }
    },
    {
      "sample_id": 3,
      "question": "What special items did Nate get for everyone at his gaming party?",
      "prediction": "Chess",
      "reference": "Custom controller decorations",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8286741971969604,
        "bert_recall": 0.863539457321167,
        "bert_f1": 0.8457476496696472,
        "meteor": 0.0,
        "sbert_similarity": -0.025119483470916748
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna write yesterday that appeared on the big screen?",
      "prediction": "Joanna's script",
      "reference": "screenplay bits",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8519259095191956,
        "bert_recall": 0.8735076189041138,
        "bert_f1": 0.862581729888916,
        "meteor": 0.0,
        "sbert_similarity": 0.4281066060066223
      }
    },
    {
      "sample_id": 3,
      "question": "What superhero is Joanna a fan of?",
      "prediction": "Spider-Man",
      "reference": "Spider-Man",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "Which superhero toy figure does Nate share a photo of?",
      "prediction": "A stuffed animal",
      "reference": "Iron Man",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8790050745010376,
        "bert_recall": 0.8732215166091919,
        "bert_f1": 0.8761037588119507,
        "meteor": 0.0,
        "sbert_similarity": 0.07015106827020645
      }
    },
    {
      "sample_id": 3,
      "question": "What is displayed on Joanna's cork board for motivation and creativity?",
      "prediction": "inspiring quotes and pictures",
      "reference": "inspiring quotes, photos, and little keepsakes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6,
        "rouge2_f": 0.25,
        "rougeL_f": 0.6,
        "bleu1": 0.27590958087858175,
        "bleu2": 0.18393972058572114,
        "bleu3": 0.0866334924408306,
        "bleu4": 0.069172428217611,
        "bert_precision": 0.970001757144928,
        "bert_recall": 0.924974262714386,
        "bert_f1": 0.9469529986381531,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.8814740180969238
      }
    },
    {
      "sample_id": 3,
      "question": "What does the photo on Joanna's cork board remind her of?",
      "prediction": "positive feelings towards Nate",
      "reference": "love and encouragement from her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8840506672859192,
        "bert_recall": 0.8607951402664185,
        "bert_f1": 0.8722679018974304,
        "meteor": 0.0,
        "sbert_similarity": 0.2782037854194641
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate make and share with his vegan diet group?",
      "prediction": "Vegan ice cream",
      "reference": "vegan ice cream",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9860954284667969,
        "bert_recall": 0.9860954284667969,
        "bert_f1": 0.9860954284667969,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "How many people attended the gaming party hosted by Nate in June 2022?",
      "prediction": "N/A",
      "reference": "7",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8194092512130737,
        "bert_recall": 0.8532542586326599,
        "bert_f1": 0.8359894156455994,
        "meteor": 0.0,
        "sbert_similarity": 0.3275170624256134
      }
    },
    {
      "sample_id": 3,
      "question": "What recipe Nate offer to share with Joanna?",
      "prediction": "recipe",
      "reference": "vegan ice cream recipe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.909270167350769,
        "bert_recall": 0.8370665907859802,
        "bert_f1": 0.8716757297515869,
        "meteor": 0.13513513513513511,
        "sbert_similarity": 0.5430232286453247
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna plan to do with the recipe Nate promised to share?",
      "prediction": "Joanna planned to try making it sometime.",
      "reference": "make it for her family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.2,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8798348903656006,
        "bert_recall": 0.8580722212791443,
        "bert_f1": 0.8688173294067383,
        "meteor": 0.3605769230769231,
        "sbert_similarity": 0.38485461473464966
      }
    },
    {
      "sample_id": 3,
      "question": "How many video game tournaments has Nate won by July 10, 2022?",
      "prediction": "Nate has won two video game tournaments as of July 10, 2022.",
      "reference": "Four",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8062593936920166,
        "bert_recall": 0.8617433905601501,
        "bert_f1": 0.8330785632133484,
        "meteor": 0.0,
        "sbert_similarity": 0.08793681859970093
      }
    },
    {
      "sample_id": 3,
      "question": "Where did Joanna go for a road trip for research?",
      "prediction": "Joanna went to California.",
      "reference": "Woodhaven",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8960394859313965,
        "bert_recall": 0.8903590440750122,
        "bert_f1": 0.8931902050971985,
        "meteor": 0.0,
        "sbert_similarity": 0.13933178782463074
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna discover at the library in Woodhaven?",
      "prediction": "Joanna discovered some lovely scenery and historic buildings.",
      "reference": "cool old book collection",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.87021803855896,
        "bert_recall": 0.861932635307312,
        "bert_f1": 0.8660554885864258,
        "meteor": 0.0,
        "sbert_similarity": 0.21444427967071533
      }
    },
    {
      "sample_id": 3,
      "question": "What specific themes are explored in Joanna's new book?",
      "prediction": "Themes like sisterhood, love, and chasing dreams were explored so well.",
      "reference": "loss, redemption, and forgiveness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.12838814775327384,
        "bleu3": 0.053149189341188084,
        "bleu4": 0.03342866121562732,
        "bert_precision": 0.8707616329193115,
        "bert_recall": 0.9005120992660522,
        "bert_f1": 0.8853870630264282,
        "meteor": 0.10638297872340427,
        "sbert_similarity": 0.26770803332328796
      }
    },
    {
      "sample_id": 3,
      "question": "What inspired Joanna's new script in July 2022?",
      "prediction": "Joanna contributed to another movie script and pitched it to producers.",
      "reference": "Woodhaven's interesting past and people",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.11764705882352942,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352942,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.86888188123703,
        "bert_recall": 0.8679165840148926,
        "bert_f1": 0.8683989644050598,
        "meteor": 0.0892857142857143,
        "sbert_similarity": 0.10914912819862366
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate do while Joanna was on her road trip?",
      "prediction": "took a road trip to heal and grow",
      "reference": "Won a video game tournament",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8746434450149536,
        "bert_recall": 0.8908703923225403,
        "bert_f1": 0.8826823234558105,
        "meteor": 0.09433962264150944,
        "sbert_similarity": 0.08694104850292206
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate do that he loves and can make money from?",
      "prediction": "Nate enjoys practicing for his future video game tournament, which he plans to enter after winning one.",
      "reference": "Competing in video game tournaments",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.2727272727272727,
        "rouge2_f": 0.2,
        "rougeL_f": 0.2727272727272727,
        "bleu1": 0.10526315789473682,
        "bleu2": 0.07647191129018724,
        "bleu3": 0.033656049636627425,
        "bleu4": 0.02153320854081468,
        "bert_precision": 0.8824968338012695,
        "bert_recall": 0.929793119430542,
        "bert_f1": 0.9055278301239014,
        "meteor": 0.3024193548387097,
        "sbert_similarity": 0.5409138202667236
      }
    },
    {
      "sample_id": 3,
      "question": "How did Joanna feel when someone wrote her a letter after reading her blog post?",
      "prediction": "It was awesome to realize my words had that kind of power.",
      "reference": "Touched",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8454238772392273,
        "bert_recall": 0.8825457096099854,
        "bert_f1": 0.8635860085487366,
        "meteor": 0.0,
        "sbert_similarity": 0.18804772198200226
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of impact does Joanna hope to have with her writing?",
      "prediction": "Joanna hopes to have a positive impact through her writing.",
      "reference": "share her stories and hopefully have an impact",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333326,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333326,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.05222329678670937,
        "bleu3": 0.03227679469808881,
        "bleu4": 0.024808415001701823,
        "bert_precision": 0.9109705686569214,
        "bert_recall": 0.9059491753578186,
        "bert_f1": 0.9084529280662537,
        "meteor": 0.24390243902439024,
        "sbert_similarity": 0.5336509943008423
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of content did Joanna share that someone wrote her a letter about?",
      "prediction": "A letter",
      "reference": "A blog post about a hard moment in her life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.00915781944436709,
        "bleu2": 0.004095501360654834,
        "bleu3": 0.003187734728290661,
        "bleu4": 0.002738826828955665,
        "bert_precision": 0.9271588921546936,
        "bert_recall": 0.8906722068786621,
        "bert_f1": 0.9085493683815002,
        "meteor": 0.05434782608695652,
        "sbert_similarity": 0.13687819242477417
      }
    },
    {
      "sample_id": 3,
      "question": "What motivates Joanna to keep writing even on tough days?",
      "prediction": "Joanna's supportive friends",
      "reference": "Knowing that her writing can make a difference",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8867970705032349,
        "bert_recall": 0.8836575746536255,
        "bert_f1": 0.8852245211601257,
        "meteor": 0.0,
        "sbert_similarity": 0.2745341658592224
      }
    },
    {
      "sample_id": 3,
      "question": "What type of ice cream does Joanna mention that Nate makes and is delicious?",
      "prediction": "Coconut milk ice cream",
      "reference": "Coconut milk ice cream",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "How did Nate feel about sharing his love for dairy-free desserts with Joanna?",
      "prediction": "Nate felt enthusiastic and rewarded.",
      "reference": "Happy to share",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8912801742553711,
        "bert_recall": 0.8859050273895264,
        "bert_f1": 0.8885844945907593,
        "meteor": 0.0,
        "sbert_similarity": 0.21299660205841064
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna share with her writers group in August 2022?",
      "prediction": "Joanna shared details about a character dealing with loss and self-discovery, taking a road trip to heal and grow.",
      "reference": "her book",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8307468891143799,
        "bert_recall": 0.8590066432952881,
        "bert_f1": 0.8446404337882996,
        "meteor": 0.0,
        "sbert_similarity": 0.4239799380302429
      }
    },
    {
      "sample_id": 3,
      "question": "How did Joanna celebrate after sharing her book with her writers group?",
      "prediction": "Joanna celebrated by making and eating a dessert, which she shared with Nate.",
      "reference": "making a delicious treat",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.13333333333333333,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.020202551616772847,
        "bleu4": 0.015718877363021206,
        "bert_precision": 0.8651029467582703,
        "bert_recall": 0.9000687599182129,
        "bert_f1": 0.8822395205497742,
        "meteor": 0.20408163265306123,
        "sbert_similarity": 0.4072504937648773
      }
    },
    {
      "sample_id": 3,
      "question": "How did Nate celebrate winning the international tournament?",
      "prediction": "Nate celebrated winning the international tournament by sharing their victory and progress updates.",
      "reference": "Taking time off to chill with pets",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8668712377548218,
        "bert_recall": 0.85877525806427,
        "bert_f1": 0.8628042936325073,
        "meteor": 0.0,
        "sbert_similarity": -0.01845824532210827
      }
    },
    {
      "sample_id": 3,
      "question": "Why is Joanna experimenting with dairy-free options in her dessert recipes?",
      "prediction": "Joanna's experimentations are driven by personal preference and dietary restrictions, aiming to create delicious desserts that cater to different tastes and preferences.",
      "reference": "lactose intolerance",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8229021430015564,
        "bert_recall": 0.83094322681427,
        "bert_f1": 0.8269031643867493,
        "meteor": 0.0,
        "sbert_similarity": 0.1858542114496231
      }
    },
    {
      "sample_id": 3,
      "question": "What substitution does Nate suggest for butter in dairy-free baking?",
      "prediction": "margarine or coconut oil instead of butter",
      "reference": "dairy-free margarine or coconut oil",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.5454545454545454,
        "rougeL_f": 0.6153846153846153,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.5345224838248488,
        "bleu3": 0.4888069165703076,
        "bleu4": 0.41113361690051975,
        "bert_precision": 0.9300235509872437,
        "bert_recall": 0.9210047721862793,
        "bert_f1": 0.9254921674728394,
        "meteor": 0.7632211538461539,
        "sbert_similarity": 0.8176300525665283
      }
    },
    {
      "sample_id": 3,
      "question": "What type of show did Nate host where he taught vegan ice cream recipes?",
      "prediction": "cooking show",
      "reference": "a cooking show",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9327027201652527,
        "bert_recall": 0.9256388545036316,
        "bert_f1": 0.9291573166847229,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.968105137348175
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's favorite dish from the cooking show he hosted?",
      "prediction": "cooking",
      "reference": "Coconut milk ice cream",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8936630487442017,
        "bert_recall": 0.8144091367721558,
        "bert_f1": 0.8521974086761475,
        "meteor": 0.0,
        "sbert_similarity": 0.27044838666915894
      }
    },
    {
      "sample_id": 3,
      "question": "What is one of Nate's favorite dairy-free treats besides coconut milk ice cream?",
      "prediction": "dairy-free chocolate mousse",
      "reference": "dairy-free chocolate mousse",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What dessert did Joanna share a photo of that has an almond flour crust, chocolate ganache, and fresh raspberries?",
      "prediction": "Chocolate Raspberry Tart",
      "reference": "chocolate raspberry tart",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9765582084655762,
        "bert_recall": 0.9765582084655762,
        "bert_f1": 0.9765582084655762,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of cake did Joanna share a photo of that she likes making for birthdays and special days?",
      "prediction": "A dairy-free chocolate cake with berries on a wooden table.",
      "reference": "chocolate cake with raspberries",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.46153846153846156,
        "rouge1_f": 0.39999999999999997,
        "rouge2_f": 0.30769230769230765,
        "rougeL_f": 0.39999999999999997,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.2335496832484569,
        "bleu3": 0.18545147779376364,
        "bleu4": 0.09329460218997072,
        "bert_precision": 0.8901535272598267,
        "bert_recall": 0.9194867014884949,
        "bert_f1": 0.9045823812484741,
        "meteor": 0.640096618357488,
        "sbert_similarity": 0.7383670806884766
      }
    },
    {
      "sample_id": 3,
      "question": "What two main ingredients are part of the dessert Joanna shared a photo of with blueberries, coconut milk, and a gluten-free crust?",
      "prediction": "blueberries, coconut milk",
      "reference": "blueberries and coconut milk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.4,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9886135458946228,
        "bert_recall": 0.9886135458946228,
        "bert_f1": 0.9886135458946228,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.9882222414016724
      }
    },
    {
      "sample_id": 3,
      "question": "What movie did Nate recently watch and enjoy on October 6, 2022?",
      "prediction": "The Lord of the Rings",
      "reference": "Little Women",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8710541725158691,
        "bert_recall": 0.8804773092269897,
        "bert_f1": 0.8757404088973999,
        "meteor": 0.0,
        "sbert_similarity": 0.15684768557548523
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna make for one of the ladies at her writing club?",
      "prediction": "Joanna made a beautiful drawing or sketch.",
      "reference": "a bookmark",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8839125633239746,
        "bert_recall": 0.8771820068359375,
        "bert_f1": 0.8805344104766846,
        "meteor": 0.2,
        "sbert_similarity": 0.17157284915447235
      }
    },
    {
      "sample_id": 3,
      "question": "What game did Nate play at the game convention he attended on 9 October, 2022?",
      "prediction": "Nate played games at the game convention that were generally relaxed and not competitive.",
      "reference": "Catan",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8383671045303345,
        "bert_recall": 0.8765766620635986,
        "bert_f1": 0.8570461869239807,
        "meteor": 0.0,
        "sbert_similarity": 0.06266622245311737
      }
    },
    {
      "sample_id": 3,
      "question": "What movie has Nate recently seen that blew his mind?",
      "prediction": "Inception",
      "reference": "\"Inception\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9575163125991821,
        "bert_recall": 0.9324093461036682,
        "bert_f1": 0.9447960257530212,
        "meteor": 0.0,
        "sbert_similarity": 0.9185203313827515
      }
    },
    {
      "sample_id": 3,
      "question": "What game has Nate been playing nonstop with a futuristic setting and gameplay on October 9, 2022?",
      "prediction": "Cyberpunk 2077",
      "reference": "Cyberpunk 2077",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate share a photo of when mentioning unwinding at home?",
      "prediction": "A bookcase filled with dvds and movies",
      "reference": "a bookcase filled with dvds and movies",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9930003881454468,
        "bert_recall": 0.9930003881454468,
        "bert_f1": 0.9930003881454468,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "How did Joanna describe the classic movie he watched?",
      "prediction": "Joanna described the classic movie as captivating and talented actors.",
      "reference": "gripping with great actors",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8635631799697876,
        "bert_recall": 0.851609468460083,
        "bert_f1": 0.8575446605682373,
        "meteor": 0.0,
        "sbert_similarity": 0.4955645203590393
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna recommend to make a living room comfy like hers?",
      "prediction": "Joanna recommends getting comfortable items such as pillows and blankets.",
      "reference": "couch for multiple people, fluffy blanket, lights that can be dimmed",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8755127787590027,
        "bert_recall": 0.8584182262420654,
        "bert_f1": 0.8668811917304993,
        "meteor": 0.0,
        "sbert_similarity": 0.4164329767227173
      }
    },
    {
      "sample_id": 3,
      "question": "What helps Joanna stay focused and brings her joy?",
      "prediction": "Tilly",
      "reference": "stuffed animal dog named Tilly",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.00579191356895553,
        "bleu3": 0.004007025187889067,
        "bleu4": 0.003257032351754417,
        "bert_precision": 0.9152151346206665,
        "bert_recall": 0.8378323912620544,
        "bert_f1": 0.8748158812522888,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.6590962409973145
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna do while she writes?",
      "prediction": "Creating stories and watching them come alive gives her happiness and fulfillment.",
      "reference": "have a stuffed animal dog named Tilly with her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09999999999999999,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8443952798843384,
        "bert_recall": 0.8366447687149048,
        "bert_f1": 0.840502142906189,
        "meteor": 0.10752688172043011,
        "sbert_similarity": 0.3038155436515808
      }
    },
    {
      "sample_id": 3,
      "question": "Why did Joanna name the stuffed animal dog Tilly?",
      "prediction": "Tilly was named after Joanna's friend.",
      "reference": "after a dog she had in Michigan",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8670626878738403,
        "bert_recall": 0.8437159061431885,
        "bert_f1": 0.8552299737930298,
        "meteor": 0.07246376811594203,
        "sbert_similarity": 0.15901587903499603
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna do after receiving a rejection from a production company?",
      "prediction": "Joanna kept going and set up meetings with movie producers, aligning with her dreams of achieving career goals.",
      "reference": "keep grinding and moving ahead",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0909090909090909,
        "rouge1_f": 0.08695652173913045,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913045,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.8428660035133362,
        "bert_recall": 0.8659632802009583,
        "bert_f1": 0.8542585372924805,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.15714550018310547
      }
    },
    {
      "sample_id": 3,
      "question": "How does Nate feel about Joanna's ability to bounce back from setbacks?",
      "prediction": "Nate feels supportive and encourages Joanna by reminding her of her strength and perseverance, which he sees as a sign of hard work and dedication.",
      "reference": "respect Joanna for being able to bounce back",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06896551724137931,
        "rouge1_f": 0.06060606060606061,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06060606060606061,
        "bleu1": 0.037037037037037035,
        "bleu2": 0.01193524790065722,
        "bleu3": 0.008697382097724691,
        "bleu4": 0.006980361417366381,
        "bert_precision": 0.857053279876709,
        "bert_recall": 0.8730932474136353,
        "bert_f1": 0.8649989366531372,
        "meteor": 0.10309278350515463,
        "sbert_similarity": 0.524319052696228
      }
    },
    {
      "sample_id": 3,
      "question": "What encouragement does Nate give to Joanna after her setback?",
      "prediction": "Nate expresses sympathy and encouragement towards Joanna after she has been rejected, emphasizing that rejection is normal but doesn't negate one's talent or dream aspirations.",
      "reference": "rejections don't define her, keep grinding and she'll find the perfect opportunity",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.05405405405405406,
        "rouge1_f": 0.1951219512195122,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0975609756097561,
        "bleu1": 0.13793103448275862,
        "bleu2": 0.022194838080923762,
        "bleu3": 0.012769569769300592,
        "bleu4": 0.009152541620698935,
        "bert_precision": 0.8671913146972656,
        "bert_recall": 0.8776293992996216,
        "bert_f1": 0.8723791241645813,
        "meteor": 0.07518796992481203,
        "sbert_similarity": 0.501387894153595
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate rely on for cheer and joy?",
      "prediction": "Turtles",
      "reference": "his turtles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9053637385368347,
        "bert_recall": 0.9053637385368347,
        "bert_f1": 0.9053637385368347,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7818246483802795
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna use to remember her dog from Michigan?",
      "prediction": "That name helps me remember her back when I used to be able to hold and squeeze animal without an allergic reaction!",
      "reference": "naming a stuffed animal dog Tilly",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07407407407407407,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.043478260869565216,
        "bleu2": 0.014058038927888336,
        "bleu3": 0.010263558503321437,
        "bleu4": 0.008282282660969604,
        "bert_precision": 0.8334833979606628,
        "bert_recall": 0.8437884449958801,
        "bert_f1": 0.8386042714118958,
        "meteor": 0.13157894736842107,
        "sbert_similarity": 0.40421074628829956
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna contribute to that was shown on the big screen on the Sunday before October 25, 2022?",
      "prediction": "Joanna's creations",
      "reference": "movie script",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8594424724578857,
        "bert_recall": 0.8748509287834167,
        "bert_f1": 0.8670782446861267,
        "meteor": 0.0,
        "sbert_similarity": 0.15848080813884735
      }
    },
    {
      "sample_id": 3,
      "question": "What inspires Joanna to create drawings of her characters?",
      "prediction": "Joanna creates drawings of her characters because she wants to explore and express herself through art, similar in concept and execution to other stories or themes that inspire her.",
      "reference": "visuals to help bring the characters alive in her head so she can write better",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2439024390243902,
        "rouge1_f": 0.22727272727272724,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13636363636363635,
        "bleu1": 0.16129032258064513,
        "bleu2": 0.023186944788008416,
        "bleu3": 0.01283717285499374,
        "bleu4": 0.009020544569112136,
        "bert_precision": 0.8611904382705688,
        "bert_recall": 0.873806357383728,
        "bert_f1": 0.8674525618553162,
        "meteor": 0.1524390243902439,
        "sbert_similarity": 0.5673936605453491
      }
    },
    {
      "sample_id": 3,
      "question": "Where does Joanna get her ideas for the characters from?",
      "prediction": "Joanna gets her ideas for the characters from her own experiences and self-discovery.",
      "reference": "people she knows, things she saw, her imagination",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473685,
        "rouge1_f": 0.09090909090909091,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8695957660675049,
        "bert_recall": 0.8567464351654053,
        "bert_f1": 0.8631232976913452,
        "meteor": 0.058823529411764705,
        "sbert_similarity": 0.49570372700691223
      }
    },
    {
      "sample_id": 3,
      "question": "How did Joanna feel on October 25, 2022 about seeing her characters come alive on the big screen?",
      "prediction": "Joanna's feelings were mixed - she was both relieved and anxious about seeing her characters in a movie.",
      "reference": "surreal and cool",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.8355593681335449,
        "bert_recall": 0.8393434882164001,
        "bert_f1": 0.8374471068382263,
        "meteor": 0.11111111111111113,
        "sbert_similarity": 0.13390928506851196
      }
    },
    {
      "sample_id": 3,
      "question": "What type of diet do Nate's turtles have?",
      "prediction": "vegetables, fruits, and insects",
      "reference": "combination of vegetables, fruits, and insects",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7499999999999999,
        "rougeL_f": 0.8,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.7165313105737893,
        "bert_precision": 0.9516983032226562,
        "bert_recall": 0.9355747103691101,
        "bert_f1": 0.9435675740242004,
        "meteor": 0.6842672413793103,
        "sbert_similarity": 0.8818746209144592
      }
    },
    {
      "sample_id": 3,
      "question": "What ingredient did Nate use to make the ice cream lactose-free?",
      "prediction": "coconut milk",
      "reference": "coconut milk",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna find in old notebooks last week that prompted her to reflect on her progress as a writer?",
      "prediction": "Joanna found old notebooks filled with her early writings, which inspired her to appreciate how far she had come since the beginning of her writing journey.",
      "reference": "early writings",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.07692307692307693,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.05143444998736396,
        "bleu3": 0.02251575432005011,
        "bleu4": 0.014203609079723397,
        "bert_precision": 0.8438493013381958,
        "bert_recall": 0.8768401741981506,
        "bert_f1": 0.8600284457206726,
        "meteor": 0.22727272727272732,
        "sbert_similarity": 0.5034378170967102
      }
    },
    {
      "sample_id": 3,
      "question": "What game is Nate currently playing and recommends to others on November 7, 2022?",
      "prediction": "Video games",
      "reference": "\"Xenoblade Chronicles\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9125477075576782,
        "bert_recall": 0.8321001529693604,
        "bert_f1": 0.8704692125320435,
        "meteor": 0.0,
        "sbert_similarity": 0.2698025703430176
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna receive from her brother that brought back childhood memories?",
      "prediction": "a teddy bear",
      "reference": "a handwritten letter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8987517952919006,
        "bert_recall": 0.9250024557113647,
        "bert_f1": 0.9116882085800171,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.25200390815734863
      }
    },
    {
      "sample_id": 3,
      "question": "What is the type of game \"Xenoblade Chronicles\" that Nate is playing?",
      "prediction": "Fantasy RPG",
      "reference": "fantasy RPG",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9816932678222656,
        "bert_recall": 0.9816932678222656,
        "bert_f1": 0.9816932678222656,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What dish did Nate make on 9 November, 2022?",
      "prediction": "Joannasays congratulates someone on a job well done and asks what the specific term for it is.",
      "reference": "Homemade coconut ice cream",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8224409818649292,
        "bert_recall": 0.8254618644714355,
        "bert_f1": 0.8239486813545227,
        "meteor": 0.0,
        "sbert_similarity": 0.017460625618696213
      }
    },
    {
      "sample_id": 3,
      "question": "What project is Joanna working on in her notebook on November 9, 2022?",
      "prediction": "Joanna's notebook contains a photo of a series that looks awesome.",
      "reference": "A suspenseful thriller set in a small Midwestern town",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11111111111111112,
        "rouge1_f": 0.1904761904761905,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1904761904761905,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8611990809440613,
        "bert_recall": 0.8541951179504395,
        "bert_f1": 0.8576827645301819,
        "meteor": 0.10869565217391303,
        "sbert_similarity": 0.2789112627506256
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate creating for YouTube on 9 November, 2022?",
      "prediction": "Creating gaming content for YouTube",
      "reference": "gaming content",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8669320344924927,
        "bert_recall": 0.9031049013137817,
        "bert_f1": 0.884648859500885,
        "meteor": 0.8152173913043478,
        "sbert_similarity": 0.7243716716766357
      }
    },
    {
      "sample_id": 3,
      "question": "What inspired Nate to start making gaming videos?",
      "prediction": "Joanna's project on small Midwestern town thriller",
      "reference": "Love of gaming and connecting with others who enjoy it too",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8343812823295593,
        "bert_recall": 0.8184495568275452,
        "bert_f1": 0.8263386487960815,
        "meteor": 0.0,
        "sbert_similarity": 0.010425839573144913
      }
    },
    {
      "sample_id": 3,
      "question": "What new content is Nate creating for YouTube?",
      "prediction": "Creating gaming content",
      "reference": "Gaming videos",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9255222082138062,
        "bert_recall": 0.9327805042266846,
        "bert_f1": 0.9291371703147888,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.6545695662498474
      }
    },
    {
      "sample_id": 3,
      "question": "What advice does Joanna give to Nate about making YouTube videos?",
      "prediction": "Joanna encourages Nate to connect with people through gaming videos.",
      "reference": "Watch other people's videos to understand what the audience likes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20000000000000004,
        "rouge1_f": 0.28571428571428564,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1904761904761905,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.05222329678670937,
        "bleu3": 0.03227679469808881,
        "bleu4": 0.024808415001701823,
        "bert_precision": 0.8750116229057312,
        "bert_recall": 0.8790905475616455,
        "bert_f1": 0.8770463466644287,
        "meteor": 0.05000000000000001,
        "sbert_similarity": 0.330506831407547
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna take a picture of near Fort Wayne last summer?",
      "prediction": "a sunset",
      "reference": "Sunset",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9230508208274841,
        "bert_recall": 0.9230508208274841,
        "bert_f1": 0.9230508208274841,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.9212944507598877
      }
    },
    {
      "sample_id": 3,
      "question": "What inspired Joanna to take a picture of the sunset in the field near Fort Wayne?",
      "prediction": "The beauty and surrounding nature was just incredible.",
      "reference": "The incredible sunset and surrounding beauty",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.7142857142857143,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.26352313834736496,
        "bleu3": 0.10206057820194947,
        "bleu4": 0.06376715693797415,
        "bert_precision": 0.9209522604942322,
        "bert_recall": 0.9159846305847168,
        "bert_f1": 0.9184616804122925,
        "meteor": 0.5090725806451614,
        "sbert_similarity": 0.6621701121330261
      }
    },
    {
      "sample_id": 3,
      "question": "Why did Nate get a third turtle?",
      "prediction": "Nate got a third turtle because he saw another turtle at a pet store and felt compelled to acquire it.",
      "reference": "He saw another one at a pet store and wanted to get it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6451612903225806,
        "rouge1_f": 0.6060606060606061,
        "rouge2_f": 0.3870967741935484,
        "rougeL_f": 0.6060606060606061,
        "bleu1": 0.47619047619047616,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.314636248794599,
        "bleu4": 0.2404315522172745,
        "bert_precision": 0.9305930137634277,
        "bert_recall": 0.9542571306228638,
        "bert_f1": 0.9422765374183655,
        "meteor": 0.7065693430656934,
        "sbert_similarity": 0.548437774181366
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate want to do when he goes over to Joanna's place?",
      "prediction": "try out different flavors and toppings",
      "reference": "Watch one of Joanna's movies together or go to the park",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.850490927696228,
        "bert_recall": 0.8493775129318237,
        "bert_f1": 0.8499338626861572,
        "meteor": 0.0,
        "sbert_similarity": 0.25518664717674255
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate take to the beach in Tampa?",
      "prediction": "a picture",
      "reference": "turtles",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8975571393966675,
        "bert_recall": 0.8745805025100708,
        "bert_f1": 0.8859198689460754,
        "meteor": 0.0,
        "sbert_similarity": 0.2934796214103699
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate love most about having turtles?",
      "prediction": "Turtles bring Nate joy and peace",
      "reference": "They make him feel calm and don't require much looking after",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352942,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0613132401952404,
        "bleu2": 0.021239529438966134,
        "bleu3": 0.016579663309725004,
        "bleu4": 0.01501861529550426,
        "bert_precision": 0.8677190542221069,
        "bert_recall": 0.8505963087081909,
        "bert_f1": 0.8590723872184753,
        "meteor": 0.04761904761904762,
        "sbert_similarity": 0.1461184024810791
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate share a photo of as a part of his experimentation in November 2022?",
      "prediction": "a sunset",
      "reference": "colorful bowls of coconut milk ice cream",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8730665445327759,
        "bert_recall": 0.8160156607627869,
        "bert_f1": 0.8435776233673096,
        "meteor": 0.0,
        "sbert_similarity": 0.12699614465236664
      }
    },
    {
      "sample_id": 3,
      "question": "What color did Joanna choose for her hair?",
      "prediction": "purple",
      "reference": "purple",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna's favorite movie trilogy?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Lord of the Rings",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8842611908912659,
        "bert_recall": 0.8674640655517578,
        "bert_f1": 0.8757821321487427,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.17697061598300934
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna's favorite book series about?",
      "prediction": "Not mentioned in the conversation",
      "reference": "dragons",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8660027384757996,
        "bert_recall": 0.861403226852417,
        "bert_f1": 0.8636968731880188,
        "meteor": 0.0,
        "sbert_similarity": 0.09364505112171173
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of lighting does Joanna's gaming room have?",
      "prediction": "Not mentioned in the conversation",
      "reference": "red and purple lighting",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8776457905769348,
        "bert_recall": 0.8527690172195435,
        "bert_f1": 0.8650285601615906,
        "meteor": 0.0,
        "sbert_similarity": -0.002829102799296379
      }
    },
    {
      "sample_id": 3,
      "question": "What game was the second tournament that Joanna won based on?",
      "prediction": "Valorant",
      "reference": "Street Fighter",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8654836416244507,
        "bert_recall": 0.8773471117019653,
        "bert_f1": 0.8713749647140503,
        "meteor": 0.0,
        "sbert_similarity": 0.24174122512340546
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's third screenplay about?",
      "prediction": "road trip",
      "reference": "loss, identity, and connection",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8743904829025269,
        "bert_recall": 0.8725011944770813,
        "bert_f1": 0.8734447956085205,
        "meteor": 0.0,
        "sbert_similarity": 0.05284113436937332
      }
    },
    {
      "sample_id": 3,
      "question": "What type of movies does Nate hate watching the most?",
      "prediction": "Not mentioned in the conversation",
      "reference": "action and sci-fi",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8836545944213867,
        "bert_recall": 0.8687281608581543,
        "bert_f1": 0.8761277794837952,
        "meteor": 0.0,
        "sbert_similarity": 0.12533436715602875
      }
    },
    {
      "sample_id": 3,
      "question": "What genre is Joanna's first novella?",
      "prediction": "drama and romance",
      "reference": "drama and romance",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What are Nate's plans for his finished screenplay in January 2022?",
      "prediction": "submit it to film festivals and get producers and directors to check it out",
      "reference": "submit it to film festivals and get producers and directors to check it out",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9998177842565598,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "For how long has Nate had his snakes?",
      "prediction": "Not mentioned in the conversation",
      "reference": "3 years",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.891991913318634,
        "bert_recall": 0.8985151052474976,
        "bert_f1": 0.8952415585517883,
        "meteor": 0.0,
        "sbert_similarity": 0.13386738300323486
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate think of the caramel ice cream he made?",
      "prediction": "Super good, rich and creamy",
      "reference": "Super good, rich and creamy",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 3,
      "question": "What flavor of cake did Nate make for his friend on 25 February, 2022?",
      "prediction": "Chocolate and Vanilla Swirl",
      "reference": "chocolate and vanilla swirl",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9638712406158447,
        "bert_recall": 0.9705341458320618,
        "bert_f1": 0.967191219329834,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What was Nate's audition for?",
      "prediction": "writing gig",
      "reference": "writing gig",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "Why did Joanna choose the hair color she did?",
      "prediction": "Bright and bold - like her personality",
      "reference": "Bright and bold - like her",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.923076923076923,
        "rouge1_f": 0.9090909090909091,
        "rouge2_f": 0.888888888888889,
        "rougeL_f": 0.9090909090909091,
        "bleu1": 0.8571428571428571,
        "bleu2": 0.8451542547285166,
        "bleu3": 0.8313759247878635,
        "bleu4": 0.8091067115702212,
        "bert_precision": 0.9562859535217285,
        "bert_recall": 0.9711865186691284,
        "bert_f1": 0.9636786580085754,
        "meteor": 0.981329690346084,
        "sbert_similarity": 0.9262881875038147
      }
    },
    {
      "sample_id": 3,
      "question": "What are the main ingredients of the ice cream recipe shared by Joanna?",
      "prediction": "Coconut milk, vanilla extract, sugar, salt",
      "reference": "Coconut milk, vanilla extract, sugar, salt",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What is Nate's project called in the writers group?",
      "prediction": "Not mentioned in the conversation",
      "reference": "\"Finding Home\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8948009610176086,
        "bert_recall": 0.8874703049659729,
        "bert_f1": 0.8911205530166626,
        "meteor": 0.0,
        "sbert_similarity": 0.07752814888954163
      }
    },
    {
      "sample_id": 3,
      "question": "Which activity helps Nate escape and numbs his mind?",
      "prediction": "Watching music",
      "reference": "watching fantasy and sci-fi movies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.11156508007421491,
        "bleu2": 0.04989342059223037,
        "bleu3": 0.03883455907572571,
        "bleu4": 0.03336574130316504,
        "bert_precision": 0.9040789604187012,
        "bert_recall": 0.8647385835647583,
        "bert_f1": 0.8839713335037231,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.42988938093185425
      }
    },
    {
      "sample_id": 3,
      "question": "What filling did Nate use in the cake he made recently in May 2022?",
      "prediction": "Not mentioned in the conversation or strawberry",
      "reference": "strawberry",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8291572332382202,
        "bert_recall": 0.8099735379219055,
        "bert_f1": 0.8194530606269836,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.7056014537811279
      }
    },
    {
      "sample_id": 3,
      "question": "Who did Joanna plan to invite to her gaming party in June 2022?",
      "prediction": "Tournament friends",
      "reference": "Tournament friends, old friends, teammates",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.0820849986238988,
        "bleu2": 0.0820849986238988,
        "bleu3": 0.038394038428908775,
        "bleu4": 0.025957555738330737,
        "bert_precision": 0.9700362682342529,
        "bert_recall": 0.9193828105926514,
        "bert_f1": 0.9440305829048157,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.8655997514724731
      }
    },
    {
      "sample_id": 3,
      "question": "What special items did Joanna get for everyone at her gaming party?",
      "prediction": "Custom controller decorations",
      "reference": "Custom controller decorations",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What supervillain is Joanna a fan of?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Spider-Man",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8877221345901489,
        "bert_recall": 0.9018644094467163,
        "bert_f1": 0.8947373628616333,
        "meteor": 0.0,
        "sbert_similarity": 0.07908296585083008
      }
    },
    {
      "sample_id": 3,
      "question": "Which superhero toy figure does Joanna share a photo of?",
      "prediction": "Iron Man",
      "reference": "Iron Man",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna make and share with her vegan diet group?",
      "prediction": "desserts",
      "reference": "vegan ice cream",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8911959528923035,
        "bert_recall": 0.8779898881912231,
        "bert_f1": 0.8845436573028564,
        "meteor": 0.0,
        "sbert_similarity": 0.46571579575538635
      }
    },
    {
      "sample_id": 3,
      "question": "How many people attended the gaming party hosted by Joanna in June 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "7",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8550496101379395,
        "bert_recall": 0.854331910610199,
        "bert_f1": 0.8546905517578125,
        "meteor": 0.0,
        "sbert_similarity": 0.12071841955184937
      }
    },
    {
      "sample_id": 3,
      "question": "Where did Nate go for a road trip for research?",
      "prediction": "Woodhaven",
      "reference": "Woodhaven",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna discover at the museum in Woodhaven?",
      "prediction": "cool old book collection",
      "reference": "cool old book collection",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 3,
      "question": "What specific themes are explored in Nate's new book?",
      "prediction": "loss, redemption, and forgiveness",
      "reference": "loss, redemption, and forgiveness",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "How did Nate feel when someone wrote him a letter after reading his blog post?",
      "prediction": "Touched",
      "reference": "Touched",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What kind of impact does Joanna hope to have with her painting?",
      "prediction": "share her stories",
      "reference": "share her stories and hopefully have an impact",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.4444444444444445,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.18887560283756186,
        "bleu2": 0.18887560283756186,
        "bleu3": 0.18887560283756186,
        "bleu4": 0.10621255679580059,
        "bert_precision": 0.974246621131897,
        "bert_recall": 0.8970350623130798,
        "bert_f1": 0.9340479373931885,
        "meteor": 0.3925925925925926,
        "sbert_similarity": 0.8145943880081177
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate share with his writers group in August 2022?",
      "prediction": "A photo of a notebook with a notepad and a piece of paper",
      "reference": "her book",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8410094976425171,
        "bert_recall": 0.8662694096565247,
        "bert_f1": 0.8534525036811829,
        "meteor": 0.0,
        "sbert_similarity": 0.17433546483516693
      }
    },
    {
      "sample_id": 3,
      "question": "How did Nate celebrate after sharing his book with a writers group?",
      "prediction": "Celebrated by finishing a presentation for producers and showing it to Joanne.",
      "reference": "making a delicious treat",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8431456089019775,
        "bert_recall": 0.8800081610679626,
        "bert_f1": 0.8611825704574585,
        "meteor": 0.10416666666666667,
        "sbert_similarity": 0.29405492544174194
      }
    },
    {
      "sample_id": 3,
      "question": "How did Joanna celebrate winning the international tournament?",
      "prediction": "Taking time off to chill with pets",
      "reference": "Taking time off to chill with pets",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What substitution does Nate suggest for sugar in dairy-free baking?",
      "prediction": "coconut oil",
      "reference": "dairy-free margarine or coconut oil",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.5,
        "bleu1": 0.22313016014842982,
        "bleu2": 0.22313016014842982,
        "bleu3": 0.10436581698246099,
        "bleu4": 0.07055995207471726,
        "bert_precision": 0.9276849031448364,
        "bert_recall": 0.8901005983352661,
        "bert_f1": 0.9085041880607605,
        "meteor": 0.39893617021276595,
        "sbert_similarity": 0.6799030303955078
      }
    },
    {
      "sample_id": 3,
      "question": "What type of show did Joanna host where she taught vegan ice cream recipes?",
      "prediction": "cooking show",
      "reference": "a cooking show",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9327027201652527,
        "bert_recall": 0.9256388545036316,
        "bert_f1": 0.9291573166847229,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.968105137348175
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna's favorite dish from the cooking show she hosted?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Coconut milk ice cream",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8849745392799377,
        "bert_recall": 0.820184588432312,
        "bert_f1": 0.8513486385345459,
        "meteor": 0.0,
        "sbert_similarity": 0.12384446710348129
      }
    },
    {
      "sample_id": 3,
      "question": "What dessert did Nate share a photo of that has an almond flour crust, chocolate ganache, and fresh raspberries?",
      "prediction": "chocolate raspberry tart",
      "reference": "chocolate raspberry tart",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What two main ingredients are part of the dessert Nate shared a photo of with blueberries, coconut milk, and a gluten-free crust?",
      "prediction": "blueberries, coconut milk",
      "reference": "blueberries and coconut milk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.4,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.75,
        "bleu2": 0.49999999999999994,
        "bleu3": 0.23549424823785386,
        "bleu4": 0.1880301546543197,
        "bert_precision": 0.9886135458946228,
        "bert_recall": 0.9886135458946228,
        "bert_f1": 0.9886135458946228,
        "meteor": 0.48076923076923084,
        "sbert_similarity": 0.9882222414016724
      }
    },
    {
      "sample_id": 3,
      "question": "What movie did Joanna recently watch and enjoy on October 6, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Little Women",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8782232999801636,
        "bert_recall": 0.8408116698265076,
        "bert_f1": 0.8591104745864868,
        "meteor": 0.0,
        "sbert_similarity": 0.08908423036336899
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate make for one of the ladies at his writing club?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a bookmark",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8886566162109375,
        "bert_recall": 0.885771632194519,
        "bert_f1": 0.8872117400169373,
        "meteor": 0.0,
        "sbert_similarity": 0.08981933444738388
      }
    },
    {
      "sample_id": 3,
      "question": "What game has Joanna been playing nonstop with a futuristic setting and gameplay on October 9, 2022?",
      "prediction": "Cyberpunk 2077",
      "reference": "Cyberpunk 2077",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "How did Nate describe the classic movie he watched?",
      "prediction": "gripping with great actors",
      "reference": "gripping with great actors",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate recommend to make a living room comfy like his?",
      "prediction": "couch for multiple people",
      "reference": "couch for multiple people, fluffy blanket, lights that can be dimmed",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.5333333333333333,
        "rouge2_f": 0.4615384615384615,
        "rougeL_f": 0.5333333333333333,
        "bleu1": 0.10539922456186433,
        "bleu2": 0.10539922456186433,
        "bleu3": 0.10539922456186433,
        "bleu4": 0.10539922456186433,
        "bert_precision": 0.9687456488609314,
        "bert_recall": 0.8929295539855957,
        "bert_f1": 0.9292938113212585,
        "meteor": 0.28586839266450914,
        "sbert_similarity": 0.6999293565750122
      }
    },
    {
      "sample_id": 3,
      "question": "What helps Joanna stay distracted and brings her sadness?",
      "prediction": "Not mentioned in the conversation",
      "reference": "stuffed animal dog named Tilly",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8730998039245605,
        "bert_recall": 0.8161594867706299,
        "bert_f1": 0.8436699509620667,
        "meteor": 0.10000000000000002,
        "sbert_similarity": 0.061712879687547684
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate do while he writes?",
      "prediction": "Not mentioned in the conversation",
      "reference": "have a stuffed animal dog named Tilly with him",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8890969157218933,
        "bert_recall": 0.8322522640228271,
        "bert_f1": 0.8597359657287598,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.018039513379335403
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate do after receiving a rejection from a production company?",
      "prediction": "Not mentioned in the conversation or keep grinding and moving ahead",
      "reference": "keep grinding and moving ahead",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.625,
        "rouge1_f": 0.625,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.625,
        "bleu1": 0.45454545454545453,
        "bleu2": 0.4264014327112209,
        "bleu3": 0.3964882289652318,
        "bleu4": 0.3508439695638686,
        "bert_precision": 0.8997876644134521,
        "bert_recall": 0.954982578754425,
        "bert_f1": 0.9265639185905457,
        "meteor": 0.8892857142857143,
        "sbert_similarity": 0.6714510321617126
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna rely on for cheer and joy?",
      "prediction": "Joanna relies on creating new items that bring her pleasure and satisfaction.",
      "reference": "her turtles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8374240398406982,
        "bert_recall": 0.8567677736282349,
        "bert_f1": 0.8469854593276978,
        "meteor": 0.16666666666666669,
        "sbert_similarity": 0.17536301910877228
      }
    },
    {
      "sample_id": 3,
      "question": "What does Nate use to remember his dog from Michigan?",
      "prediction": "the name of the dog",
      "reference": "stuffed animal dog Tilly",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.898732602596283,
        "bert_recall": 0.8658210039138794,
        "bert_f1": 0.8819699287414551,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.4703427255153656
      }
    },
    {
      "sample_id": 3,
      "question": "What inspires Joanna to create music for her characters?",
      "prediction": "Not mentioned in the conversation or visuals",
      "reference": "visuals to help bring the characters alive in her head so she can write better",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.27272727272727276,
        "rouge1_f": 0.27272727272727276,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0.13667423885313018,
        "bleu2": 0.026952523378317727,
        "bleu3": 0.017170427022579027,
        "bleu4": 0.013863533666885183,
        "bert_precision": 0.8666426539421082,
        "bert_recall": 0.8523348569869995,
        "bert_f1": 0.8594292402267456,
        "meteor": 0.10563380281690142,
        "sbert_similarity": 0.39988967776298523
      }
    },
    {
      "sample_id": 3,
      "question": "What type of diet do Joanna's turtles have?",
      "prediction": "vegetables, fruits, and insects",
      "reference": "combination of vegetables, fruits, and insects",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7499999999999999,
        "rougeL_f": 0.8,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.7165313105737893,
        "bert_precision": 0.9516983032226562,
        "bert_recall": 0.9355747103691101,
        "bert_f1": 0.9435675740242004,
        "meteor": 0.6842672413793103,
        "sbert_similarity": 0.8818746209144592
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate find in old notebooks last week that prompted him to reflect on her progress as a writer?",
      "prediction": "Joanna's favorite memories",
      "reference": "early writings",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8703944683074951,
        "bert_recall": 0.8778712749481201,
        "bert_f1": 0.8741168975830078,
        "meteor": 0.0,
        "sbert_similarity": 0.06626034528017044
      }
    },
    {
      "sample_id": 3,
      "question": "What game is Joanna currently playing and recommends to others on November 7, 2022?",
      "prediction": "Xenoblade Chronicles",
      "reference": "\"Xenoblade Chronicles\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9844865202903748,
        "bert_recall": 0.9638261198997498,
        "bert_f1": 0.9740467667579651,
        "meteor": 0.0,
        "sbert_similarity": 0.9620691537857056
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate receive from his brother that brought back childhood memories?",
      "prediction": "A handwritten letter",
      "reference": "a handwritten letter",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9782467484474182,
        "bert_recall": 0.9782467484474182,
        "bert_f1": 0.9782467484474182,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What is the type of game \"Xenoblade Chronicles\" that Joanna is playing?",
      "prediction": "fantasy RPG",
      "reference": "fantasy RPG",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 3,
      "question": "What project is Nate working on in his notebook on November 9, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A suspenseful thriller set in a small Midwestern town",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.08986579282344431,
        "bleu2": 0.03177235575108143,
        "bleu3": 0.02545508208534111,
        "bleu4": 0.02414176971688927,
        "bert_precision": 0.8836244940757751,
        "bert_recall": 0.8538800477981567,
        "bert_f1": 0.8684977293014526,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.014259420335292816
      }
    },
    {
      "sample_id": 3,
      "question": "What is Joanna creating for YouTube on 9 November, 2022?",
      "prediction": "gaming content",
      "reference": "gaming content",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What inspired Joanna to start making gaming videos?",
      "prediction": "Love of gaming and connecting with others who enjoy it too",
      "reference": "Love of gaming and connecting with others who enjoy it too",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What new content is Nate creating for television?",
      "prediction": "Gaming videos",
      "reference": "Gaming videos",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.000000238418579,
        "bert_recall": 1.000000238418579,
        "bert_f1": 1.000000238418579,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What did Nate take a picture of near Fort Wayne last summer?",
      "prediction": "Sunset",
      "reference": "Sunset",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "Why did Joanna get a third turtle?",
      "prediction": "She saw another one at a pet store and wanted to get it",
      "reference": "She saw another one at a pet store and wanted to get it",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997724169321802,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 3,
      "question": "What did Joanna take to the beach in Tampa?",
      "prediction": "turtles",
      "reference": "turtles",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 3,
      "question": "What does Joanna love most about having turtles?",
      "prediction": "They make her feel calm and don't require much looking after.",
      "reference": "They make her feel calm and don't require much looking after",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.9230769230769231,
        "bleu2": 0.9198662110077999,
        "bleu3": 0.9170619907921821,
        "bleu4": 0.912167909070388,
        "bert_precision": 0.9948723912239075,
        "bert_recall": 0.9944465160369873,
        "bert_f1": 0.994659423828125,
        "meteor": 0.9086363636363638,
        "sbert_similarity": 0.9921355247497559
      }
    },
    {
      "sample_id": 4,
      "question": "what are John's goals with regards to his basketball career?",
      "prediction": "Winning a championship and making a difference outside of the court, like through charity or inspiring people.",
      "reference": "improve shooting percentage, win a championship",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.2608695652173913,
        "rouge2_f": 0.19047619047619047,
        "rougeL_f": 0.2608695652173913,
        "bleu1": 0.15789473684210523,
        "bleu2": 0.09365858115816939,
        "bleu3": 0.03847452395409486,
        "bleu4": 0.023830412565256146,
        "bert_precision": 0.8754197955131531,
        "bert_recall": 0.9124815464019775,
        "bert_f1": 0.8935664892196655,
        "meteor": 0.2112676056338028,
        "sbert_similarity": 0.47314053773880005
      }
    },
    {
      "sample_id": 4,
      "question": "What are John's goals for his career that are not related to his basketball skills?",
      "prediction": "None provided",
      "reference": "get endorsements, build his brand, do charity work",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8082529306411743,
        "bert_recall": 0.822644829750061,
        "bert_f1": 0.8153853416442871,
        "meteor": 0.0,
        "sbert_similarity": 0.02991190366446972
      }
    },
    {
      "sample_id": 4,
      "question": "What items does John collect?",
      "prediction": "John collects books.",
      "reference": "sneakers, fantasy movie DVDs, jerseys",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.891331136226654,
        "bert_recall": 0.8261977434158325,
        "bert_f1": 0.8575294017791748,
        "meteor": 0.0,
        "sbert_similarity": 0.22527647018432617
      }
    },
    {
      "sample_id": 4,
      "question": "Would Tim enjoy reading books by C. S. Lewis or John Greene?",
      "prediction": "Tim would likely enjoy reading books by both C. S. Lewis and John Greene, as they are both well-known authors in the fantasy and science fiction genres.",
      "reference": "C. S.Lewis",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.21428571428571425,
        "rouge1_f": 0.19354838709677416,
        "rouge2_f": 0.13793103448275862,
        "rougeL_f": 0.19354838709677416,
        "bleu1": 0.034482758620689655,
        "bleu2": 0.011097419040461881,
        "bleu3": 0.008081583568827886,
        "bleu4": 0.006471824245088331,
        "bert_precision": 0.8401961326599121,
        "bert_recall": 0.9376310110092163,
        "bert_f1": 0.8862435817718506,
        "meteor": 0.11111111111111113,
        "sbert_similarity": 0.47241196036338806
      }
    },
    {
      "sample_id": 4,
      "question": "What books has Tim read?",
      "prediction": "Harry Potter series and The Hobbit",
      "reference": "Harry Potter, Game of Thrones, the Name of the Wind, The Alchemist, The Hobbit, A Dance with Dragons, and the Wheel of Time.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4347826086956522,
        "rouge1_f": 0.3448275862068966,
        "rouge2_f": 0.22222222222222218,
        "rougeL_f": 0.27586206896551724,
        "bleu1": 0.015263032407278483,
        "bleu2": 0.01295111245998798,
        "bleu3": 0.004313227611098457,
        "bleu4": 0.0026167923528216314,
        "bert_precision": 0.9242017269134521,
        "bert_recall": 0.8601759672164917,
        "bert_f1": 0.8910402059555054,
        "meteor": 0.11997913406364111,
        "sbert_similarity": 0.7151104807853699
      }
    },
    {
      "sample_id": 4,
      "question": "Based on Tim's collections, what is a shop that he would enjoy visiting in New York city?",
      "prediction": "The Empire State Building",
      "reference": "House of MinaLima",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.876397430896759,
        "bert_recall": 0.8360558152198792,
        "bert_f1": 0.8557514548301697,
        "meteor": 0.0,
        "sbert_similarity": 0.338390976190567
      }
    },
    {
      "sample_id": 4,
      "question": "In which month's game did John achieve a career-high score in points?",
      "prediction": "last week",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8512589931488037,
        "bert_recall": 0.8421604037284851,
        "bert_f1": 0.8466851711273193,
        "meteor": 0.0,
        "sbert_similarity": 0.3504534959793091
      }
    },
    {
      "sample_id": 4,
      "question": "Which geographical locations has Tim been to?",
      "prediction": "Tim has been to London, New York City, Paris, Barcelona, and Rome.",
      "reference": "California, London, the Smoky Mountains",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.10502100630210073,
        "bleu3": 0.043244627469917335,
        "bleu4": 0.026920508809559315,
        "bert_precision": 0.8580601215362549,
        "bert_recall": 0.8490995168685913,
        "bert_f1": 0.8535563349723816,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.37619635462760925
      }
    },
    {
      "sample_id": 4,
      "question": "Which outdoor gear company likely signed up John for an endorsement deal?",
      "prediction": "Nike",
      "reference": "Under Armour",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9234309196472168,
        "bert_recall": 0.9234309196472168,
        "bert_f1": 0.9234309196472168,
        "meteor": 0.0,
        "sbert_similarity": 0.5063055753707886
      }
    },
    {
      "sample_id": 4,
      "question": "Which endorsement deals has John been offered?",
      "prediction": "Nike and Gatorade",
      "reference": "basketball shoes and gear deal with Nike, potential sponsorship with Gatorade, Moxie a popular beverage company, outdoor gear company",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3157894736842105,
        "rouge1_f": 0.2727272727272727,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.0017761035457343791,
        "bleu2": 0.0003971488263340479,
        "bleu3": 0.0003091209096320501,
        "bleu4": 0.00026558942724365923,
        "bert_precision": 0.9050827026367188,
        "bert_recall": 0.8459497690200806,
        "bert_f1": 0.8745177388191223,
        "meteor": 0.028735632183908046,
        "sbert_similarity": 0.6999343633651733
      }
    },
    {
      "sample_id": 4,
      "question": "When was John in Seattle for a game?",
      "prediction": "4:21 pm on 16 July, 2023",
      "reference": "early August, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8490586280822754,
        "bert_recall": 0.9162939190864563,
        "bert_f1": 0.8813959360122681,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7067695260047913
      }
    },
    {
      "sample_id": 4,
      "question": "What sports does John like besides basketball?",
      "prediction": "football",
      "reference": "surfing",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8807239532470703,
        "bert_recall": 0.866580605506897,
        "bert_f1": 0.8735949993133545,
        "meteor": 0.0,
        "sbert_similarity": 0.38223665952682495
      }
    },
    {
      "sample_id": 4,
      "question": "What year did John start surfing?",
      "prediction": "2023",
      "reference": "2018",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8986917734146118,
        "bert_recall": 0.902716875076294,
        "bert_f1": 0.90069979429245,
        "meteor": 0.0,
        "sbert_similarity": 0.5888933539390564
      }
    },
    {
      "sample_id": 4,
      "question": "What does Tim do to escape reality?",
      "prediction": "Tim reads fantasy books and watches movies that transport him into alternate realities.",
      "reference": "Read fantasy books.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.375,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.375,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.12838814775327384,
        "bleu3": 0.053149189341188084,
        "bleu4": 0.03342866121562732,
        "bert_precision": 0.8862466216087341,
        "bert_recall": 0.9582257270812988,
        "bert_f1": 0.9208316802978516,
        "meteor": 0.46875,
        "sbert_similarity": 0.46473994851112366
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of writing does Tim do?",
      "prediction": "writing about different fantasy novels",
      "reference": "comments on favorite books in a fantasy literature forum, articles on fantasy novels, studying characters, themes, and making book recommendations, writing a fantasy novel",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.24,
        "rouge1_f": 0.20689655172413793,
        "rouge2_f": 0.07407407407407408,
        "rougeL_f": 0.20689655172413793,
        "bleu1": 0.004937848229412018,
        "bleu2": 0.003187367326435421,
        "bleu3": 0.0014323415433433353,
        "bleu4": 0.0010348359057272274,
        "bert_precision": 0.8849465847015381,
        "bert_recall": 0.8529776334762573,
        "bert_f1": 0.8686680793762207,
        "meteor": 0.11563599798893916,
        "sbert_similarity": 0.5080060958862305
      }
    },
    {
      "sample_id": 4,
      "question": "Who is Anthony?",
      "prediction": "Anthony",
      "reference": "likely John's friend, colleague or family",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8680979609489441,
        "bert_recall": 0.79185950756073,
        "bert_f1": 0.8282279968261719,
        "meteor": 0.0,
        "sbert_similarity": 0.2723030745983124
      }
    },
    {
      "sample_id": 4,
      "question": "After how many weeks did Tim reconnect with the fellow Harry Potter fan from California?",
      "prediction": "7",
      "reference": "three weeks",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8675495982170105,
        "bert_recall": 0.8542810678482056,
        "bert_f1": 0.8608642220497131,
        "meteor": 0.0,
        "sbert_similarity": 0.2737019956111908
      }
    },
    {
      "sample_id": 4,
      "question": "How many games has John mentioned winning?",
      "prediction": "2",
      "reference": "6",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9706783294677734,
        "bert_recall": 0.9706783294677734,
        "bert_f1": 0.9706783294677734,
        "meteor": 0.0,
        "sbert_similarity": 0.6696947813034058
      }
    },
    {
      "sample_id": 4,
      "question": "What authors has Tim read books from?",
      "prediction": "Timsays mentions enjoying Harry Potter and reading as a lifelong passion, indicating Tim has read books by J.K.Rowling.",
      "reference": "J.K. Rowling, R.R. Martin, Patrick Rothfuss, Paulo Coelho, and J. R. R. Tolkien.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25806451612903225,
        "rouge1_f": 0.2285714285714286,
        "rouge2_f": 0.12121212121212122,
        "rougeL_f": 0.17142857142857143,
        "bleu1": 0.15,
        "bleu2": 0.02809757434745082,
        "bleu3": 0.017056169390452146,
        "bleu4": 0.012673718536830811,
        "bert_precision": 0.8517545461654663,
        "bert_recall": 0.8485327959060669,
        "bert_f1": 0.8501406311988831,
        "meteor": 0.03703703703703704,
        "sbert_similarity": 0.45867595076560974
      }
    },
    {
      "sample_id": 4,
      "question": "What is a prominent charity organization that John might want to work with and why?",
      "prediction": "A prominent charity organization that John might want to work with could be a local organization helping disadvantaged kids with sports and school, as he hopes to use his platform for positive impact on the community.",
      "reference": "Good Sports, because they work with Nike, Gatorade, and Under Armour and they aim toprovide youth sports opportunities for kids ages 3-18 in high-need communities.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.03278688524590164,
        "rougeL_f": 0.15873015873015875,
        "bleu1": 0.21052631578947367,
        "bleu2": 0.07543142864047142,
        "bleu3": 0.026037825650146784,
        "bleu4": 0.014577516118523641,
        "bert_precision": 0.8657229542732239,
        "bert_recall": 0.8548194169998169,
        "bert_f1": 0.8602365851402283,
        "meteor": 0.11494252873563217,
        "sbert_similarity": 0.39287710189819336
      }
    },
    {
      "sample_id": 4,
      "question": "Which city was John in before traveling to Chicago?",
      "prediction": "Seattle",
      "reference": "Seattle",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "Which US cities does John mention visiting to Tim?",
      "prediction": "John mentions visiting Mount Washington and Acadia National Park in New England.",
      "reference": "Seattle, Chicago, New York",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8461449146270752,
        "bert_recall": 0.8709406852722168,
        "bert_f1": 0.8583638072013855,
        "meteor": 0.10416666666666667,
        "sbert_similarity": 0.27437102794647217
      }
    },
    {
      "sample_id": 4,
      "question": "When did John meet with his teammates after returning from Chicago?",
      "prediction": "15th",
      "reference": "August 15, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.868323802947998,
        "bert_recall": 0.8646623492240906,
        "bert_f1": 0.8664892315864563,
        "meteor": 0.0,
        "sbert_similarity": 0.5171427130699158
      }
    },
    {
      "sample_id": 4,
      "question": "When is Tim attending a book conference?",
      "prediction": "7:54 pm on 17 August, 2023",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8531646728515625,
        "bert_recall": 0.9168901443481445,
        "bert_f1": 0.8838802576065063,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6767369508743286
      }
    },
    {
      "sample_id": 4,
      "question": "Where was John between August 11 and August 15 2023?",
      "prediction": "Barcelona",
      "reference": "Chicago",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8591861724853516,
        "bert_recall": 0.9062227010726929,
        "bert_f1": 0.8820778131484985,
        "meteor": 0.0,
        "sbert_similarity": 0.5155904293060303
      }
    },
    {
      "sample_id": 4,
      "question": "What similar sports collectible do Tim and John own?",
      "prediction": "basketball jerseys",
      "reference": "signed basketball",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8920506238937378,
        "bert_recall": 0.8961971998214722,
        "bert_f1": 0.8941190838813782,
        "meteor": 0.25,
        "sbert_similarity": 0.6567060947418213
      }
    },
    {
      "sample_id": 4,
      "question": "Which TV series does Tim mention watching?",
      "prediction": "The Wheel of Time",
      "reference": "That, Wheel of Time",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 0.75,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.75,
        "bleu1": 0.5841005873035536,
        "bleu2": 0.5506953149031837,
        "bleu3": 0.49288611328092413,
        "bleu4": 0.3096787331587729,
        "bert_precision": 0.965535044670105,
        "bert_recall": 0.9399579167366028,
        "bert_f1": 0.9525747895240784,
        "meteor": 0.7361111111111112,
        "sbert_similarity": 0.8438622355461121
      }
    },
    {
      "sample_id": 4,
      "question": "Which popular time management technique does Tim use to prepare for exams?",
      "prediction": "Tim uses the 25-minute study-then-10-minute break routine.",
      "reference": "Pomodoro technique",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8450469970703125,
        "bert_recall": 0.8279066681861877,
        "bert_f1": 0.8363890647888184,
        "meteor": 0.0,
        "sbert_similarity": 0.11437660455703735
      }
    },
    {
      "sample_id": 4,
      "question": "Which popular music composer's tunes does Tim enjoy playing on the piano?",
      "prediction": "Jazz and film scores",
      "reference": "John Williams",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8730432391166687,
        "bert_recall": 0.9048060178756714,
        "bert_f1": 0.8886408805847168,
        "meteor": 0.0,
        "sbert_similarity": 0.37323814630508423
      }
    },
    {
      "sample_id": 4,
      "question": "What schools did John play basketball in and how many years was he with his team during high school?",
      "prediction": "John played basketball at a school called University of Minnesota, and he was part of their basketball program for four years while attending high school.",
      "reference": "Middle school, high school, and college and he was with his high school team for 4 years.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.39999999999999997,
        "rouge1_f": 0.38095238095238104,
        "rouge2_f": 0.15,
        "rougeL_f": 0.28571428571428564,
        "bleu1": 0.37037037037037035,
        "bleu2": 0.23870495801314426,
        "bleu3": 0.13429762959577146,
        "bleu4": 0.05551277111446364,
        "bert_precision": 0.896754264831543,
        "bert_recall": 0.9132177829742432,
        "bert_f1": 0.9049111604690552,
        "meteor": 0.35656959412978667,
        "sbert_similarity": 0.6187048554420471
      }
    },
    {
      "sample_id": 4,
      "question": "Which cities has John been to?",
      "prediction": "Barcelona",
      "reference": "Seattle, Chicago, New York, and Paris.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8317158222198486,
        "bert_recall": 0.8516613245010376,
        "bert_f1": 0.8415704369544983,
        "meteor": 0.0,
        "sbert_similarity": 0.36619406938552856
      }
    },
    {
      "sample_id": 4,
      "question": "What month did Tim plan on going to Universal Studios?",
      "prediction": "next month",
      "reference": "September, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8609551191329956,
        "bert_recall": 0.8447448015213013,
        "bert_f1": 0.8527728915214539,
        "meteor": 0.0,
        "sbert_similarity": 0.5306335687637329
      }
    },
    {
      "sample_id": 4,
      "question": "Which US states might Tim be in during September 2023 based on his plans of visiting Universal Studios?",
      "prediction": "California",
      "reference": "California or Florida",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.959608256816864,
        "bert_recall": 0.9006597995758057,
        "bert_f1": 0.9291999936103821,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7366414070129395
      }
    },
    {
      "sample_id": 4,
      "question": "When does John plan on traveling with his team on a team trip?",
      "prediction": "next month",
      "reference": "October, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8793140649795532,
        "bert_recall": 0.8456660509109497,
        "bert_f1": 0.8621618747711182,
        "meteor": 0.0,
        "sbert_similarity": 0.5741462111473083
      }
    },
    {
      "sample_id": 4,
      "question": "What could John do after his basketball career?",
      "prediction": "John could continue mentoring young athletes and promoting fair opportunities for them.",
      "reference": "become a basketball coach since he likes giving back and leadership",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8670675158500671,
        "bert_recall": 0.8693530559539795,
        "bert_f1": 0.8682087659835815,
        "meteor": 0.04504504504504504,
        "sbert_similarity": 0.5106083154678345
      }
    },
    {
      "sample_id": 4,
      "question": "What outdoor activities does John enjoy?",
      "prediction": "hiking and surfing",
      "reference": "Hiking, surfing",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9775969982147217,
        "bert_recall": 0.9775969982147217,
        "bert_f1": 0.9775969982147217,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.9766215085983276
      }
    },
    {
      "sample_id": 4,
      "question": "Who is Tim and John's favorite basketball player?",
      "prediction": "Timsays",
      "reference": "LeBron James",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.845109760761261,
        "bert_recall": 0.8572053909301758,
        "bert_f1": 0.8511146306991577,
        "meteor": 0.0,
        "sbert_similarity": 0.11066040396690369
      }
    },
    {
      "sample_id": 4,
      "question": "Which week did Tim visit the UK for the Harry Potter Conference?",
      "prediction": "last week",
      "reference": "The week before October 13th, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.024893534183931972,
        "bleu2": 0.01113272692709733,
        "bleu3": 0.008665161385860336,
        "bleu4": 0.007444903200446295,
        "bert_precision": 0.8802475929260254,
        "bert_recall": 0.8505483269691467,
        "bert_f1": 0.8651431798934937,
        "meteor": 0.08928571428571427,
        "sbert_similarity": 0.5125997066497803
      }
    },
    {
      "sample_id": 4,
      "question": "which country has Tim visited most frequently in his travels?",
      "prediction": "Italy",
      "reference": "UK",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9386536478996277,
        "bert_recall": 0.9386536478996277,
        "bert_f1": 0.9386536478996277,
        "meteor": 0.0,
        "sbert_similarity": 0.4784335494041443
      }
    },
    {
      "sample_id": 4,
      "question": "What year did Tim go to the Smoky Mountains?",
      "prediction": "]2023",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8935105204582214,
        "bert_recall": 0.9398766160011292,
        "bert_f1": 0.9161072373390198,
        "meteor": 0.0,
        "sbert_similarity": 0.7877450585365295
      }
    },
    {
      "sample_id": 4,
      "question": "Has Tim been to North Carolina and/or Tennesee states in the US?",
      "prediction": "Yes",
      "reference": "Yes",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of fiction stories does Tim write?",
      "prediction": "fantasy novels",
      "reference": "Fantasy stories with plot twists",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.11156508007421491,
        "bleu2": 0.04989342059223037,
        "bleu3": 0.03883455907572571,
        "bleu4": 0.03336574130316504,
        "bert_precision": 0.9545286893844604,
        "bert_recall": 0.9153398275375366,
        "bert_f1": 0.9345236420631409,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.6288447976112366
      }
    },
    {
      "sample_id": 4,
      "question": "What has John cooked?",
      "prediction": "John made a tasty soup recently",
      "reference": "Soup, a slow cooker meal, and honey garlic chicken with roasted veg.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0743767200494766,
        "bleu2": 0.01821850128630489,
        "bleu3": 0.012640619670384382,
        "bleu4": 0.01083278567718,
        "bert_precision": 0.8933709263801575,
        "bert_recall": 0.8463941812515259,
        "bert_f1": 0.8692482709884644,
        "meteor": 0.043859649122807015,
        "sbert_similarity": 0.5163559913635254
      }
    },
    {
      "sample_id": 4,
      "question": "What does John like about Lebron James?",
      "prediction": "John likes that LeBron James is passionate and has a strong bond with teammates, creating a second family feel.",
      "reference": "His heart, determination, skills, and leadership.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08333333333333333,
        "rouge1_f": 0.08,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.026726124191242442,
        "bleu3": 0.016210225632977325,
        "bleu4": 0.012021577610863726,
        "bert_precision": 0.8593527674674988,
        "bert_recall": 0.8935899138450623,
        "bert_f1": 0.8761370182037354,
        "meteor": 0.0684931506849315,
        "sbert_similarity": 0.25761860609054565
      }
    },
    {
      "sample_id": 4,
      "question": "When did John and his wife go on a European vacation?",
      "prediction": "6 November 2023",
      "reference": "November, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9344724416732788,
        "bert_recall": 0.9490068554878235,
        "bert_f1": 0.9416835308074951,
        "meteor": 0.0,
        "sbert_similarity": 0.9017546772956848
      }
    },
    {
      "sample_id": 4,
      "question": "Which country was Tim visiting in the second week of November?",
      "prediction": "Italy",
      "reference": "UK",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9386536478996277,
        "bert_recall": 0.9386536478996277,
        "bert_f1": 0.9386536478996277,
        "meteor": 0.0,
        "sbert_similarity": 0.4784335494041443
      }
    },
    {
      "sample_id": 4,
      "question": "Where was Tim in the week before 16 November 2023?",
      "prediction": "New York City",
      "reference": "UK",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8772516250610352,
        "bert_recall": 0.9120968580245972,
        "bert_f1": 0.8943349719047546,
        "meteor": 0.0,
        "sbert_similarity": 0.4424913823604584
      }
    },
    {
      "sample_id": 4,
      "question": "When did John get married at a greenhouse?",
      "prediction": "2023",
      "reference": "last week of September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.00579191356895553,
        "bleu3": 0.004007025187889067,
        "bleu4": 0.003257032351754417,
        "bert_precision": 0.9144033193588257,
        "bert_recall": 0.8429609537124634,
        "bert_f1": 0.8772299885749817,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.6963152885437012
      }
    },
    {
      "sample_id": 4,
      "question": "When did John get an ankle injury in 2023?",
      "prediction": "November 16, 2023",
      "reference": "around November 16, 2023",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7788007830714049,
        "bleu2": 0.7788007830714049,
        "bleu3": 0.7788007830714049,
        "bleu4": 0.7788007830714049,
        "bert_precision": 0.97687828540802,
        "bert_recall": 0.9574410915374756,
        "bert_f1": 0.9670619964599609,
        "meteor": 0.754985754985755,
        "sbert_similarity": 0.9445656538009644
      }
    },
    {
      "sample_id": 4,
      "question": "How many times has John injured his ankle?",
      "prediction": "1",
      "reference": "two times",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8628672361373901,
        "bert_recall": 0.8568217158317566,
        "bert_f1": 0.8598338961601257,
        "meteor": 0.0,
        "sbert_similarity": 0.3717716336250305
      }
    },
    {
      "sample_id": 4,
      "question": "Which book was John reading during his recovery from an ankle injury?",
      "prediction": "There is no information provided about the specific book John was reading during his recovery from an ankle injury.",
      "reference": "The Alchemist",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.8357860445976257,
        "bert_recall": 0.879858672618866,
        "bert_f1": 0.857256293296814,
        "meteor": 0.13513513513513514,
        "sbert_similarity": 0.1575573980808258
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of yoga for building core strength might John benefit from?",
      "prediction": "Warrior II",
      "reference": "Hatha Yoga",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8971190452575684,
        "bert_recall": 0.8733299970626831,
        "bert_f1": 0.8850647211074829,
        "meteor": 0.0,
        "sbert_similarity": 0.20665974915027618
      }
    },
    {
      "sample_id": 4,
      "question": "What does John do to supplement his basketball training?",
      "prediction": "John incorporates strength training into his routine.",
      "reference": "Yoga, strength training",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.9231299757957458,
        "bert_recall": 0.9101644158363342,
        "bert_f1": 0.9166013598442078,
        "meteor": 0.5514705882352942,
        "sbert_similarity": 0.4502910375595093
      }
    },
    {
      "sample_id": 4,
      "question": "What other exercises can help John with his basketball performance?",
      "prediction": "strength training",
      "reference": "Sprinting, long-distance running, and boxing.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8885424137115479,
        "bert_recall": 0.8544954061508179,
        "bert_f1": 0.871186375617981,
        "meteor": 0.0,
        "sbert_similarity": 0.5093328356742859
      }
    },
    {
      "sample_id": 4,
      "question": "When did John take a trip to the Rocky Mountains?",
      "prediction": "2023",
      "reference": "2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9776392579078674,
        "bert_recall": 0.9776392579078674,
        "bert_f1": 0.9776392579078674,
        "meteor": 0.0,
        "sbert_similarity": 0.9072221517562866
      }
    },
    {
      "sample_id": 4,
      "question": "When did John start playing professionally?",
      "prediction": "2023",
      "reference": "May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9407316446304321,
        "bert_recall": 0.8919609785079956,
        "bert_f1": 0.9156973361968994,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.814176619052887
      }
    },
    {
      "sample_id": 4,
      "question": "When did Tim start playing the violin?",
      "prediction": "4 months",
      "reference": "August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8915711641311646,
        "bert_recall": 0.8821889758110046,
        "bert_f1": 0.8868553042411804,
        "meteor": 0.0,
        "sbert_similarity": 0.38103991746902466
      }
    },
    {
      "sample_id": 4,
      "question": "What instruments does Tim play?",
      "prediction": "Timsays: I'm gonna give it a shot and see how it goes.",
      "reference": "piano, violin",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8250160813331604,
        "bert_recall": 0.8369954824447632,
        "bert_f1": 0.8309625387191772,
        "meteor": 0.0,
        "sbert_similarity": 0.14272068440914154
      }
    },
    {
      "sample_id": 4,
      "question": "When did John attend the Harry Potter trivia?",
      "prediction": "2023",
      "reference": "August 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9504785537719727,
        "bert_recall": 0.9481024742126465,
        "bert_f1": 0.9492890238761902,
        "meteor": 0.0,
        "sbert_similarity": 0.8288976550102234
      }
    },
    {
      "sample_id": 4,
      "question": "Which career-high performances did John achieve in 2023?",
      "prediction": "career-high assists",
      "reference": "highest point score, highest assist",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9145669341087341,
        "bert_recall": 0.8954808115959167,
        "bert_f1": 0.9049232006072998,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.6642884612083435
      }
    },
    {
      "sample_id": 4,
      "question": "When did John achieve a career-high assist performance?",
      "prediction": "Last Friday",
      "reference": "December 11, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8723300695419312,
        "bert_recall": 0.8655827641487122,
        "bert_f1": 0.8689432740211487,
        "meteor": 0.0,
        "sbert_similarity": 0.3228481411933899
      }
    },
    {
      "sample_id": 4,
      "question": "What books has John read?",
      "prediction": "The Hobbit",
      "reference": "inpsiring book on dreaming big, The Alchemist, fantasy series, non-fiction books on personal development, Dune",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.00010173418450532208,
        "bleu2": 4.5496910437881206e-05,
        "bleu3": 3.5412534061415736e-05,
        "bleu4": 3.042561776171364e-05,
        "bert_precision": 0.8651033639907837,
        "bert_recall": 0.8093937635421753,
        "bert_f1": 0.8363218307495117,
        "meteor": 0.0364963503649635,
        "sbert_similarity": 0.2781790494918823
      }
    },
    {
      "sample_id": 4,
      "question": "What does John do to share his knowledge?",
      "prediction": "John shares his knowledge by participating in seminars and helping aspiring professors.",
      "reference": "gives seminars, mentors younger players.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.893058180809021,
        "bert_recall": 0.9115095138549805,
        "bert_f1": 0.9021894931793213,
        "meteor": 0.0,
        "sbert_similarity": 0.5017936825752258
      }
    },
    {
      "sample_id": 4,
      "question": "When did John organize a basketball camp for kids?",
      "prediction": "a year ago",
      "reference": "summer 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8164128661155701,
        "bert_recall": 0.8160441517829895,
        "bert_f1": 0.816228449344635,
        "meteor": 0.0,
        "sbert_similarity": 0.27808669209480286
      }
    },
    {
      "sample_id": 4,
      "question": "Which month was John in Italy?",
      "prediction": "last month",
      "reference": "December, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8733218908309937,
        "bert_recall": 0.8373103141784668,
        "bert_f1": 0.8549370169639587,
        "meteor": 0.0,
        "sbert_similarity": 0.45239847898483276
      }
    },
    {
      "sample_id": 4,
      "question": "What fantasy movies does Tim like?",
      "prediction": "Star Wars",
      "reference": "Lord of the Rings, Harry Potter, and Star Wars.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.006737946999085467,
        "bleu3": 0.003151574591602894,
        "bleu4": 0.002130725927060655,
        "bert_precision": 0.9232527017593384,
        "bert_recall": 0.8926440477371216,
        "bert_f1": 0.9076904058456421,
        "meteor": 0.06024096385542168,
        "sbert_similarity": 0.5998539328575134
      }
    },
    {
      "sample_id": 4,
      "question": "What is a Star Wars book that Tim might enjoy?",
      "prediction": "Star Wars",
      "reference": "Star Wars: Jedi Apprentice by Judy Blundell and David Farland. It is a highly rated and immersive series about his favorite movies.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913042,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0909090909090909,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 1.013009359863071e-05,
        "bleu2": 1.013009359863071e-05,
        "bleu3": 4.738200760608077e-06,
        "bleu4": 3.2034168682364606e-06,
        "bert_precision": 0.9392775297164917,
        "bert_recall": 0.8197358846664429,
        "bert_f1": 0.8754447102546692,
        "meteor": 0.025,
        "sbert_similarity": 0.5762946009635925
      }
    },
    {
      "sample_id": 4,
      "question": "What would be a good hobby related to his travel dreams for Tim to pick up?",
      "prediction": "playing the piano",
      "reference": "Writing a travel blog.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9160411953926086,
        "bert_recall": 0.8947546482086182,
        "bert_f1": 0.9052728414535522,
        "meteor": 0.0,
        "sbert_similarity": 0.06039934605360031
      }
    },
    {
      "sample_id": 4,
      "question": "What day did Tim get into his study abroad program?",
      "prediction": "7 January 2024",
      "reference": "Januarty 5, 2024",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9136905670166016,
        "bert_recall": 0.8441829085350037,
        "bert_f1": 0.8775625228881836,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.6316741108894348
      }
    },
    {
      "sample_id": 4,
      "question": "When will Tim leave for Ireland?",
      "prediction": "Next month",
      "reference": "February, 2024",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8779703378677368,
        "bert_recall": 0.8634918928146362,
        "bert_f1": 0.8706709742546082,
        "meteor": 0.0,
        "sbert_similarity": 0.5266788005828857
      }
    },
    {
      "sample_id": 4,
      "question": "Which Star Wars-related locations would Tim enjoy during his visit to Ireland?",
      "prediction": "Harry Potter places",
      "reference": "Skellig Michael, Malin Head, Loop Head, Ceann Sib\u00e9al, and Brow Head because they are Star Wars filming locations.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8481040000915527,
        "bert_recall": 0.7953259944915771,
        "bert_f1": 0.8208675384521484,
        "meteor": 0.0,
        "sbert_similarity": 0.24335655570030212
      }
    },
    {
      "sample_id": 4,
      "question": "Which team did John sign with on 21 May, 2023?",
      "prediction": "Minnesota Wolves",
      "reference": "The Minnesota Wolves",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9684875011444092,
        "bert_recall": 0.941759467124939,
        "bert_f1": 0.9549365043640137,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.9709622263908386
      }
    },
    {
      "sample_id": 4,
      "question": "What is John's position on the team he signed with?",
      "prediction": "Johnsays : The Minnesota Wolves! I can't wait to play with them!",
      "reference": "shooting guard",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8094911575317383,
        "bert_recall": 0.8244732618331909,
        "bert_f1": 0.8169135451316833,
        "meteor": 0.0,
        "sbert_similarity": 0.1413232386112213
      }
    },
    {
      "sample_id": 4,
      "question": "What challenge did John encounter during pre-season training?",
      "prediction": "Fitting into the new team's style of play was a challenge during pre-season.",
      "reference": "fitting into the new team's style of play",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.761904761904762,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.7272727272727273,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6,
        "bleu2": 0.5855400437691198,
        "bleu3": 0.5726224931332392,
        "bleu4": 0.5512003357447272,
        "bert_precision": 0.9262784123420715,
        "bert_recall": 0.9642784595489502,
        "bert_f1": 0.9448965787887573,
        "meteor": 0.9402573529411764,
        "sbert_similarity": 0.7067050933837891
      }
    },
    {
      "sample_id": 4,
      "question": "What aspects of the Harry Potter universe will be discussed in John's fan project collaborations?",
      "prediction": "characters, spells, and magical creatures",
      "reference": "characters, spells, magical creatures",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.888888888888889,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.888888888888889,
        "bleu1": 0.8571428571428571,
        "bleu2": 0.7559289460184544,
        "bleu3": 0.6144368316876522,
        "bleu4": 0.488923022434901,
        "bert_precision": 0.9768998622894287,
        "bert_recall": 0.9856752157211304,
        "bert_f1": 0.9812679290771484,
        "meteor": 0.9146341463414633,
        "sbert_similarity": 0.9764874577522278
      }
    },
    {
      "sample_id": 4,
      "question": "What forum did Tim join recently?",
      "prediction": "A fantasy literature forum",
      "reference": "fantasy literature forum",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.75,
        "bleu2": 0.7071067811865475,
        "bleu3": 0.63287829698514,
        "bleu4": 0.3976353643835253,
        "bert_precision": 0.9639002084732056,
        "bert_recall": 0.9639002084732056,
        "bert_f1": 0.9639002084732056,
        "meteor": 0.9498207885304659,
        "sbert_similarity": 0.9792490601539612
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of picture did Tim share as part of their Harry Potter book collection?",
      "prediction": "[Image: a photo of a collection of harry potter books on a desk]",
      "reference": "MinaLima's creation from the Harry Potter films",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.1904761904761905,
        "rouge2_f": 0.10526315789473685,
        "rougeL_f": 0.1904761904761905,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.026012784404037925,
        "bert_precision": 0.8650431632995605,
        "bert_recall": 0.8297572731971741,
        "bert_f1": 0.8470329642295837,
        "meteor": 0.24671052631578952,
        "sbert_similarity": 0.40839457511901855
      }
    },
    {
      "sample_id": 4,
      "question": "What was the highest number of points John scored in a game recently?",
      "prediction": "40 points",
      "reference": "40 points",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What did John celebrate at a restaurant with teammates?",
      "prediction": "celebrated at a restaurant with teammates by laughing and reliving the intense moments, which felt amazing.",
      "reference": "a tough win",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473684,
        "rouge1_f": 0.10526315789473684,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473684,
        "bleu1": 0.05555555555555554,
        "bleu2": 0.018077538151554672,
        "bleu3": 0.013254163846385744,
        "bleu4": 0.010802314890908065,
        "bert_precision": 0.8522679209709167,
        "bert_recall": 0.8811262845993042,
        "bert_f1": 0.8664568662643433,
        "meteor": 0.11627906976744187,
        "sbert_similarity": 0.16108697652816772
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of deals did John sign with Nike and Gatorade?",
      "prediction": "John signed up Nike for a basketball shoe and gear deal, as well as in talks with Gatorade about a potential sponsorship.",
      "reference": "basketball shoe and gear deal with Nike, potential sponsorship deal with Gatorade",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5882352941176471,
        "rouge2_f": 0.37499999999999994,
        "rougeL_f": 0.4705882352941177,
        "bleu1": 0.4583333333333333,
        "bleu2": 0.3457820374040623,
        "bleu3": 0.2570751704707034,
        "bleu4": 0.19850823739068116,
        "bert_precision": 0.9081013798713684,
        "bert_recall": 0.9450409412384033,
        "bert_f1": 0.9262030124664307,
        "meteor": 0.5172684458398744,
        "sbert_similarity": 0.8182722330093384
      }
    },
    {
      "sample_id": 4,
      "question": "Which city is John excited to have a game at?",
      "prediction": "Seattle",
      "reference": "Seattle",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "How long has John been surfing?",
      "prediction": "five years ago",
      "reference": "five years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.848494827747345,
        "bert_recall": 0.8750934600830078,
        "bert_f1": 0.8615889549255371,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.7120558023452759
      }
    },
    {
      "sample_id": 4,
      "question": "How does John feel while surfing?",
      "prediction": "John loves the connection to nature and appreciates it as a positive experience.",
      "reference": "super exciting and free-feeling",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8665914535522461,
        "bert_recall": 0.8902491927146912,
        "bert_f1": 0.8782610297203064,
        "meteor": 0.10204081632653061,
        "sbert_similarity": 0.3227100372314453
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of articles has Tim been writing about for the online magazine?",
      "prediction": "fantasy novels",
      "reference": "different fantasy novels, characters, themes, and book recommendations",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.011108996538242306,
        "bleu2": 0.011108996538242306,
        "bleu3": 0.005196068065373761,
        "bleu4": 0.0035129731579771513,
        "bert_precision": 0.9405443668365479,
        "bert_recall": 0.8785383105278015,
        "bert_f1": 0.9084845185279846,
        "meteor": 0.06756756756756756,
        "sbert_similarity": 0.8079648613929749
      }
    },
    {
      "sample_id": 4,
      "question": "Which two fantasy novels does Tim particularly enjoy writing about?",
      "prediction": "The Name of the Wind and Harry Potter",
      "reference": "Harry Potter and Game of Thrones",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.5,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.1083896554122787,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.878754198551178,
        "bert_recall": 0.9313557147979736,
        "bert_f1": 0.9042906761169434,
        "meteor": 0.5090725806451614,
        "sbert_similarity": 0.6613955497741699
      }
    },
    {
      "sample_id": 4,
      "question": "What did Anthony and John end up playing during the charity event?",
      "prediction": "Scoring points",
      "reference": "an intense Harry Potter trivia contest",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8667158484458923,
        "bert_recall": 0.8466625809669495,
        "bert_f1": 0.8565718531608582,
        "meteor": 0.0,
        "sbert_similarity": 0.21321359276771545
      }
    },
    {
      "sample_id": 4,
      "question": "What did John share with the person he skyped about?",
      "prediction": "John shared that it was nice chatting and that they would talk to each other later.",
      "reference": "Characters from Harry Potter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8206340074539185,
        "bert_recall": 0.8325598239898682,
        "bert_f1": 0.8265538811683655,
        "meteor": 0.0,
        "sbert_similarity": 0.15776875615119934
      }
    },
    {
      "sample_id": 4,
      "question": "How did John describe the team bond?",
      "prediction": "John described the team bond as awesome and made all the hard work worth it.",
      "reference": "Awesome",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8413726687431335,
        "bert_recall": 0.8754265904426575,
        "bert_f1": 0.8580618500709534,
        "meteor": 0.20833333333333337,
        "sbert_similarity": 0.14814747869968414
      }
    },
    {
      "sample_id": 4,
      "question": "How did John get introduced to basketball?",
      "prediction": "John learned about basketball through participating in basketball camps for kids in his hometown.",
      "reference": "Dad signed him up for a local league",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.09090909090909091,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8888835310935974,
        "bert_recall": 0.9038937091827393,
        "bert_f1": 0.8963257670402527,
        "meteor": 0.058139534883720936,
        "sbert_similarity": 0.3940422236919403
      }
    },
    {
      "sample_id": 4,
      "question": "What is John's number one goal in his basketball career?",
      "prediction": "scoring",
      "reference": "Winning a championship",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8798404932022095,
        "bert_recall": 0.8880647420883179,
        "bert_f1": 0.8839334845542908,
        "meteor": 0.0,
        "sbert_similarity": 0.4691397249698639
      }
    },
    {
      "sample_id": 4,
      "question": "What organization is John teaming up with for his charity work?",
      "prediction": "local organization",
      "reference": "A local organization helping disadvantaged kids with sports and school",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.19999999999999998,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.01831563888873418,
        "bleu3": 0.008566867943387383,
        "bleu4": 0.00579191356895553,
        "bert_precision": 0.9062671661376953,
        "bert_recall": 0.8370227813720703,
        "bert_f1": 0.870269775390625,
        "meteor": 0.20380434782608695,
        "sbert_similarity": 0.4699903130531311
      }
    },
    {
      "sample_id": 4,
      "question": "When did John meet back up with his teammates after his trip in August 2023?",
      "prediction": "16 July, 2023",
      "reference": "Aug 15th",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8646599650382996,
        "bert_recall": 0.9024109244346619,
        "bert_f1": 0.8831322193145752,
        "meteor": 0.0,
        "sbert_similarity": 0.5384010672569275
      }
    },
    {
      "sample_id": 4,
      "question": "What did John's teammates give him when they met on Aug 15th?",
      "prediction": "They signed it to show our friendship and appreciation.",
      "reference": "a basketball with autographs on it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8980829119682312,
        "bert_recall": 0.8846753239631653,
        "bert_f1": 0.8913286924362183,
        "meteor": 0.07936507936507937,
        "sbert_similarity": 0.34097719192504883
      }
    },
    {
      "sample_id": 4,
      "question": "Why did John's teammates sign the basketball they gave him?",
      "prediction": "It was a reminder of their friendship and appreciation.",
      "reference": "to show their friendship and appreciation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.5333333333333333,
        "rouge2_f": 0.4615384615384615,
        "rougeL_f": 0.5333333333333333,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.9360311627388,
        "bert_recall": 0.9325965642929077,
        "bert_f1": 0.9343106746673584,
        "meteor": 0.46737213403880074,
        "sbert_similarity": 0.7021045684814453
      }
    },
    {
      "sample_id": 4,
      "question": "What is the main intention behind Tim wanting to attend the book conference?",
      "prediction": "Tim wants to learn more about literature and create a stronger bond with it.",
      "reference": "to learn more about literature and create a stronger bond to it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.88,
        "rouge1_f": 0.8461538461538461,
        "rouge2_f": 0.7500000000000001,
        "rougeL_f": 0.8461538461538461,
        "bleu1": 0.7333333333333333,
        "bleu2": 0.6866065623255951,
        "bleu3": 0.6647305464149239,
        "bleu4": 0.6413865258981679,
        "bert_precision": 0.9587516188621521,
        "bert_recall": 0.9700502157211304,
        "bert_f1": 0.9643678665161133,
        "meteor": 0.8163934426229508,
        "sbert_similarity": 0.6665709018707275
      }
    },
    {
      "sample_id": 4,
      "question": "What new activity has Tim started learning in August 2023?",
      "prediction": "playing the piano",
      "reference": "play the piano",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9801626801490784,
        "bert_recall": 0.9801626801490784,
        "bert_f1": 0.9801626801490784,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9592674970626831
      }
    },
    {
      "sample_id": 4,
      "question": "Which movie's theme is Tim's favorite to play on the piano?",
      "prediction": "Harry Potter and the Philosopher's Stone",
      "reference": "\"Harry Potter and the Philosopher's Stone\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.7514772930752859,
        "bleu2": 0.7514772930752859,
        "bleu3": 0.7514772930752859,
        "bleu4": 0.7514772930752859,
        "bert_precision": 0.9840853214263916,
        "bert_recall": 0.9671415686607361,
        "bert_f1": 0.9755398035049438,
        "meteor": 0.6614583333333333,
        "sbert_similarity": 0.9408314228057861
      }
    },
    {
      "sample_id": 4,
      "question": "What special memory does \"Harry Potter and the Philosopher's Stone\" bring to Tim?",
      "prediction": "Tim's family movie night, specifically watching 'Harry Potter and the Philosopher's Stone' together was amazing and brought back some great memories.",
      "reference": "Watching it with his family",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16000000000000003,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857142,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.017541160386140592,
        "bleu3": 0.011366029557861331,
        "bleu4": 0.008640609739997756,
        "bert_precision": 0.8457967042922974,
        "bert_recall": 0.899313747882843,
        "bert_f1": 0.871734619140625,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.38295841217041016
      }
    },
    {
      "sample_id": 4,
      "question": "Which movie does Tim mention they enjoy watching during Thanksgiving?",
      "prediction": "Elf",
      "reference": "\"Home Alone\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.887018620967865,
        "bert_recall": 0.8415249586105347,
        "bert_f1": 0.8636731505393982,
        "meteor": 0.0,
        "sbert_similarity": 0.18014220893383026
      }
    },
    {
      "sample_id": 4,
      "question": "What tradition does Tim mention they love during Thanksgiving?",
      "prediction": "favorite movies",
      "reference": "Prepping the feast and talking about what they're thankful for",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8588825464248657,
        "bert_recall": 0.8424531817436218,
        "bert_f1": 0.8505885601043701,
        "meteor": 0.0,
        "sbert_similarity": 0.03616972267627716
      }
    },
    {
      "sample_id": 4,
      "question": "How long did John and his high school basketball teammates play together?",
      "prediction": "four years",
      "reference": "Four years",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9508745074272156,
        "bert_recall": 0.9508745074272156,
        "bert_f1": 0.9508745074272156,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "How was John's experience in New York City?",
      "prediction": "An amazing adventure to explore the city's culture, food, and tourist attractions without regret.",
      "reference": "Amazing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.05555555555555554,
        "bleu2": 0.018077538151554672,
        "bleu3": 0.013254163846385744,
        "bleu4": 0.010802314890908065,
        "bert_precision": 0.8330960273742676,
        "bert_recall": 0.8946859836578369,
        "bert_f1": 0.8627932667732239,
        "meteor": 0.2173913043478261,
        "sbert_similarity": 0.1545352190732956
      }
    },
    {
      "sample_id": 4,
      "question": "What did John say about NYC, enticing Tim to visit?",
      "prediction": "Trust me, NYC is amazing! It's got so much to check out - the culture, food - you won't regret it. It's an adventure you'll never forget!",
      "reference": "It's got so much to check out - the culture, food - you won't regret it.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7111111111111111,
        "rouge2_f": 0.6976744186046512,
        "rougeL_f": 0.7111111111111111,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.549169647365276,
        "bleu3": 0.5458309920075374,
        "bleu4": 0.5355319736543218,
        "bert_precision": 0.930844783782959,
        "bert_recall": 0.972415566444397,
        "bert_f1": 0.9511761665344238,
        "meteor": 0.9347587719298245,
        "sbert_similarity": 0.5609052181243896
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of soup did John make recently?",
      "prediction": "Tomato Soup",
      "reference": "tasty soup with sage",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.9076869487762451,
        "bert_recall": 0.8943499326705933,
        "bert_f1": 0.9009690880775452,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.5408485531806946
      }
    },
    {
      "sample_id": 4,
      "question": "What spice did John add to the soup for flavor?",
      "prediction": "sage",
      "reference": "sage",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What is Tim excited to see at Universal Studios?",
      "prediction": "Universal Studios",
      "reference": "The Harry Potter stuff",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8561981916427612,
        "bert_recall": 0.8389114141464233,
        "bert_f1": 0.8474666476249695,
        "meteor": 0.0,
        "sbert_similarity": 0.29216626286506653
      }
    },
    {
      "sample_id": 4,
      "question": "Where are John and his teammates planning to explore on a team trip?",
      "prediction": "explore a new city",
      "reference": "a new city",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.75,
        "bleu2": 0.7071067811865475,
        "bleu3": 0.63287829698514,
        "bleu4": 0.3976353643835253,
        "bert_precision": 0.9422785043716431,
        "bert_recall": 0.9588260650634766,
        "bert_f1": 0.9504802823066711,
        "meteor": 0.9498207885304659,
        "sbert_similarity": 0.8359741568565369
      }
    },
    {
      "sample_id": 4,
      "question": "What city did Tim suggest to John for the team trip next month?",
      "prediction": "Seattle",
      "reference": "Edinburgh, Scotland",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9076860547065735,
        "bert_recall": 0.8747907876968384,
        "bert_f1": 0.8909348249435425,
        "meteor": 0.0,
        "sbert_similarity": 0.44447582960128784
      }
    },
    {
      "sample_id": 4,
      "question": "What does John want to do after his basketball career?",
      "prediction": "It's important for me to think about life after basketball.",
      "reference": "positively influence and inspire others, potentially start a foundation and engage in charity work",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8637648820877075,
        "bert_recall": 0.8326923251152039,
        "bert_f1": 0.8479440808296204,
        "meteor": 0.0,
        "sbert_similarity": 0.09029338508844376
      }
    },
    {
      "sample_id": 4,
      "question": "What advice did Tim give John about picking endorsements?",
      "prediction": "Look for a company that shares similar goals of making a change and helping others.",
      "reference": "Ensure they align with values and brand, look for companies that share the desire to make a change and help others, make sure the endorsement feels authentic",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3684210526315789,
        "rouge1_f": 0.5238095238095238,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5238095238095238,
        "bleu1": 0.19413944816047243,
        "bleu2": 0.13126222449877564,
        "bleu3": 0.08313351417988357,
        "bleu4": 0.036950464285784856,
        "bert_precision": 0.9463207721710205,
        "bert_recall": 0.9003788828849792,
        "bert_f1": 0.9227783679962158,
        "meteor": 0.3751937984496124,
        "sbert_similarity": 0.501404881477356
      }
    },
    {
      "sample_id": 4,
      "question": "What book recommendation did Tim give to John for the trip?",
      "prediction": "Lord of the Rings",
      "reference": "A fantasy novel by Patrick Rothfuss",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8762520551681519,
        "bert_recall": 0.816074013710022,
        "bert_f1": 0.8450930714607239,
        "meteor": 0.0,
        "sbert_similarity": 0.26536163687705994
      }
    },
    {
      "sample_id": 4,
      "question": "What type of venue did John and his girlfriend choose for their wedding ceremony?",
      "prediction": "greenhouse",
      "reference": "Greenhouse",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9439657330513,
        "bert_recall": 0.9439657330513,
        "bert_f1": 0.9439657330513,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What was the setting for John and his wife's first dance?",
      "prediction": "a cozy restaurant",
      "reference": "Cozy restaurant",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9594066739082336,
        "bert_recall": 0.9594066739082336,
        "bert_f1": 0.9594066739082336,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.9672349095344543
      }
    },
    {
      "sample_id": 4,
      "question": "Which basketball team does Tim support?",
      "prediction": "Seattle",
      "reference": "The Wolves",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8955801725387573,
        "bert_recall": 0.8882418870925903,
        "bert_f1": 0.8918959498405457,
        "meteor": 0.0,
        "sbert_similarity": 0.4500499963760376
      }
    },
    {
      "sample_id": 4,
      "question": "What passion does Tim mention connects him with people from all over the world?",
      "prediction": "traveling",
      "reference": "passion for fantasy stuff",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8785730600357056,
        "bert_recall": 0.8548012375831604,
        "bert_f1": 0.8665241003036499,
        "meteor": 0.0,
        "sbert_similarity": 0.22309480607509613
      }
    },
    {
      "sample_id": 4,
      "question": "How does John describe the game season for his team?",
      "prediction": "John describes the game season as intense, featuring both tough losses and great wins, indicating an overall satisfactory performance.",
      "reference": "intense with tough losses and great wins",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.46153846153846156,
        "rouge1_f": 0.46153846153846156,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.46153846153846156,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.2279211529192759,
        "bleu3": 0.20148743737862052,
        "bleu4": 0.16923267918690046,
        "bert_precision": 0.8890600204467773,
        "bert_recall": 0.9233188629150391,
        "bert_f1": 0.9058656692504883,
        "meteor": 0.4839939024390244,
        "sbert_similarity": 0.741094172000885
      }
    },
    {
      "sample_id": 4,
      "question": "How does John say his team handles tough opponents?",
      "prediction": "John says that playing against rivals added an extra level of intensity to the game.",
      "reference": "by backing each other up and not quitting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8803483247756958,
        "bert_recall": 0.8633546829223633,
        "bert_f1": 0.8717687129974365,
        "meteor": 0.0,
        "sbert_similarity": 0.0803498849272728
      }
    },
    {
      "sample_id": 4,
      "question": "What motivates John's team to get better, according to John?",
      "prediction": "John's team gets better through hard work and support.",
      "reference": "facing tough opponents",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.868667483329773,
        "bert_recall": 0.8723296523094177,
        "bert_f1": 0.8704947233200073,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.3306952118873596
      }
    },
    {
      "sample_id": 4,
      "question": "What did John's team win at the end of the season?",
      "prediction": "We won it by a tight score.",
      "reference": "a trophy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8944462537765503,
        "bert_recall": 0.9006470441818237,
        "bert_f1": 0.8975359201431274,
        "meteor": 0.2,
        "sbert_similarity": 0.2737625539302826
      }
    },
    {
      "sample_id": 4,
      "question": "Where did Tim capture the photography of the sunset over the mountain range?",
      "prediction": "Timsays: I took this pic last summer.",
      "reference": "Smoky Mountains",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8405051231384277,
        "bert_recall": 0.8191460371017456,
        "bert_f1": 0.8296881914138794,
        "meteor": 0.0,
        "sbert_similarity": 0.23998868465423584
      }
    },
    {
      "sample_id": 4,
      "question": "How does John feel about being seen as a mentor by some of the younger players?",
      "prediction": "John feels very rewarding and enjoys making a positive impact on their lives.",
      "reference": "It feels great",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8788149356842041,
        "bert_recall": 0.8995840549468994,
        "bert_f1": 0.8890781998634338,
        "meteor": 0.125,
        "sbert_similarity": 0.2554858922958374
      }
    },
    {
      "sample_id": 4,
      "question": "What does John find rewarding about mentoring the younger players?",
      "prediction": "John finds rewarding seeing their growth, improvement, and confidence is so fulfilling.",
      "reference": "Seeing their growth, improvement, and confidence",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5333333333333333,
        "bleu2": 0.5163977794943222,
        "bleu3": 0.5009083692610703,
        "bleu4": 0.4758733096412523,
        "bert_precision": 0.918067991733551,
        "bert_recall": 0.970518946647644,
        "bert_f1": 0.9435650706291199,
        "meteor": 0.906986531986532,
        "sbert_similarity": 0.5810348987579346
      }
    },
    {
      "sample_id": 4,
      "question": "What has John been able to help the younger players achieve?",
      "prediction": "John's mentoring and support have helped the younger players achieve growth, improvement, and confidence.",
      "reference": "reach their goals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8507965803146362,
        "bert_recall": 0.8612092137336731,
        "bert_f1": 0.8559712171554565,
        "meteor": 0.0,
        "sbert_similarity": 0.40566104650497437
      }
    },
    {
      "sample_id": 4,
      "question": "What genre is the novel that Tim is writing?",
      "prediction": "fantasy",
      "reference": "Fantasy",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9779714345932007,
        "bert_recall": 0.9779714345932007,
        "bert_f1": 0.9779714345932007,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "Who is one of Tim's sources of inspiration for writing?",
      "prediction": "J.K. Rowling",
      "reference": "J.K. Rowling",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "What J.K. Rowling quote does Tim resonate with?",
      "prediction": "There's a quote by J.K. Rowling that I really like: 'Turn on the light - happiness hides in the darkest of times.'",
      "reference": "\"Turn on the light - happiness hides in the darkest of times.\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285714,
        "rouge1_f": 0.6470588235294118,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6470588235294118,
        "bleu1": 0.46153846153846156,
        "bleu2": 0.45064056971928645,
        "bleu3": 0.44264776918161913,
        "bleu4": 0.42657068031281004,
        "bert_precision": 0.9172300696372986,
        "bert_recall": 0.9754371643066406,
        "bert_f1": 0.9454385042190552,
        "meteor": 0.768846153846154,
        "sbert_similarity": 0.7968369126319885
      }
    },
    {
      "sample_id": 4,
      "question": "What does John write on the whiteboard to help him stay motivated?",
      "prediction": "John writes motivational quotes and strategies to help him stay focused and push through tough workouts.",
      "reference": "motivational quotes and strategies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4210526315789474,
        "rouge1_f": 0.4,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.4,
        "bleu1": 0.23529411764705885,
        "bleu2": 0.21004201260420147,
        "bleu3": 0.18363347637365393,
        "bleu4": 0.1431712315455507,
        "bert_precision": 0.8855969309806824,
        "bert_recall": 0.9173700213432312,
        "bert_f1": 0.9012035131454468,
        "meteor": 0.606971153846154,
        "sbert_similarity": 0.6320689916610718
      }
    },
    {
      "sample_id": 4,
      "question": "What hobby is a therapy for John when away from the court?",
      "prediction": "reading",
      "reference": "Cooking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8856907486915588,
        "bert_recall": 0.8646198511123657,
        "bert_f1": 0.8750284314155579,
        "meteor": 0.0,
        "sbert_similarity": 0.3330439627170563
      }
    },
    {
      "sample_id": 4,
      "question": "What type of meal does John often cook using a slow cooker?",
      "prediction": "John often cooks soups using a slow cooker.",
      "reference": "honey garlic chicken with roasted veg",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8603414297103882,
        "bert_recall": 0.8144278526306152,
        "bert_f1": 0.8367552757263184,
        "meteor": 0.0,
        "sbert_similarity": 0.2797161042690277
      }
    },
    {
      "sample_id": 4,
      "question": "How will John share the honey garlic chicken recipe with the other person?",
      "prediction": "John will share the recipe through photos and detailed descriptions.",
      "reference": "write it down and mail it",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8646870851516724,
        "bert_recall": 0.8572809100151062,
        "bert_f1": 0.8609681129455566,
        "meteor": 0.078125,
        "sbert_similarity": 0.27088463306427
      }
    },
    {
      "sample_id": 4,
      "question": "What was Tim's huge writing issue last week,as mentioned on November 6, 2023?",
      "prediction": "got stuck on a plot twist and couldn't find my way out.",
      "reference": "He got stuck on a plot twist",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.631578947368421,
        "rouge1_f": 0.6,
        "rouge2_f": 0.5555555555555556,
        "rougeL_f": 0.6,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.40599897147057507,
        "bleu3": 0.38386316691836725,
        "bleu4": 0.3498761149110956,
        "bert_precision": 0.9022023677825928,
        "bert_recall": 0.9436113834381104,
        "bert_f1": 0.9224423766136169,
        "meteor": 0.7981481481481482,
        "sbert_similarity": 0.7001779079437256
      }
    },
    {
      "sample_id": 4,
      "question": "What does Tim have that serves as a reminder of hard work and is his prized possession?",
      "prediction": "The signing",
      "reference": "a basketball signed by his favorite player",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8869985342025757,
        "bert_recall": 0.8653796911239624,
        "bert_f1": 0.8760557770729065,
        "meteor": 0.07692307692307691,
        "sbert_similarity": 0.5336496829986572
      }
    },
    {
      "sample_id": 4,
      "question": "Why do Tim and John find LeBron inspiring?",
      "prediction": "Tim finds LeBron James inspirational because he sees him as someone who shows determination and heart through his basketball plays, particularly during memorable moments like a block in the Finals that changed game dynamics.",
      "reference": "LeBron's determination and the epic block in Game 7 of the '16 Finals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3043478260869565,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.08695652173913043,
        "rougeL_f": 0.2916666666666667,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.11268723396380222,
        "bleu3": 0.0345819335350256,
        "bleu4": 0.018341681391245233,
        "bert_precision": 0.8693650364875793,
        "bert_recall": 0.8913037776947021,
        "bert_f1": 0.8801977634429932,
        "meteor": 0.37910528449790515,
        "sbert_similarity": 0.6184777617454529
      }
    },
    {
      "sample_id": 4,
      "question": "How did John describe the views during their road trip out on the European coastline?",
      "prediction": "The views were spectacular.",
      "reference": "Spectacular",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.9109614491462708,
        "bert_recall": 0.9080394506454468,
        "bert_f1": 0.9094980359077454,
        "meteor": 0.0,
        "sbert_similarity": 0.5331395864486694
      }
    },
    {
      "sample_id": 4,
      "question": "What is one of Tim's favorite fantasy TV shows, as mentioned on November 11, 2023?",
      "prediction": "That",
      "reference": "\"That\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9269177913665771,
        "bert_recall": 0.8949573040008545,
        "bert_f1": 0.9106572270393372,
        "meteor": 0.0,
        "sbert_similarity": 0.5329480767250061
      }
    },
    {
      "sample_id": 4,
      "question": "How does Tim stay motivated during difficult study sessions?",
      "prediction": "Tim stays motivated during difficult study sessions by visualizing his goals and success, which helps him focus and maintain motivation through tough studying.",
      "reference": "Visualizing goals and success",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3076923076923077,
        "rouge1_f": 0.29629629629629634,
        "rouge2_f": 0.16,
        "rougeL_f": 0.29629629629629634,
        "bleu1": 0.15999999999999998,
        "bleu2": 0.11547005383792518,
        "bleu3": 0.08547903235542961,
        "bleu4": 0.04028998029112094,
        "bert_precision": 0.8710689544677734,
        "bert_recall": 0.9220713376998901,
        "bert_f1": 0.8958448171615601,
        "meteor": 0.2542372881355932,
        "sbert_similarity": 0.45667585730552673
      }
    },
    {
      "sample_id": 4,
      "question": "What did Tim say about his injury on 16 November, 2023?",
      "prediction": "Tim said he injured himself not too long ago and it sucked because he had to miss some games and couldn't help his team.",
      "reference": "The doctor said it's not too serious",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20689655172413793,
        "rouge1_f": 0.24242424242424243,
        "rouge2_f": 0.06451612903225806,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.07844645405527362,
        "bleu3": 0.03054560136346369,
        "bleu4": 0.018272664875356538,
        "bert_precision": 0.8504923582077026,
        "bert_recall": 0.8818334341049194,
        "bert_f1": 0.8658794164657593,
        "meteor": 0.2937420178799489,
        "sbert_similarity": 0.09702569991350174
      }
    },
    {
      "sample_id": 4,
      "question": "What was the setback Tim faced in his writing project on 21 November, 2023?",
      "prediction": "Tim tried writing a story based on his experiences in the UK but it didn't go as he wanted.",
      "reference": "Story based on experiences in the UK didn't go as planned",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6874999999999999,
        "rouge2_f": 0.5333333333333333,
        "rougeL_f": 0.6874999999999999,
        "bleu1": 0.5238095238095238,
        "bleu2": 0.4577377082170635,
        "bleu3": 0.3843071186262007,
        "bleu4": 0.2797704355252052,
        "bert_precision": 0.9257656931877136,
        "bert_recall": 0.9414242506027222,
        "bert_f1": 0.9335293769836426,
        "meteor": 0.8360169491525423,
        "sbert_similarity": 0.6970600485801697
      }
    },
    {
      "sample_id": 4,
      "question": "How did John overcome his ankle injury from last season?",
      "prediction": "John persevered by focusing on recovery and strengthening through strength training exercises.",
      "reference": "stayed focused on recovery and worked hard to strengthen his body",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2608695652173913,
        "rouge1_f": 0.43478260869565216,
        "rouge2_f": 0.28571428571428564,
        "rougeL_f": 0.43478260869565216,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.19611613513818407,
        "bleu3": 0.15466755103052704,
        "bleu4": 0.0768968397262906,
        "bert_precision": 0.9154524803161621,
        "bert_recall": 0.9149124026298523,
        "bert_f1": 0.9151822924613953,
        "meteor": 0.4360360360360361,
        "sbert_similarity": 0.626476526260376
      }
    },
    {
      "sample_id": 4,
      "question": "What motivated Tim to keep pushing himself to get better in writing and reading?",
      "prediction": "It's important to remember why we love what we do, such as writing and reading.",
      "reference": "Love for writing and reading",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4210526315789473,
        "rouge1_f": 0.38095238095238093,
        "rouge2_f": 0.2105263157894737,
        "rougeL_f": 0.38095238095238093,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16169041669088866,
        "bleu3": 0.12032947744440442,
        "bleu4": 0.05744990696182412,
        "bert_precision": 0.8928762674331665,
        "bert_recall": 0.9344031810760498,
        "bert_f1": 0.9131678342819214,
        "meteor": 0.42592592592592593,
        "sbert_similarity": 0.6544078588485718
      }
    },
    {
      "sample_id": 4,
      "question": "How did John overcome a mistake he made during a big game in basketball?",
      "prediction": "John worked hard to get better and chose to learn from his mistake rather than dwell on it.",
      "reference": "Worked hard to get better and focused on growth",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5384615384615384,
        "rouge1_f": 0.5185185185185185,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5185185185185185,
        "bleu1": 0.3684210526315789,
        "bleu2": 0.31990495664579766,
        "bleu3": 0.29237853973820477,
        "bleu4": 0.2592170537135687,
        "bert_precision": 0.9064210653305054,
        "bert_recall": 0.9336888790130615,
        "bert_f1": 0.9198529124259949,
        "meteor": 0.6411049268192125,
        "sbert_similarity": 0.41825512051582336
      }
    },
    {
      "sample_id": 4,
      "question": "What book did John recently finish rereading that left him feeling inspired and hopeful about following dreams?",
      "prediction": "The Alchemist",
      "reference": "The Alchemist",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "How did \"The Alchemist\" impact John's perspective on following dreams?",
      "prediction": "It inspired John to think about following his dreams and searching for his own personal legends.",
      "reference": "made him think again about following dreams and searching for personal legends",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.6428571428571429,
        "rouge2_f": 0.3846153846153846,
        "rougeL_f": 0.6428571428571429,
        "bleu1": 0.5294117647058824,
        "bleu2": 0.4067446084099803,
        "bleu3": 0.28404174646388836,
        "bleu4": 0.19923405658137924,
        "bert_precision": 0.9318051338195801,
        "bert_recall": 0.9426436424255371,
        "bert_f1": 0.9371930956840515,
        "meteor": 0.6048387096774195,
        "sbert_similarity": 0.619259238243103
      }
    },
    {
      "sample_id": 4,
      "question": "What is John trying out to improve his strength and flexibility after recovery from ankle injury?",
      "prediction": "Strength training exercises every day",
      "reference": "yoga",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.861302375793457,
        "bert_recall": 0.8717329502105713,
        "bert_f1": 0.8664862513542175,
        "meteor": 0.0,
        "sbert_similarity": 0.3695860505104065
      }
    },
    {
      "sample_id": 4,
      "question": "How long does John usually hold the yoga pose he shared with Tim?",
      "prediction": "Warrior II",
      "reference": "30-60 seconds",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8717828989028931,
        "bert_recall": 0.8481849431991577,
        "bert_f1": 0.8598220348358154,
        "meteor": 0.0,
        "sbert_similarity": 0.138164684176445
      }
    },
    {
      "sample_id": 4,
      "question": "Where was the forest picture shared by John on December 1,2023 taken?",
      "prediction": "a beautiful and peaceful place",
      "reference": "near his hometown",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8748737573623657,
        "bert_recall": 0.8958401679992676,
        "bert_f1": 0.8852328658103943,
        "meteor": 0.0,
        "sbert_similarity": 0.32284045219421387
      }
    },
    {
      "sample_id": 4,
      "question": "What did Tim recently start learning in addition to being part of a travel club and working on studies?",
      "prediction": "Tim started learning an instrument",
      "reference": "an instrument",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.890192985534668,
        "bert_recall": 0.927455723285675,
        "bert_f1": 0.9084424376487732,
        "meteor": 0.8152173913043478,
        "sbert_similarity": 0.591771125793457
      }
    },
    {
      "sample_id": 4,
      "question": "What instrument is Tim learning to play in December 2023?",
      "prediction": "The instrument that John is learning to play in December 2023 is not explicitly mentioned.",
      "reference": "violin",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8163287043571472,
        "bert_recall": 0.8349934816360474,
        "bert_f1": 0.8255556225776672,
        "meteor": 0.0,
        "sbert_similarity": 0.385628879070282
      }
    },
    {
      "sample_id": 4,
      "question": "How long has Tim been playing the piano for, as of December 2023?",
      "prediction": "Since we last chatted on August 19, 2023",
      "reference": "about four months",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8324974179267883,
        "bert_recall": 0.8594293594360352,
        "bert_f1": 0.845749020576477,
        "meteor": 0.0,
        "sbert_similarity": 0.3268417716026306
      }
    },
    {
      "sample_id": 4,
      "question": "What book did Tim just finish reading on 8th December, 2023?",
      "prediction": "That",
      "reference": "\"A Dance with Dragons\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8687494993209839,
        "bert_recall": 0.8254328966140747,
        "bert_f1": 0.8465374708175659,
        "meteor": 0.0,
        "sbert_similarity": 0.16751129925251007
      }
    },
    {
      "sample_id": 4,
      "question": "Which book did Tim recommend to John as a good story on 8th December, 2023?",
      "prediction": "A Dance with Dragons",
      "reference": "\"A Dance with Dragons\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.6065306597126334,
        "bleu4": 0.6065306597126334,
        "bert_precision": 0.979447066783905,
        "bert_recall": 0.9641860723495483,
        "bert_f1": 0.971756637096405,
        "meteor": 0.46875,
        "sbert_similarity": 0.941758394241333
      }
    },
    {
      "sample_id": 4,
      "question": "What is the topic of discussion between John and Tim on 11 December, 2023?",
      "prediction": "Writing articles about fantasy novels for an online magazine.",
      "reference": "Academic achievements and sports successes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8605064153671265,
        "bert_recall": 0.8607400059700012,
        "bert_f1": 0.8606231808662415,
        "meteor": 0.0,
        "sbert_similarity": 0.1673646718263626
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of game did John have a career-high in assists in?",
      "prediction": "a big game against our rival",
      "reference": "basketball",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8202898502349854,
        "bert_recall": 0.8809555768966675,
        "bert_f1": 0.8495410680770874,
        "meteor": 0.0,
        "sbert_similarity": 0.39031118154525757
      }
    },
    {
      "sample_id": 4,
      "question": "What was John's way of dealing with doubts and stress when he was younger?",
      "prediction": "John worked hard on improving after the mistake, showing resilience and learning from it to become a strong player and teammate.",
      "reference": "practicing basketball outside for hours",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8537172675132751,
        "bert_recall": 0.8638207912445068,
        "bert_f1": 0.8587392568588257,
        "meteor": 0.0,
        "sbert_similarity": 0.18323048949241638
      }
    },
    {
      "sample_id": 4,
      "question": "How did John feel about the atmosphere during the big game against the rival team?",
      "prediction": "John felt very excited and energized, as evidenced by the electric atmosphere and intense level of intensity added to the game due to facing rivals.",
      "reference": "electric and intense",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.24000000000000002,
        "rouge1_f": 0.21428571428571425,
        "rouge2_f": 0.07692307692307693,
        "rougeL_f": 0.21428571428571425,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.06537204504606134,
        "bleu3": 0.026720129842177108,
        "bleu4": 0.01633647013102316,
        "bert_precision": 0.8288261890411377,
        "bert_recall": 0.8502438068389893,
        "bert_f1": 0.8393983840942383,
        "meteor": 0.4914529914529915,
        "sbert_similarity": 0.513297975063324
      }
    },
    {
      "sample_id": 4,
      "question": "How did John feel after being able to jog without pain?",
      "prediction": "John felt relieved and proud.",
      "reference": "It was a huge success.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8977044820785522,
        "bert_recall": 0.8977757692337036,
        "bert_f1": 0.8977401256561279,
        "meteor": 0.0,
        "sbert_similarity": 0.3036150634288788
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of deal did John get in December?",
      "prediction": "John got something great during the holiday season.",
      "reference": "Deal with a renowned outdoor gear company",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8865002989768982,
        "bert_recall": 0.8790377378463745,
        "bert_f1": 0.8827532529830933,
        "meteor": 0.0,
        "sbert_similarity": 0.08921148627996445
      }
    },
    {
      "sample_id": 4,
      "question": "Where was the photoshoot done for John's gear deal?",
      "prediction": "a gorgeous forest",
      "reference": "In a gorgeous forest",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.40293516672844226,
        "bert_precision": 0.9733409881591797,
        "bert_recall": 0.9686988592147827,
        "bert_f1": 0.9710143804550171,
        "meteor": 0.754985754985755,
        "sbert_similarity": 0.8520327210426331
      }
    },
    {
      "sample_id": 4,
      "question": "In which area has John's team seen the most growth during training?",
      "prediction": "strength training",
      "reference": "Communication and bonding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8836457133293152,
        "bert_recall": 0.874259889125824,
        "bert_f1": 0.8789277076721191,
        "meteor": 0.0,
        "sbert_similarity": 0.08944551646709442
      }
    },
    {
      "sample_id": 4,
      "question": "What type of seminars is John conducting?",
      "prediction": "seminars on sports and marketing",
      "reference": "Sports and marketing seminars",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.888888888888889,
        "rouge1_f": 0.888888888888889,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666665,
        "bleu1": 0.8,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.5143157015215767,
        "bleu4": 0.28574404296988,
        "bert_precision": 0.9214929342269897,
        "bert_recall": 0.9528931379318237,
        "bert_f1": 0.9369300603866577,
        "meteor": 0.9146341463414633,
        "sbert_similarity": 0.9703371524810791
      }
    },
    {
      "sample_id": 4,
      "question": "What activity did Tim do after reading the stories about the Himalayan trek?",
      "prediction": "Tim read more cool stories from travelers.",
      "reference": "visited a travel agency",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8481732606887817,
        "bert_recall": 0.8608197569847107,
        "bert_f1": 0.8544497489929199,
        "meteor": 0.0,
        "sbert_similarity": 0.3716755509376526
      }
    },
    {
      "sample_id": 4,
      "question": "What is one cause that John supports with his influence and resources?",
      "prediction": "charity",
      "reference": "youth sports and fair chances in sports",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8743534088134766,
        "bert_recall": 0.834270715713501,
        "bert_f1": 0.8538419008255005,
        "meteor": 0.0,
        "sbert_similarity": 0.1840643286705017
      }
    },
    {
      "sample_id": 4,
      "question": "What new fantasy TV series is Tim excited about?",
      "prediction": "Tim is excited about 'The Wheel of Time'.",
      "reference": "\"The Wheel of Time\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.6,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.3,
        "bleu2": 0.25819888974716104,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.10445522730720382,
        "bert_precision": 0.9201375246047974,
        "bert_recall": 0.9568263292312622,
        "bert_f1": 0.938123345375061,
        "meteor": 0.42613636363636365,
        "sbert_similarity": 0.6886346936225891
      }
    },
    {
      "sample_id": 4,
      "question": "Which language is Tim learning?",
      "prediction": "German",
      "reference": "German",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "What language does Tim know besides German?",
      "prediction": "French",
      "reference": "Spanish",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9478417634963989,
        "bert_recall": 0.9478417634963989,
        "bert_f1": 0.9478417634963989,
        "meteor": 0.0,
        "sbert_similarity": 0.6289787292480469
      }
    },
    {
      "sample_id": 4,
      "question": "What book did Tim get in Italy that inspired him to cook?",
      "prediction": "Timsays made this tasty soup recently - it was real good!",
      "reference": "a cooking book",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8334956765174866,
        "bert_recall": 0.8764084577560425,
        "bert_f1": 0.8544135093688965,
        "meteor": 0.13157894736842107,
        "sbert_similarity": 0.29586654901504517
      }
    },
    {
      "sample_id": 4,
      "question": "What is John's favorite book series?",
      "prediction": "Star Wars movies",
      "reference": "Harry Potter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9021731019020081,
        "bert_recall": 0.9036479592323303,
        "bert_f1": 0.9029099345207214,
        "meteor": 0.0,
        "sbert_similarity": 0.45637404918670654
      }
    },
    {
      "sample_id": 4,
      "question": "According to John, who is his favorite character from Lord of the Rings?",
      "prediction": "Aragorn",
      "reference": "Aragorn",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "Why does John like Aragorn from Lord of the Rings?",
      "prediction": "John likes Aragorn because he grows so much throughout the story and achieves redemption, making his character inspiring.",
      "reference": "brave, selfless, down-to-earth attitude",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.8607739210128784,
        "bert_recall": 0.8511565923690796,
        "bert_f1": 0.855938196182251,
        "meteor": 0.0,
        "sbert_similarity": 0.15277434885501862
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of painting does John have in his room as a reminder?",
      "prediction": "a painting",
      "reference": "a painting of Aragorn",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9643321633338928,
        "bert_recall": 0.8651105165481567,
        "bert_f1": 0.9120306372642517,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.6406252980232239
      }
    },
    {
      "sample_id": 4,
      "question": "What is the painting of Aragorn a reminder for John to be in everything he does?",
      "prediction": "To act courageously and stand up for justice.",
      "reference": "be a leader",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.872048020362854,
        "bert_recall": 0.9196903109550476,
        "bert_f1": 0.8952357769012451,
        "meteor": 0.0,
        "sbert_similarity": 0.40917932987213135
      }
    },
    {
      "sample_id": 4,
      "question": "What map does Tim show to his friend John?",
      "prediction": "a map",
      "reference": "a map of Middle-earth from LOTR",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.9432427883148193,
        "bert_recall": 0.8554816246032715,
        "bert_f1": 0.8972212076187134,
        "meteor": 0.33482142857142855,
        "sbert_similarity": 0.5233018398284912
      }
    },
    {
      "sample_id": 4,
      "question": "Where will Tim be going for a semester abroad?",
      "prediction": "I'm off to Ireland for a semester.",
      "reference": "Ireland",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8713580369949341,
        "bert_recall": 0.8984385132789612,
        "bert_f1": 0.8846909999847412,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.6310577392578125
      }
    },
    {
      "sample_id": 4,
      "question": "Which city in Ireland will Tim be staying in during his semester abroad?",
      "prediction": "Galway",
      "reference": "Galway",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What charity event did John organize recently in 2024?",
      "prediction": "The speaker's recent memory mentions attending a local restaurant with new teammates last week, which was followed by an intense game of basketball with a tight score. The atmosphere and personal growth shared are aligned with the context provided.",
      "reference": "benefit basketball game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10810810810810811,
        "rouge1_f": 0.09302325581395349,
        "rouge2_f": 0.0,
        "rougeL_f": 0.046511627906976744,
        "bleu1": 0.04651162790697675,
        "bleu2": 0.010523401123547818,
        "bleu3": 0.006798363306330766,
        "bleu4": 0.005097615802300061,
        "bert_precision": 0.8136717677116394,
        "bert_recall": 0.8764805197715759,
        "bert_f1": 0.8439090847969055,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.5227333307266235
      }
    },
    {
      "sample_id": 4,
      "question": "What achievement did John share with Tim in January 2024?",
      "prediction": "The Harry Potter party",
      "reference": "endorsement with a popular beverage company",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8512558937072754,
        "bert_recall": 0.8648864030838013,
        "bert_f1": 0.8580170273780823,
        "meteor": 0.0,
        "sbert_similarity": 0.15596753358840942
      }
    },
    {
      "sample_id": 4,
      "question": "What was Johns's reaction to sealing the deal with the beverage company?",
      "prediction": "Johns says that it feels good and is awesome.",
      "reference": "crazy feeling, sense of accomplishment",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8893213868141174,
        "bert_recall": 0.8722798824310303,
        "bert_f1": 0.8807182312011719,
        "meteor": 0.0,
        "sbert_similarity": 0.30776870250701904
      }
    },
    {
      "sample_id": 4,
      "question": "Which city did John recommend to Tim in January 2024?",
      "prediction": "Westendell",
      "reference": "Barcelona",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8631304502487183,
        "bert_recall": 0.8650006651878357,
        "bert_f1": 0.8640645742416382,
        "meteor": 0.0,
        "sbert_similarity": 0.3866138160228729
      }
    },
    {
      "sample_id": 4,
      "question": "Which team did Tim sign with on 21 May, 2023?",
      "prediction": "The Minnesota Wolves",
      "reference": "The Minnesota Wolves",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 4,
      "question": "What is Tim's position on the team he signed with?",
      "prediction": "Not mentioned in the conversation",
      "reference": "shooting guard",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8641161918640137,
        "bert_recall": 0.8288992643356323,
        "bert_f1": 0.8461414575576782,
        "meteor": 0.0,
        "sbert_similarity": 0.1794775128364563
      }
    },
    {
      "sample_id": 4,
      "question": "What challenge did Tim encounter during pre-season training?",
      "prediction": "an ankle injury that required time off and physical therapy",
      "reference": "fitting into the new team's style of play",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8497886657714844,
        "bert_recall": 0.8704472184181213,
        "bert_f1": 0.8599939346313477,
        "meteor": 0.0,
        "sbert_similarity": 0.09913472831249237
      }
    },
    {
      "sample_id": 4,
      "question": "What cult did Tim join recently?",
      "prediction": "Travel club",
      "reference": "fantasy literature forum",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8839568495750427,
        "bert_recall": 0.8726174831390381,
        "bert_f1": 0.8782505393028259,
        "meteor": 0.0,
        "sbert_similarity": 0.2683103084564209
      }
    },
    {
      "sample_id": 4,
      "question": "What was the highest number of points Tim scored in a game recently?",
      "prediction": "40 points",
      "reference": "40 points",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What did Tim celebrate at a restaurant with teammates?",
      "prediction": "celebrating at a restaurant",
      "reference": "a tough win",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8809919357299805,
        "bert_recall": 0.8724678754806519,
        "bert_f1": 0.8767092227935791,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.19168123602867126
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of deals did Tim sign with Nike and Gatorade?",
      "prediction": "potential sponsorship deal with Gatorade",
      "reference": "basketball shoe and gear deal with Nike, potential sponsorship deal with Gatorade",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.5882352941176471,
        "rouge2_f": 0.5333333333333333,
        "rougeL_f": 0.5882352941176471,
        "bleu1": 0.20189651799465538,
        "bleu2": 0.20189651799465538,
        "bleu3": 0.20189651799465538,
        "bleu4": 0.20189651799465538,
        "bert_precision": 0.9593539237976074,
        "bert_recall": 0.9240089058876038,
        "bert_f1": 0.9413497447967529,
        "meteor": 0.4407079646017699,
        "sbert_similarity": 0.7738461494445801
      }
    },
    {
      "sample_id": 4,
      "question": "How does Tim feel while surfing?",
      "prediction": "super exciting and free-feeling",
      "reference": "super exciting and free-feeling",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9921875,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of articles has John been writing about for the online magazine?",
      "prediction": "fantasy novels",
      "reference": "different fantasy novels, characters, themes, and book recommendations",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.011108996538242306,
        "bleu2": 0.011108996538242306,
        "bleu3": 0.005196068065373761,
        "bleu4": 0.0035129731579771513,
        "bert_precision": 0.9405443668365479,
        "bert_recall": 0.8785383105278015,
        "bert_f1": 0.9084845185279846,
        "meteor": 0.06756756756756756,
        "sbert_similarity": 0.8079648613929749
      }
    },
    {
      "sample_id": 4,
      "question": "Which two mystery novels does Tim particularly enjoy writing about?",
      "prediction": "The Name of the Wind",
      "reference": "Harry Potter and Game of Thrones",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.1637461506155964,
        "bleu2": 0.05789300674674099,
        "bleu3": 0.046382183633183686,
        "bleu4": 0.0439891724758422,
        "bert_precision": 0.8836742639541626,
        "bert_recall": 0.861128568649292,
        "bert_f1": 0.872255802154541,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.20890597999095917
      }
    },
    {
      "sample_id": 4,
      "question": "What did Anthony and Tim end up playing during the charity event?",
      "prediction": "Intense basketball game",
      "reference": "an intense Harry Potter trivia contest",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9069762229919434,
        "bert_recall": 0.8816570043563843,
        "bert_f1": 0.894137442111969,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.20957013964653015
      }
    },
    {
      "sample_id": 4,
      "question": "How did Tim get introduced to basketball?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Dad signed him up for a local league",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.888435959815979,
        "bert_recall": 0.882137656211853,
        "bert_f1": 0.8852756023406982,
        "meteor": 0.0,
        "sbert_similarity": 0.042666275054216385
      }
    },
    {
      "sample_id": 4,
      "question": "What is Tim's number one goal in his basketball career?",
      "prediction": "Winning a championship",
      "reference": "Winning a championship",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What organization is Tim teaming up with for his charity work?",
      "prediction": "A local organization helping disadvantaged kids with sports and school",
      "reference": "A local organization helping disadvantaged kids with sports and school",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9995,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "What did Tim's teammates give him when they met on Aug 15th?",
      "prediction": "a basketball with autographs on it",
      "reference": "a basketball with autographs on it",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "Why did John's teammates sign the football they gave him?",
      "prediction": "to show their friendship and appreciation",
      "reference": "to show their friendship and appreciation",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "What is the main intention behind John wanting to attend the book conference?",
      "prediction": "to learn more about literature and create a stronger bond to it",
      "reference": "to learn more about literature and create a stronger bond to it",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9997106481481481,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 4,
      "question": "What new activity has John started learning in August 2023?",
      "prediction": "reading",
      "reference": "play the piano",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8683943748474121,
        "bert_recall": 0.8689419031143188,
        "bert_f1": 0.8686680197715759,
        "meteor": 0.0,
        "sbert_similarity": 0.2679218351840973
      }
    },
    {
      "sample_id": 4,
      "question": "What special memory does \"Fifty Shades of Grey\" bring to Tim?",
      "prediction": "Watching it with his family",
      "reference": "Watching it with his family",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "Which movie does John mention they enjoy watching during Thanksgiving?",
      "prediction": "Home Alone",
      "reference": "\"Home Alone\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9280412197113037,
        "bert_recall": 0.9160686731338501,
        "bert_f1": 0.9220160841941833,
        "meteor": 0.0,
        "sbert_similarity": 0.8923487663269043
      }
    },
    {
      "sample_id": 4,
      "question": "What tradition does Tim mention they love during Halloween?",
      "prediction": "Not mentioned in the conversation or Prepping the feast and talking about what they're thankful for",
      "reference": "Prepping the feast and talking about what they're thankful for",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.6470588235294118,
        "bleu2": 0.6359337738364604,
        "bleu3": 0.626674120796503,
        "bleu4": 0.6102169202557912,
        "bert_precision": 0.9389640092849731,
        "bert_recall": 0.9792611598968506,
        "bert_f1": 0.9586893320083618,
        "meteor": 0.9429245283018869,
        "sbert_similarity": 0.8942440152168274
      }
    },
    {
      "sample_id": 4,
      "question": "How long did Tim and his high school basketball teammates play together?",
      "prediction": "Four years",
      "reference": "Four years",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "How was Tim's experience in New York City?",
      "prediction": "Amazing",
      "reference": "Amazing",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999997615814209,
        "bert_recall": 0.9999997615814209,
        "bert_f1": 0.9999997615814209,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What spice did Tim add to the soup for flavor?",
      "prediction": "sage",
      "reference": "sage",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What is Tim excited to see at Disneyland?",
      "prediction": "The Harry Potter stuff",
      "reference": "The Harry Potter stuff",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "Where are John and his teammates planning to avoid on a team trip?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a new city",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8869131207466125,
        "bert_recall": 0.881231963634491,
        "bert_f1": 0.8840634226799011,
        "meteor": 0.0,
        "sbert_similarity": 0.07867977023124695
      }
    },
    {
      "sample_id": 4,
      "question": "What does Tim want to do after his basketball career?",
      "prediction": "start a foundation and engage in charity work",
      "reference": "positively influence and inspire others, potentially start a foundation and engage in charity work",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.761904761904762,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.7000000000000001,
        "rougeL_f": 0.7272727272727273,
        "bleu1": 0.4168620196785084,
        "bleu2": 0.4168620196785084,
        "bleu3": 0.4168620196785084,
        "bleu4": 0.4168620196785084,
        "bert_precision": 0.9566145539283752,
        "bert_recall": 0.8929176330566406,
        "bert_f1": 0.9236692786216736,
        "meteor": 0.5964319029850745,
        "sbert_similarity": 0.84318608045578
      }
    },
    {
      "sample_id": 4,
      "question": "What type of venue did John and his girlfriend choose for their breakup?",
      "prediction": "Greenhouse",
      "reference": "Greenhouse",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What passion does John mention connects him with people from all over the world?",
      "prediction": "surfing",
      "reference": "passion for fantasy stuff",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8207923173904419,
        "bert_recall": 0.815475344657898,
        "bert_f1": 0.8181251883506775,
        "meteor": 0.0,
        "sbert_similarity": 0.16380147635936737
      }
    },
    {
      "sample_id": 4,
      "question": "How does Tim say his team handles tough opponents?",
      "prediction": "Not mentioned in the conversation",
      "reference": "by backing each other up and not quitting",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.10976232721880529,
        "bleu2": 0.03880684294761699,
        "bleu3": 0.03109090746822916,
        "bleu4": 0.029486824119076216,
        "bert_precision": 0.8860896825790405,
        "bert_recall": 0.858714759349823,
        "bert_f1": 0.8721874952316284,
        "meteor": 0.06493506493506494,
        "sbert_similarity": 0.024054203182458878
      }
    },
    {
      "sample_id": 4,
      "question": "Where did Tim capture the painting of the sunset over the mountain range?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Smoky Mountains",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.882488489151001,
        "bert_recall": 0.8263599276542664,
        "bert_f1": 0.8535023927688599,
        "meteor": 0.0,
        "sbert_similarity": 0.1305241882801056
      }
    },
    {
      "sample_id": 4,
      "question": "What does Tim find rewarding about mentoring the younger players?",
      "prediction": "Seeing their growth, improvement, and confidence",
      "reference": "Seeing their growth, improvement, and confidence",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What has Tim been able to help the younger players achieve?",
      "prediction": "Not mentioned in the conversation or reach their goals",
      "reference": "reach their goals",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28867513459481287,
        "bleu3": 0.23173297416566233,
        "bleu4": 0.11868405219520975,
        "bert_precision": 0.8701009750366211,
        "bert_recall": 0.9178038239479065,
        "bert_f1": 0.8933160901069641,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.6458325386047363
      }
    },
    {
      "sample_id": 4,
      "question": "What genre is the novel that John is writing?",
      "prediction": "Fantasy",
      "reference": "Fantasy",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "Who is one of Tim's sources of inspiration for painting?",
      "prediction": "Not mentioned in the conversation",
      "reference": "J.K. Rowling",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8851850628852844,
        "bert_recall": 0.8949785828590393,
        "bert_f1": 0.8900548815727234,
        "meteor": 0.0,
        "sbert_similarity": 0.15399256348609924
      }
    },
    {
      "sample_id": 4,
      "question": "What does Tim write on the whiteboard to help him stay motivated?",
      "prediction": "motivational quotes",
      "reference": "motivational quotes and strategies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9858332872390747,
        "bert_recall": 0.9492732882499695,
        "bert_f1": 0.9672079682350159,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.9031543731689453
      }
    },
    {
      "sample_id": 4,
      "question": "What hobby is a therapy for Tim when away from the court?",
      "prediction": "Reading",
      "reference": "Cooking",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9412221908569336,
        "bert_recall": 0.9215259552001953,
        "bert_f1": 0.9312698841094971,
        "meteor": 0.0,
        "sbert_similarity": 0.3330439627170563
      }
    },
    {
      "sample_id": 4,
      "question": "What type of meal does Tim often cook using a slow cooker?",
      "prediction": "Not mentioned in the conversation or honey garlic chicken with roasted veg",
      "reference": "honey garlic chicken with roasted veg",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.4767312946227961,
        "bleu3": 0.45325271466910305,
        "bleu4": 0.4172261448611506,
        "bert_precision": 0.9060302972793579,
        "bert_recall": 0.9524039030075073,
        "bert_f1": 0.9286385178565979,
        "meteor": 0.906986531986532,
        "sbert_similarity": 0.8797706365585327
      }
    },
    {
      "sample_id": 4,
      "question": "How will Tim share the honey garlic chicken recipe with the other person?",
      "prediction": "Not mentioned in the conversation or write it down and mail it",
      "reference": "write it down and mail it",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.625,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.4767312946227961,
        "bleu3": 0.45325271466910305,
        "bleu4": 0.4172261448611506,
        "bert_precision": 0.8970085382461548,
        "bert_recall": 0.9288424253463745,
        "bert_f1": 0.9126479625701904,
        "meteor": 0.906986531986532,
        "sbert_similarity": 0.6229768991470337
      }
    },
    {
      "sample_id": 4,
      "question": "What is one of Tim's favorite crime TV shows, as mentioned on November 11, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "\"That\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8921865820884705,
        "bert_recall": 0.8928149938583374,
        "bert_f1": 0.8925006985664368,
        "meteor": 0.0,
        "sbert_similarity": 0.18117466568946838
      }
    },
    {
      "sample_id": 4,
      "question": "What was the setback Tim faced in his coding project on 21 November, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Story based on experiences in the UK didn't go as planned",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25000000000000006,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.13333333333333333,
        "rougeL_f": 0.23529411764705882,
        "bleu1": 0.0986387855766426,
        "bleu2": 0.07798080701378897,
        "bleu3": 0.03754375535859189,
        "bleu4": 0.028018824614254093,
        "bert_precision": 0.9013075232505798,
        "bert_recall": 0.8750630617141724,
        "bert_f1": 0.8879914879798889,
        "meteor": 0.18028846153846154,
        "sbert_similarity": 0.11717285215854645
      }
    },
    {
      "sample_id": 4,
      "question": "How did Tim overcome his ankle injury from last season?",
      "prediction": "Not mentioned in the conversation or stayed focused on recovery and worked hard to strengthen his body",
      "reference": "stayed focused on recovery and worked hard to strengthen his body",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7857142857142858,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.6470588235294118,
        "bleu2": 0.6359337738364604,
        "bleu3": 0.626674120796503,
        "bleu4": 0.6102169202557912,
        "bert_precision": 0.9284125566482544,
        "bert_recall": 0.9609010815620422,
        "bert_f1": 0.9443774223327637,
        "meteor": 0.9479196352237105,
        "sbert_similarity": 0.8653111457824707
      }
    },
    {
      "sample_id": 4,
      "question": "What motivated John to keep pushing himself to get better in writing and reading?",
      "prediction": "Love for writing and reading",
      "reference": "Love for writing and reading",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.996,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 4,
      "question": "How did Tim overcome a mistake he made during a big game in basketball?",
      "prediction": "Worked hard to get better and focused on growth",
      "reference": "Worked hard to get better and focused on growth",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What is Tim trying out to improve his strength and flexibility after recovery from ankle injury?",
      "prediction": "strength training",
      "reference": "yoga",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8996648788452148,
        "bert_recall": 0.8782562017440796,
        "bert_f1": 0.8888316750526428,
        "meteor": 0.0,
        "sbert_similarity": 0.3615325391292572
      }
    },
    {
      "sample_id": 4,
      "question": "What did John recently start learning in addition to being part of a travel club and working on studies?",
      "prediction": "an instrument",
      "reference": "an instrument",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998211860657,
        "bert_recall": 0.9999998211860657,
        "bert_f1": 0.9999998211860657,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What instrument is John learning to play in December 2023?",
      "prediction": "violin",
      "reference": "violin",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "How long has John been playing the piano for, as of December 2023?",
      "prediction": "Not mentioned in the conversation or about four months",
      "reference": "about four months",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.28867513459481287,
        "bleu3": 0.23173297416566233,
        "bleu4": 0.11868405219520975,
        "bert_precision": 0.8696920871734619,
        "bert_recall": 0.9023364782333374,
        "bert_f1": 0.8857135772705078,
        "meteor": 0.8179012345679013,
        "sbert_similarity": 0.5575175285339355
      }
    },
    {
      "sample_id": 4,
      "question": "What movie did Tim just finish watching on 8th December, 2023?",
      "prediction": "Not mentioned in the conversation or 'A Dance with Dragons'",
      "reference": "\"A Dance with Dragons\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.5,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.30151134457776363,
        "bleu3": 0.26648962915429364,
        "bleu4": 0.21200626759025185,
        "bert_precision": 0.8981999754905701,
        "bert_recall": 0.9407916069030762,
        "bert_f1": 0.9190025329589844,
        "meteor": 0.4076086956521739,
        "sbert_similarity": 0.8014737367630005
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of game did Tim have a career-high in assists in?",
      "prediction": "Last Friday",
      "reference": "basketball",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8479006886482239,
        "bert_recall": 0.8560276031494141,
        "bert_f1": 0.8519448041915894,
        "meteor": 0.0,
        "sbert_similarity": 0.19124066829681396
      }
    },
    {
      "sample_id": 4,
      "question": "What was Tim's way of dealing with doubts and stress when he was younger?",
      "prediction": "turning it into a learning experience",
      "reference": "practicing basketball outside for hours",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8939506411552429,
        "bert_recall": 0.8809303045272827,
        "bert_f1": 0.8873926997184753,
        "meteor": 0.0,
        "sbert_similarity": 0.1828019618988037
      }
    },
    {
      "sample_id": 4,
      "question": "Where was the photoshoot done for John's fragrance deal?",
      "prediction": "In a gorgeous forest",
      "reference": "In a gorgeous forest",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "In which area has Tim's team seen the most growth during training?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Communication and bonding",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8830487132072449,
        "bert_recall": 0.8746658563613892,
        "bert_f1": 0.8788372874259949,
        "meteor": 0.0,
        "sbert_similarity": 0.14412254095077515
      }
    },
    {
      "sample_id": 4,
      "question": "What type of seminars is Tim conducting?",
      "prediction": "Sports and marketing seminars",
      "reference": "Sports and marketing seminars",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What is one cause that John opposes with his influence and resources?",
      "prediction": "Not mentioned in the conversation or youth sports and fair chances in sports",
      "reference": "youth sports and fair chances in sports",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7058823529411764,
        "rouge1_f": 0.7000000000000001,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7000000000000001,
        "bleu1": 0.5384615384615384,
        "bleu2": 0.5188745216627708,
        "bleu3": 0.4999673727050369,
        "bleu4": 0.4703709593866897,
        "bert_precision": 0.9135788083076477,
        "bert_recall": 0.9610341787338257,
        "bert_f1": 0.9367058277130127,
        "meteor": 0.9197099892588615,
        "sbert_similarity": 0.831881046295166
      }
    },
    {
      "sample_id": 4,
      "question": "What new fantasy TV series is John excited about?",
      "prediction": "The Wheel of Time",
      "reference": "\"The Wheel of Time\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.6065306597126334,
        "bleu4": 0.6065306597126334,
        "bert_precision": 0.9739252328872681,
        "bert_recall": 0.9590457677841187,
        "bert_f1": 0.9664282202720642,
        "meteor": 0.46875,
        "sbert_similarity": 0.940045177936554
      }
    },
    {
      "sample_id": 4,
      "question": "Which language is John learning?",
      "prediction": "German",
      "reference": "German",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 4,
      "question": "According to John, who is his least favorite character from Lord of the Rings?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Aragorn",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8750364184379578,
        "bert_recall": 0.8163779973983765,
        "bert_f1": 0.8446900844573975,
        "meteor": 0.0,
        "sbert_similarity": 0.12168694287538528
      }
    },
    {
      "sample_id": 4,
      "question": "Why does Tim like Aragorn from Lord of the Rings?",
      "prediction": "brave, selfless, down-to-earth attitude",
      "reference": "brave, selfless, down-to-earth attitude",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 4,
      "question": "What kind of painting does Tim have in his room as a reminder?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a painting of Aragorn",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8795515894889832,
        "bert_recall": 0.8323438167572021,
        "bert_f1": 0.855296790599823,
        "meteor": 0.0,
        "sbert_similarity": 0.028259223327040672
      }
    },
    {
      "sample_id": 4,
      "question": "What is the sculpture of Aragorn a reminder for John to be in everything he does?",
      "prediction": "brave, selfless, down-to-earth",
      "reference": "be a leader",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8533759117126465,
        "bert_recall": 0.9328539371490479,
        "bert_f1": 0.8913467526435852,
        "meteor": 0.0,
        "sbert_similarity": 0.26403531432151794
      }
    },
    {
      "sample_id": 4,
      "question": "Which city in Ireland will John be staying in during his semester abroad?",
      "prediction": "Ireland",
      "reference": "Galway",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9344956278800964,
        "bert_recall": 0.9046026468276978,
        "bert_f1": 0.919306218624115,
        "meteor": 0.0,
        "sbert_similarity": 0.7373754382133484
      }
    },
    {
      "sample_id": 4,
      "question": "What charity event did Tim organize recently in 2024?",
      "prediction": "benefit basketball game",
      "reference": "benefit basketball game",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 4,
      "question": "What was Tims's reaction to sealing the deal with the beverage company?",
      "prediction": "sense of accomplishment",
      "reference": "crazy feeling, sense of accomplishment",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.36787944117144233,
        "bleu4": 0.20687381245863395,
        "bert_precision": 0.9485882520675659,
        "bert_recall": 0.9146156907081604,
        "bert_f1": 0.9312922358512878,
        "meteor": 0.6134259259259259,
        "sbert_similarity": 0.771248459815979
      }
    },
    {
      "sample_id": 5,
      "question": "Which year did Audrey adopt the first three of her dogs?",
      "prediction": "2023",
      "reference": "2020",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9242581725120544,
        "bert_recall": 0.9351127743721008,
        "bert_f1": 0.9296538233757019,
        "meteor": 0.0,
        "sbert_similarity": 0.6698888540267944
      }
    },
    {
      "sample_id": 5,
      "question": "When did Andrew start his new job as a financial analyst?",
      "prediction": "27 March 2023",
      "reference": "The week before March 27, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.2635971381157267,
        "bleu2": 0.05894211195011658,
        "bleu3": 0.045877610743154634,
        "bleu4": 0.03941696592147537,
        "bert_precision": 0.9484833478927612,
        "bert_recall": 0.9163476228713989,
        "bert_f1": 0.9321385622024536,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.8631802797317505
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of indoor activities has Andrew pursued with his girlfriend?",
      "prediction": "playing board games",
      "reference": "boardgames, volunteering at pet shelter, wine tasting, growing flowers",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8647521734237671,
        "bert_recall": 0.8347856998443604,
        "bert_f1": 0.8495047688484192,
        "meteor": 0.0,
        "sbert_similarity": 0.3901301920413971
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of places have Andrew and his girlfriend checked out around the city?",
      "prediction": "hiking trails",
      "reference": "cafes, new places to eat, open space for hikes, pet shelter, wine tasting event, park",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.901606559753418,
        "bert_recall": 0.8307332992553711,
        "bert_f1": 0.8647201657295227,
        "meteor": 0.0,
        "sbert_similarity": 0.42451024055480957
      }
    },
    {
      "sample_id": 5,
      "question": "When did Audrey make muffins for herself?",
      "prediction": "16 April",
      "reference": "The week of April 3rd to 9th",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.0410424993119494,
        "bleu2": 0.018354763685601443,
        "bleu3": 0.014286435890917338,
        "bleu4": 0.012274570264879268,
        "bert_precision": 0.9146760702133179,
        "bert_recall": 0.8519687652587891,
        "bert_f1": 0.8822094798088074,
        "meteor": 0.07692307692307691,
        "sbert_similarity": 0.5697710514068604
      }
    },
    {
      "sample_id": 5,
      "question": "When did Audrey see a hummingbird?",
      "prediction": "Last week",
      "reference": "first week of May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.11156508007421491,
        "bleu2": 0.04989342059223037,
        "bleu3": 0.03883455907572571,
        "bleu4": 0.03336574130316504,
        "bert_precision": 0.8726612329483032,
        "bert_recall": 0.8230531215667725,
        "bert_f1": 0.8471315503120422,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.4647393226623535
      }
    },
    {
      "sample_id": 5,
      "question": "When did Audrey adopt Pixie?",
      "prediction": "2023",
      "reference": "around April 2, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.00579191356895553,
        "bleu3": 0.004007025187889067,
        "bleu4": 0.003257032351754417,
        "bert_precision": 0.9352134466171265,
        "bert_recall": 0.877947211265564,
        "bert_f1": 0.905676007270813,
        "meteor": 0.13513513513513511,
        "sbert_similarity": 0.7351477742195129
      }
    },
    {
      "sample_id": 5,
      "question": "How many years passed between Audrey adopting Pixie and her other three dogs?",
      "prediction": "3",
      "reference": "three years",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8812121152877808,
        "bert_recall": 0.8638948202133179,
        "bert_f1": 0.8724675178527832,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.45293867588043213
      }
    },
    {
      "sample_id": 5,
      "question": "Did Andrew have a pet dog during March 2023?",
      "prediction": "No",
      "reference": "No",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of classes or groups has Audrey joined to take better care of her dogs?",
      "prediction": "a dog owners group",
      "reference": "positive reinforcement training workshop to bond with pets, dog training course, agility training course, grooming course, dog-owners group",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.2608695652173913,
        "rouge2_f": 0.1904761904761905,
        "rougeL_f": 0.2608695652173913,
        "bleu1": 0.005554498269121153,
        "bleu2": 0.00143416528618961,
        "bleu3": 0.0010703973376875182,
        "bleu4": 0.0010614010671086875,
        "bert_precision": 0.9275850057601929,
        "bert_recall": 0.8659141063690186,
        "bert_f1": 0.8956892490386963,
        "meteor": 0.06024096385542168,
        "sbert_similarity": 0.4631526470184326
      }
    },
    {
      "sample_id": 5,
      "question": "When did Audrey's positive reinforcement training course for dogs take place?",
      "prediction": "2023-04-16",
      "reference": "June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8623201847076416,
        "bert_recall": 0.8836439847946167,
        "bert_f1": 0.8728518486022949,
        "meteor": 0.0,
        "sbert_similarity": 0.647433876991272
      }
    },
    {
      "sample_id": 5,
      "question": "When did Andrew go rock climbing?",
      "prediction": "Last Sunday",
      "reference": "June 11, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8611956834793091,
        "bert_recall": 0.8595224618911743,
        "bert_f1": 0.8603582382202148,
        "meteor": 0.0,
        "sbert_similarity": 0.35324105620384216
      }
    },
    {
      "sample_id": 5,
      "question": "What outdoor activities has Andrew done other than hiking in nature?",
      "prediction": "Andrewsays: I've been getting into cooking more and trying out new recipes - it's been enjoyable.",
      "reference": "rock climbing, fishing, camping",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.823670506477356,
        "bert_recall": 0.8540384769439697,
        "bert_f1": 0.8385796546936035,
        "meteor": 0.0,
        "sbert_similarity": 0.23300299048423767
      }
    },
    {
      "sample_id": 5,
      "question": "When did Audrey move to a new place?",
      "prediction": "26 June 2023",
      "reference": "June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9583881497383118,
        "bert_recall": 0.96994549036026,
        "bert_f1": 0.9641321897506714,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.9385553598403931
      }
    },
    {
      "sample_id": 5,
      "question": "What is something that Andrew really misses while working in the city?",
      "prediction": "missing nature",
      "reference": "being in nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9322848320007324,
        "bert_recall": 0.9205987453460693,
        "bert_f1": 0.9264049530029297,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.6613380908966064
      }
    },
    {
      "sample_id": 5,
      "question": "What is a shared frustration regarding dog ownership for Audrey and Andrew?",
      "prediction": "finding a dog-friendly place in the city",
      "reference": "Not being able to find pet friendly spots.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9411216974258423,
        "bert_recall": 0.9128754138946533,
        "bert_f1": 0.9267833828926086,
        "meteor": 0.06329113924050632,
        "sbert_similarity": 0.6130167841911316
      }
    },
    {
      "sample_id": 5,
      "question": "When is Andrew going to go hiking with Audrey?",
      "prediction": "Next month",
      "reference": "August",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9056071639060974,
        "bert_recall": 0.9171715378761292,
        "bert_f1": 0.9113526344299316,
        "meteor": 0.0,
        "sbert_similarity": 0.5378770232200623
      }
    },
    {
      "sample_id": 5,
      "question": "How many times did Audrey and Andew plan to hike together?",
      "prediction": "Twice",
      "reference": "three times",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9271478652954102,
        "bert_recall": 0.9271478652954102,
        "bert_f1": 0.9271478652954102,
        "meteor": 0.0,
        "sbert_similarity": 0.7794877290725708
      }
    },
    {
      "sample_id": 5,
      "question": "Where did Audrey get Pixie from?",
      "prediction": "Adoption",
      "reference": "breeder",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8887450695037842,
        "bert_recall": 0.893703281879425,
        "bert_f1": 0.8912172913551331,
        "meteor": 0.0,
        "sbert_similarity": 0.3799956440925598
      }
    },
    {
      "sample_id": 5,
      "question": "What is an indoor activity that Andrew would enjoy doing while make his dog happy?",
      "prediction": "hiking",
      "reference": "cook dog treats",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8679143786430359,
        "bert_recall": 0.8606119751930237,
        "bert_f1": 0.864247739315033,
        "meteor": 0.0,
        "sbert_similarity": -0.07359732687473297
      }
    },
    {
      "sample_id": 5,
      "question": "Which meat does Audrey prefer eating more than others?",
      "prediction": "Audrey prefers dogs over other meats.",
      "reference": "chicken",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8502762317657471,
        "bert_recall": 0.8484635353088379,
        "bert_f1": 0.8493689298629761,
        "meteor": 0.0,
        "sbert_similarity": 0.2927826941013336
      }
    },
    {
      "sample_id": 5,
      "question": "What are the classes that Audrey took for her dogs to?",
      "prediction": "Agility classes",
      "reference": "Positive reinforcement training class for bonding, dog training course, agility class",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.3076923076923077,
        "rouge2_f": 0.18181818181818182,
        "rougeL_f": 0.3076923076923077,
        "bleu1": 0.0020433857192320333,
        "bleu2": 0.0009138298744910253,
        "bleu3": 0.0007112797604342066,
        "bleu4": 0.0006111148689636938,
        "bert_precision": 0.8998956084251404,
        "bert_recall": 0.863177478313446,
        "bert_f1": 0.8811542391777039,
        "meteor": 0.18564356435643564,
        "sbert_similarity": 0.6252243518829346
      }
    },
    {
      "sample_id": 5,
      "question": "Where did Andrew go during the first weekend of August 2023?",
      "prediction": "Andrew went on a hike.",
      "reference": "camping with girlfriend",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9204344749450684,
        "bert_recall": 0.8858053684234619,
        "bert_f1": 0.902787983417511,
        "meteor": 0.0,
        "sbert_similarity": 0.23047271370887756
      }
    },
    {
      "sample_id": 5,
      "question": "What are some problems that Andrew faces before he adopted Toby?",
      "prediction": "It's difficult because Toby is still young.",
      "reference": "Finding the right dog and pet-friendly apartments close to open spaces",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8523043990135193,
        "bert_recall": 0.8545385599136353,
        "bert_f1": 0.853420078754425,
        "meteor": 0.0,
        "sbert_similarity": 0.14900782704353333
      }
    },
    {
      "sample_id": 5,
      "question": "Did Audrey and Andrew grow up with a pet dog?",
      "prediction": "No",
      "reference": "Yes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9496124982833862,
        "bert_recall": 0.9496124982833862,
        "bert_f1": 0.9496124982833862,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 5,
      "question": "When did Andrew and his girlfriend go fishing?",
      "prediction": "24 August, 2023",
      "reference": "weekend before August 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.3501806396568502,
        "bleu3": 0.15705900667750333,
        "bleu4": 0.1225504602011117,
        "bert_precision": 0.9233071804046631,
        "bert_recall": 0.8855310082435608,
        "bert_f1": 0.9040246605873108,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.8143389225006104
      }
    },
    {
      "sample_id": 5,
      "question": "What is the biggest stressor in Andrew's life besides not being able to hike frequently?",
      "prediction": "work",
      "reference": "work",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "How does Andrew feel about his current work?",
      "prediction": "Andrew feels dedicated to making a positive impact despite challenges.",
      "reference": "Stressful",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8713067173957825,
        "bert_recall": 0.8575843572616577,
        "bert_f1": 0.8643910884857178,
        "meteor": 0.0,
        "sbert_similarity": 0.23391452431678772
      }
    },
    {
      "sample_id": 5,
      "question": "What is something that Audrey often dresses up her dogs with?",
      "prediction": "tags",
      "reference": "Hats",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8776581883430481,
        "bert_recall": 0.853506326675415,
        "bert_f1": 0.8654137849807739,
        "meteor": 0.0,
        "sbert_similarity": 0.32359713315963745
      }
    },
    {
      "sample_id": 5,
      "question": "What are the names of Audrey's dogs?",
      "prediction": "Pixie",
      "reference": "Pepper, Precious, Panda, and Pixie",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.0009118819655545162,
        "bleu2": 0.0002883623968383479,
        "bleu3": 0.00019949803698118587,
        "bleu4": 0.00016215809237314185,
        "bert_precision": 0.911062479019165,
        "bert_recall": 0.8700680732727051,
        "bert_f1": 0.8900935649871826,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.44096651673316956
      }
    },
    {
      "sample_id": 5,
      "question": "When is Andrew planning to go to the beach with his girlfriend?",
      "prediction": "24 August, 2023",
      "reference": "November 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9223012328147888,
        "bert_recall": 0.9532674551010132,
        "bert_f1": 0.9375287294387817,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.7884318232536316
      }
    },
    {
      "sample_id": 5,
      "question": "What has Andrew done with his dogs?",
      "prediction": "has hiked with his dogs",
      "reference": "Taking walks and hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8794431686401367,
        "bert_recall": 0.9244257211685181,
        "bert_f1": 0.901373565196991,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.49324819445610046
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of tattoo does Audrey have on her arm?",
      "prediction": "Audrey has tattoos on her arm featuring dogs and sunflowers.",
      "reference": "Tattoos of her four dogs.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.36363636363636365,
        "bleu2": 0.06030226891555274,
        "bleu3": 0.03549115908274777,
        "bleu4": 0.026658376817028855,
        "bert_precision": 0.8693751096725464,
        "bert_recall": 0.916579008102417,
        "bert_f1": 0.8923532366752625,
        "meteor": 0.18181818181818182,
        "sbert_similarity": 0.6332948207855225
      }
    },
    {
      "sample_id": 5,
      "question": "What can Andrew potentially do to improve his stress and accomodate his living situation with his dogs?",
      "prediction": "Andrew can take hikes next month as planned.",
      "reference": "Change to a hybrid or remote job so he can move away from the city to the suburbs to have a larger living space and be closer to nature.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0625,
        "rouge1_f": 0.05405405405405405,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05405405405405405,
        "bleu1": 0.02154932619209001,
        "bleu2": 0.005110871460669568,
        "bleu3": 0.003421243855513089,
        "bleu4": 0.002765401926782114,
        "bert_precision": 0.8693404793739319,
        "bert_recall": 0.8399362564086914,
        "bert_f1": 0.854385495185852,
        "meteor": 0.05576208178438661,
        "sbert_similarity": 0.2299315482378006
      }
    },
    {
      "sample_id": 5,
      "question": "How many months passed between Andrew adopting Toby and Buddy?",
      "prediction": "bytes",
      "reference": "three months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8697858452796936,
        "bert_recall": 0.863288164138794,
        "bert_f1": 0.866524875164032,
        "meteor": 0.0,
        "sbert_similarity": 0.09704671800136566
      }
    },
    {
      "sample_id": 5,
      "question": "What are the names of Andrew's dogs?",
      "prediction": "Not mentioned",
      "reference": "Toby, Scout, Buddy",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8908352851867676,
        "bert_recall": 0.8270829916000366,
        "bert_f1": 0.8577762246131897,
        "meteor": 0.0,
        "sbert_similarity": 0.11230265349149704
      }
    },
    {
      "sample_id": 5,
      "question": "What are some foods that Audrey likes eating?",
      "prediction": "sushi and pastries",
      "reference": "chicken pot pie, chicken roast, blueberry muffins, sushi",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.023161150407600514,
        "bleu2": 0.008970274980764192,
        "bleu3": 0.008415723708086742,
        "bleu4": 0.007894844292834697,
        "bert_precision": 0.912449300289154,
        "bert_recall": 0.8534106612205505,
        "bert_f1": 0.8819430470466614,
        "meteor": 0.06666666666666667,
        "sbert_similarity": 0.6697272062301636
      }
    },
    {
      "sample_id": 5,
      "question": "When did Audrey get into an accident in the park?",
      "prediction": "July 6, 2023",
      "reference": "between October 19 and 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.23618327637050734,
        "bleu2": 0.19284283762882742,
        "bleu3": 0.09730817497099951,
        "bleu4": 0.08025716728928957,
        "bert_precision": 0.9423290491104126,
        "bert_recall": 0.8921398520469666,
        "bert_f1": 0.9165478944778442,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.8388850688934326
      }
    },
    {
      "sample_id": 5,
      "question": "When did Andrew and his girlfriend go on a wine tasting trip?",
      "prediction": "2023",
      "reference": "the weekend before October 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.9093199968338013,
        "bert_recall": 0.8379428386688232,
        "bert_f1": 0.8721734881401062,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.6569377183914185
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey get wtih having so many dogs?",
      "prediction": "joy and happiness",
      "reference": "Companionship",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8479883670806885,
        "bert_recall": 0.8516013026237488,
        "bert_f1": 0.8497909903526306,
        "meteor": 0.0,
        "sbert_similarity": 0.44868990778923035
      }
    },
    {
      "sample_id": 5,
      "question": "What is a good place for dogs to run around freely and meet new friends?",
      "prediction": "a park",
      "reference": "The dog park",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9481714963912964,
        "bert_recall": 0.9228036403656006,
        "bert_f1": 0.9353156089782715,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.708055853843689
      }
    },
    {
      "sample_id": 5,
      "question": "What are the breeds of Audrey's dogs?",
      "prediction": "Pepper, Precious, Panda",
      "reference": "Mongrel mixed with Lab for Pepper and Panda. Mongrel mixed with Chihuahua for Precious and Pixie.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.42857142857142855,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.0,
        "rougeL_f": 0.21052631578947367,
        "bleu1": 0.04456414692860033,
        "bleu2": 0.009096618399790746,
        "bleu3": 0.006046366223211932,
        "bleu4": 0.0052519350818344916,
        "bert_precision": 0.9073839783668518,
        "bert_recall": 0.8492953777313232,
        "bert_f1": 0.877379298210144,
        "meteor": 0.0,
        "sbert_similarity": 0.6696561574935913
      }
    },
    {
      "sample_id": 5,
      "question": "What technique is Audrey using to discipline her dogs?",
      "prediction": "Positive Reinforcement",
      "reference": "Positive reinforcement",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9471379518508911,
        "bert_recall": 0.9607634544372559,
        "bert_f1": 0.9539020657539368,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "Which US state do Audrey and Andrew potentially live in?",
      "prediction": "California",
      "reference": "Minnesota",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9470997452735901,
        "bert_recall": 0.9470997452735901,
        "bert_f1": 0.9470997452735901,
        "meteor": 0.0,
        "sbert_similarity": 0.5743793845176697
      }
    },
    {
      "sample_id": 5,
      "question": "Which national park could Audrey and Andrew be referring to in their conversations?",
      "prediction": "national parks",
      "reference": "Voyageurs National Park",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9292975664138794,
        "bert_recall": 0.8270835876464844,
        "bert_f1": 0.8752163648605347,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.6955349445343018
      }
    },
    {
      "sample_id": 5,
      "question": "How many pets will Andrew have, as of December 2023?",
      "prediction": "No pets",
      "reference": "three",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8277311325073242,
        "bert_recall": 0.8476756811141968,
        "bert_f1": 0.8375847339630127,
        "meteor": 0.0,
        "sbert_similarity": 0.2340475171804428
      }
    },
    {
      "sample_id": 5,
      "question": "How many pets did Andrew have, as of September 2023?",
      "prediction": "1",
      "reference": "one",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9119826555252075,
        "bert_recall": 0.9119826555252075,
        "bert_f1": 0.9119826555252075,
        "meteor": 0.5,
        "sbert_similarity": 0.7313903570175171
      }
    },
    {
      "sample_id": 5,
      "question": "How many months passed between Andrew adopting Buddy and Scout",
      "prediction": "3",
      "reference": "one month",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8651463985443115,
        "bert_recall": 0.8488997220993042,
        "bert_f1": 0.8569461107254028,
        "meteor": 0.0,
        "sbert_similarity": 0.33009815216064453
      }
    },
    {
      "sample_id": 5,
      "question": "What does Andrew view his pets as?",
      "prediction": "friends and confidantes",
      "reference": "Family",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8792872428894043,
        "bert_recall": 0.9137986898422241,
        "bert_f1": 0.8962108492851257,
        "meteor": 0.0,
        "sbert_similarity": 0.3653602600097656
      }
    },
    {
      "sample_id": 5,
      "question": "What does Audrey view her pets as?",
      "prediction": "refuge",
      "reference": "Family",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8765355348587036,
        "bert_recall": 0.8947336673736572,
        "bert_f1": 0.8855411410331726,
        "meteor": 0.0,
        "sbert_similarity": 0.25834906101226807
      }
    },
    {
      "sample_id": 5,
      "question": "What is a skill that Audrey learned to take care of her dogs?",
      "prediction": "dog grooming",
      "reference": "Grooming",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9080325365066528,
        "bert_recall": 0.9147387742996216,
        "bert_f1": 0.9113732576370239,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.8116861581802368
      }
    },
    {
      "sample_id": 5,
      "question": "What items has Audrey bought or made for her dogs?",
      "prediction": "Audrey has bought toys and made a box filled with puppy pads, bed, and other doggy essentials.",
      "reference": "dog tags, toys, dog beds, collars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.1739130434782609,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1739130434782609,
        "bleu1": 0.15,
        "bleu2": 0.02809757434745082,
        "bleu3": 0.017056169390452146,
        "bleu4": 0.012673718536830811,
        "bert_precision": 0.8730259537696838,
        "bert_recall": 0.8852391839027405,
        "bert_f1": 0.8790901899337769,
        "meteor": 0.0,
        "sbert_similarity": 0.5579593181610107
      }
    },
    {
      "sample_id": 5,
      "question": "What is something that Andrew could do to make birdwatching hobby to fit in his city schedule?",
      "prediction": "Attend birdwatching events or join local bird clubs",
      "reference": "Install a bird feeder outside where he can see the birds without going outdoors.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.09090909090909091,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0.05210775245981356,
        "bleu2": 0.0176156354783007,
        "bleu3": 0.01337520252626669,
        "bleu4": 0.01157883880415623,
        "bert_precision": 0.8791707754135132,
        "bert_recall": 0.8661482334136963,
        "bert_f1": 0.872610867023468,
        "meteor": 0.11194029850746268,
        "sbert_similarity": 0.5694100856781006
      }
    },
    {
      "sample_id": 5,
      "question": "What is a career that Andrew could potentially pursue with his love for animals and nature?",
      "prediction": "animal welfare work",
      "reference": "Park ranger or a similar position working for the National Park Services.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8558123111724854,
        "bert_recall": 0.8605359196662903,
        "bert_f1": 0.8581676483154297,
        "meteor": 0.04504504504504504,
        "sbert_similarity": 0.37363553047180176
      }
    },
    {
      "sample_id": 5,
      "question": "What activity do Audrey's dogs like to do in the dog park?",
      "prediction": "playing fetch",
      "reference": "Play fetch with ball and frisbee, run around and meet other dogs",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.16666666666666669,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.0020433857192320333,
        "bleu2": 0.0009138298744910253,
        "bleu3": 0.0007112797604342066,
        "bleu4": 0.0006111148689636938,
        "bert_precision": 0.9152339696884155,
        "bert_recall": 0.8298245668411255,
        "bert_f1": 0.8704391717910767,
        "meteor": 0.17045454545454544,
        "sbert_similarity": 0.46920132637023926
      }
    },
    {
      "sample_id": 5,
      "question": "When did Andrew make his dogs a fun indoor area?",
      "prediction": "2023",
      "reference": "few days before November 22, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.9031063914299011,
        "bert_recall": 0.8431609869003296,
        "bert_f1": 0.872104823589325,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.6636579632759094
      }
    },
    {
      "sample_id": 5,
      "question": "Has Andrew moved into a new apartment for his dogs?",
      "prediction": "Yes",
      "reference": "No",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9496124982833862,
        "bert_recall": 0.9496124982833862,
        "bert_f1": 0.9496124982833862,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 5,
      "question": "When did Andrew adopt Scout?",
      "prediction": "22 November 2023",
      "reference": "few days before November 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.3422780793550613,
        "bleu2": 0.29642151188002913,
        "bleu3": 0.16711688476494407,
        "bleu4": 0.12336445138236939,
        "bert_precision": 0.9454410076141357,
        "bert_recall": 0.9043738842010498,
        "bert_f1": 0.9244515895843506,
        "meteor": 0.39062499999999994,
        "sbert_similarity": 0.8097096085548401
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey eat for dinner on October 24, 2023?",
      "prediction": "dinner",
      "reference": "sushi",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9083917140960693,
        "bert_recall": 0.9028820991516113,
        "bert_f1": 0.9056285619735718,
        "meteor": 0.0,
        "sbert_similarity": 0.2821897566318512
      }
    },
    {
      "sample_id": 5,
      "question": "How long has it been since Andrew adopted his first pet, as of November 2023?",
      "prediction": "since you last spoke",
      "reference": "4 months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8411293029785156,
        "bert_recall": 0.8657854795455933,
        "bert_f1": 0.8532792925834656,
        "meteor": 0.0,
        "sbert_similarity": 0.3578317165374756
      }
    },
    {
      "sample_id": 5,
      "question": "How many dogs does Andrew have?",
      "prediction": "four",
      "reference": "3",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9292528629302979,
        "bert_recall": 0.9292528629302979,
        "bert_f1": 0.9292528629302979,
        "meteor": 0.0,
        "sbert_similarity": 0.720665454864502
      }
    },
    {
      "sample_id": 5,
      "question": "Which specific type of bird mesmerizes Andrew?",
      "prediction": "Eagles",
      "reference": "Eagles",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew express missing about exploring nature trails with his family's dog?",
      "prediction": "exploring nature",
      "reference": "The peaceful moments",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8984553217887878,
        "bert_recall": 0.8987085819244385,
        "bert_f1": 0.8985819220542908,
        "meteor": 0.0,
        "sbert_similarity": 0.30066922307014465
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of pastries did Andrew and his girlfriend have at the cafe?",
      "prediction": "muffins",
      "reference": "croissants, muffins, and tarts",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9175961017608643,
        "bert_recall": 0.8788315653800964,
        "bert_f1": 0.897795557975769,
        "meteor": 0.0,
        "sbert_similarity": 0.685418426990509
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of flowers does Audrey have a tattoo of?",
      "prediction": "Audrey has a tattoo of sunflowers.",
      "reference": "sunflowers",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2857142857142857,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8649749755859375,
        "bert_recall": 0.8769211173057556,
        "bert_f1": 0.8709070682525635,
        "meteor": 0.0,
        "sbert_similarity": 0.5281230807304382
      }
    },
    {
      "sample_id": 5,
      "question": "What does Audrey do during dog playdates in the park?",
      "prediction": "It filled my heart with joy - their happiness brought me so much peace.",
      "reference": "chat with people while dogs make new friends",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8424216508865356,
        "bert_recall": 0.8503307104110718,
        "bert_f1": 0.8463577032089233,
        "meteor": 0.058139534883720936,
        "sbert_similarity": 0.09043923765420914
      }
    },
    {
      "sample_id": 5,
      "question": "What type of dog was Andrew looking to adopt based on his living space?",
      "prediction": "a smaller dog",
      "reference": "smaller dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9442857503890991,
        "bert_recall": 0.9442857503890991,
        "bert_f1": 0.9442857503890991,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.9558970928192139
      }
    },
    {
      "sample_id": 5,
      "question": "Where does Andrew want to live to give their dog a large, open space to run around?",
      "prediction": "Andrew wants to live in the city.",
      "reference": "near a park or woods",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8955588340759277,
        "bert_recall": 0.8720059990882874,
        "bert_f1": 0.8836255073547363,
        "meteor": 0.0,
        "sbert_similarity": 0.17502132058143616
      }
    },
    {
      "sample_id": 5,
      "question": "Why did Audrey sign up for a workshop about bonding with pets?",
      "prediction": "Audrey signed up for a workshop to learn more about the benefits and techniques for creating strong bonds with her dogs.",
      "reference": "Strengthen the bond with her pets",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23076923076923075,
        "rouge1_f": 0.2962962962962963,
        "rouge2_f": 0.16000000000000003,
        "rougeL_f": 0.2962962962962963,
        "bleu1": 0.13636363636363635,
        "bleu2": 0.08058229640253801,
        "bleu3": 0.033020090829322155,
        "bleu4": 0.020331710659744474,
        "bert_precision": 0.8777086734771729,
        "bert_recall": 0.898867666721344,
        "bert_f1": 0.8881621956825256,
        "meteor": 0.5,
        "sbert_similarity": 0.6452019214630127
      }
    },
    {
      "sample_id": 5,
      "question": "How did Audrey hear about the workshop on bonding with pets?",
      "prediction": "Audrey found out through word of mouth at the beach",
      "reference": "Saw a workshop flyer at the local pet store",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2105263157894737,
        "rouge1_f": 0.2105263157894737,
        "rouge2_f": 0.11764705882352941,
        "rougeL_f": 0.2105263157894737,
        "bleu1": 0.2,
        "bleu2": 0.14907119849998599,
        "bleu3": 0.06705330284284157,
        "bleu4": 0.0446323613785333,
        "bert_precision": 0.8732858896255493,
        "bert_recall": 0.8821230530738831,
        "bert_f1": 0.8776822090148926,
        "meteor": 0.206043956043956,
        "sbert_similarity": 0.15273305773735046
      }
    },
    {
      "sample_id": 5,
      "question": "What type of training was the workshop Audrey signed up for in May 2023?",
      "prediction": "positive reinforcement training",
      "reference": "Positive reinforcement training",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9721710681915283,
        "bert_recall": 0.9536827802658081,
        "bert_f1": 0.9628381729125977,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "How did Audrey describe she dog he met at the pet store?",
      "prediction": "Pixie",
      "reference": "Friendly and playful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8765503168106079,
        "bert_recall": 0.8677617907524109,
        "bert_f1": 0.8721339106559753,
        "meteor": 0.0,
        "sbert_similarity": 0.23985862731933594
      }
    },
    {
      "sample_id": 5,
      "question": "Why did Audrey think positive reinforcement training is important for pets?",
      "prediction": "Audrey thinks it's important because she has seen her dogs respond well to it and it helps build trust.",
      "reference": "To have pets learn how to behave in a positive way",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07142857142857142,
        "rouge1_f": 0.06451612903225806,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06451612903225806,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.009134423666564471,
        "bert_precision": 0.8721255660057068,
        "bert_recall": 0.8873553276062012,
        "bert_f1": 0.87967449426651,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.4902151823043823
      }
    },
    {
      "sample_id": 5,
      "question": "What challenge is Andrew facing in their search for a pet?",
      "prediction": "It's tough finding a pet-friendly spot in the city.",
      "reference": "Finding a pet-friendly spot in the city",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8750000000000001,
        "rouge1_f": 0.8421052631578948,
        "rouge2_f": 0.8235294117647058,
        "rougeL_f": 0.8421052631578948,
        "bleu1": 0.6363636363636364,
        "bleu2": 0.6179143806533247,
        "bleu3": 0.5994773144013525,
        "bleu4": 0.5706745777055999,
        "bert_precision": 0.95289146900177,
        "bert_recall": 0.9775697588920593,
        "bert_f1": 0.9650729298591614,
        "meteor": 0.8314043209876543,
        "sbert_similarity": 0.8914457559585571
      }
    },
    {
      "sample_id": 5,
      "question": "How does Andrew feel about their search for a pet-friendly place?",
      "prediction": "Andrew feels determined and persistent, despite being discouraged by his search thus far.",
      "reference": "Discouraged but determined",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.13333333333333333,
        "bleu2": 0.030860669992418384,
        "bleu3": 0.020202551616772847,
        "bleu4": 0.015718877363021206,
        "bert_precision": 0.8902164697647095,
        "bert_recall": 0.9028162360191345,
        "bert_f1": 0.8964720964431763,
        "meteor": 0.25,
        "sbert_similarity": 0.5166244506835938
      }
    },
    {
      "sample_id": 5,
      "question": "What outdoor activities does Andrew plan on trying after the rock climbing class?",
      "prediction": "hiking",
      "reference": "kayaking and bungee jumping",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8825106620788574,
        "bert_recall": 0.8484910130500793,
        "bert_f1": 0.8651665449142456,
        "meteor": 0.0,
        "sbert_similarity": 0.388110488653183
      }
    },
    {
      "sample_id": 5,
      "question": "How long does Audrey typically walk her dogs for?",
      "prediction": "15-20 minutes",
      "reference": "about an hour",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8857834339141846,
        "bert_recall": 0.905680239200592,
        "bert_f1": 0.8956213593482971,
        "meteor": 0.0,
        "sbert_similarity": 0.6746402978897095
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey set up in the backyard for their dogs on June 26, 2023?",
      "prediction": "A new place with a bigger backyard",
      "reference": "a doggy play area with agility stuff and toys",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.21470779802151024,
        "bleu2": 0.05185685985790754,
        "bleu3": 0.03539349040407053,
        "bleu4": 0.029519129053903102,
        "bert_precision": 0.8917272090911865,
        "bert_recall": 0.8480308055877686,
        "bert_f1": 0.8693302273750305,
        "meteor": 0.11363636363636365,
        "sbert_similarity": 0.31291618943214417
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey and her friends stumble across during a hike a few years back, as mentioned on June 26, 2023?",
      "prediction": "Scout",
      "reference": "a stunning lake in the mountains",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8601911067962646,
        "bert_recall": 0.8682202100753784,
        "bert_f1": 0.8641870021820068,
        "meteor": 0.0,
        "sbert_similarity": 0.13621068000793457
      }
    },
    {
      "sample_id": 5,
      "question": "What is Audrey's favorite recipe that she shares with Andrew on 3 July, 2023?",
      "prediction": "The recipe is healthy and reminds Audrey of her grandma's kitchen.",
      "reference": "Chicken Pot Pie",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8475916385650635,
        "bert_recall": 0.8283101320266724,
        "bert_f1": 0.8378399610519409,
        "meteor": 0.0,
        "sbert_similarity": 0.3681595027446747
      }
    },
    {
      "sample_id": 5,
      "question": "What dish is one of Audrey's favorite dishes that includes garlic and is shared with Andrew on 3 July, 2023?",
      "prediction": "Greek chicken salad",
      "reference": "Roasted Chicken",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9156439304351807,
        "bert_recall": 0.8969876766204834,
        "bert_f1": 0.9062197804450989,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.5597484707832336
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew and his GF do on the Monday before July 24, 2023?",
      "prediction": "Exploring outdoors",
      "reference": "volunteered at a pet shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8725618124008179,
        "bert_recall": 0.8580678701400757,
        "bert_f1": 0.8652541637420654,
        "meteor": 0.0,
        "sbert_similarity": 0.19800111651420593
      }
    },
    {
      "sample_id": 5,
      "question": "What is the name of Audrey's childhood dog?",
      "prediction": "Max",
      "reference": "Max",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What special memories does Audrey have with her childhood dog, Max?",
      "prediction": "Audrey has lots of great memories with her childhood dog, Max, who loved a game of fetch and had lots of energy.",
      "reference": "Long walks in the neighborhood, exploring new paths, sharing worries and hopes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06451612903225808,
        "rouge1_f": 0.05882352941176471,
        "rouge2_f": 0.0,
        "rougeL_f": 0.05882352941176471,
        "bleu1": 0.12000000000000001,
        "bleu2": 0.022360679774997897,
        "bleu3": 0.013529754476103547,
        "bleu4": 0.009970222994544455,
        "bert_precision": 0.8595380187034607,
        "bert_recall": 0.8663415312767029,
        "bert_f1": 0.8629263639450073,
        "meteor": 0.038461538461538464,
        "sbert_similarity": 0.11939572542905807
      }
    },
    {
      "sample_id": 5,
      "question": "What are some of the personalities of Audrey's four fur babies?",
      "prediction": "Pepper, Precious, Panda, and Pixie",
      "reference": "oldest is relaxed, second is playful, third can be naughty but loves cuddles, youngest is full of life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.07384187820157277,
        "bleu2": 0.014412458864714317,
        "bleu3": 0.009078822377358563,
        "bleu4": 0.007198209131192452,
        "bert_precision": 0.8509441614151001,
        "bert_recall": 0.8128129243850708,
        "bert_f1": 0.8314416408538818,
        "meteor": 0.0,
        "sbert_similarity": 0.252546101808548
      }
    },
    {
      "sample_id": 5,
      "question": "What type of classes did Audrey start with her pups recently on 4 August, 2023?",
      "prediction": "agility classes",
      "reference": "Agility classes",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9610538482666016,
        "bert_recall": 0.9610538482666016,
        "bert_f1": 0.9610538482666016,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 5,
      "question": "How often does Audrey take her pups to the park for practice?",
      "prediction": "The information provided does not mention how often Audrey takes her pups to the park for practice.",
      "reference": "Twice a week",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8588102459907532,
        "bert_recall": 0.8727719187736511,
        "bert_f1": 0.8657347559928894,
        "meteor": 0.0,
        "sbert_similarity": 0.23611053824424744
      }
    },
    {
      "sample_id": 5,
      "question": "How long did the trail hike that Audrey went on with her pups take?",
      "prediction": "It is not specified how long the trail hike took in the context provided.",
      "reference": "Two hours",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8545178174972534,
        "bert_recall": 0.886604905128479,
        "bert_f1": 0.8702656626701355,
        "meteor": 0.0,
        "sbert_similarity": 0.3230319023132324
      }
    },
    {
      "sample_id": 5,
      "question": "What advice did Audrey give to Andrew regarding grooming Toby?",
      "prediction": "Thanks! I'll remember them if I ever get to groom Toby.",
      "reference": "Grooming slowly and gently, paying attention to sensitive areas like ears and paws. And remember to stay patient and positive throughout the grooming process.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.0846680917129044,
        "bleu2": 0.016041740470847245,
        "bleu3": 0.009822526232782002,
        "bleu4": 0.007427549185526758,
        "bert_precision": 0.8585259318351746,
        "bert_recall": 0.8451434373855591,
        "bert_f1": 0.8517821431159973,
        "meteor": 0.06607929515418502,
        "sbert_similarity": 0.3921954333782196
      }
    },
    {
      "sample_id": 5,
      "question": "What is essential to keep the dogs looking good according to Audrey?",
      "prediction": "Taking them to the groomer",
      "reference": "Daily brushing, regular baths, nail trims, and lots of love",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8831592798233032,
        "bert_recall": 0.8651796579360962,
        "bert_f1": 0.8740770220756531,
        "meteor": 0.0,
        "sbert_similarity": 0.34136390686035156
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey organize with the neighbors' dogs?",
      "prediction": "hiking",
      "reference": "a doggy playdate",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.895301342010498,
        "bert_recall": 0.8646581172943115,
        "bert_f1": 0.8797129392623901,
        "meteor": 0.0,
        "sbert_similarity": 0.13173002004623413
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey do to give her dogs extra comfort as the weather cooled down?",
      "prediction": "walks them on the trails",
      "reference": "Got new beds for them",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20000000000000004,
        "rouge1_f": 0.20000000000000004,
        "rouge2_f": 0.0,
        "rougeL_f": 0.20000000000000004,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8709911704063416,
        "bert_recall": 0.8547899127006531,
        "bert_f1": 0.8628144860267639,
        "meteor": 0.10000000000000002,
        "sbert_similarity": 0.2155013531446457
      }
    },
    {
      "sample_id": 5,
      "question": "How does Audrey describe the new beds for her dogs?",
      "prediction": "The new bed for Audrey's dogs is described as luxurious and comfortable, providing them with ample space to rest in peace.",
      "reference": "Super cozy and comfy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08,
        "rouge1_f": 0.07692307692307693,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07692307692307693,
        "bleu1": 0.041666666666666664,
        "bleu2": 0.013459547551454133,
        "bleu3": 0.009821111002978165,
        "bleu4": 0.007913247271422612,
        "bert_precision": 0.8616846799850464,
        "bert_recall": 0.9086191654205322,
        "bert_f1": 0.884529709815979,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.48434358835220337
      }
    },
    {
      "sample_id": 5,
      "question": "How did Audrey calm down her dog after the leash incident?",
      "prediction": "Audrey patiently and gently rubbed soothing pressure on Toby's tired paws, praising him softly to help calm him down.",
      "reference": "Petted, hugged, spoke calmly and slowly walked the dog",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07407407407407407,
        "rouge1_f": 0.06896551724137932,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06896551724137932,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.020806259464411975,
        "bleu3": 0.013510398157576722,
        "bleu4": 0.010331208012220438,
        "bert_precision": 0.8614481091499329,
        "bert_recall": 0.8841245174407959,
        "bert_f1": 0.8726390600204468,
        "meteor": 0.05,
        "sbert_similarity": 0.5544653534889221
      }
    },
    {
      "sample_id": 5,
      "question": "How often does Audrey take her dogs for walks?",
      "prediction": "It's fulfilling",
      "reference": "Multiple times a day",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8701944351196289,
        "bert_recall": 0.891211986541748,
        "bert_f1": 0.880577802658081,
        "meteor": 0.0,
        "sbert_similarity": 0.17505697906017303
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of flowers does Audrey take care of?",
      "prediction": "Audrey takes care of flowers.",
      "reference": "Peruvian Lilies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.891676127910614,
        "bert_recall": 0.8630200028419495,
        "bert_f1": 0.8771141171455383,
        "meteor": 0.0,
        "sbert_similarity": 0.12018600851297379
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew learn from reading books about ecological systems?",
      "prediction": "Andrew learned fascinating insights into how different elements within ecosystems work together.",
      "reference": "about animals, plants, and ecosystems and how they work together",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.380952380952381,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.09999999999999999,
        "rougeL_f": 0.2727272727272727,
        "bleu1": 0.30769230769230765,
        "bleu2": 0.16012815380508713,
        "bleu3": 0.0632832964415753,
        "bleu4": 0.039073802494525016,
        "bert_precision": 0.8861253261566162,
        "bert_recall": 0.898139476776123,
        "bert_f1": 0.8920919895172119,
        "meteor": 0.14705882352941177,
        "sbert_similarity": 0.5700423717498779
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew suggest as a way to reduce carbon footprint?",
      "prediction": "biking or using public transport",
      "reference": "biking or using public transport",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "How does Andrew suggest helping the planet while also training the body?",
      "prediction": "Bike riding",
      "reference": "by biking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9170148968696594,
        "bert_recall": 0.9139493107795715,
        "bert_f1": 0.9154795408248901,
        "meteor": 0.25,
        "sbert_similarity": 0.6551218032836914
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey do with her pups over the weekend before 4th October, 2023?",
      "prediction": "took them to the beach and it was so fun to see them playing in the ocean.",
      "reference": "Took them to the beach",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5263157894736842,
        "rouge1_f": 0.45454545454545453,
        "rouge2_f": 0.4,
        "rougeL_f": 0.45454545454545453,
        "bleu1": 0.2777777777777778,
        "bleu2": 0.25565499628245686,
        "bleu3": 0.23396034203975458,
        "bleu4": 0.20105373454060027,
        "bert_precision": 0.8987714052200317,
        "bert_recall": 0.9493189454078674,
        "bert_f1": 0.9233538508415222,
        "meteor": 0.40322580645161293,
        "sbert_similarity": 0.7549705505371094
      }
    },
    {
      "sample_id": 5,
      "question": "What was the reason Audrey couldn't walk her dogs for a period of time?",
      "prediction": "A knee injury",
      "reference": "Knee injury",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.970228910446167,
        "bert_recall": 0.970228910446167,
        "bert_f1": 0.970228910446167,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.9738548994064331
      }
    },
    {
      "sample_id": 5,
      "question": "What type of jewelry does Audrey make?",
      "prediction": "jewelry made with recycled objects",
      "reference": "Jewelry made from recycled objects",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8000000000000002,
        "rouge1_f": 0.8000000000000002,
        "rouge2_f": 0.5,
        "rougeL_f": 0.8000000000000002,
        "bleu1": 0.8,
        "bleu2": 0.6324555320336759,
        "bleu3": 0.2405635273174191,
        "bleu4": 0.16068568378893033,
        "bert_precision": 0.9754878282546997,
        "bert_recall": 0.9784567952156067,
        "bert_f1": 0.9769700765609741,
        "meteor": 0.7500000000000001,
        "sbert_similarity": 0.9894618391990662
      }
    },
    {
      "sample_id": 5,
      "question": "Why does Audrey make jewelry out of recycled objects?",
      "prediction": "It is a great way to show her love of creativity and sustainability.",
      "reference": "To show love for creativity and sustainability",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.6,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3144854510165755,
        "bleu3": 0.20525160292459657,
        "bleu4": 0.09303723531853667,
        "bert_precision": 0.9280743598937988,
        "bert_recall": 0.9479844570159912,
        "bert_f1": 0.937923789024353,
        "meteor": 0.5868421052631579,
        "sbert_similarity": 0.6677093505859375
      }
    },
    {
      "sample_id": 5,
      "question": "What organization does Audrey donate a portion of his profits to?",
      "prediction": "A cause that's close to her heart.",
      "reference": "Animal shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.863017201423645,
        "bert_recall": 0.8692640066146851,
        "bert_f1": 0.8661293387413025,
        "meteor": 0.0,
        "sbert_similarity": 0.1628793179988861
      }
    },
    {
      "sample_id": 5,
      "question": "How does Audrey help out the animal shelter?",
      "prediction": "Adopted another pup from a shelter.",
      "reference": "By donating a portion of his profits frmo selling jwelery",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.09306272250443651,
        "bleu2": 0.03178697118830404,
        "bleu3": 0.02440851883376367,
        "bleu4": 0.02151810250197126,
        "bert_precision": 0.870020866394043,
        "bert_recall": 0.8148659467697144,
        "bert_f1": 0.8415406942367554,
        "meteor": 0.05208333333333333,
        "sbert_similarity": 0.17757552862167358
      }
    },
    {
      "sample_id": 5,
      "question": "What type of games do Audrey's dogs like to play at the park?",
      "prediction": "hide-and-seek",
      "reference": "Fetch and Frisbee",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9026686549186707,
        "bert_recall": 0.8603225350379944,
        "bert_f1": 0.8809869885444641,
        "meteor": 0.0,
        "sbert_similarity": 0.21634045243263245
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey make to thank her neighbors?",
      "prediction": "special items",
      "reference": "Goodies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9299485683441162,
        "bert_recall": 0.9318898320198059,
        "bert_f1": 0.9309182167053223,
        "meteor": 0.0,
        "sbert_similarity": 0.2689650356769562
      }
    },
    {
      "sample_id": 5,
      "question": "How do Audrey's dogs react to snow?",
      "prediction": "They were so confused!",
      "reference": "Confused",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8867230415344238,
        "bert_recall": 0.9188839197158813,
        "bert_f1": 0.9025170803070068,
        "meteor": 0.0,
        "sbert_similarity": 0.4934633672237396
      }
    },
    {
      "sample_id": 5,
      "question": "How does Audrey describe her dogs' response to snow?",
      "prediction": "They definitely prefer nice, sunny days in the grass.",
      "reference": "They definitely prefer nice, sunny days in the grass.",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of experiences are Audrey's dogs the best companions for?",
      "prediction": "Audrey's dogs provide emotional support and companionship, especially during tough times.",
      "reference": "Exploring the great outdoors",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8571649193763733,
        "bert_recall": 0.8862751126289368,
        "bert_f1": 0.871476948261261,
        "meteor": 0.0,
        "sbert_similarity": 0.006033333949744701
      }
    },
    {
      "sample_id": 5,
      "question": "What activity do Andrew and Buddy enjoy doing together?",
      "prediction": "walking",
      "reference": "Walking",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.907476544380188,
        "bert_recall": 0.9066905975341797,
        "bert_f1": 0.9070833921432495,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "What do Andrew and Buddy like doing on walks?",
      "prediction": "Walking",
      "reference": "Checking out new hiking trails",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9154726266860962,
        "bert_recall": 0.8974291086196899,
        "bert_f1": 0.9063610434532166,
        "meteor": 0.0,
        "sbert_similarity": 0.33732372522354126
      }
    },
    {
      "sample_id": 5,
      "question": "What cuisine did Andrew recently try at a new spot in town?",
      "prediction": "Sushi",
      "reference": "sushi",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9757910370826721,
        "bert_recall": 0.9757910370826721,
        "bert_f1": 0.9757910370826721,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "Which type of sushi did Audrey suggest trying first to someone new to sushi?",
      "prediction": "California roll",
      "reference": "California or salmon roll",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.928949773311615,
        "bert_recall": 0.8827334046363831,
        "bert_f1": 0.9052520990371704,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7313237190246582
      }
    },
    {
      "sample_id": 5,
      "question": "What type of date is Andrew going on Sunday?",
      "prediction": "Picnic date",
      "reference": "picnic date",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.969449520111084,
        "bert_recall": 0.969449520111084,
        "bert_f1": 0.969449520111084,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew and Audrey plan to do on the Saturday after October 28, 2023?",
      "prediction": "They planned to take their dogs hiking.",
      "reference": "Go hiking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8957977294921875,
        "bert_recall": 0.9239702820777893,
        "bert_f1": 0.9096659421920776,
        "meteor": 0.0,
        "sbert_similarity": 0.44194716215133667
      }
    },
    {
      "sample_id": 5,
      "question": "What aspect of autumn does Andrew find beautiful?",
      "prediction": "autumn colors",
      "reference": "The autumn colors",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9432432055473328,
        "bert_recall": 0.9432432055473328,
        "bert_f1": 0.9432432055473328,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.9775059819221497
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey do in November 2023 to better take care of her dogs?",
      "prediction": "A hike with the furry friends next month was planned.",
      "reference": "Joined a dog owners group",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8681988716125488,
        "bert_recall": 0.8988529443740845,
        "bert_f1": 0.8832600116729736,
        "meteor": 0.09090909090909091,
        "sbert_similarity": 0.34585171937942505
      }
    },
    {
      "sample_id": 5,
      "question": "How often does Audrey meet up with other dog owners for tips and playdates?",
      "prediction": "Every week",
      "reference": "Once a week",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9567930698394775,
        "bert_recall": 0.9529764652252197,
        "bert_f1": 0.954880952835083,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.8300042152404785
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey share to show ways to keep dogs active in the city?",
      "prediction": "Speaker Audreysays : [Image: a photography of a dog running with a tennis ball in its mouth] You got it! There are lots of ways to keep them happy in the city. Make sure to socialize and exercise them daily.",
      "reference": "photography of a basket full of stuffed animals",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14634146341463417,
        "rouge1_f": 0.1702127659574468,
        "rouge2_f": 0.08888888888888889,
        "rougeL_f": 0.1702127659574468,
        "bleu1": 0.08695652173913045,
        "bleu2": 0.06216698721602093,
        "bleu3": 0.045857470828745715,
        "bleu4": 0.021259332643596612,
        "bert_precision": 0.833091676235199,
        "bert_recall": 0.864544153213501,
        "bert_f1": 0.8485265374183655,
        "meteor": 0.2818080357142857,
        "sbert_similarity": 0.3867146968841553
      }
    },
    {
      "sample_id": 5,
      "question": "What type of activities does Audrey suggest for mental stimulation of the dogs?",
      "prediction": "add some mental stimulation too.",
      "reference": "puzzles, training, hide-and-seek",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8738870620727539,
        "bert_recall": 0.8344303369522095,
        "bert_f1": 0.8537030220031738,
        "meteor": 0.0,
        "sbert_similarity": 0.27463293075561523
      }
    },
    {
      "sample_id": 5,
      "question": "What is Andrew planning to do with Scout, Toby, and Buddy?",
      "prediction": "Andrew plans to take Scout, Toby, and Buddy to a nearby park",
      "reference": "Take them to a nearby park",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5882352941176471,
        "rouge1_f": 0.5555555555555556,
        "rouge2_f": 0.37499999999999994,
        "rougeL_f": 0.5555555555555556,
        "bleu1": 0.3571428571428572,
        "bleu2": 0.2870846258816073,
        "bleu3": 0.24293866735461883,
        "bleu4": 0.18798317647335086,
        "bert_precision": 0.8930833339691162,
        "bert_recall": 0.9531071186065674,
        "bert_f1": 0.9221194982528687,
        "meteor": 0.7333333333333334,
        "sbert_similarity": 0.4555394649505615
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew get for Scout to create a safe and fun space for them?",
      "prediction": "took Scout, Toby, and Buddy to a nearby park",
      "reference": "essentials like a bed, toys, and puppy pads",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.23529411764705882,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.36363636363636365,
        "bleu2": 0.19069251784911848,
        "bleu3": 0.07587875263140755,
        "bleu4": 0.04740604259878138,
        "bert_precision": 0.8692938089370728,
        "bert_recall": 0.8706420660018921,
        "bert_f1": 0.8699674010276794,
        "meteor": 0.1234567901234568,
        "sbert_similarity": 0.20400400459766388
      }
    },
    {
      "sample_id": 5,
      "question": "Which specific type of bird mesmerizes Audrey?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Eagles",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.873216450214386,
        "bert_recall": 0.8313682079315186,
        "bert_f1": 0.8517786264419556,
        "meteor": 0.0,
        "sbert_similarity": 0.1378367692232132
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of flowers does Andrew have a tattoo of?",
      "prediction": "sunflowers",
      "reference": "sunflowers",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "What type of dog was Audrey looking to adopt based on her living space?",
      "prediction": "Not mentioned in the conversation",
      "reference": "smaller dog",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8837457895278931,
        "bert_recall": 0.8777905702590942,
        "bert_f1": 0.8807581067085266,
        "meteor": 0.0,
        "sbert_similarity": 0.11759894341230392
      }
    },
    {
      "sample_id": 5,
      "question": "Why did Audrey sign up for a workshop about car maintenance?",
      "prediction": "Not mentioned in the conversation or Strengthen the bond with her pets",
      "reference": "Strengthen the bond with her pets",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7058823529411764,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.4767312946227961,
        "bleu3": 0.45325271466910305,
        "bleu4": 0.4172261448611506,
        "bert_precision": 0.9180282950401306,
        "bert_recall": 0.9746553301811218,
        "bert_f1": 0.9454946517944336,
        "meteor": 0.906986531986532,
        "sbert_similarity": 0.798656165599823
      }
    },
    {
      "sample_id": 5,
      "question": "How did Andrew hear about the workshop on bonding with pets?",
      "prediction": "Saw a workshop flyer at the local pet store",
      "reference": "Saw a workshop flyer at the local pet store",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "What type of training was the workshop Andrew signed up for in May 2023?",
      "prediction": "Positive reinforcement training",
      "reference": "Positive reinforcement training",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "How did Andrew describe the dog he met at the pet store?",
      "prediction": "Friendly and playful",
      "reference": "Friendly and playful",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "What challenge is Audrey facing in their search for a pet?",
      "prediction": "Finding a pet-friendly spot in the city",
      "reference": "Finding a pet-friendly spot in the city",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "What indoor activities does Andrew plan on trying after the rock climbing class?",
      "prediction": "Not mentioned in the conversation",
      "reference": "kayaking and bungee jumping",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8746657371520996,
        "bert_recall": 0.8068405389785767,
        "bert_f1": 0.8393852114677429,
        "meteor": 0.0,
        "sbert_similarity": -0.009816008619964123
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew set up in the backyard for their dogs on June 26, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a doggy play area with agility stuff and toys",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8784646391868591,
        "bert_recall": 0.829254686832428,
        "bert_f1": 0.8531506657600403,
        "meteor": 0.0,
        "sbert_similarity": -0.0011022631078958511
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey and her GF do on the Monday before July 24, 2023?",
      "prediction": "Not mentioned in the conversation or volunteered at a pet shelter",
      "reference": "volunteered at a pet shelter",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.625,
        "rouge1_f": 0.625,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.625,
        "bleu1": 0.45454545454545453,
        "bleu2": 0.4264014327112209,
        "bleu3": 0.3964882289652318,
        "bleu4": 0.3508439695638686,
        "bert_precision": 0.8867624998092651,
        "bert_recall": 0.922558069229126,
        "bert_f1": 0.904306173324585,
        "meteor": 0.8892857142857143,
        "sbert_similarity": 0.7958429455757141
      }
    },
    {
      "sample_id": 5,
      "question": "What is the name of Andrew's childhood dog?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Max",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.874416172504425,
        "bert_recall": 0.8788846731185913,
        "bert_f1": 0.8766447305679321,
        "meteor": 0.0,
        "sbert_similarity": 0.09914705157279968
      }
    },
    {
      "sample_id": 5,
      "question": "What special memories does Andrew have with his childhood dog, Max?",
      "prediction": "Not mentioned in the conversation or Long walks in the neighborhood, exploring new paths, sharing worries and hopes",
      "reference": "Long walks in the neighborhood, exploring new paths, sharing worries and hopes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8,
        "rouge2_f": 0.7857142857142858,
        "rougeL_f": 0.8,
        "bleu1": 0.7,
        "bleu2": 0.6920602346769048,
        "bleu3": 0.6860959499274262,
        "bleu4": 0.6741942962447748,
        "bert_precision": 0.9355446696281433,
        "bert_recall": 0.9738547205924988,
        "bert_f1": 0.9543153643608093,
        "meteor": 0.9521053791887125,
        "sbert_similarity": 0.8363491296768188
      }
    },
    {
      "sample_id": 5,
      "question": "What are some of the personalities of Andrew's four fur babies?",
      "prediction": "Not mentioned in the conversation",
      "reference": "oldest is relaxed, second is playful, third can be naughty but loves cuddles, youngest is full of life",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8641314506530762,
        "bert_recall": 0.8102274537086487,
        "bert_f1": 0.8363118171691895,
        "meteor": 0.0,
        "sbert_similarity": 0.07691003382205963
      }
    },
    {
      "sample_id": 5,
      "question": "What type of classes did Andrew start with his pups recently on 4 August, 2023?",
      "prediction": "Agility classes",
      "reference": "Agility classes",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 5,
      "question": "What is essential to keep the dogs looking good according to Andrew?",
      "prediction": "Daily brushing, regular baths, nail trims, and lots of love",
      "reference": "Daily brushing, regular baths, nail trims, and lots of love",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9995,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What did Audrey organize with the neighbors' cats?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a doggy playdate",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8800382614135742,
        "bert_recall": 0.8625089526176453,
        "bert_f1": 0.8711854219436646,
        "meteor": 0.0,
        "sbert_similarity": 0.0761180967092514
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew do to give his dogs extra comfort as the weather cooled down?",
      "prediction": "Got new beds for them",
      "reference": "Got new beds for them",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "How does Andrew describe the new beds for his dogs?",
      "prediction": "Super cozy and comfy",
      "reference": "Super cozy and comfy",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 5,
      "question": "How did Andrew calm down his dog after the leash incident?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Petted, hugged, spoke calmly and slowly walked the dog",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8824383616447449,
        "bert_recall": 0.8558719158172607,
        "bert_f1": 0.8689520955085754,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.09985148161649704
      }
    },
    {
      "sample_id": 5,
      "question": "How often does Andrew take his dogs for walks?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Multiple times a day",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8987183570861816,
        "bert_recall": 0.913589358329773,
        "bert_f1": 0.9060928225517273,
        "meteor": 0.0,
        "sbert_similarity": 0.06161557137966156
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of vegetables does Audrey take care of?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Peruvian Lilies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8752123713493347,
        "bert_recall": 0.8366943597793579,
        "bert_f1": 0.8555200695991516,
        "meteor": 0.0,
        "sbert_similarity": 0.12073709070682526
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew learn from reading books about economic systems?",
      "prediction": "Not mentioned in the conversation or about animals, plants, and ecosystems and how they work together.",
      "reference": "about animals, plants, and ecosystems and how they work together",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7692307692307693,
        "rouge2_f": 0.7499999999999999,
        "rougeL_f": 0.7692307692307693,
        "bleu1": 0.631578947368421,
        "bleu2": 0.6212607441973956,
        "bleu3": 0.6130736327213653,
        "bleu4": 0.5977992387152216,
        "bert_precision": 0.9269213676452637,
        "bert_recall": 0.9715819954872131,
        "bert_f1": 0.9487264156341553,
        "meteor": 0.848474260423946,
        "sbert_similarity": 0.8064850568771362
      }
    },
    {
      "sample_id": 5,
      "question": "What was the reason Andrew couldn't walk his dogs for a period of time?",
      "prediction": "Knee injury",
      "reference": "Knee injury",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What type of jewelry does Andrew make?",
      "prediction": "Jewelry made from recycled objects",
      "reference": "Jewelry made from recycled objects",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "Why does Andrew make jewelry out of recycled objects?",
      "prediction": "To show love for creativity and sustainability",
      "reference": "To show love for creativity and sustainability",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 5,
      "question": "What type of games do Andrew's dogs like to play at the park?",
      "prediction": "Fetch and Frisbee",
      "reference": "Fetch and Frisbee",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What did Andrew make to thank his neighbors?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Goodies",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8831706643104553,
        "bert_recall": 0.8933297395706177,
        "bert_f1": 0.8882210850715637,
        "meteor": 0.0,
        "sbert_similarity": 0.07701004296541214
      }
    },
    {
      "sample_id": 5,
      "question": "How do Andrew's dogs react to snow?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Confused",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8949558734893799,
        "bert_recall": 0.8949867486953735,
        "bert_f1": 0.8949713110923767,
        "meteor": 0.0,
        "sbert_similarity": 0.16087903082370758
      }
    },
    {
      "sample_id": 5,
      "question": "How does Andrew describe his dogs' response to snow?",
      "prediction": "They definitely prefer nice, sunny days in the grass.",
      "reference": "They definitely prefer nice, sunny days in the grass.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 5,
      "question": "What kind of experiences are Audrey's cats the best companions for?",
      "prediction": "Exploring the great outdoors",
      "reference": "Exploring the great outdoors",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 5,
      "question": "What activity do Audrey and Buddy enjoy doing together?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Walking",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8909865021705627,
        "bert_recall": 0.8921170830726624,
        "bert_f1": 0.8915514349937439,
        "meteor": 0.0,
        "sbert_similarity": 0.1288113296031952
      }
    },
    {
      "sample_id": 5,
      "question": "What type of drink did Andrew recently try at a new spot in town?",
      "prediction": "Not mentioned in the conversation",
      "reference": "sushi",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8717828989028931,
        "bert_recall": 0.8361905813217163,
        "bert_f1": 0.8536158800125122,
        "meteor": 0.0,
        "sbert_similarity": 0.09109530597925186
      }
    },
    {
      "sample_id": 5,
      "question": "Which type of pizza did Audrey suggest trying first to someone new to Italian cuisine?",
      "prediction": "Not mentioned in the conversation",
      "reference": "California or salmon roll",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8690217733383179,
        "bert_recall": 0.8354476094245911,
        "bert_f1": 0.851904034614563,
        "meteor": 0.0,
        "sbert_similarity": 0.04712406545877457
      }
    },
    {
      "sample_id": 5,
      "question": "How often does Andrew meet up with other dog owners for tips and playdates?",
      "prediction": "Once a week",
      "reference": "Once a week",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 6,
      "question": "What are John's suspected health problems?",
      "prediction": "seasonal canine disease",
      "reference": "Obesity",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8598290085792542,
        "bert_recall": 0.8807657957077026,
        "bert_f1": 0.8701714873313904,
        "meteor": 0.0,
        "sbert_similarity": -0.0011973674409091473
      }
    },
    {
      "sample_id": 6,
      "question": "Which recreational activity was James pursuing on March 16, 2022?",
      "prediction": "gaming",
      "reference": "bowling",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8914545774459839,
        "bert_recall": 0.8719031810760498,
        "bert_f1": 0.8815705180168152,
        "meteor": 0.0,
        "sbert_similarity": 0.4096030592918396
      }
    },
    {
      "sample_id": 6,
      "question": "Which places or events have John and James planned to meet at?",
      "prediction": "Johnsays : See you John, bye!",
      "reference": "VR Club, McGee's, baseball game",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.823508083820343,
        "bert_recall": 0.7947710752487183,
        "bert_f1": 0.8088844418525696,
        "meteor": 0.0,
        "sbert_similarity": 0.12207231670618057
      }
    },
    {
      "sample_id": 6,
      "question": "Do both James and John have pets?",
      "prediction": "Yes",
      "reference": "No",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9496124982833862,
        "bert_recall": 0.9496124982833862,
        "bert_f1": 0.9496124982833862,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 6,
      "question": "When did John resume playing drums in his adulthood?",
      "prediction": "2022",
      "reference": "February 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9192354679107666,
        "bert_recall": 0.9168193936347961,
        "bert_f1": 0.9180258512496948,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8130249977111816
      }
    },
    {
      "sample_id": 6,
      "question": "What are John and James' favorite games?",
      "prediction": "John: Video games. James: Board games",
      "reference": "John's favorite game is CS:GO, and James's is Apex Legends.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.20418925512707853,
        "bleu2": 0.03424357910543963,
        "bleu3": 0.020374789148516276,
        "bleu4": 0.01558062783946341,
        "bert_precision": 0.8992657661437988,
        "bert_recall": 0.861175537109375,
        "bert_f1": 0.8798086047172546,
        "meteor": 0.05208333333333333,
        "sbert_similarity": 0.644708514213562
      }
    },
    {
      "sample_id": 6,
      "question": "Does James live in Connecticut?",
      "prediction": "no",
      "reference": "Likely yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8728929162025452,
        "bert_recall": 0.8363488912582397,
        "bert_f1": 0.8542301654815674,
        "meteor": 0.0,
        "sbert_similarity": 0.5132695436477661
      }
    },
    {
      "sample_id": 6,
      "question": "In which state is the shelter from which James adopted the puppy?",
      "prediction": "Georgia",
      "reference": "Connecticut.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9316766858100891,
        "bert_recall": 0.9371824264526367,
        "bert_f1": 0.9344214797019958,
        "meteor": 0.0,
        "sbert_similarity": 0.4380294680595398
      }
    },
    {
      "sample_id": 6,
      "question": "How many pets does James have?",
      "prediction": "three",
      "reference": "Three dogs.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.8855372071266174,
        "bert_recall": 0.8994675278663635,
        "bert_f1": 0.8924480080604553,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.5097525119781494
      }
    },
    {
      "sample_id": 6,
      "question": "What are the names of James's dogs?",
      "prediction": "My two dogs",
      "reference": "Ned, Daisy, Max",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8906104564666748,
        "bert_recall": 0.8799833059310913,
        "bert_f1": 0.8852649927139282,
        "meteor": 0.0,
        "sbert_similarity": 0.24028944969177246
      }
    },
    {
      "sample_id": 6,
      "question": "When did James adopt Ned?",
      "prediction": "26 August",
      "reference": "first week of April 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8648812770843506,
        "bert_recall": 0.8399209976196289,
        "bert_f1": 0.8522184491157532,
        "meteor": 0.0,
        "sbert_similarity": 0.5040250420570374
      }
    },
    {
      "sample_id": 6,
      "question": "How was John feeling on April 10, 2022?",
      "prediction": "friendly and supportive",
      "reference": "seeking solitude",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8926730155944824,
        "bert_recall": 0.8982987403869629,
        "bert_f1": 0.8954770565032959,
        "meteor": 0.0,
        "sbert_similarity": 0.3231728672981262
      }
    },
    {
      "sample_id": 6,
      "question": "Did James have a girlfriend during April 2022?",
      "prediction": "Yes",
      "reference": "Presumably not",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8895553350448608,
        "bert_recall": 0.8593803644180298,
        "bert_f1": 0.8742075562477112,
        "meteor": 0.0,
        "sbert_similarity": 0.43804606795310974
      }
    },
    {
      "sample_id": 6,
      "question": "When did James visit Italy?",
      "prediction": "20 April 2022",
      "reference": "In 2021",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8888876438140869,
        "bert_recall": 0.9187527894973755,
        "bert_f1": 0.9035734534263611,
        "meteor": 0.0,
        "sbert_similarity": 0.6562689542770386
      }
    },
    {
      "sample_id": 6,
      "question": "When did James buy himself a new adventure book?",
      "prediction": "27 March, 2022",
      "reference": "April 26, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9225189685821533,
        "bert_recall": 0.9467458724975586,
        "bert_f1": 0.9344754219055176,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9102283716201782
      }
    },
    {
      "sample_id": 6,
      "question": "When did James start playing Civilization VI?",
      "prediction": "29 April, 2022",
      "reference": "March 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9008649587631226,
        "bert_recall": 0.9339233040809631,
        "bert_f1": 0.9170962572097778,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.812516987323761
      }
    },
    {
      "sample_id": 6,
      "question": "What is the game with different colored cards that was John talking about with James?",
      "prediction": "The game with different colored cards that was John talking about with James was a multi-colored card placement game, sometimes requiring trading or drawing extra from the deck.",
      "reference": "UNO",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7896538972854614,
        "bert_recall": 0.8199559450149536,
        "bert_f1": 0.8045196533203125,
        "meteor": 0.0,
        "sbert_similarity": 0.03383800387382507
      }
    },
    {
      "sample_id": 6,
      "question": "What is the board game where you have to find the imposter that John mentions to James?",
      "prediction": "Among several games, I remember there was a medieval castle with its own labyrinth full of puzzles and traps.",
      "reference": "Mafia",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.832451581954956,
        "bert_recall": 0.8667070865631104,
        "bert_f1": 0.8492340445518494,
        "meteor": 0.0,
        "sbert_similarity": 0.1586187779903412
      }
    },
    {
      "sample_id": 6,
      "question": "Which books has John recommended to James?",
      "prediction": "The Stormlight Archive and Kingkiller Chronicle",
      "reference": "The Name of the Wind, Stormlight Archive, Kingkiller Chronicles, Expanse",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5333333333333333,
        "rouge1_f": 0.625,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.625,
        "bleu1": 0.20760214927639847,
        "bleu2": 0.11370838014523826,
        "bleu3": 0.04741034224287906,
        "bleu4": 0.03197146058806063,
        "bert_precision": 0.9448720216751099,
        "bert_recall": 0.9025617837905884,
        "bert_f1": 0.92323237657547,
        "meteor": 0.15625,
        "sbert_similarity": 0.6796936988830566
      }
    },
    {
      "sample_id": 6,
      "question": "Was James feeling lonely before meeting Samantha?",
      "prediction": "No",
      "reference": "Most likely yes, because he mentioned that the only creatures that gave him joy are dogs and he was actively trying to date.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8417235016822815,
        "bert_recall": 0.8022416830062866,
        "bert_f1": 0.8215084671974182,
        "meteor": 0.0,
        "sbert_similarity": 0.13839133083820343
      }
    },
    {
      "sample_id": 6,
      "question": "How many charity tournaments has John organized till date?",
      "prediction": "1",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9115664958953857,
        "bert_recall": 0.9115664958953857,
        "bert_f1": 0.9115664958953857,
        "meteor": 0.0,
        "sbert_similarity": 0.6732218861579895
      }
    },
    {
      "sample_id": 6,
      "question": "When did John first organize a charity tournament with his friends?",
      "prediction": "2022",
      "reference": "May 7, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.8971660137176514,
        "bert_recall": 0.8955636620521545,
        "bert_f1": 0.896364152431488,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7144732475280762
      }
    },
    {
      "sample_id": 6,
      "question": "Who or which organizations have been the beneficiaries of John's charity tournaments?",
      "prediction": "The children's hospital and other charitable causes",
      "reference": "animal shelter, homeless, children's hospital",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.375,
        "bleu2": 0.32732683535398854,
        "bleu3": 0.2649097550589377,
        "bleu4": 0.1374708101760565,
        "bert_precision": 0.8933062553405762,
        "bert_recall": 0.8841404914855957,
        "bert_f1": 0.8886997103691101,
        "meteor": 0.3605769230769231,
        "sbert_similarity": 0.5757347345352173
      }
    },
    {
      "sample_id": 6,
      "question": "When will John start his new job?",
      "prediction": "2022-06-13",
      "reference": "In July, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8369719386100769,
        "bert_recall": 0.8472767472267151,
        "bert_f1": 0.8420928120613098,
        "meteor": 0.0,
        "sbert_similarity": 0.6912426948547363
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of games has James tried to develop?",
      "prediction": "James has tried developing board games.",
      "reference": "football simulator, virtual world inspired by Witcher 3",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8801247477531433,
        "bert_recall": 0.8591809868812561,
        "bert_f1": 0.869526743888855,
        "meteor": 0.0,
        "sbert_similarity": 0.2890711724758148
      }
    },
    {
      "sample_id": 6,
      "question": "Are John and James fans of the same football team?",
      "prediction": "No",
      "reference": "No, James is a Liverpool fan and John is a Manchester City fan.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 8.315287191035679e-07,
        "bleu2": 2.629524692209641e-07,
        "bleu3": 1.8191866208667132e-07,
        "bleu4": 1.478690400037914e-07,
        "bert_precision": 0.8675006628036499,
        "bert_recall": 0.8348996639251709,
        "bert_f1": 0.8508880138397217,
        "meteor": 0.0,
        "sbert_similarity": 0.2735685110092163
      }
    },
    {
      "sample_id": 6,
      "question": "Which countries has James visited?",
      "prediction": "James visited Italy, Turkey, and Mexico.",
      "reference": "Italy, Mexico, Turkey, Canada, Greenland",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.45643546458763845,
        "bleu3": 0.3135508904498039,
        "bleu4": 0.14923729480049117,
        "bert_precision": 0.9400078058242798,
        "bert_recall": 0.9266400337219238,
        "bert_f1": 0.9332760572433472,
        "meteor": 0.19607843137254904,
        "sbert_similarity": 0.4956166744232178
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of classes has James joined?",
      "prediction": "a course on programming",
      "reference": "game design course, cooking classes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.05536842069051654,
        "bleu3": 0.04649251600872798,
        "bleu4": 0.04873039689743777,
        "bert_precision": 0.915330708026886,
        "bert_recall": 0.9066035151481628,
        "bert_f1": 0.9109461903572083,
        "meteor": 0.0,
        "sbert_similarity": 0.48242849111557007
      }
    },
    {
      "sample_id": 6,
      "question": "When did James volunteer at an organization?",
      "prediction": "13 June 2022",
      "reference": "May 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9310647249221802,
        "bert_recall": 0.9473956823348999,
        "bert_f1": 0.9391592144966125,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8171349763870239
      }
    },
    {
      "sample_id": 6,
      "question": "When did James depart for his trip to Canada?",
      "prediction": "9 July",
      "reference": "July 11, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.9060890674591064,
        "bert_recall": 0.9002491235733032,
        "bert_f1": 0.9031596779823303,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.6010373830795288
      }
    },
    {
      "sample_id": 6,
      "question": "Which country did James book tickets for in July 2022?",
      "prediction": "Vancouver",
      "reference": "Canada",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9143856763839722,
        "bert_recall": 0.9362410306930542,
        "bert_f1": 0.9251842498779297,
        "meteor": 0.0,
        "sbert_similarity": 0.7244583368301392
      }
    },
    {
      "sample_id": 6,
      "question": "How many days did James plan to spend on his trip in Canada?",
      "prediction": "N/A",
      "reference": "19 days",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8536408543586731,
        "bert_recall": 0.8793886303901672,
        "bert_f1": 0.8663234710693359,
        "meteor": 0.0,
        "sbert_similarity": 0.17974340915679932
      }
    },
    {
      "sample_id": 6,
      "question": "Where was James at on July 12, 2022?",
      "prediction": "Seattle",
      "reference": "Toronto, Canada",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.89469313621521,
        "bert_recall": 0.8685666918754578,
        "bert_f1": 0.8814363479614258,
        "meteor": 0.0,
        "sbert_similarity": 0.5747106075286865
      }
    },
    {
      "sample_id": 6,
      "question": "Did John and James study together?",
      "prediction": "No",
      "reference": "Yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9496124982833862,
        "bert_recall": 0.9496124982833862,
        "bert_f1": 0.9496124982833862,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 6,
      "question": "Which countries did James visit in July 2022?",
      "prediction": "Italy, Turkey, and Mexico",
      "reference": "Canada, Greenland",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.9086753129959106,
        "bert_recall": 0.9100985527038574,
        "bert_f1": 0.909386396408081,
        "meteor": 0.0,
        "sbert_similarity": 0.32557767629623413
      }
    },
    {
      "sample_id": 6,
      "question": "What additional country did James visit during his trip to Canada?",
      "prediction": "Nuuk",
      "reference": "Greenland",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8456356525421143,
        "bert_recall": 0.9109514951705933,
        "bert_f1": 0.8770792484283447,
        "meteor": 0.0,
        "sbert_similarity": 0.36771899461746216
      }
    },
    {
      "sample_id": 6,
      "question": "Who is Jill?",
      "prediction": "Jill",
      "reference": "Most likely John's partner.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8894969820976257,
        "bert_recall": 0.8940940499305725,
        "bert_f1": 0.8917896151542664,
        "meteor": 0.0,
        "sbert_similarity": 0.2654361128807068
      }
    },
    {
      "sample_id": 6,
      "question": "When did John spend time with his sister and dogs?",
      "prediction": "20 April 2022",
      "reference": "July 21, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9310735464096069,
        "bert_recall": 0.9414275884628296,
        "bert_f1": 0.9362219572067261,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8470727205276489
      }
    },
    {
      "sample_id": 6,
      "question": "What happened to John's job situation in 2022?",
      "prediction": "John got his dream job after lots of interviews and late nights.",
      "reference": "quit his IT Job, secured his dream job, aspires to become an eSports competition organizer",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.24000000000000002,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.16,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.16964803398269643,
        "bleu2": 0.14417310595340352,
        "bleu3": 0.11370253246407157,
        "bleu4": 0.056530056609206736,
        "bert_precision": 0.8864901065826416,
        "bert_recall": 0.8874086141586304,
        "bert_f1": 0.8869491219520569,
        "meteor": 0.12755102040816324,
        "sbert_similarity": 0.42015668749809265
      }
    },
    {
      "sample_id": 6,
      "question": "When did John start his job in IT?",
      "prediction": "2022",
      "reference": "2019",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9083740711212158,
        "bert_recall": 0.9107277989387512,
        "bert_f1": 0.9095494747161865,
        "meteor": 0.0,
        "sbert_similarity": 0.5572811365127563
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of tricks do James's pets know?",
      "prediction": "Max",
      "reference": "swimming, catching frisbees, balancing on a skateboard, sit, stay, paw, and rollover",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.870424747467041,
        "bert_recall": 0.7655277252197266,
        "bert_f1": 0.8146132230758667,
        "meteor": 0.0,
        "sbert_similarity": 0.09819188714027405
      }
    },
    {
      "sample_id": 6,
      "question": "When did James meet Samantha?",
      "prediction": "2022",
      "reference": "August 9, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.8967989683151245,
        "bert_recall": 0.8936354517936707,
        "bert_f1": 0.8952144384384155,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.6614817380905151
      }
    },
    {
      "sample_id": 6,
      "question": "When did James take his 3 dogs to the beach?",
      "prediction": "Yesterday",
      "reference": "August 9, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.86814284324646,
        "bert_recall": 0.8739498257637024,
        "bert_f1": 0.87103670835495,
        "meteor": 0.0,
        "sbert_similarity": 0.3632773756980896
      }
    },
    {
      "sample_id": 6,
      "question": "When did John plan his next meeting with his siblings?",
      "prediction": "21 August, 2022",
      "reference": "In September, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9439993500709534,
        "bert_recall": 0.9439993500709534,
        "bert_f1": 0.9439993500709534,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7233264446258545
      }
    },
    {
      "sample_id": 6,
      "question": "Why didn't John want to go to Starbucks?",
      "prediction": "John wanted to meet up somewhere else.",
      "reference": "Possibly because he likes to drink beer on his days off.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.03624713984635017,
        "bleu3": 0.024462491968492806,
        "bleu4": 0.020034704329441457,
        "bert_precision": 0.891811728477478,
        "bert_recall": 0.8681402802467346,
        "bert_f1": 0.8798167705535889,
        "meteor": 0.04716981132075472,
        "sbert_similarity": 0.2637898921966553
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of beer does McGee's bar serve?",
      "prediction": "stout",
      "reference": "Stout, lager",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.8824471235275269,
        "bert_recall": 0.8170899152755737,
        "bert_f1": 0.848511815071106,
        "meteor": 0.0,
        "sbert_similarity": 0.8480827808380127
      }
    },
    {
      "sample_id": 6,
      "question": "When did John and James meet at McGee's bar?",
      "prediction": "9:18 pm on 26 August, 2022",
      "reference": "August 27, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.8631191253662109,
        "bert_recall": 0.9440891742706299,
        "bert_f1": 0.9017902612686157,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.7838459610939026
      }
    },
    {
      "sample_id": 6,
      "question": "When did James ask Samantha to be his girlfriend?",
      "prediction": "23 April 2022",
      "reference": "September 3, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.935433030128479,
        "bert_recall": 0.9435893297195435,
        "bert_f1": 0.9394934773445129,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7855903506278992
      }
    },
    {
      "sample_id": 6,
      "question": "When did James, Samantha and John go to the baseball game together?",
      "prediction": "Next Sunday",
      "reference": "September 11, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8805915117263794,
        "bert_recall": 0.8924486637115479,
        "bert_f1": 0.886480450630188,
        "meteor": 0.0,
        "sbert_similarity": 0.38235923647880554
      }
    },
    {
      "sample_id": 6,
      "question": "What gaming equipments did John buy or refurbish?",
      "prediction": "John bought new devices and refurbished his gaming desk.",
      "reference": "Sennheiser headphones, Logitech mouse, gaming desk",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.15384615384615385,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.2,
        "bleu2": 0.14907119849998599,
        "bleu3": 0.06705330284284157,
        "bleu4": 0.0446323613785333,
        "bert_precision": 0.8917107582092285,
        "bert_recall": 0.8520762920379639,
        "bert_f1": 0.8714431524276733,
        "meteor": 0.07936507936507937,
        "sbert_similarity": 0.39909225702285767
      }
    },
    {
      "sample_id": 6,
      "question": "When did James start taking cooking classes?",
      "prediction": "3 October 2022",
      "reference": "September 2, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9396016597747803,
        "bert_recall": 0.944914698600769,
        "bert_f1": 0.9422507286071777,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8005412220954895
      }
    },
    {
      "sample_id": 6,
      "question": "Which new games did John start play during the course of the conversation with James?",
      "prediction": "board games",
      "reference": "AC Valhalla, Witcher 3, FIFA 23, Dungeons of the Dragons, futuristic dystopian game",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.13333333333333336,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333336,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8518565893173218,
        "bert_recall": 0.7974650859832764,
        "bert_f1": 0.8237639665603638,
        "meteor": 0.04201680672268908,
        "sbert_similarity": 0.4839073419570923
      }
    },
    {
      "sample_id": 6,
      "question": "When did John start working on his 2D Adventure mobile game?",
      "prediction": "the past few months",
      "reference": "approximately summer of 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8659198880195618,
        "bert_recall": 0.8622462749481201,
        "bert_f1": 0.8640792369842529,
        "meteor": 0.0,
        "sbert_similarity": 0.21478371322155
      }
    },
    {
      "sample_id": 6,
      "question": "How long did it take for James to complete his Witcher-inspired game?",
      "prediction": "A few days",
      "reference": "six months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9043991565704346,
        "bert_recall": 0.9078044295310974,
        "bert_f1": 0.9060985445976257,
        "meteor": 0.0,
        "sbert_similarity": 0.5545676946640015
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of programming-related events has John hosted?",
      "prediction": "Programming seminars and online gaming tournaments",
      "reference": "online programming competition, programming seminar",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.08164965809277261,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.048549177170732344,
        "bert_precision": 0.912236213684082,
        "bert_recall": 0.9223536252975464,
        "bert_f1": 0.9172670245170593,
        "meteor": 0.5010893246187365,
        "sbert_similarity": 0.6665927171707153
      }
    },
    {
      "sample_id": 6,
      "question": "When did John and his programming friends host an online programming competition?",
      "prediction": "2022-10-13",
      "reference": "Last week before 13 October 2022.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8565231561660767,
        "bert_recall": 0.8562291264533997,
        "bert_f1": 0.8563761115074158,
        "meteor": 0.0,
        "sbert_similarity": 0.556950569152832
      }
    },
    {
      "sample_id": 6,
      "question": "Which of James's family members have visited him in the last year?",
      "prediction": "Siblings",
      "reference": "mother, sister",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9028276205062866,
        "bert_recall": 0.8923384547233582,
        "bert_f1": 0.8975524306297302,
        "meteor": 0.0,
        "sbert_similarity": 0.6675729751586914
      }
    },
    {
      "sample_id": 6,
      "question": "When did James' mother and her friend visit him?",
      "prediction": "8 May",
      "reference": "October 19, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8866137862205505,
        "bert_recall": 0.8917498588562012,
        "bert_f1": 0.8891744017601013,
        "meteor": 0.0,
        "sbert_similarity": 0.5111042261123657
      }
    },
    {
      "sample_id": 6,
      "question": "When did James try Cyberpunk 2077 game?",
      "prediction": "Yesterday",
      "reference": "October 20, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8590352535247803,
        "bert_recall": 0.8740193247795105,
        "bert_f1": 0.8664624691009521,
        "meteor": 0.0,
        "sbert_similarity": 0.3446264863014221
      }
    },
    {
      "sample_id": 6,
      "question": "When did John and his gaming friends organize the charity tournament?",
      "prediction": "8 May, 2022",
      "reference": "On the night of October 30 to 31, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.11156508007421491,
        "bleu2": 0.09109250643152444,
        "bleu3": 0.04596512716457053,
        "bleu4": 0.03791080144520064,
        "bert_precision": 0.9215507507324219,
        "bert_recall": 0.8699823617935181,
        "bert_f1": 0.8950243592262268,
        "meteor": 0.05952380952380952,
        "sbert_similarity": 0.7075762748718262
      }
    },
    {
      "sample_id": 6,
      "question": "What games has John played with his friends at charity tournaments?",
      "prediction": "Fortnite and other games.",
      "reference": "CS:GO, Fortnite, Overwatch and Apex Legends",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.36363636363636365,
        "rouge2_f": 0.0,
        "rougeL_f": 0.36363636363636365,
        "bleu1": 0.14715177646857694,
        "bleu2": 0.03678794411714424,
        "bleu3": 0.026197237902936488,
        "bleu4": 0.023505403213046533,
        "bert_precision": 0.9193522334098816,
        "bert_recall": 0.8667922019958496,
        "bert_f1": 0.8922988772392273,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.6219128370285034
      }
    },
    {
      "sample_id": 6,
      "question": "What was James' big moment with Samantha in October 2023?",
      "prediction": "In October 2023, John went to the theater with Samantha and she became his girlfriend.",
      "reference": "They decided to live together and rented an apartment not far from McGee's bar.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1379310344827586,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.17647058823529413,
        "bleu2": 0.03321055820775358,
        "bleu3": 0.020227031939554035,
        "bleu4": 0.015138514598766055,
        "bert_precision": 0.8804928064346313,
        "bert_recall": 0.8793575763702393,
        "bert_f1": 0.8799248337745667,
        "meteor": 0.10638297872340424,
        "sbert_similarity": 0.16134607791900635
      }
    },
    {
      "sample_id": 6,
      "question": "How long did James and Samantha date for before deciding to move in together?",
      "prediction": "about 2 years",
      "reference": "nearly three months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.935910165309906,
        "bert_recall": 0.9002556204795837,
        "bert_f1": 0.9177367091178894,
        "meteor": 0.0,
        "sbert_similarity": 0.8194704055786133
      }
    },
    {
      "sample_id": 6,
      "question": "When did James, his family and his dogs start on a road trip together?",
      "prediction": "5 November 2022",
      "reference": "November 4, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9550645351409912,
        "bert_recall": 0.9552234411239624,
        "bert_f1": 0.9551439881324768,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9141481518745422
      }
    },
    {
      "sample_id": 6,
      "question": "How long did John practice chess for before winning the chess tournament?",
      "prediction": "John started playing chess in November 2022.",
      "reference": "nearly four months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8444336652755737,
        "bert_recall": 0.8323922157287598,
        "bert_f1": 0.8383696675300598,
        "meteor": 0.0,
        "sbert_similarity": 0.2365453839302063
      }
    },
    {
      "sample_id": 6,
      "question": "When did James and his family visit Mark and Josh?",
      "prediction": "7 November, 2022",
      "reference": "November 7, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 1.0,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.2589465250642332,
        "bleu4": 0.20205155046766235,
        "bert_precision": 0.9418423771858215,
        "bert_recall": 0.9598530530929565,
        "bert_f1": 0.9507623910903931,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.9810649752616882
      }
    },
    {
      "sample_id": 6,
      "question": "When did John work with a game developer on a project?",
      "prediction": "18 September 2022",
      "reference": "November 5-6, 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9297937154769897,
        "bert_recall": 0.9014447927474976,
        "bert_f1": 0.9153998494148254,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7723152041435242
      }
    },
    {
      "sample_id": 6,
      "question": "What programming languages has James worked with?",
      "prediction": "Python and C++",
      "reference": "Python and C++",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What type of mobile application does James plan to build with John?",
      "prediction": "mobile game",
      "reference": "An app for dog walking and pet care",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8808814287185669,
        "bert_recall": 0.843686580657959,
        "bert_f1": 0.8618829250335693,
        "meteor": 0.0,
        "sbert_similarity": 0.30450910329818726
      }
    },
    {
      "sample_id": 6,
      "question": "How does James plan to make his dog-sitting app unique?",
      "prediction": "Making it unique for each owner and pup",
      "reference": "By allowing users to customize their pup's preferences/needs",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.11111111111111112,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11111111111111112,
        "bleu1": 0.11031211282307446,
        "bleu2": 0.03729230060020541,
        "bleu3": 0.02831530397030447,
        "bleu4": 0.024512401940754224,
        "bert_precision": 0.892668604850769,
        "bert_recall": 0.8854105472564697,
        "bert_f1": 0.8890247941017151,
        "meteor": 0.0,
        "sbert_similarity": 0.6194953322410583
      }
    },
    {
      "sample_id": 6,
      "question": "What has John mostly found with the metal detector so far?",
      "prediction": "Bottle caps and occasionally coins or gold rings.",
      "reference": "bottle caps",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16666666666666669,
        "bleu3": 0.07542890820012248,
        "bleu4": 0.050712153369465586,
        "bert_precision": 0.873624861240387,
        "bert_recall": 0.927715539932251,
        "bert_f1": 0.8998581171035767,
        "meteor": 0.7211538461538463,
        "sbert_similarity": 0.7003047466278076
      }
    },
    {
      "sample_id": 6,
      "question": "What did James offer to do for John regarding pets?",
      "prediction": "James offered to take John's next pet.",
      "reference": "help find the perfect pet",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.869879961013794,
        "bert_recall": 0.89064621925354,
        "bert_f1": 0.8801406025886536,
        "meteor": 0.0,
        "sbert_similarity": 0.34673282504081726
      }
    },
    {
      "sample_id": 6,
      "question": "What instrument is John learning to play as of 27 March, 2022?",
      "prediction": "guitar",
      "reference": "Drums",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8900895118713379,
        "bert_recall": 0.9023063778877258,
        "bert_f1": 0.8961563110351562,
        "meteor": 0.0,
        "sbert_similarity": 0.529847264289856
      }
    },
    {
      "sample_id": 6,
      "question": "How long has John been playing the drums as of 27 March, 2022?",
      "prediction": "a month",
      "reference": "One month",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9500500559806824,
        "bert_recall": 0.9500500559806824,
        "bert_f1": 0.9500500559806824,
        "meteor": 0.25,
        "sbert_similarity": 0.8867592811584473
      }
    },
    {
      "sample_id": 6,
      "question": "What game did John play in an intense tournament at the gaming convention in March 2022?",
      "prediction": "CS:GO",
      "reference": "CS:GO",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What game was James playing in the online gaming tournament in April 2022?",
      "prediction": "CS:GO game",
      "reference": "Apex Legends",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8460224866867065,
        "bert_recall": 0.8387027978897095,
        "bert_f1": 0.842346727848053,
        "meteor": 0.0,
        "sbert_similarity": 0.2523263990879059
      }
    },
    {
      "sample_id": 6,
      "question": "How does James communicate with his gaming team?",
      "prediction": "James uses voice chat to communicate with his team.",
      "reference": "voice chat",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.2,
        "bleu2": 0.14907119849998599,
        "bleu3": 0.06705330284284157,
        "bleu4": 0.0446323613785333,
        "bert_precision": 0.8789592981338501,
        "bert_recall": 0.8980923891067505,
        "bert_f1": 0.8884227871894836,
        "meteor": 0.6944444444444445,
        "sbert_similarity": 0.6222437024116516
      }
    },
    {
      "sample_id": 6,
      "question": "What advice did James receive from the famous players he met at the tournament?",
      "prediction": "The advice received was to persevere and have patience when working on game development projects.",
      "reference": "never put your ego above team success",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694126605987549,
        "bert_recall": 0.8537801504135132,
        "bert_f1": 0.8615254759788513,
        "meteor": 0.0,
        "sbert_similarity": 0.30626630783081055
      }
    },
    {
      "sample_id": 6,
      "question": "What did James adopt in April 2022?",
      "prediction": "a puppy",
      "reference": "a pup",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9779256582260132,
        "bert_recall": 0.9779256582260132,
        "bert_f1": 0.9779256582260132,
        "meteor": 0.25,
        "sbert_similarity": 0.9239083528518677
      }
    },
    {
      "sample_id": 6,
      "question": "What is the name of the pup that was adopted by James?",
      "prediction": "Ned",
      "reference": "Ned",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "Why did James embody the appearance of the game character from the woman he saw during a walk?",
      "prediction": "James embodied the appearance of the game character because the woman he saw during a walk was playing the video game where players control an avatar that resembles a specific character.",
      "reference": "He found her appearance and eyes amazing.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.10526315789473685,
        "rouge2_f": 0.0,
        "rougeL_f": 0.052631578947368425,
        "bleu1": 0.09374999999999999,
        "bleu2": 0.0173902085910063,
        "bleu3": 0.010499077661193047,
        "bleu4": 0.007678432706586176,
        "bert_precision": 0.8447285294532776,
        "bert_recall": 0.888744056224823,
        "bert_f1": 0.8661774396896362,
        "meteor": 0.10638297872340427,
        "sbert_similarity": 0.33622580766677856
      }
    },
    {
      "sample_id": 6,
      "question": "What inspired James to create the game character in the virtual world?",
      "prediction": "I work with one of my gaming pals on a programming project.",
      "reference": "Appearance of a woman he saw during a walk",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.1904761904761905,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1904761904761905,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8739665746688843,
        "bert_recall": 0.880600094795227,
        "bert_f1": 0.8772708177566528,
        "meteor": 0.10752688172043011,
        "sbert_similarity": 0.01085745170712471
      }
    },
    {
      "sample_id": 6,
      "question": "Which country did James visit in 2021?",
      "prediction": "Nuuk",
      "reference": "Italy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8278186321258545,
        "bert_recall": 0.9175035357475281,
        "bert_f1": 0.8703567981719971,
        "meteor": 0.0,
        "sbert_similarity": 0.1378350853919983
      }
    },
    {
      "sample_id": 6,
      "question": "What impresses John about Japan?",
      "prediction": "technologically advanced megacities and mesmerizing screens",
      "reference": "Technologically advanced megacities and tasty street food",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.6153846153846153,
        "rouge2_f": 0.5454545454545454,
        "rougeL_f": 0.6153846153846153,
        "bleu1": 0.5643211499270759,
        "bleu2": 0.5353620496724769,
        "bleu3": 0.4976883615946352,
        "bleu4": 0.43012508513132625,
        "bert_precision": 0.9429727792739868,
        "bert_recall": 0.9430986046791077,
        "bert_f1": 0.9430356621742249,
        "meteor": 0.5751811594202899,
        "sbert_similarity": 0.4219512939453125
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of assignment was giving John a hard time at work?",
      "prediction": "coding",
      "reference": "Coding assignment",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9546873569488525,
        "bert_recall": 0.922709584236145,
        "bert_f1": 0.93842613697052,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8304872512817383
      }
    },
    {
      "sample_id": 6,
      "question": "What breed is Daisy, one of James' dogs?",
      "prediction": "Daisy",
      "reference": "Labrador",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9211388230323792,
        "bert_recall": 0.9181081652641296,
        "bert_f1": 0.9196209907531738,
        "meteor": 0.0,
        "sbert_similarity": 0.28172922134399414
      }
    },
    {
      "sample_id": 6,
      "question": "What type of pizza is James' favorite?",
      "prediction": "Hawaiian",
      "reference": "Pepperoni",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8436772227287292,
        "bert_recall": 0.8550646901130676,
        "bert_f1": 0.8493327498435974,
        "meteor": 0.0,
        "sbert_similarity": 0.3553980588912964
      }
    },
    {
      "sample_id": 6,
      "question": "What type of pizza is John's favorite?",
      "prediction": "Hawaiian",
      "reference": "Hawaiian",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What did John organize with his friends on May 8, 2022?",
      "prediction": "Josh and Mark",
      "reference": "A tournament for CS:GO",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8721527457237244,
        "bert_recall": 0.8257529735565186,
        "bert_f1": 0.8483188152313232,
        "meteor": 0.0,
        "sbert_similarity": 0.10813425481319427
      }
    },
    {
      "sample_id": 6,
      "question": "What did John and his friends do with the remaining money after helping the dog shelter?",
      "prediction": "John and his friends used leftover money to buy groceries and cook meals for homeless individuals, which they found very rewarding.",
      "reference": "Bought groceries and cooked food for the homeless",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.3448275862068965,
        "rouge2_f": 0.14814814814814817,
        "rougeL_f": 0.3448275862068965,
        "bleu1": 0.17391304347826086,
        "bleu2": 0.08891084489487741,
        "bleu3": 0.034671909414527975,
        "bleu4": 0.020828838183973034,
        "bert_precision": 0.8979167342185974,
        "bert_recall": 0.9454213380813599,
        "bert_f1": 0.9210569262504578,
        "meteor": 0.4795698924731184,
        "sbert_similarity": 0.637483537197113
      }
    },
    {
      "sample_id": 6,
      "question": "What was the main goal of the money raised from the charity tournament organized by John and his friends in May 2022?",
      "prediction": "To raise funds for a children's hospital.",
      "reference": "Raise money for a dog shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.20412414523193154,
        "bleu3": 0.08622792534209153,
        "bleu4": 0.056122223243057295,
        "bert_precision": 0.9313393235206604,
        "bert_recall": 0.9309357404708862,
        "bert_f1": 0.9311375021934509,
        "meteor": 0.4189435336976321,
        "sbert_similarity": 0.4738849699497223
      }
    },
    {
      "sample_id": 6,
      "question": "What did the system John created help the charitable foundation with?",
      "prediction": "helped streamline their operations and make them run more smoothly",
      "reference": "tracking inventory, resources, and donations",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8367782831192017,
        "bert_recall": 0.8765114545822144,
        "bert_f1": 0.856184184551239,
        "meteor": 0.09090909090909091,
        "sbert_similarity": 0.11738048493862152
      }
    },
    {
      "sample_id": 6,
      "question": "What did John create for the charitable foundation that helped generate reports for analysis?",
      "prediction": "a software tool",
      "reference": "computer application on smartphones",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.877752423286438,
        "bert_recall": 0.8661707043647766,
        "bert_f1": 0.87192302942276,
        "meteor": 0.0,
        "sbert_similarity": 0.2657948136329651
      }
    },
    {
      "sample_id": 6,
      "question": "What did John receive for achieving second place in the tournament?",
      "prediction": "John received money as a prize for finishing second in the tournament.",
      "reference": "money and a trophy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8753539323806763,
        "bert_recall": 0.8873468637466431,
        "bert_f1": 0.8813096284866333,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.48667317628860474
      }
    },
    {
      "sample_id": 6,
      "question": "What project is James working on in his game design course?",
      "prediction": "exploring different types of games",
      "reference": "a new part of the football simulator, collecting player databases",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.060238842382440407,
        "bleu2": 0.02129764696972561,
        "bleu3": 0.01706305179528683,
        "bleu4": 0.016182712188007015,
        "bert_precision": 0.8670861721038818,
        "bert_recall": 0.8797942399978638,
        "bert_f1": 0.8733940124511719,
        "meteor": 0.052631578947368425,
        "sbert_similarity": 0.3299868106842041
      }
    },
    {
      "sample_id": 6,
      "question": "Who does James support in football matches?",
      "prediction": "Manchester City fan",
      "reference": "Liverpool",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8780317306518555,
        "bert_recall": 0.9080944657325745,
        "bert_f1": 0.8928101062774658,
        "meteor": 0.0,
        "sbert_similarity": 0.6073188781738281
      }
    },
    {
      "sample_id": 6,
      "question": "Which football club does John support?",
      "prediction": "Manchester City",
      "reference": "Manchester City",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 6,
      "question": "What disagreement do James and John have about their football teams?",
      "prediction": "James and John disagree on which team will win, with one saying their team will definitely win while the other believes Manchester City's chances are slim.",
      "reference": "debating on which team will perform better in the championship",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30303030303030304,
        "rouge1_f": 0.2702702702702703,
        "rouge2_f": 0.17142857142857146,
        "rougeL_f": 0.2702702702702703,
        "bleu1": 0.1724137931034483,
        "bleu2": 0.13591507055489058,
        "bleu3": 0.1134870957807143,
        "bleu4": 0.08517399705356635,
        "bert_precision": 0.88044273853302,
        "bert_recall": 0.9112602472305298,
        "bert_f1": 0.8955864906311035,
        "meteor": 0.38448275862068965,
        "sbert_similarity": 0.48425042629241943
      }
    },
    {
      "sample_id": 6,
      "question": "What is Max good at doing according to James?",
      "prediction": "Max is good at catching frisbees.",
      "reference": "catching frisbees in mid-air",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.22222222222222224,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.9263671040534973,
        "bert_recall": 0.9190605878829956,
        "bert_f1": 0.9226993322372437,
        "meteor": 0.11904761904761905,
        "sbert_similarity": 0.6039890050888062
      }
    },
    {
      "sample_id": 6,
      "question": "What is the main focus of the organization that James volunteered with?",
      "prediction": "game dev non-profit",
      "reference": "providing necessary items to those who are less fortunate",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8857812881469727,
        "bert_recall": 0.8367705345153809,
        "bert_f1": 0.8605786561965942,
        "meteor": 0.0,
        "sbert_similarity": 0.196200430393219
      }
    },
    {
      "sample_id": 6,
      "question": "Will there be an interview required to volunteer with the organization James volunteered for?",
      "prediction": "No",
      "reference": "No",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "How did John relax in his free time on 9 July, 2022?",
      "prediction": "playing with dogs",
      "reference": "Reading",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8699827194213867,
        "bert_recall": 0.901566207408905,
        "bert_f1": 0.8854928612709045,
        "meteor": 0.0,
        "sbert_similarity": 0.19931545853614807
      }
    },
    {
      "sample_id": 6,
      "question": "What did James enjoy doing on cold winter days?",
      "prediction": "read",
      "reference": "Reading while snuggled under the covers",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8375880122184753,
        "bert_recall": 0.8197087049484253,
        "bert_f1": 0.8285520076751709,
        "meteor": 0.0909090909090909,
        "sbert_similarity": 0.4188542664051056
      }
    },
    {
      "sample_id": 6,
      "question": "What new hobby did James become interested in on 9 July, 2022?",
      "prediction": "James became interested in board games",
      "reference": "Extreme sports",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8714959621429443,
        "bert_recall": 0.8920973539352417,
        "bert_f1": 0.8816763758659363,
        "meteor": 0.0,
        "sbert_similarity": 0.21133564412593842
      }
    },
    {
      "sample_id": 6,
      "question": "Where did James plan to visit after Toronto?",
      "prediction": "Johns is planning to visit Vancouver and perhaps somewhere else.",
      "reference": "Vancouver",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8559865355491638,
        "bert_recall": 0.9046977162361145,
        "bert_f1": 0.8796682953834534,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.5375077724456787
      }
    },
    {
      "sample_id": 6,
      "question": "When did James plan to return from his trip to Toronto and Vancouver?",
      "prediction": "The speaker is unsure.",
      "reference": "July 20",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8621188998222351,
        "bert_recall": 0.8387808799743652,
        "bert_f1": 0.8502897620201111,
        "meteor": 0.0,
        "sbert_similarity": 0.0778670459985733
      }
    },
    {
      "sample_id": 6,
      "question": "What online game did John start playing recently for improving strategy?",
      "prediction": "board games",
      "reference": "Chess",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9085742235183716,
        "bert_recall": 0.8559161424636841,
        "bert_f1": 0.8814594745635986,
        "meteor": 0.0,
        "sbert_similarity": 0.7376891374588013
      }
    },
    {
      "sample_id": 6,
      "question": "What made John leave his IT job?",
      "prediction": "John left his IT job after 3 years and found something that made a difference.",
      "reference": "to focus on things that align with his values and passions",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23076923076923075,
        "rouge1_f": 0.23076923076923075,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.1875,
        "bleu2": 0.03535533905932738,
        "bleu3": 0.021565426544625413,
        "bleu4": 0.01618861356572822,
        "bert_precision": 0.882124125957489,
        "bert_recall": 0.8733456134796143,
        "bert_f1": 0.8777128458023071,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.17995092272758484
      }
    },
    {
      "sample_id": 6,
      "question": "Which game tournaments does John plan to organize besides CS:GO?",
      "prediction": "Fortnite competitions",
      "reference": "Fortnite competitions",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What happened to James's puppy during the recent visit to the clinic?",
      "prediction": "Don't worry; this was just a routine examination and the puppy was vaccinated to prevent it from contracting the seasonal canine disease.",
      "reference": "routine examination and vaccination",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.24999999999999997,
        "rouge1_f": 0.29629629629629634,
        "rouge2_f": 0.16,
        "rougeL_f": 0.29629629629629634,
        "bleu1": 0.12000000000000001,
        "bleu2": 0.09999999999999998,
        "bleu3": 0.07773736473060514,
        "bleu4": 0.03749405143204497,
        "bert_precision": 0.8502677083015442,
        "bert_recall": 0.9106067419052124,
        "bert_f1": 0.8794034123420715,
        "meteor": 0.646551724137931,
        "sbert_similarity": 0.4763520061969757
      }
    },
    {
      "sample_id": 6,
      "question": "What game genre did John start exploring instead of shooters?",
      "prediction": "RPGs and strategy games",
      "reference": "strategy and RPG games",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.75,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.75,
        "bleu2": 0.15811388300841897,
        "bleu3": 0.11014893547185509,
        "bleu4": 0.10573712634405641,
        "bert_precision": 0.9545760750770569,
        "bert_recall": 0.9450799226760864,
        "bert_f1": 0.9498042464256287,
        "meteor": 0.5,
        "sbert_similarity": 0.9755458831787109
      }
    },
    {
      "sample_id": 6,
      "question": "Which RPG game is John playing and enjoying on 10 August, 2022?",
      "prediction": "The RPG game",
      "reference": "The Witcher 3",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9177300333976746,
        "bert_recall": 0.9109202027320862,
        "bert_f1": 0.9143124222755432,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.4749082326889038
      }
    },
    {
      "sample_id": 6,
      "question": "What aspect of \"The Witcher 3\" does John find immersive?",
      "prediction": "John finds immersive aspects like shaping the world and feeling the impact through choices in The Witcher 3.",
      "reference": "shaping the world with choices",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.38095238095238093,
        "rouge1_f": 0.3478260869565218,
        "rouge2_f": 0.19047619047619047,
        "rougeL_f": 0.3478260869565218,
        "bleu1": 0.21052631578947367,
        "bleu2": 0.15294382258037448,
        "bleu3": 0.11369541118456825,
        "bleu4": 0.05415315253510895,
        "bert_precision": 0.8646664619445801,
        "bert_recall": 0.9160534143447876,
        "bert_f1": 0.889618456363678,
        "meteor": 0.3174603174603175,
        "sbert_similarity": 0.38759398460388184
      }
    },
    {
      "sample_id": 6,
      "question": "Whose phone number did James receive during the beach outing?",
      "prediction": "Samantha",
      "reference": "Samantha",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What is James planning to do after receiving Samantha's phone number?",
      "prediction": "Call Samantha",
      "reference": "call her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.9064450263977051,
        "bert_recall": 0.9064450263977051,
        "bert_f1": 0.9064450263977051,
        "meteor": 0.25,
        "sbert_similarity": 0.6882882118225098
      }
    },
    {
      "sample_id": 6,
      "question": "What is John organizing with his siblings?",
      "prediction": "An event next month.",
      "reference": "a gaming night",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9004004001617432,
        "bert_recall": 0.8914936780929565,
        "bert_f1": 0.8959248661994934,
        "meteor": 0.0,
        "sbert_similarity": 0.3568355441093445
      }
    },
    {
      "sample_id": 6,
      "question": "What type of beer does John not like?",
      "prediction": "dark beer",
      "reference": "dark beer",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 6,
      "question": "What were some difficulties James faced during the development of his game?",
      "prediction": "bugs in the code, frustration with debugging, project malfunction due to bugs",
      "reference": "balancing mechanics and ensuring fairness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8532692193984985,
        "bert_recall": 0.8521982431411743,
        "bert_f1": 0.8527334332466125,
        "meteor": 0.0,
        "sbert_similarity": 0.08867393434047699
      }
    },
    {
      "sample_id": 6,
      "question": "What has John been teaching his siblings?",
      "prediction": "coding",
      "reference": "coding",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of programs are John's siblings making?",
      "prediction": "basic games and stories",
      "reference": "basic games and stories",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "Which company's headphones did John choose for gaming?",
      "prediction": "Sennheiser",
      "reference": "Sennheiser",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What did James and Samantha discover they both enjoy at McGee's bar?",
      "prediction": "They discovered they both enjoy lager beer.",
      "reference": "Lager beer",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.2857142857142857,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.25,
        "bleu2": 0.18898223650461357,
        "bleu3": 0.08622792534209149,
        "bleu4": 0.05873949094699214,
        "bert_precision": 0.8993747234344482,
        "bert_recall": 0.9420315623283386,
        "bert_f1": 0.9202090501785278,
        "meteor": 0.2,
        "sbert_similarity": 0.6424862146377563
      }
    },
    {
      "sample_id": 6,
      "question": "How much does James pay per cooking class?",
      "prediction": "$10",
      "reference": "$10",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What did James learn to make in the cooking class besides omelette and meringue?",
      "prediction": "gingerbread",
      "reference": "Dough",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8024917840957642,
        "bert_recall": 0.859704852104187,
        "bert_f1": 0.8301136493682861,
        "meteor": 0.0,
        "sbert_similarity": 0.49677586555480957
      }
    },
    {
      "sample_id": 6,
      "question": "Why did James sign up for a cooking class?",
      "prediction": "James signed up for a cooking class because he wanted to learn how to make delicious meals and impress his friends.",
      "reference": "He wanted to learn something new",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.2962962962962963,
        "rouge2_f": 0.24,
        "rougeL_f": 0.2962962962962963,
        "bleu1": 0.18181818181818182,
        "bleu2": 0.16116459280507606,
        "bleu3": 0.14021612449732193,
        "bleu4": 0.1081300533795917,
        "bert_precision": 0.8865625262260437,
        "bert_recall": 0.9144002199172974,
        "bert_f1": 0.9002662897109985,
        "meteor": 0.42083333333333334,
        "sbert_similarity": 0.48285216093063354
      }
    },
    {
      "sample_id": 6,
      "question": "What did James prepare for the first time in the cooking class?",
      "prediction": "omelette",
      "reference": "Omelette",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.861977219581604,
        "bert_recall": 0.8277419805526733,
        "bert_f1": 0.8445127606391907,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What is the name of the board game John tried in September 2022?",
      "prediction": "The Legend of Zelda",
      "reference": "Dungeons of the Dragon",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.5,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.09635409769034631,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.8863517642021179,
        "bert_recall": 0.8846974968910217,
        "bert_f1": 0.8855238556861877,
        "meteor": 0.25,
        "sbert_similarity": 0.5050713419914246
      }
    },
    {
      "sample_id": 6,
      "question": "Where does James get his ideas from?",
      "prediction": "James gets his ideas from board games.",
      "reference": "books, movies, dreams",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8752132654190063,
        "bert_recall": 0.8734692335128784,
        "bert_f1": 0.8743404150009155,
        "meteor": 0.0,
        "sbert_similarity": 0.25163185596466064
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of dream did James have recently?",
      "prediction": "A crazy dream",
      "reference": "a dream with a medieval castle full of puzzles and traps",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.04632230081520103,
        "bleu2": 0.012685884536012774,
        "bleu3": 0.010578677257345471,
        "bleu4": 0.009388605004877648,
        "bert_precision": 0.9071385264396667,
        "bert_recall": 0.8743154406547546,
        "bert_f1": 0.8904246091842651,
        "meteor": 0.09803921568627451,
        "sbert_similarity": 0.5188905000686646
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of music does John like?",
      "prediction": "John likes music.",
      "reference": "electronic and rock music",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9087408781051636,
        "bert_recall": 0.8587163686752319,
        "bert_f1": 0.8830206990242004,
        "meteor": 0.0,
        "sbert_similarity": 0.4164631962776184
      }
    },
    {
      "sample_id": 6,
      "question": "What instrument did James used to play when he was younger?",
      "prediction": "guitar",
      "reference": "guitar",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What did John use to play when he was younger to let off steam?",
      "prediction": "The Super Nintendo Entertainment System (SNES)",
      "reference": "drums",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7967143058776855,
        "bert_recall": 0.8537309765815735,
        "bert_f1": 0.8242377638816833,
        "meteor": 0.0,
        "sbert_similarity": 0.09049151837825775
      }
    },
    {
      "sample_id": 6,
      "question": "What career milestone did John achieve recently in September 2022?",
      "prediction": "Achieved a major career milestone",
      "reference": "making his first mobile game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8650438785552979,
        "bert_recall": 0.8748364448547363,
        "bert_f1": 0.8699126243591309,
        "meteor": 0.0,
        "sbert_similarity": 0.1411053091287613
      }
    },
    {
      "sample_id": 6,
      "question": "What type of game is John's upcoming mobile game?",
      "prediction": "mobile game",
      "reference": "2D adventure",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.898331880569458,
        "bert_recall": 0.8904357552528381,
        "bert_f1": 0.894366443157196,
        "meteor": 0.0,
        "sbert_similarity": 0.4323604702949524
      }
    },
    {
      "sample_id": 6,
      "question": "What does John do to stay informed and constantly learn about game design?",
      "prediction": "John collaborates with game developers and participates in coding mentorship programs.",
      "reference": "watch tutorials and keep up with developer forums",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2105263157894737,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2105263157894737,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03892494720807617,
        "bleu3": 0.025677367760996977,
        "bleu4": 0.020255986027125642,
        "bert_precision": 0.8850990533828735,
        "bert_recall": 0.882599413394928,
        "bert_f1": 0.8838474154472351,
        "meteor": 0.18072289156626503,
        "sbert_similarity": 0.25318101048469543
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of gig was John offered at the game dev non-profit organization?",
      "prediction": "A volunteer gig",
      "reference": "programming mentor for game developers",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8698199391365051,
        "bert_recall": 0.8499826192855835,
        "bert_f1": 0.859786868095398,
        "meteor": 0.0,
        "sbert_similarity": 0.1726897954940796
      }
    },
    {
      "sample_id": 6,
      "question": "What does John feel about starting the journey as a programming mentor for game developers?",
      "prediction": "John feels excited and enthusiastic.",
      "reference": "excited and inspired",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9272842407226562,
        "bert_recall": 0.9302600622177124,
        "bert_f1": 0.9287697076797485,
        "meteor": 0.5859375,
        "sbert_similarity": 0.5629985332489014
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of games is James excited to play with his new video card?",
      "prediction": "RPGs and strategy games",
      "reference": "RPGs",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9252787828445435,
        "bert_recall": 0.961755633354187,
        "bert_f1": 0.9431646466255188,
        "meteor": 0.3846153846153847,
        "sbert_similarity": 0.788531482219696
      }
    },
    {
      "sample_id": 6,
      "question": "What inspired James to create his game?",
      "prediction": "Witcher 3",
      "reference": "Witcher 3",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What sparked James' passion for gaming when he was a kid?",
      "prediction": "Super Nintendo Entertainment System (SNES)",
      "reference": "Super Mario and The Legend of Zelda games",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.12383969996431167,
        "bleu2": 0.04229931028018368,
        "bleu3": 0.03248071373371268,
        "bleu4": 0.028634401465295497,
        "bert_precision": 0.0,
        "bert_recall": 0.0,
        "bert_f1": 0.0,
        "meteor": 0.06493506493506494,
        "sbert_similarity": 0.48424607515335083
      }
    },
    {
      "sample_id": 6,
      "question": "What did James lose progress on due to a power outage?",
      "prediction": "progress",
      "reference": "a game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9164218306541443,
        "bert_recall": 0.8934584259986877,
        "bert_f1": 0.904794454574585,
        "meteor": 0.0,
        "sbert_similarity": 0.3279617428779602
      }
    },
    {
      "sample_id": 6,
      "question": "What games were played at the gaming tournament organized by John on 31 October, 2022?",
      "prediction": "CS:GO",
      "reference": "Fortnite, Overwatch, Apex Legends",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8144277334213257,
        "bert_recall": 0.8078725934028625,
        "bert_f1": 0.8111369013786316,
        "meteor": 0.0,
        "sbert_similarity": 0.23394306004047394
      }
    },
    {
      "sample_id": 6,
      "question": "What was the purpose of the gaming tournament organized by John on 31 October, 2022?",
      "prediction": "to raise funds for children's hospital",
      "reference": "To raise money for a children's hospital",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7692307692307692,
        "rouge1_f": 0.7999999999999999,
        "rouge2_f": 0.4615384615384615,
        "rougeL_f": 0.7999999999999999,
        "bleu1": 0.7430381997858699,
        "bleu2": 0.5675047991270782,
        "bleu3": 0.38535898603131136,
        "bleu4": 0.18651176671349295,
        "bert_precision": 0.9793937802314758,
        "bert_recall": 0.9819457530975342,
        "bert_f1": 0.9806681871414185,
        "meteor": 0.6463768115942029,
        "sbert_similarity": 0.962029218673706
      }
    },
    {
      "sample_id": 6,
      "question": "What decision did James and Samantha make on 31 October, 2022?",
      "prediction": "James and Samantha decided to move in together.",
      "reference": "To move in together",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.6,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.4444444444444444,
        "bleu2": 0.408248290463863,
        "bleu3": 0.3661572458236839,
        "bleu4": 0.2984745896009823,
        "bert_precision": 0.9098565578460693,
        "bert_recall": 0.9347595572471619,
        "bert_f1": 0.9221399426460266,
        "meteor": 0.6691919191919192,
        "sbert_similarity": 0.4088248014450073
      }
    },
    {
      "sample_id": 6,
      "question": "Where did James and Samantha decide to live together on 31 October, 2022?",
      "prediction": "Move in together",
      "reference": "In an apartment not far from McGee's bar",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.045111761078870896,
        "bleu2": 0.017471709937655493,
        "bleu3": 0.01639159154635143,
        "bleu4": 0.015377056977980369,
        "bert_precision": 0.9004674553871155,
        "bert_recall": 0.8696005344390869,
        "bert_f1": 0.8847648501396179,
        "meteor": 0.06666666666666667,
        "sbert_similarity": 0.24933969974517822
      }
    },
    {
      "sample_id": 6,
      "question": "Why did James and Samantha choose an apartment near McGee's bar?",
      "prediction": "They chose an apartment near McGee's bar because it was suggested by Speaker John in a previous conversation involving moving decisions.",
      "reference": "They love spending time together at the bar",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13793103448275862,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.08695652173913045,
        "bleu2": 0.01988106931218861,
        "bleu3": 0.012901430308862295,
        "bleu4": 0.009849349468888718,
        "bert_precision": 0.8527758121490479,
        "bert_recall": 0.8892775774002075,
        "bert_f1": 0.8706442713737488,
        "meteor": 0.10752688172043011,
        "sbert_similarity": 0.2391054630279541
      }
    },
    {
      "sample_id": 6,
      "question": "What game is John hooked on playing on 5 November, 2022?",
      "prediction": "FIFA 23",
      "reference": "FIFA 23",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What did John suggest James practice before playing FIFA 23 together?",
      "prediction": "practice",
      "reference": "Control with a gamepad and timing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8401081562042236,
        "bert_recall": 0.8331465125083923,
        "bert_f1": 0.8366128206253052,
        "meteor": 0.0,
        "sbert_similarity": 0.10299625992774963
      }
    },
    {
      "sample_id": 6,
      "question": "What project did John work on with a game developer by 7 November, 2022?",
      "prediction": "A simple text-based adventure game",
      "reference": "An online board game",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.916171669960022,
        "bert_recall": 0.9279123544692993,
        "bert_f1": 0.9220045804977417,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.5135916471481323
      }
    },
    {
      "sample_id": 6,
      "question": "What is the name of John's cousin's dog?",
      "prediction": "Ned",
      "reference": "Luna",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9078763127326965,
        "bert_recall": 0.8700360655784607,
        "bert_f1": 0.8885535597801208,
        "meteor": 0.0,
        "sbert_similarity": 0.2593216300010681
      }
    },
    {
      "sample_id": 6,
      "question": "What did John adopt in April 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a pup",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8783146142959595,
        "bert_recall": 0.8729425668716431,
        "bert_f1": 0.8756203651428223,
        "meteor": 0.0,
        "sbert_similarity": 0.06366374343633652
      }
    },
    {
      "sample_id": 6,
      "question": "What is the name of the kitten that was adopted by James?",
      "prediction": "Ned",
      "reference": "Ned",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What inspired John to create the game character in the virtual world?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Appearance of a woman he saw during a walk",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8949553966522217,
        "bert_recall": 0.8794163465499878,
        "bert_f1": 0.8871178030967712,
        "meteor": 0.0,
        "sbert_similarity": 0.11221454292535782
      }
    },
    {
      "sample_id": 6,
      "question": "Which country did John visit in 2021?",
      "prediction": "Italy",
      "reference": "Italy",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of assignment was giving James a hard time at work?",
      "prediction": "Coding assignment",
      "reference": "Coding assignment",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 6,
      "question": "What did James and his friends do with the remaining money after helping the dog shelter?",
      "prediction": "Bought groceries and cooked food for the homeless",
      "reference": "Bought groceries and cooked food for the homeless",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 6,
      "question": "What was the main goal of the money raised from the political campaign organized by John and his friends in May 2022?",
      "prediction": "Raise money for a dog shelter",
      "reference": "Raise money for a dog shelter",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What did the system John created help the illegal organization with?",
      "prediction": "tracking inventory",
      "reference": "tracking inventory, resources, and donations",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.0820849986238988,
        "bleu2": 0.0820849986238988,
        "bleu3": 0.038394038428908775,
        "bleu4": 0.025957555738330737,
        "bert_precision": 0.9739487767219543,
        "bert_recall": 0.906989336013794,
        "bert_f1": 0.9392772316932678,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.723423957824707
      }
    },
    {
      "sample_id": 6,
      "question": "What did James create for the charitable foundation that helped generate reports for analysis?",
      "prediction": "computer application on smartphones",
      "reference": "computer application on smartphones",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "Who does James support in cricket matches?",
      "prediction": "Liverpool",
      "reference": "Liverpool",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What is Max good at doing according to John?",
      "prediction": "swimming and playing",
      "reference": "catching frisbees in mid-air",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8822979927062988,
        "bert_recall": 0.846760630607605,
        "bert_f1": 0.8641641139984131,
        "meteor": 0.0,
        "sbert_similarity": 0.3393091559410095
      }
    },
    {
      "sample_id": 6,
      "question": "Will there be a background check required to volunteer with the organization James volunteered for?",
      "prediction": "Not mentioned in the conversation",
      "reference": "No",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8635950088500977,
        "bert_recall": 0.8855196833610535,
        "bert_f1": 0.8744199275970459,
        "meteor": 0.0,
        "sbert_similarity": 0.19934098422527313
      }
    },
    {
      "sample_id": 6,
      "question": "How did James relax in his free time on 9 July, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Reading",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8792884945869446,
        "bert_recall": 0.8841918706893921,
        "bert_f1": 0.8817333579063416,
        "meteor": 0.0,
        "sbert_similarity": 0.16945023834705353
      }
    },
    {
      "sample_id": 6,
      "question": "What new hobby did John become interested in on 9 July, 2022?",
      "prediction": "Metal detecting",
      "reference": "Extreme sports",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8791165351867676,
        "bert_recall": 0.8948351144790649,
        "bert_f1": 0.8869062066078186,
        "meteor": 0.0,
        "sbert_similarity": 0.039515361189842224
      }
    },
    {
      "sample_id": 6,
      "question": "When did John plan to return from his trip to Toronto and Vancouver?",
      "prediction": "Not mentioned in the conversation",
      "reference": "July 20",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8707113265991211,
        "bert_recall": 0.8555174469947815,
        "bert_f1": 0.8630475997924805,
        "meteor": 0.0,
        "sbert_similarity": 0.13742542266845703
      }
    },
    {
      "sample_id": 6,
      "question": "What made James leave his IT job?",
      "prediction": "3 years",
      "reference": "to focus on things that align with his values and passions",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8846795558929443,
        "bert_recall": 0.840173602104187,
        "bert_f1": 0.8618524074554443,
        "meteor": 0.0,
        "sbert_similarity": 0.10658764839172363
      }
    },
    {
      "sample_id": 6,
      "question": "Which game tournaments does James plan to organize besides CS:GO?",
      "prediction": "RPGs and strategy games",
      "reference": "Fortnite competitions",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.85274338722229,
        "bert_recall": 0.8401670455932617,
        "bert_f1": 0.846408486366272,
        "meteor": 0.0,
        "sbert_similarity": 0.4118877053260803
      }
    },
    {
      "sample_id": 6,
      "question": "What happened to James's kitten during the recent visit to the clinic?",
      "prediction": "Not mentioned in the conversation",
      "reference": "routine examination and vaccination",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8876481652259827,
        "bert_recall": 0.8590662479400635,
        "bert_f1": 0.8731233477592468,
        "meteor": 0.0,
        "sbert_similarity": -0.012585669755935669
      }
    },
    {
      "sample_id": 6,
      "question": "What aspect of \"The Witcher 3\" does John find boring?",
      "prediction": "Not mentioned in the conversation",
      "reference": "shaping the world with choices",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20000000000000004,
        "rouge1_f": 0.20000000000000004,
        "rouge2_f": 0.0,
        "rougeL_f": 0.20000000000000004,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8921261429786682,
        "bert_recall": 0.8857342004776001,
        "bert_f1": 0.8889186382293701,
        "meteor": 0.10000000000000002,
        "sbert_similarity": 0.03287171572446823
      }
    },
    {
      "sample_id": 6,
      "question": "What is John planning to do after receiving Samantha's phone number?",
      "prediction": "call her",
      "reference": "call her",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What has James been teaching his siblings?",
      "prediction": "coding",
      "reference": "coding",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "How much does James pay per dance class?",
      "prediction": "$10",
      "reference": "$10",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What did James learn to make in the chemistry class besides omelette and meringue?",
      "prediction": "Dough",
      "reference": "Dough",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 6,
      "question": "Why did James sign up for a ballet class?",
      "prediction": "Not mentioned in the conversation",
      "reference": "He wanted to learn something new",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8976225852966309,
        "bert_recall": 0.8849304914474487,
        "bert_f1": 0.8912313580513,
        "meteor": 0.0,
        "sbert_similarity": 0.07536203414201736
      }
    },
    {
      "sample_id": 6,
      "question": "What did John prepare for the first time in the cooking class?",
      "prediction": "Omelette",
      "reference": "Omelette",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What is the name of the board game James tried in September 2022?",
      "prediction": "Dungeons of the Dragon",
      "reference": "Dungeons of the Dragon",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9921875,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 6,
      "question": "Where does John get his ideas from?",
      "prediction": "dreams",
      "reference": "books, movies, dreams",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.00579191356895553,
        "bleu3": 0.004007025187889067,
        "bleu4": 0.003257032351754417,
        "bert_precision": 0.9176623225212097,
        "bert_recall": 0.889258861541748,
        "bert_f1": 0.9032372832298279,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.6353172659873962
      }
    },
    {
      "sample_id": 6,
      "question": "What did James use to play when he was younger to let off steam?",
      "prediction": "Nintendo Game Console and Mario-themed Controller",
      "reference": "drums",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8282593488693237,
        "bert_recall": 0.8380228281021118,
        "bert_f1": 0.8331124782562256,
        "meteor": 0.0,
        "sbert_similarity": 0.04789913818240166
      }
    },
    {
      "sample_id": 6,
      "question": "What does James do to stay informed and constantly learn about game design?",
      "prediction": "watch tutorials and keep up with developer forums",
      "reference": "watch tutorials and keep up with developer forums",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What kind of gig was James offered at the game dev non-profit organization?",
      "prediction": "A volunteer gig",
      "reference": "programming mentor for game developers",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8698199391365051,
        "bert_recall": 0.8499826192855835,
        "bert_f1": 0.859786868095398,
        "meteor": 0.0,
        "sbert_similarity": 0.1726897954940796
      }
    },
    {
      "sample_id": 6,
      "question": "What does James feel about starting the journey as a programming mentor for game developers?",
      "prediction": "excited and inspired",
      "reference": "excited and inspired",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What inspired James to create his painting?",
      "prediction": "Witcher 3",
      "reference": "Witcher 3",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What games were played at the gaming tournament organized by James on 31 October, 2022?",
      "prediction": "Not mentioned in the conversation or Fortnite",
      "reference": "Fortnite, Overwatch, Apex Legends",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8832355737686157,
        "bert_recall": 0.8776820302009583,
        "bert_f1": 0.8804500102996826,
        "meteor": 0.0,
        "sbert_similarity": 0.46437349915504456
      }
    },
    {
      "sample_id": 6,
      "question": "What was the purpose of the gaming tournament organized by James on 31 October, 2022?",
      "prediction": "To raise money for a children's hospital",
      "reference": "To raise money for a children's hospital",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What decision did John and Samantha make on 31 October, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "To move in together",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.900063157081604,
        "bert_recall": 0.8989711999893188,
        "bert_f1": 0.8995168209075928,
        "meteor": 0.12195121951219512,
        "sbert_similarity": 0.13424353301525116
      }
    },
    {
      "sample_id": 6,
      "question": "Where did John and Samantha decide to live together on 31 October, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "In an apartment not far from McGee's bar",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3076923076923077,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.17973158564688863,
        "bleu2": 0.04493289641172216,
        "bleu3": 0.03199737863082477,
        "bleu4": 0.028709564316081952,
        "bert_precision": 0.0,
        "bert_recall": 0.0,
        "bert_f1": 0.0,
        "meteor": 0.12987012987012989,
        "sbert_similarity": 0.2339479774236679
      }
    },
    {
      "sample_id": 6,
      "question": "Why did John and Samantha choose an apartment near McGee's bar?",
      "prediction": "They love spending time together at the bar",
      "reference": "They love spending time together at the bar",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 6,
      "question": "What game is James hooked on playing on 5 November, 2022?",
      "prediction": "Not mentioned in the conversation",
      "reference": "FIFA 23",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.88297438621521,
        "bert_recall": 0.8528927564620972,
        "bert_f1": 0.8676729202270508,
        "meteor": 0.0,
        "sbert_similarity": 0.16172394156455994
      }
    },
    {
      "sample_id": 6,
      "question": "What project did James work on with a game developer by 7 November, 2022?",
      "prediction": "An online board game",
      "reference": "An online board game",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 6,
      "question": "What is the name of James's cousin's dog?",
      "prediction": "Luna",
      "reference": "Luna",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of project was Jolene working on in the beginning of January 2023?",
      "prediction": "A cleanup project",
      "reference": "electricity engineering project",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9169092774391174,
        "bert_recall": 0.8825581073760986,
        "bert_f1": 0.8994058966636658,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.19550320506095886
      }
    },
    {
      "sample_id": 7,
      "question": "Which of Deborah`s family and friends have passed away?",
      "prediction": "My mother",
      "reference": "mother, father, her friend Karlie",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.0410424993119494,
        "bleu2": 0.018354763685601443,
        "bleu3": 0.014286435890917338,
        "bleu4": 0.012274570264879268,
        "bert_precision": 0.8944444060325623,
        "bert_recall": 0.8740653991699219,
        "bert_f1": 0.8841374516487122,
        "meteor": 0.0,
        "sbert_similarity": 0.46463093161582947
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah`s mother pass away?",
      "prediction": "2023",
      "reference": "a few years before 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.01831563888873418,
        "bleu2": 0.00579191356895553,
        "bleu3": 0.004007025187889067,
        "bleu4": 0.003257032351754417,
        "bert_precision": 0.9424537420272827,
        "bert_recall": 0.8595757484436035,
        "bert_f1": 0.89910888671875,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.8021894097328186
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene`s mother pass away?",
      "prediction": "2023",
      "reference": "in 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9216490983963013,
        "bert_recall": 0.8981533646583557,
        "bert_f1": 0.9097495079040527,
        "meteor": 0.0,
        "sbert_similarity": 0.787651002407074
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene's mom gift her a pendant?",
      "prediction": "2010 in Paris",
      "reference": "in 2010",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9049160480499268,
        "bert_recall": 0.9196622371673584,
        "bert_f1": 0.9122295379638672,
        "meteor": 0.4761904761904762,
        "sbert_similarity": 0.5943842530250549
      }
    },
    {
      "sample_id": 7,
      "question": "In what country did Jolene's mother buy her the pendant?",
      "prediction": "Paris",
      "reference": "In France",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9166306853294373,
        "bert_recall": 0.9159747958183289,
        "bert_f1": 0.9163026213645935,
        "meteor": 0.0,
        "sbert_similarity": 0.6891964077949524
      }
    },
    {
      "sample_id": 7,
      "question": "What symbolic gifts do Deborah and Jolene have from their mothers?",
      "prediction": "Deborah has a pendant that reminds her of her mother, given in 2010 in Paris; Jolene wears a heart-shaped pendant with a bird on it as a reminder of her mother who passed away last year.",
      "reference": "pendants",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.052631578947368425,
        "rouge2_f": 0.0,
        "rougeL_f": 0.052631578947368425,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.7811535596847534,
        "bert_recall": 0.7924253940582275,
        "bert_f1": 0.7867491245269775,
        "meteor": 0.11111111111111113,
        "sbert_similarity": 0.520450234413147
      }
    },
    {
      "sample_id": 7,
      "question": "Which country were Jolene and her mother visiting in 2010?",
      "prediction": "France",
      "reference": "France",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What helped Deborah find peace when grieving deaths of her loved ones?",
      "prediction": "Nature and gardening",
      "reference": "yoga, old photos, the roses and dahlias in a flower garden, nature",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.012210425925822785,
        "bleu2": 0.0033439628581595207,
        "bleu3": 0.0027885090500861915,
        "bleu4": 0.0024748094101847366,
        "bert_precision": 0.914995014667511,
        "bert_recall": 0.8620312809944153,
        "bert_f1": 0.8877238631248474,
        "meteor": 0.09009009009009009,
        "sbert_similarity": 0.5666378140449524
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah's father pass away?",
      "prediction": "27 January 2023",
      "reference": "January 25, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9549353122711182,
        "bert_recall": 0.9549475908279419,
        "bert_f1": 0.95494145154953,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9345912337303162
      }
    },
    {
      "sample_id": 7,
      "question": "When was Deborah's parents' wedding?",
      "prediction": "27 January, 2023",
      "reference": "in 1993",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.857518196105957,
        "bert_recall": 0.871554970741272,
        "bert_f1": 0.8644796013832092,
        "meteor": 0.0,
        "sbert_similarity": 0.2864096760749817
      }
    },
    {
      "sample_id": 7,
      "question": "Is Deborah married?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 3,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah receive an appreciation letter from her community?",
      "prediction": "9 July 2023",
      "reference": "January 26, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9325313568115234,
        "bert_recall": 0.9371914863586426,
        "bert_f1": 0.934855580329895,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7726092338562012
      }
    },
    {
      "sample_id": 7,
      "question": "What places give Deborah peace?",
      "prediction": "Nature and her favorite studio",
      "reference": "sitting in a spot by the window in her Mom's house, sitting by the beach, Bali, forest trail in a nearby park",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.07142857142857142,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857142,
        "bleu1": 0.0029991153640955407,
        "bleu2": 0.0010603474057563594,
        "bleu3": 0.000849519326296349,
        "bleu4": 0.0008056897980817704,
        "bert_precision": 0.8602738380432129,
        "bert_recall": 0.8410794734954834,
        "bert_f1": 0.8505683541297913,
        "meteor": 0.024630541871921183,
        "sbert_similarity": 0.3167651295661926
      }
    },
    {
      "sample_id": 7,
      "question": "What were Deborah's mother's hobbies?",
      "prediction": "Cooking",
      "reference": "reading, traveling, art, cooking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.8986334800720215,
        "bert_recall": 0.8588293790817261,
        "bert_f1": 0.8782806992530823,
        "meteor": 0.13513513513513511,
        "sbert_similarity": 0.5289499759674072
      }
    },
    {
      "sample_id": 7,
      "question": "What pets does Jolene have?",
      "prediction": "A snake",
      "reference": "snakes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9272060394287109,
        "bert_recall": 0.9272060394287109,
        "bert_f1": 0.9272060394287109,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.837969958782196
      }
    },
    {
      "sample_id": 7,
      "question": "What are the names of Jolene's snakes?",
      "prediction": "Susie and Seraphim",
      "reference": "Susie, Seraphim",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9801365733146667,
        "bert_recall": 0.9801365733146667,
        "bert_f1": 0.9801365733146667,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.9173301458358765
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene buy her pet Seraphim?",
      "prediction": "26 June 2023",
      "reference": "in 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8675390481948853,
        "bert_recall": 0.8906127214431763,
        "bert_f1": 0.8789244890213013,
        "meteor": 0.0,
        "sbert_similarity": 0.66136634349823
      }
    },
    {
      "sample_id": 7,
      "question": "In what country did Jolene buy snake Seraphim?",
      "prediction": "the day before yesterday",
      "reference": "In France",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.822805643081665,
        "bert_recall": 0.8660706281661987,
        "bert_f1": 0.8438839912414551,
        "meteor": 0.0,
        "sbert_similarity": 0.2286974936723709
      }
    },
    {
      "sample_id": 7,
      "question": "How many times has Jolene been to France?",
      "prediction": "3",
      "reference": "two times",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8667084574699402,
        "bert_recall": 0.8612631559371948,
        "bert_f1": 0.8639772534370422,
        "meteor": 0.0,
        "sbert_similarity": 0.4421578347682953
      }
    },
    {
      "sample_id": 7,
      "question": "Which games have Jolene and her partner played together?",
      "prediction": "Zelda BOTW, Animal Crossing: New Horizons",
      "reference": "Detroit, Walking Dead, Battlefield 1, It Takes Two, Overcooked 2",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.05904581909262685,
        "bleu2": 0.01996113009202554,
        "bleu3": 0.015156090052094605,
        "bleu4": 0.013120543279239774,
        "bert_precision": 0.8351364135742188,
        "bert_recall": 0.8511798977851868,
        "bert_f1": 0.8430818319320679,
        "meteor": 0.0,
        "sbert_similarity": 0.3122527003288269
      }
    },
    {
      "sample_id": 7,
      "question": "When do Jolene and her partner plan to complete the game \"Walking Dead\"?",
      "prediction": "Next Saturday",
      "reference": "Saturday after 27 January, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.06766764161830635,
        "bleu2": 0.030261889307125377,
        "bleu3": 0.02355435073584915,
        "bleu4": 0.020237345084409754,
        "bert_precision": 0.8675883412361145,
        "bert_recall": 0.8404374122619629,
        "bert_f1": 0.8537970781326294,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.6846432685852051
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah meet Anna?",
      "prediction": "7:03 pm on 1 February, 2023",
      "reference": "31 January, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.868489146232605,
        "bert_recall": 0.9136882424354553,
        "bert_f1": 0.8905155062675476,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.745705246925354
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Jolene sometimes put off doing yoga?",
      "prediction": "Jolen was overwhelmed by exams and deadlines, feeling stressed.",
      "reference": "She's more interested in playing video games",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8775160312652588,
        "bert_recall": 0.8752255439758301,
        "bert_f1": 0.876369297504425,
        "meteor": 0.0,
        "sbert_similarity": 0.06757029891014099
      }
    },
    {
      "sample_id": 7,
      "question": "What new yoga poses did Deborah try?",
      "prediction": "Deborah tried Dancer Pose (Natarajasana)",
      "reference": "Warrior II, Dancer Pose (Natarajasana), Tree pose",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.4033700871483995,
        "bleu2": 0.38969252673404825,
        "bleu3": 0.37349885088316026,
        "bleu4": 0.34718201116725705,
        "bert_precision": 0.9379616975784302,
        "bert_recall": 0.9222207069396973,
        "bert_f1": 0.9300246238708496,
        "meteor": 0.14705882352941174,
        "sbert_similarity": 0.6213185787200928
      }
    },
    {
      "sample_id": 7,
      "question": "What are Jolene's favorite books?",
      "prediction": "It was one of her favorite places in the house.",
      "reference": "Sapiens, Avalanche by Neal Stephenson",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8480362296104431,
        "bert_recall": 0.8105754852294922,
        "bert_f1": 0.8288827538490295,
        "meteor": 0.0,
        "sbert_similarity": 0.02497236244380474
      }
    },
    {
      "sample_id": 7,
      "question": "Which book did Jolene read in January 2023?",
      "prediction": "A photo of a bouquet",
      "reference": "Avalanche by Neal Stephenson",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8820351958274841,
        "bert_recall": 0.8479273915290833,
        "bert_f1": 0.8646450638771057,
        "meteor": 0.0,
        "sbert_similarity": 0.04326232522726059
      }
    },
    {
      "sample_id": 7,
      "question": "When was Jolene in Bogota?",
      "prediction": "2015",
      "reference": "in summer 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8897391557693481,
        "bert_recall": 0.850380539894104,
        "bert_f1": 0.8696147799491882,
        "meteor": 0.0,
        "sbert_similarity": 0.35242244601249695
      }
    },
    {
      "sample_id": 7,
      "question": "In what country was Jolene during summer 2022?",
      "prediction": "United States",
      "reference": "Colombia",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8978496789932251,
        "bert_recall": 0.8706410527229309,
        "bert_f1": 0.8840360045433044,
        "meteor": 0.0,
        "sbert_similarity": 0.46340233087539673
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene have a mini-retreat to reflect on her career?",
      "prediction": "27 January 2023",
      "reference": "Wednesday before 9 February, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9159243106842041,
        "bert_recall": 0.8822264671325684,
        "bert_f1": 0.8987596035003662,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7850603461265564
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene have a dinner and drinks with her friends?",
      "prediction": "7:03 pm on 1 February, 2023",
      "reference": "21 February, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.30935528891357367,
        "bleu4": 0.16348126556655487,
        "bert_precision": 0.8816361427307129,
        "bert_recall": 0.9354877471923828,
        "bert_f1": 0.9077639579772949,
        "meteor": 0.5681818181818182,
        "sbert_similarity": 0.7800086140632629
      }
    },
    {
      "sample_id": 7,
      "question": "When was the last photo of Deborah and Karlie taken?",
      "prediction": "2023",
      "reference": "in summer 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8978161811828613,
        "bert_recall": 0.854293704032898,
        "bert_f1": 0.8755143880844116,
        "meteor": 0.0,
        "sbert_similarity": 0.6418472528457642
      }
    },
    {
      "sample_id": 7,
      "question": "When was Deborah in Bali?",
      "prediction": "2023",
      "reference": "in 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9216490983963013,
        "bert_recall": 0.8981533646583557,
        "bert_f1": 0.9097495079040527,
        "meteor": 0.0,
        "sbert_similarity": 0.787651002407074
      }
    },
    {
      "sample_id": 7,
      "question": "How long have Jolene and her partner been together?",
      "prediction": "three years",
      "reference": "for three years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9619745016098022,
        "bert_recall": 0.9451601505279541,
        "bert_f1": 0.9534932374954224,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.8376699686050415
      }
    },
    {
      "sample_id": 7,
      "question": "Which year did Jolene and her partner start dating?",
      "prediction": "2023",
      "reference": "2020",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9242581725120544,
        "bert_recall": 0.9351127743721008,
        "bert_f1": 0.9296538233757019,
        "meteor": 0.0,
        "sbert_similarity": 0.6698888540267944
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah go for her first morning jog in a nearby park?",
      "prediction": "26 June 2023",
      "reference": "24 February, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9556549787521362,
        "bert_recall": 0.9356202483177185,
        "bert_f1": 0.9455315470695496,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8899750709533691
      }
    },
    {
      "sample_id": 7,
      "question": "How old is Jolene?",
      "prediction": "two years",
      "reference": "likely no more than 30; since she's in school",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.855825662612915,
        "bert_recall": 0.812488317489624,
        "bert_f1": 0.8335941433906555,
        "meteor": 0.0,
        "sbert_similarity": 0.27855902910232544
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene take Seraphim to the park?",
      "prediction": "2 March",
      "reference": "Sunday before 2 March, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.8538123369216919,
        "bert_recall": 0.8408135175704956,
        "bert_f1": 0.8472630381584167,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.6331202983856201
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah start the yoga class in the neighborhood?",
      "prediction": "4 February 2023",
      "reference": "Friday before 13 March, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9066122770309448,
        "bert_recall": 0.8810995817184448,
        "bert_f1": 0.8936738967895508,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7988916039466858
      }
    },
    {
      "sample_id": 7,
      "question": "What time management techniques do Deborah and Jolene use?",
      "prediction": "Deborah uses a bullet journal and the Pomodoro Technique, while Jolene practices mindful breathing exercises.",
      "reference": "the Pomodoro Technique - 25 minutes work and 5-minute break, scheduler or to-do list, The Eisenhower Matrix, bullet journal",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.34285714285714286,
        "rouge2_f": 0.18181818181818182,
        "rougeL_f": 0.17142857142857143,
        "bleu1": 0.3068424540643743,
        "bleu2": 0.20705776610363366,
        "bleu3": 0.13094256450042316,
        "bleu4": 0.05802619629587293,
        "bert_precision": 0.8884105682373047,
        "bert_recall": 0.8716933131217957,
        "bert_f1": 0.879972517490387,
        "meteor": 0.20000000000000004,
        "sbert_similarity": 0.2417144626379013
      }
    },
    {
      "sample_id": 7,
      "question": "Does Deborah live close to the beach or the mountains?",
      "prediction": "No",
      "reference": "beach",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8422957062721252,
        "bert_recall": 0.8096333742141724,
        "bert_f1": 0.8256415724754333,
        "meteor": 0.0,
        "sbert_similarity": 0.1658223569393158
      }
    },
    {
      "sample_id": 7,
      "question": "What ways do Deborah and Jolene use to enhance their yoga practice?",
      "prediction": "Deborah practices mindful breathing exercises, while Jolenenotes that they try new poses in the yoga class after a trip to Rio de Janeiro.",
      "reference": "candles, music, essential oils",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.04000000000000001,
        "bleu2": 0.012909944487358061,
        "bleu3": 0.009415424419114833,
        "bleu4": 0.007575731225158965,
        "bert_precision": 0.8158747553825378,
        "bert_recall": 0.838129997253418,
        "bert_f1": 0.8268526196479797,
        "meteor": 0.0,
        "sbert_similarity": 0.1824861466884613
      }
    },
    {
      "sample_id": 7,
      "question": "What music pieces does Deborah listen to during her yoga practice?",
      "prediction": "new tunes",
      "reference": "Savana, Sleep",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8563718199729919,
        "bert_recall": 0.8410944938659668,
        "bert_f1": 0.848664402961731,
        "meteor": 0.0,
        "sbert_similarity": 0.15650853514671326
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah go for a bicycle ride with Anna?",
      "prediction": "7 January 2023",
      "reference": "first week of April, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.9167120456695557,
        "bert_recall": 0.8912070989608765,
        "bert_f1": 0.9037796854972839,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7324909567832947
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah go to an art show with Anna?",
      "prediction": "4 February, 2023",
      "reference": "on 9 April, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.38940039153570244,
        "bleu2": 0.3179440883008189,
        "bleu3": 0.16043405788769674,
        "bleu4": 0.13232169883599026,
        "bert_precision": 0.9280070066452026,
        "bert_recall": 0.924961507320404,
        "bert_f1": 0.9264817833900452,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.7694055438041687
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene finish her robotics project?",
      "prediction": "last month",
      "reference": "May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8554883599281311,
        "bert_recall": 0.8440782427787781,
        "bert_f1": 0.8497450351715088,
        "meteor": 0.0,
        "sbert_similarity": 0.41467922925949097
      }
    },
    {
      "sample_id": 7,
      "question": "How long did Jolene work on the robotics project given to her by her Professor?",
      "prediction": "about 3 years",
      "reference": "four months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9277584552764893,
        "bert_recall": 0.9494340419769287,
        "bert_f1": 0.9384711384773254,
        "meteor": 0.0,
        "sbert_similarity": 0.7090445160865784
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene do yoga at Talkeetna?",
      "prediction": "4 February, 2023",
      "reference": "on 5 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.38940039153570244,
        "bleu2": 0.3179440883008189,
        "bleu3": 0.16043405788769674,
        "bleu4": 0.13232169883599026,
        "bert_precision": 0.9251745939254761,
        "bert_recall": 0.9226252436637878,
        "bert_f1": 0.9238981604576111,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.8255261182785034
      }
    },
    {
      "sample_id": 7,
      "question": "Which US state did Jolene visit during her internship?",
      "prediction": "California",
      "reference": "Alaska",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9424716234207153,
        "bert_recall": 0.9010062217712402,
        "bert_f1": 0.9212725758552551,
        "meteor": 0.0,
        "sbert_similarity": 0.533314049243927
      }
    },
    {
      "sample_id": 7,
      "question": "How long has Jolene been doing yoga and meditation?",
      "prediction": "Jolen is focusing on her well-being through activities that promote balance and grounding, such as yoga and meditation.",
      "reference": "about 3 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8109856843948364,
        "bert_recall": 0.8385275602340698,
        "bert_f1": 0.8245267271995544,
        "meteor": 0.0,
        "sbert_similarity": -0.041301429271698
      }
    },
    {
      "sample_id": 7,
      "question": "Which year did Jolene start practicing yoga?",
      "prediction": "2023",
      "reference": "2020",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9242581725120544,
        "bert_recall": 0.9351127743721008,
        "bert_f1": 0.9296538233757019,
        "meteor": 0.0,
        "sbert_similarity": 0.6698888540267944
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene buy a new aquarium for Seraphim?",
      "prediction": "9 July, 2023",
      "reference": "24 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9807250499725342,
        "bert_recall": 0.9807250499725342,
        "bert_f1": 0.9807250499725342,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.7848917245864868
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene lose a lot of progress in her work?",
      "prediction": "21 August 2023",
      "reference": "last week of July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9286594390869141,
        "bert_recall": 0.8892961144447327,
        "bert_f1": 0.9085516333580017,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7627706527709961
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene adopt her snake Susie?",
      "prediction": "two years ago",
      "reference": "in 2021",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8103641867637634,
        "bert_recall": 0.8422422409057617,
        "bert_f1": 0.8259957432746887,
        "meteor": 0.0,
        "sbert_similarity": 0.4026127755641937
      }
    },
    {
      "sample_id": 7,
      "question": "Which pet did Jolene adopt first - Susie or Seraphim?",
      "prediction": "Jolenesays: 'Aww, that's adorable!' What's the second one's name?",
      "reference": "Susie",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8128957748413086,
        "bert_recall": 0.8582199811935425,
        "bert_f1": 0.8349432349205017,
        "meteor": 0.0,
        "sbert_similarity": 0.339964896440506
      }
    },
    {
      "sample_id": 7,
      "question": "Which pet did Jolene adopt more recently - Susie or Seraphim?",
      "prediction": "Seraphim",
      "reference": "Seraphim",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah lead a meditation session during the sunset?",
      "prediction": "9 February",
      "reference": "week before 16 August, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8455974459648132,
        "bert_recall": 0.8461349010467529,
        "bert_f1": 0.8458660244941711,
        "meteor": 0.0,
        "sbert_similarity": 0.5732079744338989
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene gift her partner a new console?",
      "prediction": "9 July, 2023",
      "reference": "17 August, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.20600140803015504,
        "bleu4": 0.16990442448471224,
        "bert_precision": 0.9771870970726013,
        "bert_recall": 0.9771870970726013,
        "bert_f1": 0.9771870970726013,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8217921257019043
      }
    },
    {
      "sample_id": 7,
      "question": "What games does Jolene recommend for Deborah?",
      "prediction": "Zelda BOTW for Switch and Animal Crossing: New Horizons",
      "reference": "Zelda BOTW for Switch , Animal Crossing: New Horizons, Overcooked 2",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8421052631578948,
        "rouge1_f": 0.8421052631578948,
        "rouge2_f": 0.7058823529411765,
        "rougeL_f": 0.8421052631578948,
        "bleu1": 0.666736398613546,
        "bleu2": 0.6198129921724927,
        "bleu3": 0.5639419664458528,
        "bleu4": 0.4874858042804568,
        "bert_precision": 0.9778013229370117,
        "bert_recall": 0.9430531859397888,
        "bert_f1": 0.9601129293441772,
        "meteor": 0.6405895691609977,
        "sbert_similarity": 0.906976580619812
      }
    },
    {
      "sample_id": 7,
      "question": "What do Deborah and her husband do together?",
      "prediction": "read books",
      "reference": "play detective games together, spend time outdoors and explore nature",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8920890688896179,
        "bert_recall": 0.8463734984397888,
        "bert_f1": 0.8686301708221436,
        "meteor": 0.0,
        "sbert_similarity": 0.3385353684425354
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah go to a yoga retreat near her mom's place?",
      "prediction": "26 June 2023",
      "reference": "a week before 24 August,2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9274881482124329,
        "bert_recall": 0.8788635730743408,
        "bert_f1": 0.9025214314460754,
        "meteor": 0.0,
        "sbert_similarity": 0.8070520758628845
      }
    },
    {
      "sample_id": 7,
      "question": "What projects is Jolene planning for next year?",
      "prediction": "solidify the plan first",
      "reference": "developing renewable energy finding ways to supply clean water to those with limited access",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8713690042495728,
        "bert_recall": 0.8358443379402161,
        "bert_f1": 0.8532370328903198,
        "meteor": 0.0,
        "sbert_similarity": 0.12394563853740692
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Deborah get her cats?",
      "prediction": "Deborah got Max when her mother passed away.",
      "reference": "Luna is from the shelter and Max is her mother's cat",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.3,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.3,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.1462613413027904,
        "bleu3": 0.0617850083534277,
        "bleu4": 0.04021333017266262,
        "bert_precision": 0.8969714045524597,
        "bert_recall": 0.8843802213668823,
        "bert_f1": 0.890631377696991,
        "meteor": 0.09345794392523364,
        "sbert_similarity": 0.5177405476570129
      }
    },
    {
      "sample_id": 7,
      "question": "How old are Deborah's cats?",
      "prediction": "Max is 8 years old",
      "reference": "Max is 8 years old and Luna is 5 years old",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7692307692307693,
        "rouge1_f": 0.625,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.625,
        "bleu1": 0.301194211912202,
        "bleu2": 0.301194211912202,
        "bleu3": 0.301194211912202,
        "bleu4": 0.301194211912202,
        "bert_precision": 0.9693425893783569,
        "bert_recall": 0.9475716948509216,
        "bert_f1": 0.9583335518836975,
        "meteor": 0.3576923076923077,
        "sbert_similarity": 0.7519575357437134
      }
    },
    {
      "sample_id": 7,
      "question": "Does Deborah like cats?",
      "prediction": "Yes",
      "reference": "Yes",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "Which country was Jolene located in during the last week of August 2023?",
      "prediction": "USA",
      "reference": "Brazil",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9160431623458862,
        "bert_recall": 0.9160431623458862,
        "bert_f1": 0.9160431623458862,
        "meteor": 0.0,
        "sbert_similarity": 0.6210944652557373
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene and her partner return home from Rio de Janeiro?",
      "prediction": "Three years ago",
      "reference": "29 August, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8249427080154419,
        "bert_recall": 0.8489859700202942,
        "bert_f1": 0.8367916345596313,
        "meteor": 0.0,
        "sbert_similarity": 0.31565260887145996
      }
    },
    {
      "sample_id": 7,
      "question": "What was Jolene doing with her partner in Rio de Janeiro?",
      "prediction": "Sitting in that spot by the window gives me peace.",
      "reference": "they went on excursions, checked out some cool yoga classes, visited a lot of delicious cafes, visited an old temple",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8648375272750854,
        "bert_recall": 0.8456847667694092,
        "bert_f1": 0.8551539182662964,
        "meteor": 0.0,
        "sbert_similarity": 0.20480293035507202
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah visit Brazil?",
      "prediction": "2023",
      "reference": "2020",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9242581725120544,
        "bert_recall": 0.9351127743721008,
        "bert_f1": 0.9296538233757019,
        "meteor": 0.0,
        "sbert_similarity": 0.6698888540267944
      }
    },
    {
      "sample_id": 7,
      "question": "Have Deborah and Jolene been to Rio de Janeiro?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "Is the friend who wrote Deborah the motivational quote no longer alive?",
      "prediction": "Yes",
      "reference": "likely yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9414836168289185,
        "bert_recall": 0.906818687915802,
        "bert_f1": 0.9238260388374329,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.6378452777862549
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah go to a community meetup?",
      "prediction": "1 February 2023",
      "reference": "last week of August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.931658148765564,
        "bert_recall": 0.8978387117385864,
        "bert_f1": 0.914435863494873,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7539353370666504
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene's parents give her first console?",
      "prediction": "2010",
      "reference": "when she was 10",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8762254118919373,
        "bert_recall": 0.8594107031822205,
        "bert_f1": 0.8677366375923157,
        "meteor": 0.0,
        "sbert_similarity": 0.41514405608177185
      }
    },
    {
      "sample_id": 7,
      "question": "Did Jolene teach herself how to play the console?",
      "prediction": "No",
      "reference": "yes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9297295212745667,
        "bert_recall": 0.9297295212745667,
        "bert_f1": 0.9297295212745667,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 7,
      "question": "What do Deborah and Jolene plan to try when they meet in a new cafe?",
      "prediction": "They plan to meet up at that cafe next Monday.",
      "reference": "coffee and fresh pastries",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8386664986610413,
        "bert_recall": 0.8505679368972778,
        "bert_f1": 0.8445752859115601,
        "meteor": 0.0,
        "sbert_similarity": 0.2959255874156952
      }
    },
    {
      "sample_id": 7,
      "question": "What card game is Deborah talking about?",
      "prediction": "Monster Hunter: World",
      "reference": "Exploding Kittens",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8297680616378784,
        "bert_recall": 0.8559218049049377,
        "bert_f1": 0.8426420092582703,
        "meteor": 0.0,
        "sbert_similarity": 0.10224998742341995
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene and her partner try scuba diving lessons?",
      "prediction": "5:35 pm on 22 March, 2023",
      "reference": "Friday before 17 September, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8624553680419922,
        "bert_recall": 0.8948516845703125,
        "bert_f1": 0.8783549070358276,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.7231330275535583
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Jolene and her partner find a cool diving spot?",
      "prediction": "Jolene and her partner found a cool dive spot near their mother's old house.",
      "reference": "Phuket",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8284720182418823,
        "bert_recall": 0.8092008829116821,
        "bert_f1": 0.8187230229377747,
        "meteor": 0.0,
        "sbert_similarity": 0.1567317098379135
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Jolene and her partner spend most of September 2023?",
      "prediction": "Austin",
      "reference": "Phuket",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9179789423942566,
        "bert_recall": 0.8112086057662964,
        "bert_f1": 0.8612974882125854,
        "meteor": 0.0,
        "sbert_similarity": 0.2256106585264206
      }
    },
    {
      "sample_id": 7,
      "question": "Has Deborah tried surfing?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "Has Jolene tried surfing?",
      "prediction": "Definitely!",
      "reference": "no",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8132236003875732,
        "bert_recall": 0.825743556022644,
        "bert_f1": 0.8194357752799988,
        "meteor": 0.0,
        "sbert_similarity": 0.24575598537921906
      }
    },
    {
      "sample_id": 7,
      "question": "When did the Deboran and Jolene agree to go surfing?",
      "prediction": "5:35 pm on 22 March, 2023",
      "reference": "in October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2222222222222222,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8560400605201721,
        "bert_recall": 0.8909922242164612,
        "bert_f1": 0.8731665015220642,
        "meteor": 0.15151515151515152,
        "sbert_similarity": 0.6785389184951782
      }
    },
    {
      "sample_id": 7,
      "question": "Which locations does Deborah practice her yoga at?",
      "prediction": "Deborah practices yoga in parks and on beaches.",
      "reference": "at her mother's old home, park, yoga studio, beach",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.33333333333333326,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.07124226538110606,
        "bleu2": 0.023895382239484966,
        "bleu3": 0.01799608986719412,
        "bleu4": 0.015375695992413887,
        "bert_precision": 0.908122181892395,
        "bert_recall": 0.9039863348007202,
        "bert_f1": 0.9060495495796204,
        "meteor": 0.05617977528089887,
        "sbert_similarity": 0.6217598915100098
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of professional activities does Jolene participate in to gain more experience in her field?",
      "prediction": "studying and gaining experience",
      "reference": "present work at virtual conference, attend workshops  and intern at firms",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.033833820809153176,
        "bleu2": 0.012354364575840592,
        "bleu3": 0.010373882542730911,
        "bleu4": 0.010873221263821838,
        "bert_precision": 0.8610578775405884,
        "bert_recall": 0.8332856297492981,
        "bert_f1": 0.8469441533088684,
        "meteor": 0.04854368932038835,
        "sbert_similarity": 0.345734179019928
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of engineering projects has Jolene worked on?",
      "prediction": "Jolenesays that her projects have focused on sustainable solutions, including developing renewable energy systems like solar power to assist communities and reduce dependence on non-renewable sources.",
      "reference": "electrical engineering, robotics, sustainable water purifier, productive and affordable aerial surveillance system",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10810810810810811,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10256410256410256,
        "bleu1": 0.10714285714285714,
        "bleu2": 0.01992047682223989,
        "bleu3": 0.012039171932923394,
        "bleu4": 0.008839374326825921,
        "bert_precision": 0.8292306661605835,
        "bert_recall": 0.8455010056495667,
        "bert_f1": 0.8372867703437805,
        "meteor": 0.1119402985074627,
        "sbert_similarity": 0.23305398225784302
      }
    },
    {
      "sample_id": 7,
      "question": "Which community activities have Deborah and Anna participated in?",
      "prediction": "Community meetup, gardening class",
      "reference": "yoga, running",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8564732074737549,
        "bert_recall": 0.8606665134429932,
        "bert_f1": 0.8585647344589233,
        "meteor": 0.0,
        "sbert_similarity": 0.09966044127941132
      }
    },
    {
      "sample_id": 7,
      "question": "What gifts has Deborah received?",
      "prediction": "A bouquet of flowers and a pendant",
      "reference": "an appreciate letter from her community, a flower bouqet from her friend, a motivational quote from a friend",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.24,
        "rouge2_f": 0.0,
        "rougeL_f": 0.24,
        "bleu1": 0.04460515580456316,
        "bleu2": 0.010773168626440057,
        "bleu3": 0.007352933468130085,
        "bleu4": 0.006132545547006367,
        "bert_precision": 0.9007977247238159,
        "bert_recall": 0.8460484743118286,
        "bert_f1": 0.8725650906562805,
        "meteor": 0.08875739644970415,
        "sbert_similarity": 0.4121716618537903
      }
    },
    {
      "sample_id": 7,
      "question": "Which countries has Deborah traveled to?",
      "prediction": "United Kingdom",
      "reference": "Thailand, Brazil",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8900291919708252,
        "bert_recall": 0.8512243032455444,
        "bert_f1": 0.8701943755149841,
        "meteor": 0.0,
        "sbert_similarity": 0.49545055627822876
      }
    },
    {
      "sample_id": 7,
      "question": "What activities does Deborah pursue besides practicing and teaching yoga?",
      "prediction": "Deborah practices running and meditation.",
      "reference": "biking, going to art shows, running, organizing workshops to practice mindfulness and self-care, surfing, gardening",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.21052631578947364,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19047619047619047,
        "bleu1": 0.03232398928813501,
        "bleu2": 0.007917728020712006,
        "bleu3": 0.005493590663168386,
        "bleu4": 0.004707909248443564,
        "bert_precision": 0.9147569537162781,
        "bert_recall": 0.8677397966384888,
        "bert_f1": 0.8906283378601074,
        "meteor": 0.10714285714285712,
        "sbert_similarity": 0.3720052242279053
      }
    },
    {
      "sample_id": 7,
      "question": "What are the names of Jolene's snakes?",
      "prediction": "Susie and Seraphim",
      "reference": "Susie, Seraphim",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9801365733146667,
        "bert_recall": 0.9801365733146667,
        "bert_f1": 0.9801365733146667,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.9173301458358765
      }
    },
    {
      "sample_id": 7,
      "question": "What are Jolene's favorite books?",
      "prediction": "Jolen recommends two games and mentions a difficult choice for their favorite game.",
      "reference": "Sapiens, Avalanche by Neal Stephenson",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8428675532341003,
        "bert_recall": 0.8139672875404358,
        "bert_f1": 0.8281653523445129,
        "meteor": 0.0,
        "sbert_similarity": 0.16743481159210205
      }
    },
    {
      "sample_id": 7,
      "question": "What music pieces does Deborah listen to during her yoga practice?",
      "prediction": "Deborah listens to new tunes during her yoga practice.",
      "reference": "Savana, Sleep",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8377907872200012,
        "bert_recall": 0.8394707441329956,
        "bert_f1": 0.8386298418045044,
        "meteor": 0.0,
        "sbert_similarity": 0.2304932028055191
      }
    },
    {
      "sample_id": 7,
      "question": "What games does Jolene recommend for Deborah?",
      "prediction": "Zelda BOTW for Switch and Animal Crossing: New Horizons",
      "reference": "Zelda BOTW for Switch , Animal Crossing: New Horizons, Overcooked 2",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8421052631578948,
        "rouge1_f": 0.8421052631578948,
        "rouge2_f": 0.7058823529411765,
        "rougeL_f": 0.8421052631578948,
        "bleu1": 0.666736398613546,
        "bleu2": 0.6198129921724927,
        "bleu3": 0.5639419664458528,
        "bleu4": 0.4874858042804568,
        "bert_precision": 0.9778013229370117,
        "bert_recall": 0.9430531859397888,
        "bert_f1": 0.9601129293441772,
        "meteor": 0.6405895691609977,
        "sbert_similarity": 0.906976580619812
      }
    },
    {
      "sample_id": 7,
      "question": "What projects is Jolene planning for next year?",
      "prediction": "Plans to solidify the plan first before starting to reach out",
      "reference": "developing renewable energy finding ways to supply clean water to those with limited access",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08695652173913043,
        "rouge1_f": 0.16,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16,
        "bleu1": 0.1384182521267043,
        "bleu2": 0.03246195756111535,
        "bleu3": 0.021494944828486344,
        "bleu4": 0.01706602014357155,
        "bert_precision": 0.8622725009918213,
        "bert_recall": 0.8409859538078308,
        "bert_f1": 0.851496160030365,
        "meteor": 0.072992700729927,
        "sbert_similarity": 0.10041402280330658
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Deborah get her cats?",
      "prediction": "Max is my mother's cat, I took him when my mother passed away.",
      "reference": "Luna is from the shelter and Max is her mother's cat",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3846153846153846,
        "rouge2_f": 0.24999999999999994,
        "rougeL_f": 0.3846153846153846,
        "bleu1": 0.3125,
        "bleu2": 0.25,
        "bleu3": 0.16765563463645106,
        "bleu4": 0.07655122720591222,
        "bert_precision": 0.9034183025360107,
        "bert_recall": 0.9255605936050415,
        "bert_f1": 0.914355456829071,
        "meteor": 0.22817460317460317,
        "sbert_similarity": 0.6961168646812439
      }
    },
    {
      "sample_id": 7,
      "question": "How old are Deborah's cats?",
      "prediction": "8 years old",
      "reference": "Max is 8 years old and Luna is 5 years old",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.33333333333333337,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.06948345122280154,
        "bleu2": 0.06948345122280154,
        "bleu3": 0.06948345122280154,
        "bleu4": 0.0390734160394292,
        "bert_precision": 0.9318113923072815,
        "bert_recall": 0.8960236310958862,
        "bert_f1": 0.9135671854019165,
        "meteor": 0.2505446623093682,
        "sbert_similarity": 0.41491031646728516
      }
    },
    {
      "sample_id": 7,
      "question": "What was Jolene doing with her partner in Rio de Janeiro?",
      "prediction": "They went on many excursions there.",
      "reference": "they went on excursions, checked out some cool yoga classes, visited a lot of delicious cafes, visited an old temple",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.32,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.058115081316701046,
        "bleu2": 0.04438612652538274,
        "bleu3": 0.03459507105594247,
        "bleu4": 0.017866092317439283,
        "bert_precision": 0.938111424446106,
        "bert_recall": 0.8730555772781372,
        "bert_f1": 0.9044151306152344,
        "meteor": 0.15830346475507767,
        "sbert_similarity": 0.5208386778831482
      }
    },
    {
      "sample_id": 7,
      "question": "Have Deborah and Jolene been to Rio de Janeiro?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene's parents give her first console?",
      "prediction": "In 2010 when she was 10 years old",
      "reference": "when she was 10",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.6,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.46291004988627577,
        "bleu3": 0.41857930082433814,
        "bleu4": 0.345720784641941,
        "bert_precision": 0.9218360781669617,
        "bert_recall": 0.9358676075935364,
        "bert_f1": 0.9287987947463989,
        "meteor": 0.9019886363636364,
        "sbert_similarity": 0.797307014465332
      }
    },
    {
      "sample_id": 7,
      "question": "What do Deborah and Jolene plan to try when they meet in a new cafe?",
      "prediction": "fresh pastries",
      "reference": "coffee and fresh pastries",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9568957090377808,
        "bert_recall": 0.9137409925460815,
        "bert_f1": 0.9348205924034119,
        "meteor": 0.4934210526315789,
        "sbert_similarity": 0.8167983889579773
      }
    },
    {
      "sample_id": 7,
      "question": "What project did Jolene finish last week before 23 January, 2023?",
      "prediction": "Cleanup project",
      "reference": "an electrical engineering project",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.9005769491195679,
        "bert_recall": 0.886799693107605,
        "bert_f1": 0.8936352133750916,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.22314588725566864
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene buy her pet snake?",
      "prediction": "last year",
      "reference": "A year ago",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9042181372642517,
        "bert_recall": 0.8827134370803833,
        "bert_f1": 0.8933364152908325,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.6964527368545532
      }
    },
    {
      "sample_id": 7,
      "question": "What project was Jolene working on as of 1 February, 2023?",
      "prediction": "A cleanup project in their community",
      "reference": "Robotics project",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8680117726325989,
        "bert_recall": 0.8933984041213989,
        "bert_f1": 0.8805221319198608,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.3097747564315796
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Deborah meet her new neighbor Anna?",
      "prediction": "yesterday",
      "reference": "yoga in the park",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8890511989593506,
        "bert_recall": 0.8693917989730835,
        "bert_f1": 0.879111647605896,
        "meteor": 0.0,
        "sbert_similarity": 0.10992424190044403
      }
    },
    {
      "sample_id": 7,
      "question": "What activity did Jolene and her partner plan to do together instead of resuming yoga?",
      "prediction": "a meditation retreat",
      "reference": "play the console",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8739688396453857,
        "bert_recall": 0.8959865570068359,
        "bert_f1": 0.884840726852417,
        "meteor": 0.0,
        "sbert_similarity": 0.06870462745428085
      }
    },
    {
      "sample_id": 7,
      "question": "What milestone did Jolene achieve recently on 4 February, 2023?",
      "prediction": "a major milestone",
      "reference": "Design and build a sustainable water purifier for a rural community",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.023161150407600514,
        "bleu2": 0.008970274980764192,
        "bleu3": 0.008415723708086742,
        "bleu4": 0.007894844292834697,
        "bert_precision": 0.8781065940856934,
        "bert_recall": 0.853636622428894,
        "bert_f1": 0.8656987547874451,
        "meteor": 0.049019607843137254,
        "sbert_similarity": 0.04774603247642517
      }
    },
    {
      "sample_id": 7,
      "question": "What is Jolene's favorite book which she mentioned on 4 February, 2023?",
      "prediction": "Avalanche",
      "reference": "\"Sapiens\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.867900550365448,
        "bert_recall": 0.868726909160614,
        "bert_f1": 0.8683134913444519,
        "meteor": 0.0,
        "sbert_similarity": 0.12014491111040115
      }
    },
    {
      "sample_id": 7,
      "question": "What does Deborah bring with her whenever she comes to reflect on her mom?",
      "prediction": "traditions that you do when you come to reflect on your mom's life.",
      "reference": "amulet",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8118014335632324,
        "bert_recall": 0.8591028451919556,
        "bert_f1": 0.834782600402832,
        "meteor": 0.0,
        "sbert_similarity": 0.1049259752035141
      }
    },
    {
      "sample_id": 7,
      "question": "What new outlook did Jolene gain after her mini retreat on 9 February, 2023?",
      "prediction": "a new outlook that totally gave me a new outlook.",
      "reference": "A confidence boost",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8688220977783203,
        "bert_recall": 0.8947657346725464,
        "bert_f1": 0.8816031217575073,
        "meteor": 0.13513513513513514,
        "sbert_similarity": 0.16945207118988037
      }
    },
    {
      "sample_id": 7,
      "question": "What cool stuff did Jolene accomplish at the retreat on 9 February, 2023?",
      "prediction": "Jolen is doing their sporadically for about 3 years now and they've had a real positive effect on them.",
      "reference": "Came up with neat solutions for her engineering project",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07142857142857142,
        "rouge1_f": 0.06896551724137932,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06896551724137932,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.009134423666564471,
        "bert_precision": 0.8280749320983887,
        "bert_recall": 0.8361245393753052,
        "bert_f1": 0.8320802450180054,
        "meteor": 0.05,
        "sbert_similarity": 0.07646076381206512
      }
    },
    {
      "sample_id": 7,
      "question": "What idea did Jolene have to help underprivileged kids learn about STEM subjects on 9 February, 2023?",
      "prediction": "a volunteer program where engineers teach STEM to underprivileged kids",
      "reference": "A volunteer program where engineers teach STEM to underprivileged kids",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9928032159805298,
        "bert_recall": 0.9928032159805298,
        "bert_f1": 0.9928032159805298,
        "meteor": 0.9995,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene plan to involve local engineers in her idea of teaching STEM to underprivileged kids?",
      "prediction": "Jolenesuggests creating a volunteer program where engineers can teach STEM skills to underprivileged children",
      "reference": "As guest speakers for workshops",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8337060213088989,
        "bert_recall": 0.8619046211242676,
        "bert_f1": 0.8475708365440369,
        "meteor": 0.0,
        "sbert_similarity": 0.1848682165145874
      }
    },
    {
      "sample_id": 7,
      "question": "What gave Deborah peace in the garden she visited?",
      "prediction": "calming",
      "reference": "Roses and dahlias",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.87714684009552,
        "bert_recall": 0.8051950335502625,
        "bert_f1": 0.8396323323249817,
        "meteor": 0.0,
        "sbert_similarity": 0.1486213058233261
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Deborah spend time in the garden?",
      "prediction": "Deborah spent time in the garden because it brought her joy and helped others take care of the world.",
      "reference": "to find comfort after losing a friend",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8564214110374451,
        "bert_recall": 0.8818398118019104,
        "bert_f1": 0.8689447641372681,
        "meteor": 0.0,
        "sbert_similarity": 0.21691396832466125
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene and her partner initially meet?",
      "prediction": "They met through community events.",
      "reference": "In an engineering class in college",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8814305663108826,
        "bert_recall": 0.8857336640357971,
        "bert_f1": 0.8835768699645996,
        "meteor": 0.0,
        "sbert_similarity": -0.0020183641463518143
      }
    },
    {
      "sample_id": 7,
      "question": "What activity does Deborah incorporate into her daily routine after going for a morning jog in the park?",
      "prediction": "Incorporating this into her daily routine",
      "reference": "spending time with loved ones",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8368583917617798,
        "bert_recall": 0.8814248442649841,
        "bert_f1": 0.8585637211799622,
        "meteor": 0.0,
        "sbert_similarity": 0.29436010122299194
      }
    },
    {
      "sample_id": 7,
      "question": "According to Jolene, what does exercise help her to feel?",
      "prediction": "Jolenesays: 'It's amazing how something so easy can make such a big difference to our health!'",
      "reference": "connected to her body",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.09523809523809523,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809523,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.8233084678649902,
        "bert_recall": 0.8489631414413452,
        "bert_f1": 0.8359389901161194,
        "meteor": 0.09615384615384617,
        "sbert_similarity": 0.2146415263414383
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deb share a photo of, which brought a smile to Jolene's face?",
      "prediction": "a heart shaped pendant",
      "reference": "a yellow coffee cup with a handwritten message",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.09196986029286058,
        "bleu2": 0.033582644728665616,
        "bleu3": 0.02819913640647395,
        "bleu4": 0.029556479778261396,
        "bert_precision": 0.8852468729019165,
        "bert_recall": 0.8869988322257996,
        "bert_f1": 0.8861219882965088,
        "meteor": 0.06578947368421052,
        "sbert_similarity": 0.2947169840335846
      }
    },
    {
      "sample_id": 7,
      "question": "What is one of Jolene's favorite dishes?",
      "prediction": "vegan stir-fry",
      "reference": "lasagna",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8222032189369202,
        "bert_recall": 0.7715162038803101,
        "bert_f1": 0.7960536479949951,
        "meteor": 0.0,
        "sbert_similarity": 0.0883394330739975
      }
    },
    {
      "sample_id": 7,
      "question": "What picture did Jolene share related to feeling overwhelmed?",
      "prediction": "A photo of a bouquet of flowers",
      "reference": "a photo of a desk with a notebook and a computer monitor",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.42857142857142855,
        "rouge1_f": 0.4210526315789474,
        "rouge2_f": 0.3529411764705882,
        "rougeL_f": 0.4210526315789474,
        "bleu1": 0.27973809117540177,
        "bleu2": 0.26167102380212115,
        "bleu3": 0.23929134914074554,
        "bleu4": 0.20126703311713304,
        "bert_precision": 0.9223567247390747,
        "bert_recall": 0.9094387888908386,
        "bert_f1": 0.9158521890640259,
        "meteor": 0.17391304347826086,
        "sbert_similarity": 0.2963297367095947
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene and Deb discuss as a helpful strategy for studying and time management?",
      "prediction": "Jolen started using a bullet journal to manage her time and stay organized.",
      "reference": "breaking tasks into smaller pieces and setting goals, using planners or schedulers",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16,
        "rouge1_f": 0.16,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8790441751480103,
        "bert_recall": 0.8588427305221558,
        "bert_f1": 0.8688260316848755,
        "meteor": 0.08264462809917354,
        "sbert_similarity": 0.32642921805381775
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene ask Deb to help with on 13 March, 2023?",
      "prediction": "Jolen asked Deb to help keep her updated on the progress of their project.",
      "reference": "time management",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8440341949462891,
        "bert_recall": 0.8632189035415649,
        "bert_f1": 0.8535187244415283,
        "meteor": 0.0,
        "sbert_similarity": 0.17896077036857605
      }
    },
    {
      "sample_id": 7,
      "question": "What method does Deb suggest Jolene to try for organizing tasks based on importance and urgency?",
      "prediction": "The Eisenhower Matrix",
      "reference": "The Eisenhower Matrix",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene and Anna discuss while watching the sunset by the sea?",
      "prediction": "They talked about inspiring each other.",
      "reference": "They realized they inspire each other",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.4000000000000001,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.26726124191242434,
        "bleu3": 0.11511121735118796,
        "bleu4": 0.07730551756939454,
        "bert_precision": 0.95018070936203,
        "bert_recall": 0.945292055606842,
        "bert_f1": 0.9477300643920898,
        "meteor": 0.42592592592592593,
        "sbert_similarity": 0.7825015783309937
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene plan to pursue her dream of learning to surf?",
      "prediction": "Definitely! I've been gathering information, watching videos, and even got a beginners' guide to surfing. Now just need the right time and place for a lesson.",
      "reference": "gathering information, watching videos, getting a beginners' guide",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.43750000000000006,
        "rouge1_f": 0.39999999999999997,
        "rouge2_f": 0.30303030303030304,
        "rougeL_f": 0.39999999999999997,
        "bleu1": 0.30303030303030304,
        "bleu2": 0.27524094128159016,
        "bleu3": 0.24822822189755211,
        "bleu4": 0.21027545940631823,
        "bert_precision": 0.879856526851654,
        "bert_recall": 0.9377490282058716,
        "bert_f1": 0.9078808426856995,
        "meteor": 0.7653061224489797,
        "sbert_similarity": 0.5909045934677124
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah buy to enhance her yoga practice besides the props?",
      "prediction": "a candle and essential oils",
      "reference": "candle",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8449679613113403,
        "bert_recall": 0.8758275508880615,
        "bert_f1": 0.8601210117340088,
        "meteor": 0.35714285714285715,
        "sbert_similarity": 0.7141815423965454
      }
    },
    {
      "sample_id": 7,
      "question": "What type of music does Deborah find helpful during her yoga practice?",
      "prediction": "new tunes",
      "reference": "instrumental tracks with mellow melodies and rhythms",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9222951531410217,
        "bert_recall": 0.8609940409660339,
        "bert_f1": 0.8905909657478333,
        "meteor": 0.0,
        "sbert_similarity": 0.5142954587936401
      }
    },
    {
      "sample_id": 7,
      "question": "Who are the musicians mentioned by Jolene that she enjoys listening to during her yoga practice?",
      "prediction": "The musician mentioned is not explicitly named, but it refers to a soft, soothing song being played in the background.",
      "reference": "Nils Frahm and Olafur Arnalds",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8263105750083923,
        "bert_recall": 0.7794389724731445,
        "bert_f1": 0.8021906018257141,
        "meteor": 0.0,
        "sbert_similarity": 0.1861354261636734
      }
    },
    {
      "sample_id": 7,
      "question": "What album does Deborah recommend for meditation and deep relaxation?",
      "prediction": "Sleep",
      "reference": "'Sleep'",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.94764244556427,
        "bert_recall": 0.9125727415084839,
        "bert_f1": 0.9297770261764526,
        "meteor": 0.0,
        "sbert_similarity": 0.8468328714370728
      }
    },
    {
      "sample_id": 7,
      "question": "Which show did Deborah go to with a friend on 9 April, 2023?",
      "prediction": "an art show",
      "reference": "an art show",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What does Deborah find comforting about going to art shows?",
      "prediction": "Artwork reminds her of someone special and their grace and strength during hardships, bringing back powerful emotions and reminding her of those she has lost.",
      "reference": "It makes her feel like she's still experiencing it with her mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06451612903225806,
        "rouge1_f": 0.15789473684210525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473685,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.02067245576486808,
        "bleu3": 0.012497943706942991,
        "bleu4": 0.009186672262412125,
        "bert_precision": 0.8649351596832275,
        "bert_recall": 0.8798028826713562,
        "bert_f1": 0.8723056316375732,
        "meteor": 0.07518796992481203,
        "sbert_similarity": 0.32381269335746765
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene describe the time spent with her snakes and partner?",
      "prediction": "teaches me to take time and be in tune with myself.",
      "reference": "Valuable and relaxing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8528947234153748,
        "bert_recall": 0.8540462255477905,
        "bert_f1": 0.8534700870513916,
        "meteor": 0.13157894736842107,
        "sbert_similarity": 0.24054674804210663
      }
    },
    {
      "sample_id": 7,
      "question": "What does Jolene enjoy doing with her partner after a long day?",
      "prediction": "shared hobbies and interests",
      "reference": "Playing video games",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8817669153213501,
        "bert_recall": 0.8795899152755737,
        "bert_f1": 0.8806770443916321,
        "meteor": 0.0,
        "sbert_similarity": 0.33393552899360657
      }
    },
    {
      "sample_id": 7,
      "question": "What is Jolene currently doing in June 2023?",
      "prediction": "Jolenesays : Thanks, Deb! Your support means a lot to me.",
      "reference": "interning at a well-known engineering firm",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352942,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.07142857142857141,
        "bleu2": 0.023440361546924773,
        "bleu3": 0.017300021018422027,
        "bleu4": 0.01428363257865929,
        "bert_precision": 0.8221350312232971,
        "bert_recall": 0.8702782392501831,
        "bert_f1": 0.845521867275238,
        "meteor": 0.07692307692307693,
        "sbert_similarity": 0.008420240134000778
      }
    },
    {
      "sample_id": 7,
      "question": "For how long has Jolene had Seraphim as a pet?",
      "prediction": "one year",
      "reference": "one year",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene feel when spending time with Seraphim?",
      "prediction": "Jolene feels comforted and joyful.",
      "reference": "comforted",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.874070405960083,
        "bert_recall": 0.905444860458374,
        "bert_f1": 0.8894810676574707,
        "meteor": 0.35714285714285715,
        "sbert_similarity": 0.4902491569519043
      }
    },
    {
      "sample_id": 7,
      "question": "Which new yoga pose did Deborah share a photo of?",
      "prediction": "Dancer Pose (Natarajasana)",
      "reference": "tree pose",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8228862285614014,
        "bert_recall": 0.9039286375045776,
        "bert_f1": 0.8615057468414307,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.4658343195915222
      }
    },
    {
      "sample_id": 7,
      "question": "What group activity did Deborah start with Anna?",
      "prediction": "running group",
      "reference": "running group",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 7,
      "question": "What made being part of the running group easy for Deborah to stay motivated?",
      "prediction": "It's awesome connecting with people who care about fitness!",
      "reference": "helping and pushing each other during runs",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8509365320205688,
        "bert_recall": 0.8560831546783447,
        "bert_f1": 0.8535020351409912,
        "meteor": 0.0,
        "sbert_similarity": 0.2052116096019745
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Jolene decide to get a snake as a pet?",
      "prediction": "to feel calm and happy",
      "reference": "fascinated by reptiles and it felt like the perfect pet",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.07357588823428847,
        "bleu2": 0.02601300475114445,
        "bleu3": 0.020840858525393174,
        "bleu4": 0.01976560930094397,
        "bert_precision": 0.8506012558937073,
        "bert_recall": 0.8360973000526428,
        "bert_f1": 0.8432868719100952,
        "meteor": 0.052631578947368425,
        "sbert_similarity": 0.2734754979610443
      }
    },
    {
      "sample_id": 7,
      "question": "What is the favorite game Jolene plays with her partner?",
      "prediction": "It's hard to choose just one!",
      "reference": "It takes two",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8563222885131836,
        "bert_recall": 0.8772956728935242,
        "bert_f1": 0.8666820526123047,
        "meteor": 0.0,
        "sbert_similarity": 0.34918391704559326
      }
    },
    {
      "sample_id": 7,
      "question": "What activity does Deborah do with her cats?",
      "prediction": "Having pets around brings a calming and comforting feeling.",
      "reference": "take them out for a run in the park every morning and evening",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818185,
        "rouge1_f": 0.18181818181818185,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818185,
        "bleu1": 0.1481636441363436,
        "bleu2": 0.034922505831373,
        "bleu3": 0.023234419705932024,
        "bleu4": 0.01859351593152887,
        "bert_precision": 0.8785148859024048,
        "bert_recall": 0.8530611395835876,
        "bert_f1": 0.8656009435653687,
        "meteor": 0.11904761904761907,
        "sbert_similarity": 0.189370796084404
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene describe the feeling of finding her snake snuggled under the bed after it got out?",
      "prediction": "relieved",
      "reference": "It really showed how much she loves her.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8435138463973999,
        "bert_recall": 0.8357299566268921,
        "bert_f1": 0.839603841304779,
        "meteor": 0.0,
        "sbert_similarity": 0.10609966516494751
      }
    },
    {
      "sample_id": 7,
      "question": "Why does Deborah take her cats out for a run in the park every day?",
      "prediction": "It is likely to keep them healthy and improve their quality of life.",
      "reference": "Exercise and nature are important to her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.033149677206589796,
        "bleu3": 0.021746357799665227,
        "bleu4": 0.0169861974906263,
        "bert_precision": 0.8848934173583984,
        "bert_recall": 0.897753894329071,
        "bert_f1": 0.8912771940231323,
        "meteor": 0.13157894736842107,
        "sbert_similarity": 0.4261528253555298
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene come to have her pet, Susie?",
      "prediction": "Jolenesays: 'Aww, that's adorable!' What's the second one's name?",
      "reference": "She adopted her two years ago when feeling lonely.",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8322644233703613,
        "bert_recall": 0.8537529706954956,
        "bert_f1": 0.8428717851638794,
        "meteor": 0.0,
        "sbert_similarity": 0.2436847686767578
      }
    },
    {
      "sample_id": 7,
      "question": "What activities have been helping Jolene stay distracted during tough times?",
      "prediction": "Taking beach walks and surfing",
      "reference": "Video games and spending time with her pet, Susie",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.07357588823428847,
        "bleu2": 0.02601300475114445,
        "bleu3": 0.020840858525393174,
        "bleu4": 0.01976560930094397,
        "bert_precision": 0.8785406351089478,
        "bert_recall": 0.8480762243270874,
        "bert_f1": 0.8630396723747253,
        "meteor": 0.05813953488372093,
        "sbert_similarity": 0.2631131410598755
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of yoga routine does Deborah recommend to Jolene?",
      "prediction": "A gentle flow that's all about breathing and grounding",
      "reference": "A gentle flow routine focused on breathing and grounding",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.631578947368421,
        "rouge2_f": 0.47058823529411764,
        "rougeL_f": 0.631578947368421,
        "bleu1": 0.6,
        "bleu2": 0.5163977794943222,
        "bleu3": 0.4091569047284194,
        "bleu4": 0.17567205239427922,
        "bert_precision": 0.9521719217300415,
        "bert_recall": 0.9561353325843811,
        "bert_f1": 0.9541494846343994,
        "meteor": 0.654320987654321,
        "sbert_similarity": 0.8637233376502991
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene design inspired by their love for space and engines?",
      "prediction": "Jolenesets notebooks",
      "reference": "Notebooks",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.223606797749979,
        "bleu3": 0.1740444189610778,
        "bleu4": 0.1495348781221221,
        "bert_precision": 0.8156789541244507,
        "bert_recall": 0.9041086435317993,
        "bert_f1": 0.8576202988624573,
        "meteor": 0.45454545454545453,
        "sbert_similarity": 0.6451603770256042
      }
    },
    {
      "sample_id": 7,
      "question": "What journal has Jolene been using to help track tasks and stay organized?",
      "prediction": "Bullet Journal",
      "reference": "bullet journal",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9531815052032471,
        "bert_recall": 0.9531815052032471,
        "bert_f1": 0.9531815052032471,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What game did Jolene recommend for being calming and cute?",
      "prediction": "Animal Crossing: New Horizons",
      "reference": "Animal Crossing: New Horizons",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What game did Jolene suggest as an awesome open-world game for the Nintendo Switch?",
      "prediction": "Zelda BOTW",
      "reference": "Zelda BOTW",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah and her husband use to play to bond and make memories?",
      "prediction": "video games",
      "reference": "video games",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What is special about the bench at the park near Deborah's house?",
      "prediction": "The special bench holds meaning as it was where Deborah\u2019s mother loved to sit and take in views every morning.",
      "reference": "It holds special memories of conversations with her mom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20689655172413793,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06666666666666667,
        "bleu1": 0.13043478260869565,
        "bleu2": 0.024349237677883696,
        "bleu3": 0.014748504201164714,
        "bleu4": 0.010900096978029118,
        "bert_precision": 0.8695132732391357,
        "bert_recall": 0.9048323035240173,
        "bert_f1": 0.8868212699890137,
        "meteor": 0.1485148514851485,
        "sbert_similarity": 0.4910581707954407
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah and her mom chat about at their special bench in the park?",
      "prediction": "dreams",
      "reference": "dreams and life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9737743735313416,
        "bert_recall": 0.9291372895240784,
        "bert_f1": 0.9509322643280029,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.824173092842102
      }
    },
    {
      "sample_id": 7,
      "question": "What feeling does Deborah get when she thinks about the time spent with her mom at their special spot?",
      "prediction": "peace",
      "reference": "peace and gratitude",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9425317049026489,
        "bert_recall": 0.8733675479888916,
        "bert_f1": 0.9066324830055237,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7389997839927673
      }
    },
    {
      "sample_id": 7,
      "question": "What habits does Jolene practice to feel balanced?",
      "prediction": "Yoga and meditation",
      "reference": "yoga, meditation, walks, and mindfulness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.18887560283756186,
        "bleu2": 0.04223386872360405,
        "bleu3": 0.03287274455178675,
        "bleu4": 0.028243490250557148,
        "bert_precision": 0.9584802389144897,
        "bert_recall": 0.9420990943908691,
        "bert_f1": 0.9502190947532654,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.8141062259674072
      }
    },
    {
      "sample_id": 7,
      "question": "Which yoga pose is Jolene a fan of for rest and calmness?",
      "prediction": "The gentle flow pose",
      "reference": "savasana (the corpse pose)",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.07830277146770757,
        "bleu3": 0.05844171443814127,
        "bleu4": 0.05795053470733952,
        "bert_precision": 0.8910770416259766,
        "bert_recall": 0.8622912168502808,
        "bert_f1": 0.8764477968215942,
        "meteor": 0.0,
        "sbert_similarity": 0.4056508243083954
      }
    },
    {
      "sample_id": 7,
      "question": "How long has Jolene been doing yoga?",
      "prediction": "Since we last spoke",
      "reference": "3 years",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8573063611984253,
        "bert_recall": 0.885634183883667,
        "bert_f1": 0.8712400794029236,
        "meteor": 0.0,
        "sbert_similarity": 0.33727818727493286
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene participate in recently that provided her with a rewarding experience?",
      "prediction": "Studying and gaining experience.",
      "reference": "presenting at a virtual conference",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9022157788276672,
        "bert_recall": 0.8988685607910156,
        "bert_f1": 0.9005391001701355,
        "meteor": 0.0,
        "sbert_similarity": 0.10741373896598816
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene feel after receiving positive feedback at the virtual conference?",
      "prediction": "Jolene felt very positive and motivated to continue pursuing success and making a positive impact.",
      "reference": "thrilled and rewarded",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8590507507324219,
        "bert_recall": 0.8618656396865845,
        "bert_f1": 0.8604558706283569,
        "meteor": 0.11904761904761904,
        "sbert_similarity": 0.3324952721595764
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of event did Jolene present at recently?",
      "prediction": "virtual conference",
      "reference": "virtual conference",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene's mom stress the value of, which she wants to keep in mind for her engineering projects?",
      "prediction": "helping others",
      "reference": "Helping others",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9699387550354004,
        "bert_recall": 0.9699387550354004,
        "bert_f1": 0.9699387550354004,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What type of projects is Jolene interested in getting involved in the future?",
      "prediction": "sustainable initiatives and developing innovative solutions for environmental issues",
      "reference": "Sustainable initiatives and developing innovative solutions for environmental issues",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9897410273551941,
        "bert_recall": 0.9897410273551941,
        "bert_f1": 0.9897410273551941,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "How did Deborah get Luna, one of her cats?",
      "prediction": "Deborah took Luna from the shelter.",
      "reference": "From the shelter",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5714285714285715,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.42857142857142855,
        "bleu2": 0.3779644730092272,
        "bleu3": 0.30935528891357367,
        "bleu4": 0.16348126556655487,
        "bert_precision": 0.8896304965019226,
        "bert_recall": 0.9338451623916626,
        "bert_f1": 0.9112017750740051,
        "meteor": 0.5681818181818182,
        "sbert_similarity": 0.4781571626663208
      }
    },
    {
      "sample_id": 7,
      "question": "How old is Max?",
      "prediction": "Max is 8 years old.",
      "reference": "8 years old",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.5,
        "bleu2": 0.447213595499958,
        "bleu3": 0.3721003696282278,
        "bleu4": 0.20205155046766235,
        "bert_precision": 0.9427616596221924,
        "bert_recall": 0.9527181386947632,
        "bert_f1": 0.9477137923240662,
        "meteor": 0.5859375,
        "sbert_similarity": 0.5282630920410156
      }
    },
    {
      "sample_id": 7,
      "question": "What type of classes did Jolene and her partner check out during their trip to Rio de Janeiro on 30 August, 2023?",
      "prediction": "a dance class",
      "reference": "Yoga classes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9312087893486023,
        "bert_recall": 0.9312087893486023,
        "bert_f1": 0.9312087893486023,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.4872134327888489
      }
    },
    {
      "sample_id": 7,
      "question": "What type of place does Jolene visit to meditate?",
      "prediction": "a nearby park",
      "reference": "A tranquil spot by a pond",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.12262648039048077,
        "bleu2": 0.04749303163563625,
        "bleu3": 0.044556965439969995,
        "bleu4": 0.0417991745584234,
        "bert_precision": 0.8993647694587708,
        "bert_recall": 0.8992670178413391,
        "bert_f1": 0.8993158340454102,
        "meteor": 0.08771929824561403,
        "sbert_similarity": 0.4673607349395752
      }
    },
    {
      "sample_id": 7,
      "question": "What was the new plant Jolene got used as a reminder for on 30 August, 2023?",
      "prediction": "A notebook",
      "reference": "To nurture herself and embrace fresh starts",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8981940746307373,
        "bert_recall": 0.8605464696884155,
        "bert_f1": 0.87896728515625,
        "meteor": 0.0,
        "sbert_similarity": 0.1280166059732437
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Jolene get the new plant on 30 August, 2023?",
      "prediction": "A photo of a snake and a window",
      "reference": "As a reminder to nurture herself and embrace fresh starts",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.19470019576785122,
        "bleu2": 0.04654224884494861,
        "bleu3": 0.03141046144306451,
        "bleu4": 0.025725069574826766,
        "bert_precision": 0.8961938619613647,
        "bert_recall": 0.8670682311058044,
        "bert_f1": 0.8813905119895935,
        "meteor": 0.10204081632653061,
        "sbert_similarity": 0.015336452051997185
      }
    },
    {
      "sample_id": 7,
      "question": "What has Jolene been focusing on lately besides studying?",
      "prediction": "studying and gaining experience",
      "reference": "relationship with her partner",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8818639516830444,
        "bert_recall": 0.8808597922325134,
        "bert_f1": 0.8813616037368774,
        "meteor": 0.0,
        "sbert_similarity": 0.2221321016550064
      }
    },
    {
      "sample_id": 7,
      "question": "How did Deborah's mom support her yoga practice when she first started?",
      "prediction": "When you first started doing yoga, your mother was there to come to your classes with you.",
      "reference": "attended classes with her",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.19047619047619047,
        "rouge2_f": 0.10526315789473684,
        "rougeL_f": 0.19047619047619047,
        "bleu1": 0.10526315789473682,
        "bleu2": 0.07647191129018724,
        "bleu3": 0.033656049636627425,
        "bleu4": 0.02153320854081468,
        "bert_precision": 0.8808799982070923,
        "bert_recall": 0.8920649290084839,
        "bert_f1": 0.886437177658081,
        "meteor": 0.35377358490566035,
        "sbert_similarity": 0.4793914258480072
      }
    },
    {
      "sample_id": 7,
      "question": "What was the video game console that Jolene's parents got her at age 10?",
      "prediction": "Nintendo Game System",
      "reference": "nintendo game console",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9303680658340454,
        "bert_recall": 0.9193791151046753,
        "bert_f1": 0.9248409867286682,
        "meteor": 0.625,
        "sbert_similarity": 0.7919241786003113
      }
    },
    {
      "sample_id": 7,
      "question": "What was one of Jolene's favorite games to play with her mom on the nintendo wii game system?",
      "prediction": "Super Mario Bros.",
      "reference": "Monster Hunter: World",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9016331434249878,
        "bert_recall": 0.8623614311218262,
        "bert_f1": 0.8815601468086243,
        "meteor": 0.0,
        "sbert_similarity": 0.39191848039627075
      }
    },
    {
      "sample_id": 7,
      "question": "What course did Jolene sign up for on 6 September 2023?",
      "prediction": "Jolene signed up for an electrical engineering project.",
      "reference": "meditation",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8222160339355469,
        "bert_recall": 0.7854915857315063,
        "bert_f1": 0.8034343719482422,
        "meteor": 0.0,
        "sbert_similarity": -0.010329503566026688
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Jolene have to reschedule their meeting with Deborah on September 8, 2023?",
      "prediction": "Jolen needs to solidify the plan first before proceeding.",
      "reference": "Jolene already had plans",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.0,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8821396827697754,
        "bert_recall": 0.8834621906280518,
        "bert_f1": 0.8828004598617554,
        "meteor": 0.22222222222222224,
        "sbert_similarity": 0.6202230453491211
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Jolene and her partner travel for a few weeks in September 2023?",
      "prediction": "Paris",
      "reference": "Phuket",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9318501949310303,
        "bert_recall": 0.8258138298988342,
        "bert_f1": 0.8756335377693176,
        "meteor": 0.0,
        "sbert_similarity": 0.2684350311756134
      }
    },
    {
      "sample_id": 7,
      "question": "What was the main focus of the session that stood out to Jolene during the retreat?",
      "prediction": "releasing expectations and judgments, savoring the present",
      "reference": "releasing expectations and judgments and savoring the present",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 1.0,
        "rouge1_f": 0.9333333333333333,
        "rouge2_f": 0.7692307692307692,
        "rougeL_f": 0.9333333333333333,
        "bleu1": 0.875,
        "bleu2": 0.7905694150420949,
        "bleu3": 0.6812405754951913,
        "bleu4": 0.5,
        "bert_precision": 0.9931137561798096,
        "bert_recall": 0.9931137561798096,
        "bert_f1": 0.9931137561798096,
        "meteor": 0.7120253164556962,
        "sbert_similarity": 0.996201753616333
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene feel about her progress in practicing mindfulness and gratitude?",
      "prediction": "Jolene felt proud and accomplished",
      "reference": "experiencing a new level of joy and happiness",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.10976232721880529,
        "bleu2": 0.03880684294761699,
        "bleu3": 0.03109090746822916,
        "bleu4": 0.029486824119076216,
        "bert_precision": 0.8629164695739746,
        "bert_recall": 0.8651353716850281,
        "bert_f1": 0.8640244603157043,
        "meteor": 0.06493506493506494,
        "sbert_similarity": 0.29072415828704834
      }
    },
    {
      "sample_id": 7,
      "question": "What positive change did Jolene experience during the retreat?",
      "prediction": "It was a dope experience that totally gave me a new outlook.",
      "reference": "finding inner peace",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8594951629638672,
        "bert_recall": 0.8678871393203735,
        "bert_f1": 0.8636707663536072,
        "meteor": 0.0,
        "sbert_similarity": 0.12098472565412521
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene recently play that she described to Deb?",
      "prediction": "video games",
      "reference": "a card game about cats",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9330918788909912,
        "bert_recall": 0.8792035579681396,
        "bert_f1": 0.9053465723991394,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.3608322739601135
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah do with their mom's old friends?",
      "prediction": "Reconnected with her mom's old friends.",
      "reference": "reminisced and looked through photos",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8554433584213257,
        "bert_recall": 0.8214446306228638,
        "bert_f1": 0.8380993604660034,
        "meteor": 0.0,
        "sbert_similarity": 0.3147732615470886
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Deborah get married?",
      "prediction": "beach",
      "reference": "on the beach",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9095892906188965,
        "bert_recall": 0.9240610003471375,
        "bert_f1": 0.9167680144309998,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.8137428760528564
      }
    },
    {
      "sample_id": 7,
      "question": "What does yoga on the beach provide for Deborah?",
      "prediction": "relaxation and self-care",
      "reference": "a peaceful atmosphere",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9003688097000122,
        "bert_recall": 0.9126933813095093,
        "bert_f1": 0.9064891934394836,
        "meteor": 0.0,
        "sbert_similarity": 0.24575772881507874
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene describe their home room?",
      "prediction": "It's where they feel connected to her.",
      "reference": "little haven for peace and rest",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8682832717895508,
        "bert_recall": 0.8623154163360596,
        "bert_f1": 0.865289032459259,
        "meteor": 0.0,
        "sbert_similarity": 0.18691964447498322
      }
    },
    {
      "sample_id": 7,
      "question": "What new activity did Deborah and her neighbor organize for the community on 16 September, 2023?",
      "prediction": "A free gardening class",
      "reference": "Free gardening class",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.75,
        "bleu2": 0.7071067811865475,
        "bleu3": 0.63287829698514,
        "bleu4": 0.3976353643835253,
        "bert_precision": 0.969430685043335,
        "bert_recall": 0.9817464351654053,
        "bert_f1": 0.9755496978759766,
        "meteor": 0.9498207885304659,
        "sbert_similarity": 0.9691208600997925
      }
    },
    {
      "sample_id": 7,
      "question": "What was Deborah's mom passionate about?",
      "prediction": "travel",
      "reference": "Cooking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8759351968765259,
        "bert_recall": 0.8683636784553528,
        "bert_f1": 0.8721329569816589,
        "meteor": 0.0,
        "sbert_similarity": 0.25563955307006836
      }
    },
    {
      "sample_id": 7,
      "question": "What food did Deborah's mom make for her on birthdays?",
      "prediction": "pineapple birthday cakes",
      "reference": "Pineapple cakes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.13512001548070346,
        "bert_precision": 0.9287896156311035,
        "bert_recall": 0.9315871000289917,
        "bert_f1": 0.9301862716674805,
        "meteor": 0.4761904761904762,
        "sbert_similarity": 0.878207266330719
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of cookies did Jolene used to bake with someone close to her?",
      "prediction": "chocolate chip cookies",
      "reference": "Chocolate chip cookies",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9886189699172974,
        "bert_recall": 0.9886189699172974,
        "bert_f1": 0.9886189699172974,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What outdoor activity did Jolene suggest doing together with Deborah?",
      "prediction": "Surfing",
      "reference": "Surfing",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What activity did Deborah enjoy at the music festival with their pals on September 20, 2023?",
      "prediction": "freedom dancing",
      "reference": "Dancing and bopping around",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.8862946033477783,
        "bert_recall": 0.8650137186050415,
        "bert_f1": 0.8755248785018921,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.5409018993377686
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah find freeing at the music festival?",
      "prediction": "freedom dancing",
      "reference": "Dancing and bopping around",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.082260343798398,
        "bleu3": 0.06402736358640969,
        "bleu4": 0.05501080739920602,
        "bert_precision": 0.8862946033477783,
        "bert_recall": 0.8650137186050415,
        "bert_f1": 0.8755248785018921,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.5409018993377686
      }
    },
    {
      "sample_id": 7,
      "question": "What are the names of Deborah's snakes?",
      "prediction": "Seraphim",
      "reference": "Susie, Seraphim",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9324479103088379,
        "bert_recall": 0.9160676002502441,
        "bert_f1": 0.9241852164268494,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7425289750099182
      }
    },
    {
      "sample_id": 7,
      "question": "What are Deborah's favorite books?",
      "prediction": "Neal Stephenson",
      "reference": "Sapiens, Avalanche by Neal Stephenson",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.1353352832366127,
        "bleu3": 0.06330106782582003,
        "bleu4": 0.04279677428117006,
        "bert_precision": 0.8900831937789917,
        "bert_recall": 0.8260875940322876,
        "bert_f1": 0.8568922281265259,
        "meteor": 0.39893617021276595,
        "sbert_similarity": 0.5375036597251892
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Deborah get her dogs?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Luna is from the shelter and Max is her mother's cat",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.0493193927883213,
        "bleu2": 0.017437038542312454,
        "bleu3": 0.013970045246163798,
        "bleu4": 0.013249284136531224,
        "bert_precision": 0.8849023580551147,
        "bert_recall": 0.8471750020980835,
        "bert_f1": 0.8656278252601624,
        "meteor": 0.04807692307692308,
        "sbert_similarity": 0.09645657986402512
      }
    },
    {
      "sample_id": 7,
      "question": "How old are Jolene's cats?",
      "prediction": "Max is 8 years old and Luna is 5 years old",
      "reference": "Max is 8 years old and Luna is 5 years old",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "When did Deborah's parents give her first console?",
      "prediction": "Not mentioned in the conversation",
      "reference": "when she was 10",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8862900137901306,
        "bert_recall": 0.8732632398605347,
        "bert_f1": 0.8797284364700317,
        "meteor": 0.0,
        "sbert_similarity": 0.10233408212661743
      }
    },
    {
      "sample_id": 7,
      "question": "When did Jolene release her pet snake?",
      "prediction": "A year ago",
      "reference": "A year ago",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Jolene meet her new friend Anna?",
      "prediction": "Not mentioned in the conversation",
      "reference": "yoga in the park",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444445,
        "rouge1_f": 0.4444444444444445,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444445,
        "bleu1": 0.4,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1522474354853864,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8937099575996399,
        "bert_recall": 0.8907894492149353,
        "bert_f1": 0.8922473192214966,
        "meteor": 0.45731707317073167,
        "sbert_similarity": 0.07275734096765518
      }
    },
    {
      "sample_id": 7,
      "question": "What is Deborah's favorite book which she mentioned on 4 February, 2023?",
      "prediction": "Neal Stephenson",
      "reference": "\"Sapiens\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8623151779174805,
        "bert_recall": 0.8440194129943848,
        "bert_f1": 0.8530691862106323,
        "meteor": 0.0,
        "sbert_similarity": 0.10906723886728287
      }
    },
    {
      "sample_id": 7,
      "question": "What cool stuff did Deborah accomplish at the retreat on 9 February, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Came up with neat solutions for her engineering project",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8922132849693298,
        "bert_recall": 0.8488575220108032,
        "bert_f1": 0.8699955940246582,
        "meteor": 0.0,
        "sbert_similarity": 0.07819128036499023
      }
    },
    {
      "sample_id": 7,
      "question": "How does Deborah plan to involve local engineers in her idea of teaching STEM to underprivileged kids?",
      "prediction": "As guest speakers for workshops",
      "reference": "As guest speakers for workshops",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "What gave Deborah anxiety in the garden she visited?",
      "prediction": "friend",
      "reference": "Roses and dahlias",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8229767084121704,
        "bert_recall": 0.779107391834259,
        "bert_f1": 0.8004413843154907,
        "meteor": 0.0,
        "sbert_similarity": 0.07273277640342712
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Jolene spend time in the garden?",
      "prediction": "to find comfort after losing a friend",
      "reference": "to find comfort after losing a friend",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene and her rival initially meet?",
      "prediction": "Not mentioned in the conversation",
      "reference": "In an engineering class in college",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20000000000000004,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.1637461506155964,
        "bleu2": 0.05789300674674099,
        "bleu3": 0.046382183633183686,
        "bleu4": 0.0439891724758422,
        "bert_precision": 0.8906236290931702,
        "bert_recall": 0.886425256729126,
        "bert_f1": 0.8885195255279541,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.09315694868564606
      }
    },
    {
      "sample_id": 7,
      "question": "What activity does Jolene incorporate into her daily routine after going for a morning jog in the park?",
      "prediction": "teaching classes",
      "reference": "spending time with loved ones",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8967806100845337,
        "bert_recall": 0.8757728338241577,
        "bert_f1": 0.8861522078514099,
        "meteor": 0.0,
        "sbert_similarity": 0.07069994509220123
      }
    },
    {
      "sample_id": 7,
      "question": "What method does Jolene suggest Deborah to try for organizing tasks based on importance and urgency?",
      "prediction": "The Eisenhower Matrix",
      "reference": "The Eisenhower Matrix",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "How does Jolene plan to pursue her dream of climbing mountains?",
      "prediction": "getting a beginners' guide",
      "reference": "gathering information, watching videos, getting a beginners' guide",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.6,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.301194211912202,
        "bleu2": 0.301194211912202,
        "bleu3": 0.301194211912202,
        "bleu4": 0.301194211912202,
        "bert_precision": 0.9617597460746765,
        "bert_recall": 0.8919069170951843,
        "bert_f1": 0.925517201423645,
        "meteor": 0.522203947368421,
        "sbert_similarity": 0.7756885886192322
      }
    },
    {
      "sample_id": 7,
      "question": "Who are the authors mentioned by Jolene that she enjoys reading during her yoga practice?",
      "prediction": "Nils Frahm and Olafur Arnalds",
      "reference": "Nils Frahm and Olafur Arnalds",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.996,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 7,
      "question": "Which show did Jolene go to with a friend on 9 April, 2023?",
      "prediction": "an art show",
      "reference": "an art show",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What does Deborah find comforting about going to horror movie screenings?",
      "prediction": "Not mentioned in the conversation",
      "reference": "It makes her feel like she's still experiencing it with her mom",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8937363028526306,
        "bert_recall": 0.8624415397644043,
        "bert_f1": 0.877810001373291,
        "meteor": 0.0,
        "sbert_similarity": 0.037503018975257874
      }
    },
    {
      "sample_id": 7,
      "question": "How does Deborah describe the time spent with her snakes and partner?",
      "prediction": "Valuable and relaxing",
      "reference": "Valuable and relaxing",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "For how long has Jolene had Lucifer as a pet?",
      "prediction": "One year",
      "reference": "one year",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9747847318649292,
        "bert_recall": 0.9747847318649292,
        "bert_f1": 0.9747847318649292,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "How does Deborah feel when spending time with Seraphim?",
      "prediction": "comforted",
      "reference": "comforted",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What made being part of the running group easy for Jolene to stay motivated?",
      "prediction": "helping and pushing each other during runs",
      "reference": "helping and pushing each other during runs",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Jolene decide to get a tarantula as a pet?",
      "prediction": "fascinated by reptiles and it felt like the perfect pet",
      "reference": "fascinated by reptiles and it felt like the perfect pet",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9995,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "How did Deborah come to have her pet, Susie?",
      "prediction": "She adopted her two years ago when feeling lonely.",
      "reference": "She adopted her two years ago when feeling lonely.",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah design inspired by their love for space and engines?",
      "prediction": "Notebooks",
      "reference": "Notebooks",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What journal has Deborah been using to help track tasks and stay organized?",
      "prediction": "bullet journal",
      "reference": "bullet journal",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What game did Jolene recommend to Deborah for being thrilling and intense?",
      "prediction": "Zelda BOTW",
      "reference": "Animal Crossing: New Horizons",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.870960533618927,
        "bert_recall": 0.8673465847969055,
        "bert_f1": 0.8691497445106506,
        "meteor": 0.0,
        "sbert_similarity": 0.2902653217315674
      }
    },
    {
      "sample_id": 7,
      "question": "What game did Deborah suggest as an awesome open-world game for the Nintendo Switch?",
      "prediction": "Zelda BOTW",
      "reference": "Zelda BOTW",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What is special about the bench at the park near Jolene's house?",
      "prediction": "special memories",
      "reference": "It holds special memories of conversations with her mom",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3636363636363636,
        "rouge1_f": 0.3636363636363636,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.3636363636363636,
        "bleu1": 0.0301973834223185,
        "bleu2": 0.0301973834223185,
        "bleu3": 0.014124377401541842,
        "bleu4": 0.009549251099193677,
        "bert_precision": 0.9090702533721924,
        "bert_recall": 0.8806525468826294,
        "bert_f1": 0.894635796546936,
        "meteor": 0.2259036144578313,
        "sbert_similarity": 0.5678738355636597
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene and her mom chat about at their special bench in the park?",
      "prediction": "dreams and life",
      "reference": "dreams and life",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "How did Deborah feel after receiving positive feedback at the virtual conference?",
      "prediction": "thrilled and rewarded",
      "reference": "thrilled and rewarded",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of event did Deborah present at recently?",
      "prediction": "None",
      "reference": "virtual conference",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8353156447410583,
        "bert_recall": 0.8349641561508179,
        "bert_f1": 0.8351398706436157,
        "meteor": 0.0,
        "sbert_similarity": 0.17046722769737244
      }
    },
    {
      "sample_id": 7,
      "question": "What did Deborah's mom stress the value of, which she wants to keep in mind for her engineering projects?",
      "prediction": "Helping others",
      "reference": "Helping others",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "What type of projects is Deborah interested in getting involved in the future?",
      "prediction": "Sustainable initiatives and developing innovative solutions for environmental issues",
      "reference": "Sustainable initiatives and developing innovative solutions for environmental issues",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9993141289437586,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene get Luna, one of her cats?",
      "prediction": "From the shelter",
      "reference": "From the shelter",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What type of classes did Deborah and her partner check out during their trip to Rio de Janeiro on 30 August, 2023?",
      "prediction": "Yoga classes",
      "reference": "Yoga classes",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "Why did Deborah get the new plant on 30 August, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "As a reminder to nurture herself and embrace fresh starts",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9065911173820496,
        "bert_recall": 0.8643708825111389,
        "bert_f1": 0.8849777579307556,
        "meteor": 0.0,
        "sbert_similarity": 0.01093505509197712
      }
    },
    {
      "sample_id": 7,
      "question": "How did Jolene's mom support her yoga practice when she first started?",
      "prediction": "Not mentioned in the conversation or attended classes with her",
      "reference": "attended classes with her",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.5,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.4,
        "bleu2": 0.36514837167011077,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.2626909894424158,
        "bert_precision": 0.8915004134178162,
        "bert_recall": 0.9280546307563782,
        "bert_f1": 0.9094103574752808,
        "meteor": 0.8627717391304348,
        "sbert_similarity": 0.740433931350708
      }
    },
    {
      "sample_id": 7,
      "question": "What was the video game console that Deborah's parents got her at age 10?",
      "prediction": "Nintendo Game Console",
      "reference": "nintendo game console",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9336614608764648,
        "bert_recall": 0.9209400415420532,
        "bert_f1": 0.9272571206092834,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What was one of Deborah's favorite games to play with her mom on the PlayStation game system?",
      "prediction": "Monster Hunter: World",
      "reference": "Monster Hunter: World",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Deborah and her partner travel for a few weeks in September 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Phuket",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.870922863483429,
        "bert_recall": 0.798068642616272,
        "bert_f1": 0.8329055905342102,
        "meteor": 0.0,
        "sbert_similarity": 0.15100598335266113
      }
    },
    {
      "sample_id": 7,
      "question": "What did Jolene do with their mom's old friends?",
      "prediction": "Not mentioned in the conversation",
      "reference": "reminisced and looked through photos",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8737488985061646,
        "bert_recall": 0.7974449396133423,
        "bert_f1": 0.8338549137115479,
        "meteor": 0.0,
        "sbert_similarity": 0.08628302067518234
      }
    },
    {
      "sample_id": 7,
      "question": "Where did Jolene get married?",
      "prediction": "Not mentioned in the conversation",
      "reference": "on the beach",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.8921806812286377,
        "bert_recall": 0.9198952913284302,
        "bert_f1": 0.9058260321617126,
        "meteor": 0.15625,
        "sbert_similarity": 0.21214985847473145
      }
    },
    {
      "sample_id": 7,
      "question": "What new activity did Jolene and her neighbor organize for the community on 16 September, 2023?",
      "prediction": "Cleanup project",
      "reference": "Free gardening class",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8859589695930481,
        "bert_recall": 0.886350154876709,
        "bert_f1": 0.8861545324325562,
        "meteor": 0.0,
        "sbert_similarity": 0.2127363681793213
      }
    },
    {
      "sample_id": 7,
      "question": "What food did Jolene's mom make for her on holidays?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Pineapple cakes",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8890999555587769,
        "bert_recall": 0.8359236717224121,
        "bert_f1": 0.8616921901702881,
        "meteor": 0.0,
        "sbert_similarity": 0.1015155017375946
      }
    },
    {
      "sample_id": 7,
      "question": "What kind of cookies did Deborah used to bake with someone close to her?",
      "prediction": "Chocolate chip cookies",
      "reference": "Chocolate chip cookies",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 7,
      "question": "What activity did Jolene enjoy at the music festival with their pals on September 20, 2023?",
      "prediction": "Dancing and bopping around",
      "reference": "Dancing and bopping around",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of car does Evan drive?",
      "prediction": "Samsays",
      "reference": "Prius",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8582882285118103,
        "bert_recall": 0.9062603712081909,
        "bert_f1": 0.8816222548484802,
        "meteor": 0.0,
        "sbert_similarity": 0.2966863512992859
      }
    },
    {
      "sample_id": 8,
      "question": "What kinds of things did Evan have broken?",
      "prediction": "Evan had a hard time adjusting to changes and understanding others.",
      "reference": "His old Prius and his new Prius.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.12500000000000003,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03892494720807617,
        "bleu3": 0.025677367760996977,
        "bleu4": 0.020255986027125642,
        "bert_precision": 0.8781844973564148,
        "bert_recall": 0.8682245016098022,
        "bert_f1": 0.8731761574745178,
        "meteor": 0.06756756756756757,
        "sbert_similarity": 0.20653057098388672
      }
    },
    {
      "sample_id": 8,
      "question": "Where has Evan been on roadtrips with his family?",
      "prediction": "Jasper",
      "reference": "Rockies, Jasper",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.1353352832366127,
        "bleu2": 0.04279677428117006,
        "bleu3": 0.02960813390314045,
        "bleu4": 0.024066394763145416,
        "bert_precision": 0.9127894639968872,
        "bert_recall": 0.8710055947303772,
        "bert_f1": 0.8914080858230591,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.6613983511924744
      }
    },
    {
      "sample_id": 8,
      "question": "How many Prius has Evan owned?",
      "prediction": "one",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9268282055854797,
        "bert_recall": 0.9268282055854797,
        "bert_f1": 0.9268282055854797,
        "meteor": 0.0,
        "sbert_similarity": 0.7806955575942993
      }
    },
    {
      "sample_id": 8,
      "question": "Which hobby did Sam take up in May 2023?",
      "prediction": "hiking",
      "reference": "painting",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8978094458580017,
        "bert_recall": 0.909110426902771,
        "bert_f1": 0.9034246206283569,
        "meteor": 0.0,
        "sbert_similarity": 0.19972947239875793
      }
    },
    {
      "sample_id": 8,
      "question": "Which country was Evan visiting in May 2023?",
      "prediction": "Canada",
      "reference": "Canada",
      "category": 3,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999997019767761,
        "bert_recall": 0.9999997019767761,
        "bert_f1": 0.9999997019767761,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "How many roadtrips did Evan take in May 2023?",
      "prediction": "1",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9115664958953857,
        "bert_recall": 0.9115664958953857,
        "bert_f1": 0.9115664958953857,
        "meteor": 0.0,
        "sbert_similarity": 0.6732218861579895
      }
    },
    {
      "sample_id": 8,
      "question": "What new hobbies did Sam consider trying?",
      "prediction": "Sam considered trying painting",
      "reference": "Painting, kayaking, hiking, cooking, running",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.07162619921504752,
        "bleu2": 0.02615419001229357,
        "bleu3": 0.021961509515299276,
        "bleu4": 0.023018609596144116,
        "bert_precision": 0.8489516973495483,
        "bert_recall": 0.8517907857894897,
        "bert_f1": 0.850368857383728,
        "meteor": 0.0,
        "sbert_similarity": 0.22649060189723969
      }
    },
    {
      "sample_id": 8,
      "question": "What hobby did Evan start practicing a few years ago that he enjoys?",
      "prediction": "painting",
      "reference": "Watercolor painting",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9191173315048218,
        "bert_recall": 0.8778599500656128,
        "bert_f1": 0.898015022277832,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8051043748855591
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan go to Jasper with his family?",
      "prediction": "2023",
      "reference": "weekend before May 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.9328055381774902,
        "bert_recall": 0.8386180400848389,
        "bert_f1": 0.8832077980041504,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.6241807341575623
      }
    },
    {
      "sample_id": 8,
      "question": "Which type of vacation would Evan prefer with his family, walking tours in metropolitan cities or camping trip in the outdoors?",
      "prediction": "Evan prefers a camping trip in the outdoors.",
      "reference": "camping trip in the outdoors",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7692307692307693,
        "rouge1_f": 0.7692307692307693,
        "rouge2_f": 0.7272727272727273,
        "rougeL_f": 0.7692307692307693,
        "bleu1": 0.5555555555555556,
        "bleu2": 0.5270462766947299,
        "bleu3": 0.49543631365378615,
        "bleu4": 0.44632361378533286,
        "bert_precision": 0.9272789359092712,
        "bert_recall": 0.9452950954437256,
        "bert_f1": 0.9362003207206726,
        "meteor": 0.7488207547169812,
        "sbert_similarity": 0.6442492008209229
      }
    },
    {
      "sample_id": 8,
      "question": "What health issue did Sam face that motivated him to change his lifestyle?",
      "prediction": "Sam faced issues related to weight and felt this weight was keeping him from fully living.",
      "reference": "Weight problem",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11764705882352941,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8669424653053284,
        "bert_recall": 0.914023756980896,
        "bert_f1": 0.8898608088493347,
        "meteor": 0.1470588235294118,
        "sbert_similarity": 0.3520888686180115
      }
    },
    {
      "sample_id": 8,
      "question": "When did Sam first go to the doctor and find out he had a weight problem?",
      "prediction": "2023-10-6",
      "reference": "A few days before May 24, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705184459686279,
        "bert_recall": 0.8835731148719788,
        "bert_f1": 0.8769972324371338,
        "meteor": 0.0,
        "sbert_similarity": 0.5964210033416748
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan have his sudden heart palpitation incident that really shocked him up?",
      "prediction": "27 July 2023",
      "reference": "first week of June 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9326823353767395,
        "bert_recall": 0.8927247524261475,
        "bert_f1": 0.9122662544250488,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.697934627532959
      }
    },
    {
      "sample_id": 8,
      "question": "What is Evan's favorite food?",
      "prediction": "strawberries",
      "reference": "Ginger snaps",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8452395796775818,
        "bert_recall": 0.8532724976539612,
        "bert_f1": 0.8492370247840881,
        "meteor": 0.0,
        "sbert_similarity": 0.3538723886013031
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of unhealthy snacks does Sam enjoy eating?",
      "prediction": "sodas",
      "reference": "soda, candy",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8594776391983032,
        "bert_recall": 0.8517363667488098,
        "bert_f1": 0.8555894494056702,
        "meteor": 0.0,
        "sbert_similarity": 0.7546108961105347
      }
    },
    {
      "sample_id": 8,
      "question": "What recurring issue frustrates Sam at the grocery store?",
      "prediction": "self-checkout machines",
      "reference": "Malfunctioning self-checkout machines.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9627211093902588,
        "bert_recall": 0.9082533121109009,
        "bert_f1": 0.9346944093704224,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.8113178610801697
      }
    },
    {
      "sample_id": 8,
      "question": "When did Sam's friends mock him for being overweight?",
      "prediction": "2023-07-27",
      "reference": "Friday before 27 July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8785983324050903,
        "bert_recall": 0.8464900851249695,
        "bert_f1": 0.8622454404830933,
        "meteor": 0.0,
        "sbert_similarity": 0.601879894733429
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of healthy food suggestions has Evan given to Sam?",
      "prediction": "Samsays received healthier snack ideas",
      "reference": "flavored seltzer water, dark chocolate with high cocoa content, air-popped popcorn and fruit, veggies, healthy sandwich snacks, energy balls, grilled chicken salad with avocado",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.06666666666666667,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06666666666666667,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8369158506393433,
        "bert_recall": 0.8024771213531494,
        "bert_f1": 0.8193347454071045,
        "meteor": 0.0,
        "sbert_similarity": 0.4629366397857666
      }
    },
    {
      "sample_id": 8,
      "question": "Considering their conversations and personal growth, what advice might Evan and Sam give to someone facing a major life transition or challenge?",
      "prediction": "Be patient, keep trying new things, and remember that progress takes time. Also, don't put too much pressure on yourself and focus on feeling good.",
      "reference": "Evan and Sam would likely advise embracing small, consistent changes\u200b\u200b, finding stress-relieving activities like hiking\u200b\u200b, painting, and road trips\u200b\u200b, and the importance of friendship and support in navigating challenges\u200b\u200b.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.04081632653061224,
        "rouge1_f": 0.07142857142857144,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857144,
        "bleu1": 0.17011850554916363,
        "bleu2": 0.07059861046156587,
        "bleu3": 0.025618942717264136,
        "bleu4": 0.014756727937572161,
        "bert_precision": 0.8764004111289978,
        "bert_recall": 0.8452975749969482,
        "bert_f1": 0.8605680465698242,
        "meteor": 0.06993006993006994,
        "sbert_similarity": 0.15283341705799103
      }
    },
    {
      "sample_id": 8,
      "question": "In light of the health and dietary changes discussed, what would be an appropriate gift for both Evan and Sam to encourage their healthy lifestyles?",
      "prediction": "A set of yoga mat or a reusable water bottle.",
      "reference": "a cookbook with healthy recipes or a subscription to a healthy meal delivery service.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.19999999999999998,
        "rouge1_f": 0.25,
        "rouge2_f": 0.09090909090909093,
        "rougeL_f": 0.25,
        "bleu1": 0.252779610326865,
        "bleu2": 0.13255874597390954,
        "bleu3": 0.052746654186203404,
        "bleu4": 0.03295402268196149,
        "bert_precision": 0.8740909695625305,
        "bert_recall": 0.875808596611023,
        "bert_f1": 0.8749489188194275,
        "meteor": 0.11029411764705882,
        "sbert_similarity": 0.24078409373760223
      }
    },
    {
      "sample_id": 8,
      "question": "How does Evan describe the woman and his feelings for her that he met in Canada?",
      "prediction": "It was like something out of a movie.",
      "reference": "He says she's cool, incredible, like something out of a movie, and that he feels alive around her. Every moment with her is fun and energetic, also Evan feels really lucky to have someone who gets him.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2926829268292683,
        "rouge1_f": 0.2608695652173913,
        "rouge2_f": 0.22727272727272727,
        "rougeL_f": 0.2608695652173913,
        "bleu1": 0.015919614444494815,
        "bleu2": 0.014270683975224947,
        "bleu3": 0.013412112560505084,
        "bleu4": 0.012495219530342359,
        "bert_precision": 0.9394689202308655,
        "bert_recall": 0.854752242565155,
        "bert_f1": 0.895110547542572,
        "meteor": 0.1460410557184751,
        "sbert_similarity": 0.13452960550785065
      }
    },
    {
      "sample_id": 8,
      "question": "When Evan did meet his future wife?",
      "prediction": "2023",
      "reference": "week before August 7, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.9274994134902954,
        "bert_recall": 0.8685771226882935,
        "bert_f1": 0.8970717191696167,
        "meteor": 0.0,
        "sbert_similarity": 0.6708895564079285
      }
    },
    {
      "sample_id": 8,
      "question": "When did Sam start working out at the gym?",
      "prediction": "2023",
      "reference": "July 28, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9342549443244934,
        "bert_recall": 0.9003336429595947,
        "bert_f1": 0.9169807434082031,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7725918889045715
      }
    },
    {
      "sample_id": 8,
      "question": "What significant event happened in Sam's life towards the end of summer 2023?",
      "prediction": "Sam got married last week.",
      "reference": "He fell in love with a Canadian woman",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9009770154953003,
        "bert_recall": 0.884492814540863,
        "bert_f1": 0.8926588892936707,
        "meteor": 0.0,
        "sbert_similarity": 0.2472875416278839
      }
    },
    {
      "sample_id": 8,
      "question": "Which year did Evan start taking care of his health seriously?",
      "prediction": "2023",
      "reference": "2021",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9707317352294922,
        "bert_recall": 0.9707317352294922,
        "bert_f1": 0.9707317352294922,
        "meteor": 0.0,
        "sbert_similarity": 0.7438403367996216
      }
    },
    {
      "sample_id": 8,
      "question": "What motivates Evan to take care of his health?",
      "prediction": "Evan struggles with his health and sticks to it despite challenges.",
      "reference": "family, fitness tracker, thirst for adventure on interesting hikes",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8728777170181274,
        "bert_recall": 0.8486781716346741,
        "bert_f1": 0.8606078624725342,
        "meteor": 0.0,
        "sbert_similarity": 0.1163797527551651
      }
    },
    {
      "sample_id": 8,
      "question": "What electronic device could Evan gift Sam to help him keep up with his fitness goals?",
      "prediction": "a smart watch",
      "reference": "fitness tracker",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9241222143173218,
        "bert_recall": 0.9145779013633728,
        "bert_f1": 0.9193253517150879,
        "meteor": 0.0,
        "sbert_similarity": 0.39024853706359863
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of writing does Sam do to relax and cope with his health issues?",
      "prediction": "Creative writing",
      "reference": "journalling, creative writing",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9282413721084595,
        "bert_recall": 0.8610664010047913,
        "bert_f1": 0.8933929204940796,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.8285381197929382
      }
    },
    {
      "sample_id": 8,
      "question": "Who did Evan meet on his trip to Canada, and who did he come back from Canada with?",
      "prediction": "Evan met an awesome Canadian woman; he returned home with his new SO.",
      "reference": "Evan met the woman he fell in love with and returned with her.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.46153846153846156,
        "rouge2_f": 0.16666666666666666,
        "rougeL_f": 0.46153846153846156,
        "bleu1": 0.4666666666666667,
        "bleu2": 0.18257418583505536,
        "bleu3": 0.06530533771613309,
        "bleu4": 0.038233014078303,
        "bert_precision": 0.9164475202560425,
        "bert_recall": 0.9314830899238586,
        "bert_f1": 0.9239040613174438,
        "meteor": 0.28615384615384615,
        "sbert_similarity": 0.7712769508361816
      }
    },
    {
      "sample_id": 8,
      "question": "When Evan get back from a vacation with his SO?",
      "prediction": "2023",
      "reference": "August 13, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9341787099838257,
        "bert_recall": 0.9019284248352051,
        "bert_f1": 0.9177703261375427,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7351039052009583
      }
    },
    {
      "sample_id": 8,
      "question": "How might Evan and Sam's experiences with health and lifestyle changes influence their approach to stress and challenges?",
      "prediction": "Evan's experience as he dealt with a health scare and improved through making positive lifestyle changes can motivate him to support Sam on her journey, providing encouragement and practical advice. Similarly, Sam's personal growth from overcoming health challenges can inspire Evan to continue his own progress and remind him that small steps matter.",
      "reference": "Their experiences likely lead them to view challenges as opportunities for growth and change. They both have embraced healthier lifestyles, indicating a proactive approach to managing stress and challenges.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1643835616438356,
        "rouge1_f": 0.26190476190476186,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.18644067796610173,
        "bleu2": 0.01792901126022127,
        "bleu3": 0.008667792019043009,
        "bleu4": 0.005633294412316096,
        "bert_precision": 0.8669561743736267,
        "bert_recall": 0.8927651047706604,
        "bert_f1": 0.8796713948249817,
        "meteor": 0.15923566878980894,
        "sbert_similarity": 0.5704571008682251
      }
    },
    {
      "sample_id": 8,
      "question": "What recurring frustration does Evan experience?",
      "prediction": "Work stress",
      "reference": "Evan consistently misplaces his keys every week.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8844325542449951,
        "bert_recall": 0.8692691922187805,
        "bert_f1": 0.8767853379249573,
        "meteor": 0.0,
        "sbert_similarity": 0.07336048036813736
      }
    },
    {
      "sample_id": 8,
      "question": "What is the recurring dream that Sam keeps having?",
      "prediction": "Sam has been having a recurring dream where they are flying over skyscrapers.",
      "reference": "he's flying over a cityscape.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.11764705882352941,
        "rougeL_f": 0.21052631578947367,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.14824986333222023,
        "bleu3": 0.05844218274062023,
        "bleu4": 0.03592143420342892,
        "bert_precision": 0.8906100988388062,
        "bert_recall": 0.9162889122962952,
        "bert_f1": 0.903266966342926,
        "meteor": 0.4406130268199234,
        "sbert_similarity": 0.49172329902648926
      }
    },
    {
      "sample_id": 8,
      "question": "What accidents has Evan's son faced lately?",
      "prediction": "soccer accident and soccer",
      "reference": "injured at a soccer game, fell off his bike",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.055782540037107455,
        "bleu2": 0.020368903662172547,
        "bleu3": 0.017103640807945183,
        "bleu4": 0.017926911178691992,
        "bert_precision": 0.8735213875770569,
        "bert_recall": 0.8494718074798584,
        "bert_f1": 0.8613287806510925,
        "meteor": 0.058823529411764705,
        "sbert_similarity": 0.6177678108215332
      }
    },
    {
      "sample_id": 8,
      "question": "When was Evan's son injured at soccer?",
      "prediction": "Last Saturday",
      "reference": "Saturday before August 15, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.0410424993119494,
        "bleu2": 0.018354763685601443,
        "bleu3": 0.014286435890917338,
        "bleu4": 0.012274570264879268,
        "bert_precision": 0.8906459808349609,
        "bert_recall": 0.8631699085235596,
        "bert_f1": 0.8766927123069763,
        "meteor": 0.10638297872340426,
        "sbert_similarity": 0.5789365768432617
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of foods or recipes has Sam recommended to Evan?",
      "prediction": "Sam has recommended healthy and flavorful dishes such as grilled chicken and veggie stir-fry, a salad with chicken, avocado, tomatoes, corn, and cheese, and various healthy snacks like flavored seltzer water.",
      "reference": "grilled vegetables, grilled chicken and veggie stir-fry, poutine",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30303030303030304,
        "rouge1_f": 0.29268292682926833,
        "rouge2_f": 0.2564102564102564,
        "rougeL_f": 0.29268292682926833,
        "bleu1": 0.1842105263157895,
        "bleu2": 0.1577761527196001,
        "bleu3": 0.14315524701981472,
        "bleu4": 0.12408616318856693,
        "bert_precision": 0.8584371209144592,
        "bert_recall": 0.9178043603897095,
        "bert_f1": 0.8871285915374756,
        "meteor": 0.4330097087378641,
        "sbert_similarity": 0.5433225631713867
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of healthy meals did Sam start eating after getting a health scare?",
      "prediction": "A salad idea",
      "reference": "salad, grilled salmon and vegetables, grilled chicken and veggie stir-fry, Beef Merlot, fruit bowl, smoothie bowl",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.0008262507255554528,
        "bleu2": 0.0003200055299868064,
        "bleu3": 0.0003002224715746007,
        "bleu4": 0.00028164062278017855,
        "bert_precision": 0.8708139061927795,
        "bert_recall": 0.8077674508094788,
        "bert_f1": 0.8381066918373108,
        "meteor": 0.0,
        "sbert_similarity": 0.5264828205108643
      }
    },
    {
      "sample_id": 8,
      "question": "What role does nature and the outdoors play in Evan and Sam's mental well-being?",
      "prediction": "Nature and outdoor activities are rejuvenating to both Evan and Sam.",
      "reference": "Nature and outdoor activities seem to be significant stress relievers and sources of joy for both Evan and Sam. These activities likely contribute positively to their mental well-being.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.47058823529411764,
        "rouge1_f": 0.45000000000000007,
        "rouge2_f": 0.3157894736842105,
        "rougeL_f": 0.45000000000000007,
        "bleu1": 0.18594180012369152,
        "bleu2": 0.16248764022344098,
        "bleu3": 0.14398303855416053,
        "bleu4": 0.12166103078676026,
        "bert_precision": 0.9591292142868042,
        "bert_recall": 0.9159519672393799,
        "bert_f1": 0.9370434880256653,
        "meteor": 0.31501901140684413,
        "sbert_similarity": 0.7882907390594482
      }
    },
    {
      "sample_id": 8,
      "question": "How many months lapsed between Sam's first and second doctor's appointment?",
      "prediction": "bytes",
      "reference": "three months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8697858452796936,
        "bert_recall": 0.863288164138794,
        "bert_f1": 0.866524875164032,
        "meteor": 0.0,
        "sbert_similarity": 0.09704671800136566
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan start taking painting classes?",
      "prediction": "few days ago",
      "reference": "Few days before 19 August, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.12591706855837456,
        "bleu2": 0.10904738014161917,
        "bleu3": 0.06147886617763995,
        "bleu4": 0.04538324543496762,
        "bert_precision": 0.8554314374923706,
        "bert_recall": 0.8352404236793518,
        "bert_f1": 0.8452153205871582,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.3887724280357361
      }
    },
    {
      "sample_id": 8,
      "question": "Which classes did Evan join in mid-August 2023?",
      "prediction": "painting",
      "reference": "painting classes",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9423125982284546,
        "bert_recall": 0.9316391944885254,
        "bert_f1": 0.9369454979896545,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7466496229171753
      }
    },
    {
      "sample_id": 8,
      "question": "How did Evan get into painting?",
      "prediction": "Evan enjoys painting landscapes and nature scenes.",
      "reference": "His friend got him into it by gifting him a painting and giving him some advice. The painting inspired Evan.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14814814814814817,
        "bleu1": 0.08688697172522257,
        "bleu2": 0.014686578766809639,
        "bleu3": 0.008809930701162177,
        "bleu4": 0.006826095093209212,
        "bert_precision": 0.8908791542053223,
        "bert_recall": 0.8621447086334229,
        "bert_f1": 0.876276433467865,
        "meteor": 0.053475935828877004,
        "sbert_similarity": 0.5478959083557129
      }
    },
    {
      "sample_id": 8,
      "question": "How often does Sam get health checkups?",
      "prediction": "Sam had to go to the hospital last week due to a health scare and was later found to have been experiencing a misunderstanding, which made him realize how important it is to keep an eye on his health.",
      "reference": "every three months",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8020142316818237,
        "bert_recall": 0.8356373906135559,
        "bert_f1": 0.8184806108474731,
        "meteor": 0.0,
        "sbert_similarity": 0.10815839469432831
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of subjects does Evan enjoy painting?",
      "prediction": "Nature",
      "reference": "nature landscapes, portraits, abstract minimalism",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.0,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.8969185948371887,
        "bert_recall": 0.8444709181785583,
        "bert_f1": 0.8699049353599548,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5352623462677002
      }
    },
    {
      "sample_id": 8,
      "question": "Which places in Canada was Evan visiting in July 2023?",
      "prediction": "mountains and valleys",
      "reference": "Banff, Rocky Mountains",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.8754605054855347,
        "bert_recall": 0.8635473251342773,
        "bert_f1": 0.8694631457328796,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.5223459005355835
      }
    },
    {
      "sample_id": 8,
      "question": "How do Evan and Sam use creative outlets to cope with life's challenges?",
      "prediction": "Evan uses art through painting to express emotions, and Sam supports him by encouraging new hobbies such as cooking and photography.",
      "reference": "Evan and Sam use creative activities, like painting and writing, as therapeutic tools to express themselves and cope with stress.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.36842105263157887,
        "rouge1_f": 0.4390243902439024,
        "rouge2_f": 0.10256410256410256,
        "rougeL_f": 0.2926829268292683,
        "bleu1": 0.43478260869565216,
        "bleu2": 0.19881069312188604,
        "bleu3": 0.058970914233203414,
        "bleu4": 0.03114637779265809,
        "bert_precision": 0.9128391742706299,
        "bert_recall": 0.9153908491134644,
        "bert_f1": 0.9141132235527039,
        "meteor": 0.29052269516614465,
        "sbert_similarity": 0.7986295819282532
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan go skiing in Banff?",
      "prediction": "2023",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9425233602523804,
        "bert_recall": 0.9230665564537048,
        "bert_f1": 0.9326935410499573,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8580030202865601
      }
    },
    {
      "sample_id": 8,
      "question": "What new diet and lifestyle change did Sam adopt over time?",
      "prediction": "Sam adopted a healthier diet by cutting down on sugary snacks, eating more vegetables and fruits, and focusing on balanced nutrition.",
      "reference": "Healthy eating, exercise routine, running, hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08,
        "rouge1_f": 0.07407407407407407,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407407,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.02331262020600785,
        "bleu3": 0.014112717030906382,
        "bleu4": 0.01041441909198652,
        "bert_precision": 0.8602637052536011,
        "bert_recall": 0.8828957080841064,
        "bert_f1": 0.8714327812194824,
        "meteor": 0.0,
        "sbert_similarity": 0.33275264501571655
      }
    },
    {
      "sample_id": 8,
      "question": "Who was injured in Evan's family?",
      "prediction": "Son",
      "reference": "Evan's son and Evan himself",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.2857142857142857,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2857142857142857,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.002130725927060655,
        "bleu3": 0.0014741021868805238,
        "bleu4": 0.0011981952414407235,
        "bert_precision": 0.8665679693222046,
        "bert_recall": 0.841913104057312,
        "bert_f1": 0.8540626168251038,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.34741413593292236
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of hobbies does Evan pursue?",
      "prediction": "Evan pursues painting and hiking as his hobbies.",
      "reference": "painting, hiking, reading books, biking, skiing, snowboarding, ice skating, swimming, camping, kayaking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.058577141803494825,
        "bleu2": 0.013892789019128023,
        "bleu3": 0.009299905003168392,
        "bleu4": 0.00751714180595745,
        "bert_precision": 0.8633720278739929,
        "bert_recall": 0.8602444529533386,
        "bert_f1": 0.8618054389953613,
        "meteor": 0.0,
        "sbert_similarity": 0.3352704346179962
      }
    },
    {
      "sample_id": 8,
      "question": "What challenges does Sam face in his quest for a healthier lifestyle, and how does he address them?",
      "prediction": "Sam faces health challenges such as dealing with heart palpitations and taking on new exercise routines.",
      "reference": "Sam faces challenges like maintaining motivation and making dietary changes. He addresses them by enrolling in cooking classes and seeking support from friends like Evan.",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.20512820512820512,
        "rouge1_f": 0.19512195121951217,
        "rouge2_f": 0.05128205128205127,
        "rougeL_f": 0.19512195121951217,
        "bleu1": 0.16332540382410313,
        "bleu2": 0.07528929107808163,
        "bleu3": 0.028423326595652637,
        "bleu4": 0.01698549113873794,
        "bert_precision": 0.8988224864006042,
        "bert_recall": 0.8962567448616028,
        "bert_f1": 0.8975377678871155,
        "meteor": 0.17692485016136464,
        "sbert_similarity": 0.634779691696167
      }
    },
    {
      "sample_id": 8,
      "question": "Which activity do Evan and Sam plan on doing together during September 2023?",
      "prediction": "Kayaking trip",
      "reference": "painting",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8449456691741943,
        "bert_recall": 0.8809976577758789,
        "bert_f1": 0.8625950813293457,
        "meteor": 0.0,
        "sbert_similarity": 0.17369124293327332
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan and Sam decide to paint together?",
      "prediction": "9:28 am on 11 September, 2023",
      "reference": "Saturday after 11 September, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5,
        "bleu1": 0.5714285714285714,
        "bleu2": 0.5345224838248488,
        "bleu3": 0.4888069165703076,
        "bleu4": 0.41113361690051975,
        "bert_precision": 0.883411169052124,
        "bert_recall": 0.9232504367828369,
        "bert_f1": 0.9028915762901306,
        "meteor": 0.36764705882352944,
        "sbert_similarity": 0.7652654051780701
      }
    },
    {
      "sample_id": 8,
      "question": "What personal health incidents does Evan face in 2023?",
      "prediction": "In 2023, Evan faces a health scare that requires him to go to the hospital due to gastritis and then a misunderstanding about his weight, which is threatening to make it difficult for him to maintain a healthy lifestyle.",
      "reference": "heart palpitations, twisted ankle, twisted ankle",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.010777013435226059,
        "bleu3": 0.006962563401049795,
        "bleu4": 0.005223577740745087,
        "bert_precision": 0.8211983442306519,
        "bert_recall": 0.8370668888092041,
        "bert_f1": 0.8290566802024841,
        "meteor": 0.0,
        "sbert_similarity": 0.1876191347837448
      }
    },
    {
      "sample_id": 8,
      "question": "What recurring adventure does Evan have with strangers?",
      "prediction": "Helped a lost tourist find their way and took an unexpected tour around the city.",
      "reference": "Helping lost tourists and experiencing unexpected adventures in the city.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.56,
        "rouge2_f": 0.17391304347826086,
        "rougeL_f": 0.56,
        "bleu1": 0.375,
        "bleu2": 0.223606797749979,
        "bleu3": 0.15575351255546135,
        "bleu4": 0.07239768078888709,
        "bert_precision": 0.9197855591773987,
        "bert_recall": 0.9454699754714966,
        "bert_f1": 0.9324508905410767,
        "meteor": 0.5451895043731778,
        "sbert_similarity": 0.7854326963424683
      }
    },
    {
      "sample_id": 8,
      "question": "What is Sam's persistent problem with his phone?",
      "prediction": "frustrating issue",
      "reference": "His new phone malfunctioning, particularly with the navigation app.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8494800329208374,
        "bert_recall": 0.8670558929443359,
        "bert_f1": 0.8581780195236206,
        "meteor": 0.0,
        "sbert_similarity": 0.33868712186813354
      }
    },
    {
      "sample_id": 8,
      "question": "Which US state was Sam travelling in during October 2023?",
      "prediction": "New York",
      "reference": "California",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9032689332962036,
        "bert_recall": 0.9336265921592712,
        "bert_f1": 0.9181969165802002,
        "meteor": 0.0,
        "sbert_similarity": 0.5260796546936035
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan start lifting weights?",
      "prediction": "one year ago",
      "reference": "October 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8085016012191772,
        "bert_recall": 0.8338226079940796,
        "bert_f1": 0.820966899394989,
        "meteor": 0.0,
        "sbert_similarity": 0.31805872917175293
      }
    },
    {
      "sample_id": 8,
      "question": "When did Sam and his friend decide to try kayaking?",
      "prediction": "2023",
      "reference": "October 14, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9323793649673462,
        "bert_recall": 0.8985769152641296,
        "bert_f1": 0.9151661396026611,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.742584764957428
      }
    },
    {
      "sample_id": 8,
      "question": "Which new activity does Sam take up in October 2023?",
      "prediction": "hiking",
      "reference": "kayaking",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8781994581222534,
        "bert_recall": 0.8483098745346069,
        "bert_f1": 0.8629959225654602,
        "meteor": 0.0,
        "sbert_similarity": 0.423862099647522
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of stress was Sam dealing with in October 2023?",
      "prediction": "Sam was stressed about their weight concern.",
      "reference": "work-related stress",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8784564733505249,
        "bert_recall": 0.894183874130249,
        "bert_f1": 0.886250376701355,
        "meteor": 0.2,
        "sbert_similarity": 0.32814157009124756
      }
    },
    {
      "sample_id": 8,
      "question": "What health scares did Sam and Evan experience?",
      "prediction": "Sam experienced pressure to keep up with a new health routine pushed by their family, while Evan had challenges keeping healthy.",
      "reference": "Sam faced a health scare with stomach pains that turned out to be gastritis, prompting him to rethink his health habits. Evan, on the other hand, experienced two separate incidents: a sudden heart palpitation incident and a different event involving a misunderstanding during a medical check-up. These experiences have significantly influenced their perspectives on health and well-being.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2318840579710145,
        "rouge1_f": 0.22500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.07312991086277484,
        "bleu2": 0.007477348486180845,
        "bleu3": 0.0036908028072459562,
        "bleu4": 0.0024772676324714073,
        "bert_precision": 0.891139566898346,
        "bert_recall": 0.8621808290481567,
        "bert_f1": 0.8764209747314453,
        "meteor": 0.0749063670411985,
        "sbert_similarity": 0.6605721712112427
      }
    },
    {
      "sample_id": 8,
      "question": "When was Sam in the ER?",
      "prediction": "Last week",
      "reference": "weekend before 17 October, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8700404167175293,
        "bert_recall": 0.8200072050094604,
        "bert_f1": 0.8442832231521606,
        "meteor": 0.0,
        "sbert_similarity": 0.432847797870636
      }
    },
    {
      "sample_id": 8,
      "question": "Which ailment does Sam have to face due to his weight?",
      "prediction": "gastritis",
      "reference": "gastritis",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "Does Evan live close to a beach or mountains?",
      "prediction": "No",
      "reference": "beach",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8422957062721252,
        "bert_recall": 0.8096333742141724,
        "bert_f1": 0.8256415724754333,
        "meteor": 0.0,
        "sbert_similarity": 0.1658223569393158
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan lose his job?",
      "prediction": "9 November, 2023",
      "reference": "end of October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9215839505195618,
        "bert_recall": 0.9215839505195618,
        "bert_f1": 0.9215839505195618,
        "meteor": 0.12820512820512822,
        "sbert_similarity": 0.7798018455505371
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan and Sam planned a trip to the beach together?",
      "prediction": "10:52 am on 27 July, 2023",
      "reference": "December, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.2857142857142857,
        "bleu2": 0.2182178902359924,
        "bleu3": 0.1006949130683619,
        "bleu4": 0.06985342056580097,
        "bert_precision": 0.8317242860794067,
        "bert_recall": 0.8945399522781372,
        "bert_f1": 0.8619892597198486,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.6484804749488831
      }
    },
    {
      "sample_id": 8,
      "question": "What was Sam doing on December 4, 2023?",
      "prediction": "Saying it's not a good time as they were fine.",
      "reference": "Attending a Weight Watchers meeting",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.12500000000000003,
        "rouge2_f": 0.0,
        "rougeL_f": 0.12500000000000003,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.8534805774688721,
        "bert_recall": 0.8623760938644409,
        "bert_f1": 0.8579052686691284,
        "meteor": 0.09090909090909091,
        "sbert_similarity": 0.11361002177000046
      }
    },
    {
      "sample_id": 8,
      "question": "Which two significant life events occur in Evan's life in December 2023 with his partner?",
      "prediction": "Finding someone who understands their feelings and having supportive family members significantly enhances one's emotional well-being, especially during challenging times.",
      "reference": "his partner gets pregnant and they get married",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07142857142857144,
        "rouge1_f": 0.06666666666666667,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06666666666666667,
        "bleu1": 0.043478260869565216,
        "bleu2": 0.014058038927888336,
        "bleu3": 0.010263558503321437,
        "bleu4": 0.008282282660969604,
        "bert_precision": 0.8420587778091431,
        "bert_recall": 0.8517857789993286,
        "bert_f1": 0.8468943238258362,
        "meteor": 0.16304347826086957,
        "sbert_similarity": 0.09231918305158615
      }
    },
    {
      "sample_id": 8,
      "question": "How long did Evan and his partner date before getting married?",
      "prediction": "at first sight",
      "reference": "four months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8987976312637329,
        "bert_recall": 0.9196738004684448,
        "bert_f1": 0.9091158509254456,
        "meteor": 0.0,
        "sbert_similarity": 0.19957560300827026
      }
    },
    {
      "sample_id": 8,
      "question": "Which major holiday season conincides with Evan's wedding?",
      "prediction": "December",
      "reference": "Christmas",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9219793081283569,
        "bert_recall": 0.9219793081283569,
        "bert_f1": 0.9219793081283569,
        "meteor": 0.0,
        "sbert_similarity": 0.7330178022384644
      }
    },
    {
      "sample_id": 8,
      "question": "Which activity did Sam resume in December 2023 after a long time?",
      "prediction": "painting",
      "reference": "hiking",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.909110426902771,
        "bert_recall": 0.8978094458580017,
        "bert_f1": 0.9034246206283569,
        "meteor": 0.0,
        "sbert_similarity": 0.19972947239875793
      }
    },
    {
      "sample_id": 8,
      "question": "When is Evan planning a big family reunion?",
      "prediction": "2023-10-09",
      "reference": "Summer 2024",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8294141888618469,
        "bert_recall": 0.8855259418487549,
        "bert_f1": 0.8565521240234375,
        "meteor": 0.0,
        "sbert_similarity": 0.5971108675003052
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan's son fall off his bike?",
      "prediction": "last Tuesday",
      "reference": "Thursday before December 17, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8655998706817627,
        "bert_recall": 0.846581220626831,
        "bert_f1": 0.8559849262237549,
        "meteor": 0.0,
        "sbert_similarity": 0.5847231149673462
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan announce his marriage to his extended family?",
      "prediction": "9 December, 2023",
      "reference": "January 5, 2024",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.883442223072052,
        "bert_recall": 0.9077010154724121,
        "bert_f1": 0.8954073786735535,
        "meteor": 0.0,
        "sbert_similarity": 0.6883493661880493
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan finish the painting that's hanging in the exhibit?",
      "prediction": "a few days ago",
      "reference": "few days before 17 December, 2023.",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.1501861529550426,
        "bleu3": 0.0757836828666637,
        "bleu4": 0.06250434473199146,
        "bert_precision": 0.8382309675216675,
        "bert_recall": 0.8322659134864807,
        "bert_f1": 0.8352377414703369,
        "meteor": 0.32327586206896547,
        "sbert_similarity": 0.3565448224544525
      }
    },
    {
      "sample_id": 8,
      "question": "How does Evan spend his time with his bride after the wedding?",
      "prediction": "They went to Rockies",
      "reference": "family get-together, honeymoon in Canada to see snowy landscapes, ski, taste local cuisine and do some snowshoeing",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.0909090909090909,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0909090909090909,
        "bleu1": 0.004578909722183545,
        "bleu2": 0.0016719814290797606,
        "bleu3": 0.0014039523321838397,
        "bleu4": 0.001471530479433688,
        "bert_precision": 0.8482773303985596,
        "bert_recall": 0.7974687218666077,
        "bert_f1": 0.8220887184143066,
        "meteor": 0.03184713375796178,
        "sbert_similarity": 0.26371365785598755
      }
    },
    {
      "sample_id": 8,
      "question": "Who did Evan tell about his marriage?",
      "prediction": "Samsays",
      "reference": "To Sam, to his friends from work, and to his and his wife's families.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8384667634963989,
        "bert_recall": 0.8295340538024902,
        "bert_f1": 0.8339765071868896,
        "meteor": 0.0,
        "sbert_similarity": 0.3898354172706604
      }
    },
    {
      "sample_id": 8,
      "question": "When will Evan and his partner have their honeymoon in Canada?",
      "prediction": "2024",
      "reference": "February 2024",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.8973242044448853,
        "bert_recall": 0.8973242044448853,
        "bert_f1": 0.8973242044448853,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.8075103759765625
      }
    },
    {
      "sample_id": 8,
      "question": "When did Evan have a drunken night with his friends?",
      "prediction": "10 January 2024",
      "reference": "January 9, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.91961270570755,
        "bert_recall": 0.9192774295806885,
        "bert_f1": 0.9194450974464417,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.8751344680786133
      }
    },
    {
      "sample_id": 8,
      "question": "What is a stress reliever for Evan?",
      "prediction": "a trip to the mountains",
      "reference": "Drawing, traveling, places with a beautiful view, yoga, sunsets or something comfortable for Evan",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473682,
        "rouge1_f": 0.10526315789473682,
        "rouge2_f": 0.0,
        "rougeL_f": 0.10526315789473682,
        "bleu1": 0.014854715642866776,
        "bleu2": 0.0052519350818344916,
        "bleu3": 0.00420769676829611,
        "bleu4": 0.003990607693903362,
        "bert_precision": 0.8927549719810486,
        "bert_recall": 0.8396754264831543,
        "bert_f1": 0.8654020428657532,
        "meteor": 0.038167938931297704,
        "sbert_similarity": 0.3333278298377991
      }
    },
    {
      "sample_id": 8,
      "question": "What is a stress reliever for Sam?",
      "prediction": "hiking",
      "reference": "Unhealthy snacks, sweets, yoga, places with beautiful views",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8654667139053345,
        "bert_recall": 0.8454318642616272,
        "bert_f1": 0.8553319573402405,
        "meteor": 0.0,
        "sbert_similarity": 0.3420400321483612
      }
    },
    {
      "sample_id": 8,
      "question": "What type of car did Evan get after his old Prius broke down?",
      "prediction": "Evan got a new Prius.",
      "reference": "new Prius",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9233887195587158,
        "bert_recall": 0.9475177526473999,
        "bert_f1": 0.9352976679801941,
        "meteor": 0.21739130434782608,
        "sbert_similarity": 0.7149540781974792
      }
    },
    {
      "sample_id": 8,
      "question": "How did Evan get into watercolor painting?",
      "prediction": "By observing nature and painting what they see.",
      "reference": "friend's advice",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8444955348968506,
        "bert_recall": 0.8387815952301025,
        "bert_f1": 0.8416288495063782,
        "meteor": 0.0,
        "sbert_similarity": 0.10255938768386841
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan start doing a few years back as a stress-buster?",
      "prediction": "Evan struggled with his health a few years ago, but stuck with it.",
      "reference": "watercolor painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8420026898384094,
        "bert_recall": 0.8471112251281738,
        "bert_f1": 0.844549298286438,
        "meteor": 0.0,
        "sbert_similarity": -0.012068265117704868
      }
    },
    {
      "sample_id": 8,
      "question": "What advice did Evan give Sam about finding a passion?",
      "prediction": "Focus on trying new things and keep going until something sparks excitement.",
      "reference": "keep trying new things until something sparks excitement",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.5555555555555556,
        "rougeL_f": 0.7000000000000001,
        "bleu1": 0.6153846153846154,
        "bleu2": 0.5063696835418333,
        "bleu3": 0.41566083684640853,
        "bleu4": 0.2891784933232572,
        "bert_precision": 0.9549934267997742,
        "bert_recall": 0.9763337969779968,
        "bert_f1": 0.9655457139015198,
        "meteor": 0.8005344995140914,
        "sbert_similarity": 0.8852683305740356
      }
    },
    {
      "sample_id": 8,
      "question": "Where did Evan take his family for a road trip on 24 May, 2023?",
      "prediction": "Jasper",
      "reference": "Jasper",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan find relaxing about his road trip to Jasper?",
      "prediction": "Relaxed and enjoyed breathtaking views.",
      "reference": "fresh air, peacefulness, cozy cabin surrounded by mountains and forests",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.0613132401952404,
        "bleu2": 0.021239529438966134,
        "bleu3": 0.016579663309725004,
        "bleu4": 0.01501861529550426,
        "bert_precision": 0.8951092958450317,
        "bert_recall": 0.8694862723350525,
        "bert_f1": 0.8821117877960205,
        "meteor": 0.052631578947368425,
        "sbert_similarity": 0.5105069875717163
      }
    },
    {
      "sample_id": 8,
      "question": "What habit is Sam trying to change in terms of diet?",
      "prediction": "Sam is trying to make healthier choices",
      "reference": "consuming soda and candy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8428646326065063,
        "bert_recall": 0.8283361792564392,
        "bert_f1": 0.8355372548103333,
        "meteor": 0.0,
        "sbert_similarity": 0.30596211552619934
      }
    },
    {
      "sample_id": 8,
      "question": "What new suggestion did Evan give to Sam regarding his soda and candy consumption?",
      "prediction": "Consider swapping soda for flavored seltzer water.",
      "reference": "try flavored seltzer water and dark chocolate with high cocoa content",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333326,
        "rouge1_f": 0.33333333333333326,
        "rouge2_f": 0.25,
        "rougeL_f": 0.33333333333333326,
        "bleu1": 0.2577334795466146,
        "bleu2": 0.2249682245993741,
        "bleu3": 0.1820696344991504,
        "bleu4": 0.09448221398071252,
        "bert_precision": 0.9251478910446167,
        "bert_recall": 0.8885005712509155,
        "bert_f1": 0.9064539670944214,
        "meteor": 0.1768867924528302,
        "sbert_similarity": 0.5704774856567383
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam agree to try instead of soda and candy?",
      "prediction": "flavored seltzer water",
      "reference": "flavored seltzer water and dark chocolate with high cocoa content",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.3636363636363636,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.09697196786440505,
        "bleu2": 0.09697196786440505,
        "bleu3": 0.09697196786440505,
        "bleu4": 0.054531344915185485,
        "bert_precision": 0.9762287735939026,
        "bert_recall": 0.8883081674575806,
        "bert_f1": 0.9301955103874207,
        "meteor": 0.31660692951015534,
        "sbert_similarity": 0.6588467359542847
      }
    },
    {
      "sample_id": 8,
      "question": "What frustrating issue did Sam face at the supermarket?",
      "prediction": "The self-checkout machines were all broken",
      "reference": "broken self-checkout machines",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.4444444444444444,
        "rougeL_f": 0.5454545454545454,
        "bleu1": 0.5,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1384586850774637,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.9265007972717285,
        "bert_recall": 0.9503294229507446,
        "bert_f1": 0.9382638335227966,
        "meteor": 0.7744107744107744,
        "sbert_similarity": 0.9207964539527893
      }
    },
    {
      "sample_id": 8,
      "question": "What novel is Evan reading that he finds gripping?",
      "prediction": "Evan",
      "reference": "The Great Gatsby",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.894935131072998,
        "bert_recall": 0.8192286491394043,
        "bert_f1": 0.855410099029541,
        "meteor": 0.0,
        "sbert_similarity": 0.19678524136543274
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of water does Evan suggest Sam try as an alternative to soda?",
      "prediction": "flavored seltzer water",
      "reference": "Flavored seltzer water",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9904090166091919,
        "bert_recall": 0.9904090166091919,
        "bert_f1": 0.9904090166091919,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "What does the smartwatch help Evan with?",
      "prediction": "progress tracking",
      "reference": "tracks progress and serves as a constant reminder to keep going",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.3076923076923077,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 0.005554498269121153,
        "bleu2": 0.0024840471421319643,
        "bleu3": 0.001933458847739007,
        "bleu4": 0.0016611824434051396,
        "bert_precision": 0.8835128545761108,
        "bert_recall": 0.8512882590293884,
        "bert_f1": 0.8671013116836548,
        "meteor": 0.09900990099009901,
        "sbert_similarity": 0.6794304847717285
      }
    },
    {
      "sample_id": 8,
      "question": "What does the bonsai tree symbolize for Evan?",
      "prediction": "Motivates and makes Evan happy.",
      "reference": "strength and resilience",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.8622655868530273,
        "bert_recall": 0.8710002899169922,
        "bert_f1": 0.8666109442710876,
        "meteor": 0.15625,
        "sbert_similarity": 0.270885169506073
      }
    },
    {
      "sample_id": 8,
      "question": "Why did Evan decide to get the bonsai tree?",
      "prediction": "find something that motivates you and makes you happy",
      "reference": "motivates him to keep going through tough times",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.11764705882352941,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11764705882352941,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8766918182373047,
        "bert_recall": 0.8764325380325317,
        "bert_f1": 0.8765621185302734,
        "meteor": 0.0617283950617284,
        "sbert_similarity": 0.39997613430023193
      }
    },
    {
      "sample_id": 8,
      "question": "According to Sam, what is more important than perfection?",
      "prediction": "progress",
      "reference": "progress",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan suggest Sam to check out for insights into his dream?",
      "prediction": "Sam suggested Evansays to check out painting classes.",
      "reference": "dream interpretation book",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8417336344718933,
        "bert_recall": 0.8607598543167114,
        "bert_f1": 0.8511404395103455,
        "meteor": 0.0,
        "sbert_similarity": 0.12578850984573364
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan mention he had been searching for fruitlessly for half an hour?",
      "prediction": "new hobbies",
      "reference": "his keys",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9008388519287109,
        "bert_recall": 0.9008388519287109,
        "bert_f1": 0.9008388519287109,
        "meteor": 0.0,
        "sbert_similarity": 0.16961491107940674
      }
    },
    {
      "sample_id": 8,
      "question": "What class is Sam taking to learn how to make healthier meals?",
      "prediction": "Sam's class",
      "reference": "cooking class",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.8948842287063599,
        "bert_recall": 0.886642336845398,
        "bert_f1": 0.8907442092895508,
        "meteor": 0.25,
        "sbert_similarity": 0.4232325851917267
      }
    },
    {
      "sample_id": 8,
      "question": "What dish did Sam make on 18 August, 2023 that turned out flavorful?",
      "prediction": "A grilled dish",
      "reference": "grilled dish with salmon and vegetables",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.28571428571428575,
        "rougeL_f": 0.4444444444444444,
        "bleu1": 0.24525296078096154,
        "bleu2": 0.21239529438966132,
        "bleu3": 0.11974448045963429,
        "bleu4": 0.08839449202722564,
        "bert_precision": 0.9270079135894775,
        "bert_recall": 0.8926774859428406,
        "bert_f1": 0.909518837928772,
        "meteor": 0.3289473684210526,
        "sbert_similarity": 0.7247238159179688
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of recipe did Evan request from Sam on 19 August, 2023?",
      "prediction": "A cousin's wedding image with sentimental value",
      "reference": "recipes with more vegetables",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8529090881347656,
        "bert_recall": 0.8611125946044922,
        "bert_f1": 0.8569912314414978,
        "meteor": 0.11627906976744187,
        "sbert_similarity": 0.10382816195487976
      }
    },
    {
      "sample_id": 8,
      "question": "What food did Sam share a photo of on 19 August, 2023?",
      "prediction": "a salad with chicken, avocado, tomatoes, corn, and cheese",
      "reference": "bowl of spinach, avocado, and strawberries",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.26666666666666666,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.26666666666666666,
        "bleu1": 0.30769230769230765,
        "bleu2": 0.2773500981126146,
        "bleu3": 0.1944191802499099,
        "bleu4": 0.09144626892372869,
        "bert_precision": 0.902592658996582,
        "bert_recall": 0.928459882736206,
        "bert_f1": 0.9153435826301575,
        "meteor": 0.15873015873015875,
        "sbert_similarity": 0.6758102178573608
      }
    },
    {
      "sample_id": 8,
      "question": "What type of painting classes did Evan start taking in 2023?",
      "prediction": "painting classes",
      "reference": "watercolor painting classes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.9533590078353882,
        "bert_recall": 0.9297395944595337,
        "bert_f1": 0.9414011836051941,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.8567944169044495
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan start painting years ago due to being inspired by a friend's gift?",
      "prediction": "years ago, Evan started painting after being inspired by a friend's gift.",
      "reference": "forest scene",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8252607583999634,
        "bert_recall": 0.8340952396392822,
        "bert_f1": 0.8296545147895813,
        "meteor": 0.0,
        "sbert_similarity": 0.08356520533561707
      }
    },
    {
      "sample_id": 8,
      "question": "What nature concept do watercolor painting classes emphasize according to Evan?",
      "prediction": "observing nature and painting what we see",
      "reference": "observing nature and painting what is seen",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7142857142857143,
        "rouge1_f": 0.7142857142857143,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.7142857142857143,
        "bleu1": 0.7142857142857143,
        "bleu2": 0.6900655593423543,
        "bleu3": 0.6613898798842307,
        "bleu4": 0.6147881529512643,
        "bert_precision": 0.9762028455734253,
        "bert_recall": 0.9762028455734253,
        "bert_f1": 0.9762028455734253,
        "meteor": 0.7114285714285714,
        "sbert_similarity": 0.9685412645339966
      }
    },
    {
      "sample_id": 8,
      "question": "What type of landscapes does Evan love painting the most?",
      "prediction": "Landscape",
      "reference": "sunsets over the ocean",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8522652983665466,
        "bert_recall": 0.8383519053459167,
        "bert_f1": 0.8452513217926025,
        "meteor": 0.0,
        "sbert_similarity": 0.26389703154563904
      }
    },
    {
      "sample_id": 8,
      "question": "What fun activity did Evan mention doing in July 2023?",
      "prediction": "Evan mentioned kayaking.",
      "reference": "skiing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9002830982208252,
        "bert_recall": 0.8832936882972717,
        "bert_f1": 0.8917074799537659,
        "meteor": 0.0,
        "sbert_similarity": 0.37451452016830444
      }
    },
    {
      "sample_id": 8,
      "question": "What injury did Evan suffer from in August 2023?",
      "prediction": "Evan suffered from an ankle injury last Saturday when his son fell off his bike.",
      "reference": "Twisted knee",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8526209592819214,
        "bert_recall": 0.8624579906463623,
        "bert_f1": 0.8575112819671631,
        "meteor": 0.0,
        "sbert_similarity": 0.34769830107688904
      }
    },
    {
      "sample_id": 8,
      "question": "What sports activity has Evan been doing to stay active while dealing with the knee injury?",
      "prediction": "Swimming",
      "reference": "Swimming",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What suggestion did Sam give to Evan to help with his knee issue?",
      "prediction": "Evan suggested that Sam consider getting PT (physical therapy) for his knee.",
      "reference": "Consider low-impact exercises or physical therapy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.3157894736842105,
        "rouge2_f": 0.11764705882352942,
        "rougeL_f": 0.3157894736842105,
        "bleu1": 0.2,
        "bleu2": 0.11952286093343935,
        "bleu3": 0.04937604958709526,
        "bleu4": 0.030934588294313718,
        "bert_precision": 0.8829782009124756,
        "bert_recall": 0.8989093899726868,
        "bert_f1": 0.890872597694397,
        "meteor": 0.07575757575757576,
        "sbert_similarity": 0.3516016900539398
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan suggest Sam try as a calming hobby?",
      "prediction": "hiking",
      "reference": "Painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.896251916885376,
        "bert_recall": 0.8936378955841064,
        "bert_f1": 0.8949429988861084,
        "meteor": 0.0,
        "sbert_similarity": 0.19972947239875793
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan recommend Sam acquire to get started with painting?",
      "prediction": "acrylic paints, brushes, canvas/paper, palette",
      "reference": "Acrylic paints, brushes, canvas/paper, palette",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9939742088317871,
        "bert_recall": 0.9939742088317871,
        "bert_f1": 0.9939742088317871,
        "meteor": 0.996,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What activity does Evan do to keep himself busy while healing his knee?",
      "prediction": "Evan stays active by going on hikes.",
      "reference": "Watercolor painting",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8624651432037354,
        "bert_recall": 0.8505070805549622,
        "bert_f1": 0.8564444184303284,
        "meteor": 0.0,
        "sbert_similarity": 0.008319243788719177
      }
    },
    {
      "sample_id": 8,
      "question": "What painting did Evan share with Sam in October?",
      "prediction": "An amazing nature painting",
      "reference": "a cactus in the desert",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8868908882141113,
        "bert_recall": 0.9007624387741089,
        "bert_f1": 0.8937728404998779,
        "meteor": 0.0,
        "sbert_similarity": 0.33727186918258667
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of writing does Sam enjoy as a form of expression?",
      "prediction": "Creative writing",
      "reference": "creative writing",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.971829891204834,
        "bert_recall": 0.971829891204834,
        "bert_f1": 0.971829891204834,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "What electronics issue has been frustrating Sam lately?",
      "prediction": "frustration",
      "reference": "malfunctioning navigation app on the new phone",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.898788571357727,
        "bert_recall": 0.8462923169136047,
        "bert_f1": 0.8717508316040039,
        "meteor": 0.0,
        "sbert_similarity": 0.11548042297363281
      }
    },
    {
      "sample_id": 8,
      "question": "What activity did Evan start one year ago?",
      "prediction": "hiking",
      "reference": "lifting weights",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8853205442428589,
        "bert_recall": 0.8762547969818115,
        "bert_f1": 0.880764365196228,
        "meteor": 0.0,
        "sbert_similarity": 0.18753542006015778
      }
    },
    {
      "sample_id": 8,
      "question": "What advice did Evan give to Sam to avoid injuries while starting weightlifting?",
      "prediction": "Find someone who can help prevent injuries as you progress through different intensities of workouts.",
      "reference": "Find a trainer",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8647526502609253,
        "bert_recall": 0.9201506972312927,
        "bert_f1": 0.8915920257568359,
        "meteor": 0.23809523809523808,
        "sbert_similarity": 0.5286648273468018
      }
    },
    {
      "sample_id": 8,
      "question": "Where did Sam and his mate plan to try kayaking?",
      "prediction": "on the lake",
      "reference": "Lake Tahoe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.12909944487358058,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.11362193664674995,
        "bert_precision": 0.9012715816497803,
        "bert_recall": 0.8277774453163147,
        "bert_f1": 0.862962543964386,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.5813841819763184
      }
    },
    {
      "sample_id": 8,
      "question": "What digestive issue did Sam experience lately?",
      "prediction": "gastritis",
      "reference": "Gastritis",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9811792373657227,
        "bert_recall": 0.9811792373657227,
        "bert_f1": 0.9811792373657227,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "What adventurous theme is emerging in Evan's life as mentioned by Sam?",
      "prediction": "Evan plans to visit nature and join a hike.",
      "reference": "helping lost tourists",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8798657655715942,
        "bert_recall": 0.8800544142723083,
        "bert_f1": 0.8799600601196289,
        "meteor": 0.0,
        "sbert_similarity": 0.23329852521419525
      }
    },
    {
      "sample_id": 8,
      "question": "What does Evan mention about his progress at the gym to Sam?",
      "prediction": "Sam mentions that Evan has been doing easy exercises and is building strength without intense workouts.",
      "reference": "gaining strength",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1111111111111111,
        "rouge1_f": 0.1111111111111111,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1111111111111111,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8706678152084351,
        "bert_recall": 0.9011350870132446,
        "bert_f1": 0.885639488697052,
        "meteor": 0.1470588235294118,
        "sbert_similarity": 0.4712063670158386
      }
    },
    {
      "sample_id": 8,
      "question": "How did Evan start his transformation journey two years ago?",
      "prediction": "By changing his mindset.",
      "reference": "Changed his diet and started walking regularly",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.36363636363636365,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.36363636363636365,
        "bleu1": 0.13406400920712788,
        "bleu2": 0.04739878501170794,
        "bleu3": 0.037974520135397725,
        "bleu4": 0.036015288308423515,
        "bert_precision": 0.9399101734161377,
        "bert_recall": 0.9015116691589355,
        "bert_f1": 0.9203105568885803,
        "meteor": 0.2798507462686567,
        "sbert_similarity": 0.3537501096725464
      }
    },
    {
      "sample_id": 8,
      "question": "What gift did Evan receive from a close friend?",
      "prediction": "A painting",
      "reference": "1968 Kustom K-200A vintage guitar",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8825964331626892,
        "bert_recall": 0.8094996809959412,
        "bert_f1": 0.8444691896438599,
        "meteor": 0.0,
        "sbert_similarity": 0.07441194355487823
      }
    },
    {
      "sample_id": 8,
      "question": "Why had Evan been going through a tough time lately?",
      "prediction": "Evan has been dealing with work-related stress.",
      "reference": "Lost their job due to downsizing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8937505483627319,
        "bert_recall": 0.8861337900161743,
        "bert_f1": 0.8899258375167847,
        "meteor": 0.0,
        "sbert_similarity": 0.20526254177093506
      }
    },
    {
      "sample_id": 8,
      "question": "How does Evan describe the island he grew up on?",
      "prediction": "Evan describes the island as serene and beautiful.",
      "reference": "A happy place",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.886681079864502,
        "bert_recall": 0.9053677320480347,
        "bert_f1": 0.8959270119667053,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.2994080185890198
      }
    },
    {
      "sample_id": 8,
      "question": "What was the main reason for Evan's frustration with his new Prius breaking down?",
      "prediction": "It is frustrating when new things go awry so soon.",
      "reference": "He relied on it for his active lifestyle and road trips",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809525,
        "rouge1_f": 0.09523809523809525,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809525,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8597172498703003,
        "bert_recall": 0.8697421550750732,
        "bert_f1": 0.8647006750106812,
        "meteor": 0.045871559633027525,
        "sbert_similarity": 0.002301238477230072
      }
    },
    {
      "sample_id": 8,
      "question": "How did Sam suggest Evan view the setback with his broken Prius?",
      "prediction": "Keep moving forward slowly and taking small steps.",
      "reference": "As a chance to explore other ways of staying active and traveling",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.07961459006375435,
        "bleu2": 0.026703545307500107,
        "bleu3": 0.02011097358938166,
        "bleu4": 0.017182633466700372,
        "bert_precision": 0.8790450096130371,
        "bert_recall": 0.8709911108016968,
        "bert_f1": 0.8749995231628418,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.3068956434726715
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam suggest Evan try for stress relief and flexibility?",
      "prediction": "yoga",
      "reference": "Yoga",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9783459901809692,
        "bert_recall": 0.9783459901809692,
        "bert_f1": 0.9783459901809692,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam offer Evan regarding yoga?",
      "prediction": "Sam offered to provide yoga tips and support.",
      "reference": "Support and tips",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5454545454545454,
        "rouge1_f": 0.5454545454545454,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.06454972243679029,
        "bleu3": 0.04033183084278514,
        "bleu4": 0.03155984539112946,
        "bert_precision": 0.9037001132965088,
        "bert_recall": 0.9327520132064819,
        "bert_f1": 0.9179962873458862,
        "meteor": 0.28571428571428575,
        "sbert_similarity": 0.409309446811676
      }
    },
    {
      "sample_id": 8,
      "question": "What news did Evan share with Sam on 9th December 2023?",
      "prediction": "Evan shared that he is moving to Canada.",
      "reference": "partner is pregnant",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8847562074661255,
        "bert_recall": 0.8715724945068359,
        "bert_f1": 0.8781148791313171,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.07077273726463318
      }
    },
    {
      "sample_id": 8,
      "question": "What family event is Evan planning for next summer?",
      "prediction": "Family reunion",
      "reference": "big family reunion",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.28369570381570614,
        "bleu4": 0.19180183554164504,
        "bert_precision": 0.964436411857605,
        "bert_recall": 0.943527102470398,
        "bert_f1": 0.9538671970367432,
        "meteor": 0.6465517241379309,
        "sbert_similarity": 0.8687882423400879
      }
    },
    {
      "sample_id": 8,
      "question": "What is the motto of Evan's family?",
      "prediction": "The motto of Evan's family is 'family means everything - they bring so much love and happiness.'",
      "reference": "'Bring it on Home'",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.09523809523809523,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09523809523809523,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.845317006111145,
        "bert_recall": 0.8609834909439087,
        "bert_f1": 0.8530783653259277,
        "meteor": 0.0,
        "sbert_similarity": 0.2704755961894989
      }
    },
    {
      "sample_id": 8,
      "question": "According to Evan, what is important for Sam to believe in concerning his weight?",
      "prediction": "Believe in yourself and take it one day at a time.",
      "reference": "Your worth is not defined by your weight",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8710710406303406,
        "bert_recall": 0.867646336555481,
        "bert_f1": 0.869355320930481,
        "meteor": 0.0,
        "sbert_similarity": 0.17343147099018097
      }
    },
    {
      "sample_id": 8,
      "question": "Who helped Evan get the painting published in the exhibition?",
      "prediction": "A close friend",
      "reference": "a close friend",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9796299338340759,
        "bert_recall": 0.9796299338340759,
        "bert_f1": 0.9796299338340759,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam recently start enjoying to clear his head?",
      "prediction": "hiking",
      "reference": "running in the mornings",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9017581343650818,
        "bert_recall": 0.9078662395477295,
        "bert_f1": 0.9048018455505371,
        "meteor": 0.0,
        "sbert_similarity": 0.3598868250846863
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam suggest Evan should do with his keys?",
      "prediction": "Sam suggested Evan lock his keys.",
      "reference": "put a GPS sensor on them",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705376386642456,
        "bert_recall": 0.8821015357971191,
        "bert_f1": 0.8762814402580261,
        "meteor": 0.0,
        "sbert_similarity": 0.0689430758357048
      }
    },
    {
      "sample_id": 8,
      "question": "How did Evan feel when he painted the piece with the bird flying over it?",
      "prediction": "Evan felt happy and excited.",
      "reference": "a sense of joy and freedom",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.9120528697967529,
        "bert_recall": 0.9021264314651489,
        "bert_f1": 0.9070624709129333,
        "meteor": 0.0847457627118644,
        "sbert_similarity": 0.34360334277153015
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan suggest Sam should keep doing to find his own version of love?",
      "prediction": "keep going and take it one step at a time!",
      "reference": "Keep trying new things",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714288,
        "rouge1_f": 0.14285714285714288,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714288,
        "bleu1": 0.0909090909090909,
        "bleu2": 0.03015113445777637,
        "bleu3": 0.022461584318318094,
        "bleu4": 0.018850319022747353,
        "bert_precision": 0.8768695592880249,
        "bert_recall": 0.9032790660858154,
        "bert_f1": 0.8898784518241882,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.42080384492874146
      }
    },
    {
      "sample_id": 8,
      "question": "How did Evan describe the process of creating the painting with the bird flying over it?",
      "prediction": "Expressing emotions through art is truly fascinating.",
      "reference": "embracing the creative process without restraint",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8881880044937134,
        "bert_recall": 0.8742746114730835,
        "bert_f1": 0.8811764121055603,
        "meteor": 0.0,
        "sbert_similarity": 0.4715779721736908
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan want to share with his work friends?",
      "prediction": "Details about the event",
      "reference": "getting married",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.884547233581543,
        "bert_recall": 0.8638778328895569,
        "bert_f1": 0.8740903735160828,
        "meteor": 0.0,
        "sbert_similarity": 0.22100061178207397
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan share with Sam after their hiking trip?",
      "prediction": "Evan shared photos and stories about their hike.",
      "reference": "a photo of a man standing on a rock looking out over a valley",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.09090909090909091,
        "rouge2_f": 0.0,
        "rougeL_f": 0.09090909090909091,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8946743607521057,
        "bert_recall": 0.8765256404876709,
        "bert_f1": 0.8855070471763611,
        "meteor": 0.03731343283582089,
        "sbert_similarity": 0.2442503720521927
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan offer to share with Sam after talking about healthy snacks?",
      "prediction": "Some new healthy snacks",
      "reference": "the recipes for cookies",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8938877582550049,
        "bert_recall": 0.8883612155914307,
        "bert_f1": 0.8911159038543701,
        "meteor": 0.0,
        "sbert_similarity": 0.4258502125740051
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan and his partner share with their extended family on January 5, 2024?",
      "prediction": "Evan shared news that he and his partner were having a joyful time together in photos.",
      "reference": "their marriage",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8365763425827026,
        "bert_recall": 0.8520830869674683,
        "bert_f1": 0.8442585468292236,
        "meteor": 0.0,
        "sbert_similarity": 0.3603494167327881
      }
    },
    {
      "sample_id": 8,
      "question": "What was Evan limiting himself to on his new diet?",
      "prediction": "sugary drinks and snacks",
      "reference": "just two ginger snaps a day",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8568407297134399,
        "bert_recall": 0.8590638041496277,
        "bert_f1": 0.8579508662223816,
        "meteor": 0.0,
        "sbert_similarity": 0.1742229163646698
      }
    },
    {
      "sample_id": 8,
      "question": "What sports activity did Evan and his partner try in a recent weekend?",
      "prediction": "Kayaking",
      "reference": "Snowshoeing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8695887327194214,
        "bert_recall": 0.8475954532623291,
        "bert_f1": 0.858451247215271,
        "meteor": 0.0,
        "sbert_similarity": 0.2824550271034241
      }
    },
    {
      "sample_id": 8,
      "question": "What advice did Evan suggest Sam seek from a doctor?",
      "prediction": "Ask a doctor about diet and exercise tips.",
      "reference": "diet plan and low-impact exercises",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3076923076923077,
        "rouge1_f": 0.42857142857142855,
        "rouge2_f": 0.0,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.9165952205657959,
        "bert_recall": 0.9038320779800415,
        "bert_f1": 0.9101689457893372,
        "meteor": 0.28301886792452824,
        "sbert_similarity": 0.47297415137290955
      }
    },
    {
      "sample_id": 8,
      "question": "What suggestions did Evan give for low-impact exercises?",
      "prediction": "Swimming, yoga, and walking are good options.",
      "reference": "swimming, yoga, walking",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6,
        "rouge1_f": 0.6,
        "rouge2_f": 0.25,
        "rougeL_f": 0.6,
        "bleu1": 0.5,
        "bleu2": 0.408248290463863,
        "bleu3": 0.350372724490772,
        "bleu4": 0.2777619034011791,
        "bert_precision": 0.9267909526824951,
        "bert_recall": 0.9493142366409302,
        "bert_f1": 0.9379173517227173,
        "meteor": 0.7516339869281047,
        "sbert_similarity": 0.9097570180892944
      }
    },
    {
      "sample_id": 8,
      "question": "What movie did Sam watch that motivated him to keep up with his routine?",
      "prediction": "Hiking",
      "reference": "The Godfather",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9105342030525208,
        "bert_recall": 0.8643200397491455,
        "bert_f1": 0.8868255019187927,
        "meteor": 0.0,
        "sbert_similarity": 0.18769431114196777
      }
    },
    {
      "sample_id": 8,
      "question": "What activity helped Evan with stress and flexibility?",
      "prediction": "Yoga",
      "reference": "Yoga",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan share a photo of that was taken on a camping trip?",
      "prediction": "A photo of a stream running through a snowy forest filled with snow",
      "reference": "a kayak",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615385,
        "rouge1_f": 0.13333333333333336,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333336,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8483400344848633,
        "bert_recall": 0.8942586779594421,
        "bert_f1": 0.8706943392753601,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.2270040661096573
      }
    },
    {
      "sample_id": 8,
      "question": "Why did Evan apologize to his partner?",
      "prediction": "Because he had to apologize to his partner for that drunken night.",
      "reference": "for a drunken night",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.39999999999999997,
        "rouge1_f": 0.375,
        "rouge2_f": 0.14285714285714288,
        "rougeL_f": 0.375,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.1386750490563073,
        "bleu3": 0.057551852907951895,
        "bleu4": 0.036362270465000714,
        "bert_precision": 0.8714531064033508,
        "bert_recall": 0.9305934906005859,
        "bert_f1": 0.9000528454780579,
        "meteor": 0.20833333333333334,
        "sbert_similarity": 0.5301796197891235
      }
    },
    {
      "sample_id": 8,
      "question": "How does Evan describe being out on the water while kayaking and watching the sunset?",
      "prediction": "Evan describes being out on the water while kayaking as peaceful.",
      "reference": "peaceful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666669,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.08333333333333333,
        "bleu2": 0.027524094128159027,
        "bleu3": 0.020427282860665447,
        "bleu4": 0.017033186037639283,
        "bert_precision": 0.83836829662323,
        "bert_recall": 0.883651077747345,
        "bert_f1": 0.8604142665863037,
        "meteor": 0.0,
        "sbert_similarity": 0.43211647868156433
      }
    },
    {
      "sample_id": 8,
      "question": "What type of car did Sam get after his old Prius broke down?",
      "prediction": "new Prius",
      "reference": "new Prius",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "How did Sam get into watercolor painting?",
      "prediction": "Not mentioned in the conversation",
      "reference": "friend's advice",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8825992941856384,
        "bert_recall": 0.85787034034729,
        "bert_f1": 0.8700591325759888,
        "meteor": 0.0,
        "sbert_similarity": 0.12193245440721512
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam start doing a few years back as a stress-buster?",
      "prediction": "sticking with it",
      "reference": "watercolor painting",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.862892746925354,
        "bert_recall": 0.8622822761535645,
        "bert_f1": 0.8625873923301697,
        "meteor": 0.0,
        "sbert_similarity": 0.07748149335384369
      }
    },
    {
      "sample_id": 8,
      "question": "Where did Sam take his family for a road trip on 24 May, 2023?",
      "prediction": "Jasper",
      "reference": "Jasper",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam find relaxing about his road trip to Jasper?",
      "prediction": "peacefulness",
      "reference": "fresh air, peacefulness, cozy cabin surrounded by mountains and forests",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 1.670170079024566e-05,
        "bleu2": 5.281541529581043e-06,
        "bleu3": 3.6539340043587375e-06,
        "bleu4": 2.9700290627924664e-06,
        "bert_precision": 0.88093101978302,
        "bert_recall": 0.8122708797454834,
        "bert_f1": 0.8452088236808777,
        "meteor": 0.0,
        "sbert_similarity": 0.3783338665962219
      }
    },
    {
      "sample_id": 8,
      "question": "What habit is Evan trying to change in terms of diet?",
      "prediction": "soda and candy",
      "reference": "consuming soda and candy",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.8571428571428571,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8571428571428571,
        "bleu1": 0.7165313105737893,
        "bleu2": 0.7165313105737893,
        "bleu3": 0.7165313105737893,
        "bleu4": 0.40293516672844226,
        "bert_precision": 0.9308584332466125,
        "bert_recall": 0.9308584332466125,
        "bert_f1": 0.9308584332466125,
        "meteor": 0.754985754985755,
        "sbert_similarity": 0.8857855200767517
      }
    },
    {
      "sample_id": 8,
      "question": "What frustrating issue did Evan face at the supermarket?",
      "prediction": "broken self-checkout machines",
      "reference": "broken self-checkout machines",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "What novel is Sam reading that he finds gripping?",
      "prediction": "The Great Gatsby",
      "reference": "The Great Gatsby",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "What does the smartwatch help Sam with?",
      "prediction": "tracks progress and serves as a constant reminder to keep going",
      "reference": "tracks progress and serves as a constant reminder to keep going",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9996243425995492,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "Why did Sam decide to get the bonsai tree?",
      "prediction": "motivates him to keep going through tough times",
      "reference": "motivates him to keep going through tough times",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam mention he had been searching for fruitlessly for half an hour?",
      "prediction": "Not mentioned in the conversation",
      "reference": "his keys",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8705978393554688,
        "bert_recall": 0.8557699918746948,
        "bert_f1": 0.8631202578544617,
        "meteor": 0.0,
        "sbert_similarity": 0.16109803318977356
      }
    },
    {
      "sample_id": 8,
      "question": "What class is Evan taking to learn how to make healthier meals?",
      "prediction": "painting classes",
      "reference": "cooking class",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.933561384677887,
        "bert_recall": 0.933561384677887,
        "bert_f1": 0.933561384677887,
        "meteor": 0.25,
        "sbert_similarity": 0.4371415376663208
      }
    },
    {
      "sample_id": 8,
      "question": "What dish did Sam make on 18 August, 2023 that turned out bland?",
      "prediction": "Not mentioned in the conversation",
      "reference": "grilled dish with salmon and vegetables",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8882460594177246,
        "bert_recall": 0.8305126428604126,
        "bert_f1": 0.8584097027778625,
        "meteor": 0.0,
        "sbert_similarity": 0.029989253729581833
      }
    },
    {
      "sample_id": 8,
      "question": "What food did Evan share a photo of on 19 August, 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "bowl of spinach, avocado, and strawberries",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8839023113250732,
        "bert_recall": 0.8369781374931335,
        "bert_f1": 0.859800398349762,
        "meteor": 0.0,
        "sbert_similarity": 0.09258361160755157
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan start sculpting years ago due to being inspired by a friend's gift?",
      "prediction": "forest scene",
      "reference": "forest scene",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What nature concept do watercolor painting classes emphasize according to Sam?",
      "prediction": "observing nature and painting what is seen",
      "reference": "observing nature and painting what is seen",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What type of landscapes does Sam love painting the most?",
      "prediction": "nature",
      "reference": "sunsets over the ocean",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8977022767066956,
        "bert_recall": 0.831329345703125,
        "bert_f1": 0.8632419109344482,
        "meteor": 0.0,
        "sbert_similarity": 0.23300762474536896
      }
    },
    {
      "sample_id": 8,
      "question": "What sports activity has Sam been doing to stay active while dealing with the knee injury?",
      "prediction": "Not mentioned in the conversation or Swimming",
      "reference": "Swimming",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8689801692962646,
        "bert_recall": 0.9366071224212646,
        "bert_f1": 0.9015271663665771,
        "meteor": 0.31250000000000006,
        "sbert_similarity": 0.6654490232467651
      }
    },
    {
      "sample_id": 8,
      "question": "What activity does Sam do to keep himself busy while healing his knee?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Watercolor painting",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8706415891647339,
        "bert_recall": 0.8414559960365295,
        "bert_f1": 0.8558000326156616,
        "meteor": 0.0,
        "sbert_similarity": 0.0003467886708676815
      }
    },
    {
      "sample_id": 8,
      "question": "What kind of writing does Evan enjoy as a form of expression?",
      "prediction": "art",
      "reference": "creative writing",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.855078935623169,
        "bert_recall": 0.8514349460601807,
        "bert_f1": 0.8532530665397644,
        "meteor": 0.0,
        "sbert_similarity": 0.5127240419387817
      }
    },
    {
      "sample_id": 8,
      "question": "What electronics issue has been frustrating Evan lately?",
      "prediction": "frustrating issues with self-checkout machines",
      "reference": "malfunctioning navigation app on the new phone",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8893411159515381,
        "bert_recall": 0.8801993727684021,
        "bert_f1": 0.8847466111183167,
        "meteor": 0.0,
        "sbert_similarity": 0.037105247378349304
      }
    },
    {
      "sample_id": 8,
      "question": "What activity did Evan quit one year ago?",
      "prediction": "Not mentioned in the conversation",
      "reference": "lifting weights",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8614088296890259,
        "bert_recall": 0.8380167484283447,
        "bert_f1": 0.849551796913147,
        "meteor": 0.0,
        "sbert_similarity": 0.0013808812946081161
      }
    },
    {
      "sample_id": 8,
      "question": "Where did Sam and his mate plan to try skydiving?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Lake Tahoe",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694390058517456,
        "bert_recall": 0.8107545375823975,
        "bert_f1": 0.8390719294548035,
        "meteor": 0.0,
        "sbert_similarity": 0.10798929631710052
      }
    },
    {
      "sample_id": 8,
      "question": "What digestive issue did Evan experience lately?",
      "prediction": "Gastritis",
      "reference": "Gastritis",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 8,
      "question": "How did Sam start his transformation journey two years ago?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Changed his diet and started walking regularly",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8907637596130371,
        "bert_recall": 0.8651552796363831,
        "bert_f1": 0.8777728080749512,
        "meteor": 0.0,
        "sbert_similarity": 0.07096841186285019
      }
    },
    {
      "sample_id": 8,
      "question": "What gift did Sam receive from a close friend?",
      "prediction": "Not mentioned in the conversation",
      "reference": "1968 Kustom K-200A vintage guitar",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8753578662872314,
        "bert_recall": 0.8001434803009033,
        "bert_f1": 0.836062490940094,
        "meteor": 0.0,
        "sbert_similarity": -0.06546469032764435
      }
    },
    {
      "sample_id": 8,
      "question": "How does Sam describe the island he grew up on?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A happy place",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8928142189979553,
        "bert_recall": 0.8989651203155518,
        "bert_f1": 0.8958791494369507,
        "meteor": 0.0,
        "sbert_similarity": 0.06581200659275055
      }
    },
    {
      "sample_id": 8,
      "question": "What was the main reason for Evan's frustration with his new Prius getting stolen?",
      "prediction": "Not mentioned in the conversation or He relied on it for his active lifestyle and road trips",
      "reference": "He relied on it for his active lifestyle and road trips",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7857142857142858,
        "rouge1_f": 0.7857142857142858,
        "rouge2_f": 0.7692307692307693,
        "rougeL_f": 0.7857142857142858,
        "bleu1": 0.6470588235294118,
        "bleu2": 0.6359337738364604,
        "bleu3": 0.626674120796503,
        "bleu4": 0.6102169202557912,
        "bert_precision": 0.9233535528182983,
        "bert_recall": 0.9827169179916382,
        "bert_f1": 0.9521108269691467,
        "meteor": 0.9479196352237105,
        "sbert_similarity": 0.8536375761032104
      }
    },
    {
      "sample_id": 8,
      "question": "What family event is Sam planning for next summer?",
      "prediction": "big family reunion",
      "reference": "big family reunion",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What is the motto of Sam's family?",
      "prediction": "'Bring it on Home'",
      "reference": "'Bring it on Home'",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9921875,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "Who helped Sam get the painting published in the exhibition?",
      "prediction": "A close friend of mine who helped me get this painting published in the exhibition!",
      "reference": "a close friend",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.25,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.1875,
        "bleu2": 0.15811388300841897,
        "bleu3": 0.12390760171084929,
        "bleu4": 0.06087895024805787,
        "bert_precision": 0.8612369298934937,
        "bert_recall": 0.9217795133590698,
        "bert_f1": 0.8904803395271301,
        "meteor": 0.701058201058201,
        "sbert_similarity": 0.3160511553287506
      }
    },
    {
      "sample_id": 8,
      "question": "How did Sam feel when he painted the piece with the bird flying over it?",
      "prediction": "Not mentioned in the conversation or a sense of joy and freedom",
      "reference": "a sense of joy and freedom",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.4767312946227961,
        "bleu3": 0.45325271466910305,
        "bleu4": 0.4172261448611506,
        "bert_precision": 0.9034974575042725,
        "bert_recall": 0.956279456615448,
        "bert_f1": 0.9291393756866455,
        "meteor": 0.906986531986532,
        "sbert_similarity": 0.7421060800552368
      }
    },
    {
      "sample_id": 8,
      "question": "How did Sam describe the process of creating the painting with the bird flying over it?",
      "prediction": "embracing the creative process without restraint",
      "reference": "embracing the creative process without restraint",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9976851851851852,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Evan and his partner keep from their extended family on January 5, 2024?",
      "prediction": "Congratulations, Evan! Is that the woman from Canada?",
      "reference": "their marriage",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8182910084724426,
        "bert_recall": 0.8416522741317749,
        "bert_f1": 0.8298072218894958,
        "meteor": 0.0,
        "sbert_similarity": 0.24948331713676453
      }
    },
    {
      "sample_id": 8,
      "question": "What was Sam limiting himself to on his new diet?",
      "prediction": "Not mentioned in the conversation or just two ginger snaps a day",
      "reference": "just two ginger snaps a day",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.625,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.5,
        "bleu2": 0.4767312946227961,
        "bleu3": 0.45325271466910305,
        "bleu4": 0.4172261448611506,
        "bert_precision": 0.8955729007720947,
        "bert_recall": 0.9502121210098267,
        "bert_f1": 0.9220837950706482,
        "meteor": 0.906986531986532,
        "sbert_similarity": 0.8834742307662964
      }
    },
    {
      "sample_id": 8,
      "question": "What dance activity did Evan and his partner try in a recent weekend?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Snowshoeing",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8871304988861084,
        "bert_recall": 0.8223966360092163,
        "bert_f1": 0.853537917137146,
        "meteor": 0.0,
        "sbert_similarity": 0.16821372509002686
      }
    },
    {
      "sample_id": 8,
      "question": "What suggestions did Evan give for high-impact exercises?",
      "prediction": "Not mentioned in the conversation",
      "reference": "swimming, yoga, walking",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8821713924407959,
        "bert_recall": 0.8439853191375732,
        "bert_f1": 0.8626559376716614,
        "meteor": 0.0,
        "sbert_similarity": 0.08148659765720367
      }
    },
    {
      "sample_id": 8,
      "question": "What movie did Evan watch that motivated him to keep up with his routine?",
      "prediction": "The Godfather",
      "reference": "The Godfather",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.000000238418579
      }
    },
    {
      "sample_id": 8,
      "question": "What activity hindered Evan's stress and flexibility?",
      "prediction": "Yoga",
      "reference": "Yoga",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 8,
      "question": "What did Sam share a photo of that was taken on a camping trip?",
      "prediction": "a tent pitched up in a grassy field",
      "reference": "a kayak",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8718161582946777,
        "bert_recall": 0.9148337841033936,
        "bert_f1": 0.892807126045227,
        "meteor": 0.19230769230769235,
        "sbert_similarity": 0.1309557855129242
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin first travel to Tokyo?",
      "prediction": "10:11 am on 19 October, 2023",
      "reference": "between 26 March and 20 April 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8696480989456177,
        "bert_recall": 0.8776823878288269,
        "bert_f1": 0.8736468553543091,
        "meteor": 0.07246376811594203,
        "sbert_similarity": 0.7068084478378296
      }
    },
    {
      "sample_id": 9,
      "question": "What items did Calvin buy in March 2023?",
      "prediction": "A beautiful diamond pendant with gold around it",
      "reference": "mansion in Japan, luxury car Ferrari 488 GTB",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8402856588363647,
        "bert_recall": 0.8249276876449585,
        "bert_f1": 0.8325358629226685,
        "meteor": 0.0,
        "sbert_similarity": 0.18551011383533478
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave see Aerosmith perform live?",
      "prediction": "last weekend",
      "reference": "on the weekend before March 26, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.024893534183931972,
        "bleu2": 0.01113272692709733,
        "bleu3": 0.008665161385860336,
        "bleu4": 0.007444903200446295,
        "bert_precision": 0.9233944416046143,
        "bert_recall": 0.8537274599075317,
        "bert_f1": 0.8871954083442688,
        "meteor": 0.07692307692307691,
        "sbert_similarity": 0.5193874835968018
      }
    },
    {
      "sample_id": 9,
      "question": "Which bands has Dave enjoyed listening to?",
      "prediction": "Dave has enjoyed listening to Frank Ocean's music.",
      "reference": "Aerosmith, The Fireworks",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8665582537651062,
        "bert_recall": 0.8442815542221069,
        "bert_f1": 0.8552748560905457,
        "meteor": 0.0,
        "sbert_similarity": 0.13568788766860962
      }
    },
    {
      "sample_id": 9,
      "question": "Which country do Calvin and Dave want to meet in?",
      "prediction": "USA",
      "reference": "United States",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9341935515403748,
        "bert_recall": 0.9173891544342041,
        "bert_f1": 0.9257150292396545,
        "meteor": 0.0,
        "sbert_similarity": 0.893727719783783
      }
    },
    {
      "sample_id": 9,
      "question": "What are Dave's dreams?",
      "prediction": "Dave's dream is to make music that connects people and creates inspiration.",
      "reference": "open a car maintenance shop, work on classic cars, build a custom car from scratch",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8670260906219482,
        "bert_recall": 0.8559495806694031,
        "bert_f1": 0.8614522218704224,
        "meteor": 0.03401360544217687,
        "sbert_similarity": 0.06408234685659409
      }
    },
    {
      "sample_id": 9,
      "question": "Which types of cars does Dave like the most?",
      "prediction": "classic cars",
      "reference": "classic vintage cars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.0,
        "rougeL_f": 0.8,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.19180183554164504,
        "bleu3": 0.13269445010681044,
        "bleu4": 0.10785809837243004,
        "bert_precision": 0.9909715056419373,
        "bert_recall": 0.9564724564552307,
        "bert_f1": 0.9734163880348206,
        "meteor": 0.3448275862068965,
        "sbert_similarity": 0.8889626860618591
      }
    },
    {
      "sample_id": 9,
      "question": "Does Dave's shop employ a lot of people?",
      "prediction": "No",
      "reference": "Yes",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9496124982833862,
        "bert_recall": 0.9496124982833862,
        "bert_f1": 0.9496124982833862,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave start his car maintenance shop?",
      "prediction": "2023",
      "reference": "May 1, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9371988773345947,
        "bert_recall": 0.8885849714279175,
        "bert_f1": 0.9122447371482849,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7325514554977417
      }
    },
    {
      "sample_id": 9,
      "question": "When did a mishap occur with Calvin's musical gear and favorite mic?",
      "prediction": "1 May 2023",
      "reference": "On a week before 16 May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.12591706855837456,
        "bleu2": 0.034483809412173126,
        "bleu3": 0.028755826157775166,
        "bleu4": 0.025520874379338556,
        "bert_precision": 0.9415557980537415,
        "bert_recall": 0.884575366973877,
        "bert_f1": 0.9121766090393066,
        "meteor": 0.07575757575757576,
        "sbert_similarity": 0.7795214653015137
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin's place get flooded in Tokyo?",
      "prediction": "20 April, 2023",
      "reference": "On a week before 16 May, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.18393972058572117,
        "bleu2": 0.1501861529550426,
        "bleu3": 0.0757836828666637,
        "bleu4": 0.06250434473199146,
        "bert_precision": 0.9485023021697998,
        "bert_recall": 0.9040162563323975,
        "bert_f1": 0.9257251620292664,
        "meteor": 0.07575757575757576,
        "sbert_similarity": 0.7843563556671143
      }
    },
    {
      "sample_id": 9,
      "question": "What mishaps has Calvin run into?",
      "prediction": "Calvin ran into a car accident last Friday, which was upsetting but not serious.",
      "reference": "flooding of his mansion, car accident",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.1111111111111111,
        "rougeL_f": 0.2,
        "bleu1": 0.1875,
        "bleu2": 0.11180339887498951,
        "bleu3": 0.04610606439635425,
        "bleu4": 0.02878787818101127,
        "bert_precision": 0.8614488840103149,
        "bert_recall": 0.8612098097801208,
        "bert_f1": 0.8613293766975403,
        "meteor": 0.2757352941176471,
        "sbert_similarity": 0.4244762659072876
      }
    },
    {
      "sample_id": 9,
      "question": "When was Calvin's concert in Tokyo?",
      "prediction": "20 April 2023",
      "reference": "last week of May 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.17113903967753066,
        "bleu2": 0.06628186505570068,
        "bleu3": 0.0621842545921536,
        "bleu4": 0.05833544737207805,
        "bert_precision": 0.9358367323875427,
        "bert_recall": 0.9013558626174927,
        "bert_f1": 0.9182727932929993,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.7354710102081299
      }
    },
    {
      "sample_id": 9,
      "question": "Would Calvin enjoy performing at the Hollywood Bowl?",
      "prediction": "Yes",
      "reference": "Yes; because he enjoys the rush of performing onstage to large crowds",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.15384615384615385,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615385,
        "bleu1": 6.14421235332821e-06,
        "bleu2": 1.9429705464260385e-06,
        "bleu3": 1.3442071996008229e-06,
        "bleu4": 1.0926126318830351e-06,
        "bert_precision": 0.897238552570343,
        "bert_recall": 0.8143314719200134,
        "bert_f1": 0.8537770509719849,
        "meteor": 0.0,
        "sbert_similarity": 0.19612710177898407
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin meet with the creative team for his new album?",
      "prediction": "2023",
      "reference": "8 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.0,
        "rougeL_f": 0.5,
        "bleu1": 0.049787068367863944,
        "bleu2": 0.015744053406497194,
        "bleu3": 0.010892223754416544,
        "bleu4": 0.008853531856477262,
        "bert_precision": 0.9435789585113525,
        "bert_recall": 0.8973141312599182,
        "bert_f1": 0.9198651909828186,
        "meteor": 0.17857142857142855,
        "sbert_similarity": 0.7407665252685547
      }
    },
    {
      "sample_id": 9,
      "question": "Why does Dave regularly visit parks?",
      "prediction": "Dave visits parks regularly because he enjoys spending time outdoors and appreciates the natural beauty.",
      "reference": "because it relaxes and calms him",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.19047619047619044,
        "rouge1_f": 0.19047619047619044,
        "rouge2_f": 0.0,
        "rougeL_f": 0.19047619047619044,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.028867513459481287,
        "bleu3": 0.018864614597621485,
        "bleu4": 0.014628063653657535,
        "bert_precision": 0.865525484085083,
        "bert_recall": 0.8683327436447144,
        "bert_f1": 0.8669268488883972,
        "meteor": 0.14492753623188406,
        "sbert_similarity": 0.2529650330543518
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave take a trip to mountainous regions?",
      "prediction": "21 June, 2023",
      "reference": "July 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9502365589141846,
        "bert_recall": 0.9557973146438599,
        "bert_f1": 0.953008770942688,
        "meteor": 0.2380952380952381,
        "sbert_similarity": 0.8768180012702942
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin have a car incident?",
      "prediction": "2023",
      "reference": "on the Friday before 21 June, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.0009118819655545162,
        "bleu2": 0.0002883623968383479,
        "bleu3": 0.00019949803698118587,
        "bleu4": 0.00016215809237314185,
        "bert_precision": 0.9142402410507202,
        "bert_recall": 0.8341010808944702,
        "bert_f1": 0.8723340034484863,
        "meteor": 0.078125,
        "sbert_similarity": 0.6935858726501465
      }
    },
    {
      "sample_id": 9,
      "question": "How many times has Calvin had to deal with insurance paperwork?",
      "prediction": "2",
      "reference": "two times",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8937985301017761,
        "bert_recall": 0.8797558546066284,
        "bert_f1": 0.886721670627594,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.4988831877708435
      }
    },
    {
      "sample_id": 9,
      "question": "Which places or events has Calvin visited in Tokyo?",
      "prediction": "Tokyo",
      "reference": "music festival, car museum, Shibuya crossing, Shinjuku",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8744895458221436,
        "bert_recall": 0.8218656778335571,
        "bert_f1": 0.8473613262176514,
        "meteor": 0.0,
        "sbert_similarity": 0.4555788040161133
      }
    },
    {
      "sample_id": 9,
      "question": "Who inspired Dave's passion for car engineering?",
      "prediction": "Dave was inspired by his fascination with how machines work and saw an old car in a neighbor's garage when he was ten years old.",
      "reference": "His Dad",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.08,
        "rouge1_f": 0.07142857142857144,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07142857142857144,
        "bleu1": 0.037037037037037035,
        "bleu2": 0.01193524790065722,
        "bleu3": 0.008697382097724691,
        "bleu4": 0.006980361417366381,
        "bert_precision": 0.8269245028495789,
        "bert_recall": 0.8884752988815308,
        "bert_f1": 0.8565956354141235,
        "meteor": 0.11627906976744187,
        "sbert_similarity": 0.274332731962204
      }
    },
    {
      "sample_id": 9,
      "question": "Does Calvin wish to become more popular?",
      "prediction": "Calvin is impressed by the transformative power of music and finds it an effective means to convey deep emotions.",
      "reference": "Yes; he want's to grow his fanbase",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07692307692307693,
        "rouge1_f": 0.07407407407407407,
        "rouge2_f": 0.0,
        "rougeL_f": 0.07407407407407407,
        "bleu1": 0.05000000000000001,
        "bleu2": 0.01622214211307625,
        "bleu3": 0.01186947435439869,
        "bleu4": 0.009629943614188135,
        "bert_precision": 0.866462767124176,
        "bert_recall": 0.8641652464866638,
        "bert_f1": 0.8653125166893005,
        "meteor": 0.06097560975609756,
        "sbert_similarity": 0.15022359788417816
      }
    },
    {
      "sample_id": 9,
      "question": "Does Calvin want to expand his brand?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 9,
      "question": "What is Dave's main passion?",
      "prediction": "Fixing up things",
      "reference": "auto engineering",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.848471999168396,
        "bert_recall": 0.8361421823501587,
        "bert_f1": 0.8422619700431824,
        "meteor": 0.0,
        "sbert_similarity": 0.22594672441482544
      }
    },
    {
      "sample_id": 9,
      "question": "Can Dave work with engines?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave host a card-playing night with his friends?",
      "prediction": "Last Friday",
      "reference": "on the Friday before 22 August, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.22222222222222224,
        "rouge1_f": 0.22222222222222224,
        "rouge2_f": 0.0,
        "rougeL_f": 0.22222222222222224,
        "bleu1": 0.024893534183931972,
        "bleu2": 0.01113272692709733,
        "bleu3": 0.008665161385860336,
        "bleu4": 0.007444903200446295,
        "bert_precision": 0.8709855675697327,
        "bert_recall": 0.8506980538368225,
        "bert_f1": 0.8607223033905029,
        "meteor": 0.07692307692307691,
        "sbert_similarity": 0.5111087560653687
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin record a podcast with his friends?",
      "prediction": "29 October",
      "reference": "21 August, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9287139177322388,
        "bert_recall": 0.882594108581543,
        "bert_f1": 0.9050668478012085,
        "meteor": 0.0,
        "sbert_similarity": 0.6028081178665161
      }
    },
    {
      "sample_id": 9,
      "question": "Which city was Calvin visiting in August 2023?",
      "prediction": "Boston",
      "reference": "Miami",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9421009421348572,
        "bert_recall": 0.9421009421348572,
        "bert_f1": 0.9421009421348572,
        "meteor": 0.0,
        "sbert_similarity": 0.5421573519706726
      }
    },
    {
      "sample_id": 9,
      "question": "What does Calvin do to relax?",
      "prediction": "driving long drives",
      "reference": "take long drives in his car, embrace nature, fixing cars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.30769230769230765,
        "rouge1_f": 0.30769230769230765,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.30769230769230765,
        "bleu1": 0.03319137891190929,
        "bleu2": 0.02874457732434855,
        "bleu3": 0.016205653179025642,
        "bleu4": 0.011962893615061086,
        "bert_precision": 0.9267600774765015,
        "bert_recall": 0.8620187640190125,
        "bert_f1": 0.8932178616523743,
        "meteor": 0.31660692951015534,
        "sbert_similarity": 0.3333030939102173
      }
    },
    {
      "sample_id": 9,
      "question": "What are Dave's hobbies other than fixing cars?",
      "prediction": "hiking and nature",
      "reference": "take a walk, go hiking, listen to favorite albums, live concerts, photography",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.004374576245646989,
        "bleu2": 0.0016942660946105358,
        "bleu3": 0.0015895248886792451,
        "bleu4": 0.0014911434751178384,
        "bert_precision": 0.8986464738845825,
        "bert_recall": 0.8598654270172119,
        "bert_f1": 0.8788283467292786,
        "meteor": 0.0,
        "sbert_similarity": 0.5800562500953674
      }
    },
    {
      "sample_id": 9,
      "question": "What kind of music does Dave listen to?",
      "prediction": "classic rock",
      "reference": "classic rock, Japanese music",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.5,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.22313016014842982,
        "bleu2": 0.22313016014842982,
        "bleu3": 0.10436581698246099,
        "bleu4": 0.07055995207471726,
        "bert_precision": 0.9706515073776245,
        "bert_recall": 0.9158539772033691,
        "bert_f1": 0.942456841468811,
        "meteor": 0.13157894736842105,
        "sbert_similarity": 0.6025806665420532
      }
    },
    {
      "sample_id": 9,
      "question": "Where was Dave in the last two weeks of August 2023?",
      "prediction": "Dave was on a tour with Frank Ocean.",
      "reference": "San Francisco",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8659490346908569,
        "bert_recall": 0.8686057329177856,
        "bert_f1": 0.8672753572463989,
        "meteor": 0.0,
        "sbert_similarity": 0.15372541546821594
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Dave return from with new knowledge of different techniques of car restoration?",
      "prediction": "San Francisco",
      "reference": "San Francisco",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 9,
      "question": "What was Dave doing in San Francisco?",
      "prediction": "Dave was touring with Frank Ocean.",
      "reference": "attending a car modification workshop",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8670331239700317,
        "bert_recall": 0.8805484175682068,
        "bert_f1": 0.8737385272979736,
        "meteor": 0.0,
        "sbert_similarity": -0.0053523872047662735
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave return from San Francisco?",
      "prediction": "9 June",
      "reference": "September 1, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.870445191860199,
        "bert_recall": 0.8603373765945435,
        "bert_f1": 0.865361750125885,
        "meteor": 0.0,
        "sbert_similarity": 0.5741761922836304
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin book flight tickets to Boston?",
      "prediction": "last week",
      "reference": "last week of August 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5714285714285715,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.4,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.22313016014842982,
        "bleu2": 0.22313016014842982,
        "bleu3": 0.10436581698246099,
        "bleu4": 0.07055995207471726,
        "bert_precision": 0.9221218228340149,
        "bert_recall": 0.8456090688705444,
        "bert_f1": 0.8822095394134521,
        "meteor": 0.39893617021276595,
        "sbert_similarity": 0.6461917161941528
      }
    },
    {
      "sample_id": 9,
      "question": "When was Calvin's album released?",
      "prediction": "11th",
      "reference": "September 11, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8629804849624634,
        "bert_recall": 0.8591118454933167,
        "bert_f1": 0.8610418438911438,
        "meteor": 0.0,
        "sbert_similarity": 0.4404260516166687
      }
    },
    {
      "sample_id": 9,
      "question": "Does Calvin love music tours?",
      "prediction": "Yes",
      "reference": "yes",
      "category": 3,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9670930504798889,
        "bert_recall": 0.9670930504798889,
        "bert_f1": 0.9670929908752441,
        "meteor": 0.5,
        "sbert_similarity": 0.9999999403953552
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave have a great jam session with his band?",
      "prediction": "15 September 2023",
      "reference": "September 14, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.47768754038252614,
        "bleu2": 0.13082012065333476,
        "bleu3": 0.10909005447984235,
        "bleu4": 0.09681772177713914,
        "bert_precision": 0.9633575081825256,
        "bert_recall": 0.9619240164756775,
        "bert_f1": 0.9626402854919434,
        "meteor": 0.3333333333333333,
        "sbert_similarity": 0.9491608142852783
      }
    },
    {
      "sample_id": 9,
      "question": "Would Dave prefer working on a Dodge Charger or a Subaru Forester?",
      "prediction": "Dave's recent restoration and modification of his car focuses on classic cars, not specific models like a Dodge Charger or Subaru Forester.",
      "reference": "Dodge Charger",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666669,
        "rouge1_f": 0.16,
        "rouge2_f": 0.08695652173913045,
        "rougeL_f": 0.16,
        "bleu1": 0.07999999999999999,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.025303453550777042,
        "bleu4": 0.016020720994064933,
        "bert_precision": 0.8473892211914062,
        "bert_recall": 0.9016733169555664,
        "bert_f1": 0.8736888766288757,
        "meteor": 0.4687500000000001,
        "sbert_similarity": 0.5180256962776184
      }
    },
    {
      "sample_id": 9,
      "question": "What was the artists Calvin used to listen to when he was a kid?",
      "prediction": "The context does not provide specific details about what songs Calvin listened to as a child.",
      "reference": "Tupac and Dr. Dre",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8496799468994141,
        "bert_recall": 0.861263632774353,
        "bert_f1": 0.8554325699806213,
        "meteor": 0.0,
        "sbert_similarity": 0.13183936476707458
      }
    },
    {
      "sample_id": 9,
      "question": "Which of their family member do Calvin and Dave have nostalgic memories about?",
      "prediction": "Road trips",
      "reference": "Dad",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8750970363616943,
        "bert_recall": 0.8801243305206299,
        "bert_f1": 0.8776034712791443,
        "meteor": 0.0,
        "sbert_similarity": 0.21916261315345764
      }
    },
    {
      "sample_id": 9,
      "question": "Based on the conversation, did Calvin and Dave have a meeting in Boston between August and November 2023? Answer in yes or no.",
      "prediction": "yes",
      "reference": "No",
      "category": 3,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9297295212745667,
        "bert_recall": 0.9297295212745667,
        "bert_f1": 0.9297295212745667,
        "meteor": 0.0,
        "sbert_similarity": 0.7334951162338257
      }
    },
    {
      "sample_id": 9,
      "question": "Which city was Calvin at on October 3, 2023?",
      "prediction": "Boston",
      "reference": "Boston",
      "category": 2,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin met with local artists in Boston?",
      "prediction": "20 April",
      "reference": "October 3, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8775217533111572,
        "bert_recall": 0.881210446357727,
        "bert_f1": 0.8793622255325317,
        "meteor": 0.0,
        "sbert_similarity": 0.5098097324371338
      }
    },
    {
      "sample_id": 9,
      "question": "What shared activities do Dave and Calvin have?",
      "prediction": "Dave and Calvin share events like performing together, celebrating milestones, collaborating on music projects, and attending parties.",
      "reference": "Working on cars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.10526315789473684,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.047619047619047616,
        "bleu2": 0.01543033499620919,
        "bleu3": 0.011280777824436245,
        "bleu4": 0.009134423666564471,
        "bert_precision": 0.8505945205688477,
        "bert_recall": 0.8807761073112488,
        "bert_f1": 0.865422248840332,
        "meteor": 0.11363636363636363,
        "sbert_similarity": 0.06382867693901062
      }
    },
    {
      "sample_id": 9,
      "question": "What is Dave's favorite activity?",
      "prediction": "Dave's favorite activity is music.",
      "reference": "Restoring cars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8660768270492554,
        "bert_recall": 0.8736456632614136,
        "bert_f1": 0.8698447346687317,
        "meteor": 0.0,
        "sbert_similarity": -0.033974725753068924
      }
    },
    {
      "sample_id": 9,
      "question": "How many car shows has Dave attended?",
      "prediction": "1",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9115664958953857,
        "bert_recall": 0.9115664958953857,
        "bert_f1": 0.9115664958953857,
        "meteor": 0.0,
        "sbert_similarity": 0.6732218861579895
      }
    },
    {
      "sample_id": 9,
      "question": "What was Dave doing in the first weekend of October 2023?",
      "prediction": "Dave likely had a farewell visit to see Calvin off for an upcoming tour.",
      "reference": "attending a car show",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.11111111111111112,
        "rouge1_f": 0.11111111111111112,
        "rouge2_f": 0.0,
        "rougeL_f": 0.11111111111111112,
        "bleu1": 0.06666666666666667,
        "bleu2": 0.02182178902359924,
        "bleu3": 0.016071866876084712,
        "bleu4": 0.013217947626377298,
        "bert_precision": 0.8467516899108887,
        "bert_recall": 0.8560795187950134,
        "bert_f1": 0.85139000415802,
        "meteor": 0.2,
        "sbert_similarity": 0.206790030002594
      }
    },
    {
      "sample_id": 9,
      "question": "When Dave was a child, what did he and his father do in the garage?",
      "prediction": "Dave and his dad worked on cars together.",
      "reference": "tinkering with car engines, restoration and refurbishing cars",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.05270462766947299,
        "bleu3": 0.03528075103412339,
        "bleu4": 0.028517539529041507,
        "bert_precision": 0.8884040117263794,
        "bert_recall": 0.8823327422142029,
        "bert_f1": 0.8853580355644226,
        "meteor": 0.125,
        "sbert_similarity": 0.1835436075925827
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin buy his second Ferrari?",
      "prediction": "1 May, 2023",
      "reference": "first week of October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.19470019576785122,
        "bleu2": 0.07109445944848267,
        "bleu3": 0.05969757224096844,
        "bleu4": 0.06257106818159155,
        "bert_precision": 0.9365298748016357,
        "bert_recall": 0.9059438109397888,
        "bert_f1": 0.9209829568862915,
        "meteor": 0.10416666666666666,
        "sbert_similarity": 0.6945575475692749
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin and Frank Ocean start collaborating?",
      "prediction": "2023",
      "reference": "August 2022",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.910536527633667,
        "bert_recall": 0.9058054685592651,
        "bert_f1": 0.9081648588180542,
        "meteor": 0.0,
        "sbert_similarity": 0.7633687257766724
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin plan on travelling to Tokyo the second time?",
      "prediction": "2023",
      "reference": "November 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9486313462257385,
        "bert_recall": 0.930962085723877,
        "bert_f1": 0.9397136569023132,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.7661337852478027
      }
    },
    {
      "sample_id": 9,
      "question": "Who supports Calvin in tough times?",
      "prediction": "Dave",
      "reference": "friends and team",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8688262104988098,
        "bert_recall": 0.8674207925796509,
        "bert_f1": 0.8681228756904602,
        "meteor": 0.0,
        "sbert_similarity": 0.2629203498363495
      }
    },
    {
      "sample_id": 9,
      "question": "What does help Calvin stay connected to the creative process?",
      "prediction": "Dave's support through collaborations and sharing updates on his own music studio setup.",
      "reference": "Calvin stays connected to the creative process by always staying up-to-date on world events and watching documentaries about artists.",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.125,
        "rouge1_f": 0.11428571428571427,
        "rouge2_f": 0.0,
        "rougeL_f": 0.057142857142857134,
        "bleu1": 0.14330626211475786,
        "bleu2": 0.027082337919563323,
        "bleu3": 0.01654822865911292,
        "bleu4": 0.012464633492045013,
        "bert_precision": 0.8768079280853271,
        "bert_recall": 0.8710054159164429,
        "bert_f1": 0.8738970756530762,
        "meteor": 0.05434782608695652,
        "sbert_similarity": 0.2356574684381485
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin visit some of the sights in Boston with a former high school friend?",
      "prediction": "9 June 2023",
      "reference": "October 24, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9349602460861206,
        "bert_recall": 0.9390308856964111,
        "bert_f1": 0.9369911551475525,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.813954770565033
      }
    },
    {
      "sample_id": 9,
      "question": "Which cities did Dave travel to in 2023?",
      "prediction": "Boston",
      "reference": "San Francsico, Detroit",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8974703550338745,
        "bert_recall": 0.8152117729187012,
        "bert_f1": 0.8543657064437866,
        "meteor": 0.0,
        "sbert_similarity": 0.31189650297164917
      }
    },
    {
      "sample_id": 9,
      "question": "Which hobby did Dave pick up in October 2023?",
      "prediction": "Dave picked up engineering cars.",
      "reference": "photography",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8458929061889648,
        "bert_recall": 0.8347020149230957,
        "bert_f1": 0.8402602076530457,
        "meteor": 0.0,
        "sbert_similarity": 0.11506868898868561
      }
    },
    {
      "sample_id": 9,
      "question": "Which events in Dave's life inspired him to take up auto engineering?",
      "prediction": "Dave started working on cars when he was ten, fixing an old car in his neighbor's garage.",
      "reference": "attending a car show with Dad, working on an old car in a neighbor's garage when he was young, spent a summer restoring an old car with Dad",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5945945945945946,
        "rouge1_f": 0.5531914893617021,
        "rouge2_f": 0.35555555555555557,
        "rougeL_f": 0.3829787234042554,
        "bleu1": 0.37501737674731633,
        "bleu2": 0.30183034928407726,
        "bleu3": 0.22901837061545138,
        "bleu4": 0.1411026310485698,
        "bert_precision": 0.9195349812507629,
        "bert_recall": 0.8994597792625427,
        "bert_f1": 0.9093865752220154,
        "meteor": 0.37574119020553626,
        "sbert_similarity": 0.6095417737960815
      }
    },
    {
      "sample_id": 9,
      "question": "How many Ferraris does Calvin own?",
      "prediction": "One",
      "reference": "two",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9345704317092896,
        "bert_recall": 0.9345704317092896,
        "bert_f1": 0.9345704317092896,
        "meteor": 0.0,
        "sbert_similarity": 0.7806955575942993
      }
    },
    {
      "sample_id": 9,
      "question": "What gifts has Calvin received from his artist friends?",
      "prediction": "Calvin has received a new vintage camera and a mansion as gifts from his artist friends.",
      "reference": "gold chain, custom-made guitar with an octopus on it",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8664025068283081,
        "bert_recall": 0.8480641841888428,
        "bert_f1": 0.8571352958679199,
        "meteor": 0.0,
        "sbert_similarity": 0.07589365541934967
      }
    },
    {
      "sample_id": 9,
      "question": "How long did Dave's work on the Ford Mustang take?",
      "prediction": "Dave worked on the Ford Mustang for several weeks.",
      "reference": "nearly two months",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8737753629684448,
        "bert_recall": 0.8610405325889587,
        "bert_f1": 0.8673612475395203,
        "meteor": 0.0,
        "sbert_similarity": 0.1682470738887787
      }
    },
    {
      "sample_id": 9,
      "question": "How long was the car modification workshop in San Francisco?",
      "prediction": "an hour or two",
      "reference": "two weeks",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.9144852161407471,
        "bert_recall": 0.9466915130615234,
        "bert_f1": 0.9303097128868103,
        "meteor": 0.22727272727272727,
        "sbert_similarity": 0.6187509298324585
      }
    },
    {
      "sample_id": 9,
      "question": "What style of guitars does Calvin own?",
      "prediction": "shiny finish",
      "reference": "custom-made yellow guitar with an octopus on it, shiny purple guitar",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666669,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.0033689734995427335,
        "bleu2": 0.001506650751874582,
        "bleu3": 0.001172702070446368,
        "bleu4": 0.0010075580833015638,
        "bert_precision": 0.8861795663833618,
        "bert_recall": 0.8381428122520447,
        "bert_f1": 0.8614920973777771,
        "meteor": 0.04950495049504951,
        "sbert_similarity": 0.2968063950538635
      }
    },
    {
      "sample_id": 9,
      "question": "What activities has Dave participated in with his friends?",
      "prediction": "Dave has participated in hiking and walking with his friends.",
      "reference": "weekly visits to local parks, countryside roadtrip, celebration of the opening of his car maintenance shop, card-playing nights",
      "category": 1,
      "metrics": {
        "exact_match": 0,
        "f1": 0.07407407407407408,
        "rouge1_f": 0.06896551724137931,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06896551724137931,
        "bleu1": 0.03662639286628481,
        "bleu2": 0.01214760025616164,
        "bleu3": 0.009049554928060907,
        "bleu4": 0.007594611092001413,
        "bert_precision": 0.8839156627655029,
        "bert_recall": 0.8556699156761169,
        "bert_f1": 0.8695634007453918,
        "meteor": 0.029069767441860465,
        "sbert_similarity": 0.2918121814727783
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave take a photo of a Boston clock tower?",
      "prediction": "Last month",
      "reference": "September 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8668073415756226,
        "bert_recall": 0.8554688692092896,
        "bert_f1": 0.8611007928848267,
        "meteor": 0.0,
        "sbert_similarity": 0.4157176911830902
      }
    },
    {
      "sample_id": 9,
      "question": "Do all of Dave's car restoration projects go smoothly?",
      "prediction": "No",
      "reference": "No",
      "category": 1,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.5,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 9,
      "question": "Where was Calvin located in the last week of October 2023?",
      "prediction": "New York City",
      "reference": "Tokyo",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9050716161727905,
        "bert_recall": 0.9169247150421143,
        "bert_f1": 0.9109596610069275,
        "meteor": 0.0,
        "sbert_similarity": 0.5367722511291504
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave find the car he repaired and started sharing in his blog?",
      "prediction": "5:46 pm on 2 November, 2023",
      "reference": "last week of October 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8564745783805847,
        "bert_recall": 0.8823839426040649,
        "bert_f1": 0.8692362308502197,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.6588137149810791
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave buy a vintage camera?",
      "prediction": "17 November 2023",
      "reference": "November 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8,
        "rouge1_f": 0.8,
        "rouge2_f": 0.6666666666666666,
        "rougeL_f": 0.8,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5773502691896257,
        "bleu3": 0.3254992452916931,
        "bleu4": 0.24028114141347542,
        "bert_precision": 0.9594914317131042,
        "bert_recall": 0.9708876013755798,
        "bert_f1": 0.965155839920044,
        "meteor": 0.8928571428571429,
        "sbert_similarity": 0.9312126040458679
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin attend a gala in Boston?",
      "prediction": "20 April 2023",
      "reference": "November 16, 2023",
      "category": 2,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3333333333333333,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.23884377019126307,
        "bleu2": 0.09250379442961536,
        "bleu3": 0.08678511835352629,
        "bleu4": 0.0814136751754278,
        "bert_precision": 0.9449870586395264,
        "bert_recall": 0.9545514583587646,
        "bert_f1": 0.9497451782226562,
        "meteor": 0.16666666666666666,
        "sbert_similarity": 0.6924683451652527
      }
    },
    {
      "sample_id": 9,
      "question": "How long did Calvin plan to stay in Japan?",
      "prediction": "several months",
      "reference": "A few months",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9058219194412231,
        "bert_recall": 0.9218713045120239,
        "bert_f1": 0.913776159286499,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.8958836793899536
      }
    },
    {
      "sample_id": 9,
      "question": "Which band was Dave's favorite at the music festival in April 2023?",
      "prediction": "Frank Ocean",
      "reference": "Aerosmith",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8902741074562073,
        "bert_recall": 0.8653098344802856,
        "bert_f1": 0.877614438533783,
        "meteor": 0.0,
        "sbert_similarity": 0.2591286897659302
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Calvin attend a music festival in April 2023?",
      "prediction": "Calvin attended a music festival in Boston, Massachusetts on April 20, 2023.",
      "reference": "Tokyo",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8337302207946777,
        "bert_recall": 0.8514602780342102,
        "bert_f1": 0.8425019979476929,
        "meteor": 0.0,
        "sbert_similarity": 0.10829485952854156
      }
    },
    {
      "sample_id": 9,
      "question": "What advice did Calvin receive from the producer at the music festival?",
      "prediction": "Stay true to yourself and sound unique.",
      "reference": "to stay true to himself and sound unique",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8571428571428571,
        "rouge1_f": 0.7999999999999999,
        "rouge2_f": 0.6153846153846153,
        "rougeL_f": 0.7999999999999999,
        "bleu1": 0.75,
        "bleu2": 0.6546536707079771,
        "bleu3": 0.5261597794341889,
        "bleu4": 0.23119742295813953,
        "bert_precision": 0.9694135189056396,
        "bert_recall": 0.9530813694000244,
        "bert_f1": 0.9611781239509583,
        "meteor": 0.6126582278481013,
        "sbert_similarity": 0.6664596796035767
      }
    },
    {
      "sample_id": 9,
      "question": "What is Dave's new business venture as of 1 May, 2023?",
      "prediction": "Dave's new business venture is an opening of his own car maintenance shop.",
      "reference": "Car maintenance shop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.375,
        "rouge1_f": 0.35294117647058826,
        "rouge2_f": 0.2666666666666667,
        "rougeL_f": 0.35294117647058826,
        "bleu1": 0.2,
        "bleu2": 0.1690308509457033,
        "bleu3": 0.13269551340792207,
        "bleu4": 0.06541868941754436,
        "bert_precision": 0.8785406947135925,
        "bert_recall": 0.9360532760620117,
        "bert_f1": 0.9063855409622192,
        "meteor": 0.46875,
        "sbert_similarity": 0.5350120067596436
      }
    },
    {
      "sample_id": 9,
      "question": "What type of cars does Dave work on at his shop?",
      "prediction": "classic cars and full restorations",
      "reference": "all kinds of cars, from regular maintenance to full restorations of classic cars",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5000000000000001,
        "rouge1_f": 0.4444444444444444,
        "rouge2_f": 0.25,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.13223911057726925,
        "bleu2": 0.10454419629475865,
        "bleu3": 0.03976488361223264,
        "bleu4": 0.026561164883435597,
        "bert_precision": 0.9279583692550659,
        "bert_recall": 0.8956810832023621,
        "bert_f1": 0.9115341305732727,
        "meteor": 0.3073770491803278,
        "sbert_similarity": 0.9076365828514099
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin receive as a gift from another artist?",
      "prediction": "A new vintage camera",
      "reference": "a gold necklace with a diamond pendant",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.11809163818525367,
        "bleu2": 0.043120969391201254,
        "bleu3": 0.036208407874557175,
        "bleu4": 0.037951271263104894,
        "bert_precision": 0.8765111565589905,
        "bert_recall": 0.8703998923301697,
        "bert_f1": 0.8734447956085205,
        "meteor": 0.07462686567164178,
        "sbert_similarity": 0.21069830656051636
      }
    },
    {
      "sample_id": 9,
      "question": "What was the necklace Calvin received meant to remind him of?",
      "prediction": "Calvin's hard work and talent",
      "reference": "why he keeps hustling as a musician",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8767809867858887,
        "bert_recall": 0.8865984678268433,
        "bert_f1": 0.8816624283790588,
        "meteor": 0.0,
        "sbert_similarity": 0.27881956100463867
      }
    },
    {
      "sample_id": 9,
      "question": "What does Dave do when he feels his creativity is frozen?",
      "prediction": "Dave's feelings are frozen through experimenting and pushing boundaries.",
      "reference": "immerse himself in something he loves",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8780259490013123,
        "bert_recall": 0.8506256341934204,
        "bert_f1": 0.8641086220741272,
        "meteor": 0.0,
        "sbert_similarity": 0.32732364535331726
      }
    },
    {
      "sample_id": 9,
      "question": "How does Calvin plan to jumpstart his inspiration?",
      "prediction": "By staying true to himself and sounding unique",
      "reference": "explore other things and have some fun",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8678505420684814,
        "bert_recall": 0.8710916042327881,
        "bert_f1": 0.869468092918396,
        "meteor": 0.07042253521126761,
        "sbert_similarity": 0.2696513235569
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin manage to save during the flood incident?",
      "prediction": "Calvin managed to rescue several items and keep them dry",
      "reference": "music gear and favorite microphone",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8442044854164124,
        "bert_recall": 0.8457466959953308,
        "bert_f1": 0.844974935054779,
        "meteor": 0.09090909090909091,
        "sbert_similarity": 0.02716256119310856
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave open in May 2023?",
      "prediction": "Dave opened his car shop.",
      "reference": "a car shop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.5,
        "rouge2_f": 0.3333333333333333,
        "rougeL_f": 0.5,
        "bleu1": 0.3333333333333333,
        "bleu2": 0.25819888974716115,
        "bleu3": 0.12111838948674812,
        "bleu4": 0.08633400213704504,
        "bert_precision": 0.9322291612625122,
        "bert_recall": 0.9436990022659302,
        "bert_f1": 0.9379290342330933,
        "meteor": 0.15625,
        "sbert_similarity": 0.5392917990684509
      }
    },
    {
      "sample_id": 9,
      "question": "What gives Dave a sense of achievement and purpose?",
      "prediction": "transforming something that's not working into something that runs smoothly",
      "reference": "Fixing up things",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8730586767196655,
        "bert_recall": 0.8871933817863464,
        "bert_f1": 0.8800693154335022,
        "meteor": 0.0,
        "sbert_similarity": 0.4264504015445709
      }
    },
    {
      "sample_id": 9,
      "question": "What fuels Calvin's soul?",
      "prediction": "Performing live",
      "reference": "Performing live",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What is Dave doing to relax on weekends?",
      "prediction": "Dave drives his red sports car in long distances.",
      "reference": "exploring parks",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8639639616012573,
        "bert_recall": 0.880722165107727,
        "bert_f1": 0.8722625970840454,
        "meteor": 0.0,
        "sbert_similarity": 0.07094070315361023
      }
    },
    {
      "sample_id": 9,
      "question": "What sports activity is Calvin planning to try after the tour with Frank Ocean?",
      "prediction": "hiking",
      "reference": "Skiing",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9191206693649292,
        "bert_recall": 0.895740807056427,
        "bert_f1": 0.9072800874710083,
        "meteor": 0.0,
        "sbert_similarity": 0.555260956287384
      }
    },
    {
      "sample_id": 9,
      "question": "What was Calvin excited to do after getting his car fixed on 7 July, 2023?",
      "prediction": "Dave opened his own car maintenance shop.",
      "reference": "get back on the road",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8731034994125366,
        "bert_recall": 0.8931462168693542,
        "bert_f1": 0.8830111622810364,
        "meteor": 0.0,
        "sbert_similarity": 0.10292568802833557
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin and his friends arrange for in the park?",
      "prediction": "Calvin's old high school friend invited him to see him perform in Boston, which was an eye-opening experience.",
      "reference": "regular walks together",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.830065131187439,
        "bert_recall": 0.8506518006324768,
        "bert_f1": 0.8402323722839355,
        "meteor": 0.0,
        "sbert_similarity": 0.10557497292757034
      }
    },
    {
      "sample_id": 9,
      "question": "What kind of music has Calvin been creating lately?",
      "prediction": "Calvin's recent creations involve experimenting between different genres.",
      "reference": "experimenting with different genres",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5,
        "rouge1_f": 0.46153846153846156,
        "rouge2_f": 0.18181818181818182,
        "rougeL_f": 0.46153846153846156,
        "bleu1": 0.3,
        "bleu2": 0.18257418583505533,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.04939382737115371,
        "bert_precision": 0.9122997522354126,
        "bert_recall": 0.9327968955039978,
        "bert_f1": 0.9224345088005066,
        "meteor": 0.22727272727272727,
        "sbert_similarity": 0.5786930918693542
      }
    },
    {
      "sample_id": 9,
      "question": "How does Calvin describe his process of adding electronic elements to his songs?",
      "prediction": "Calvin describes his process as being inspired by driving and listening to music while in the car, allowing him to incorporate unique soundscapes into his tracks.",
      "reference": "gives them a fresh vibe",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8436031341552734,
        "bert_recall": 0.858366847038269,
        "bert_f1": 0.8509209752082825,
        "meteor": 0.07042253521126761,
        "sbert_similarity": 0.19929014146327972
      }
    },
    {
      "sample_id": 9,
      "question": "What car brand does Calvin own that he is proud of?",
      "prediction": "Calvin owns a red sports car that he is proud of.",
      "reference": "Ferrari",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8511788845062256,
        "bert_recall": 0.8333303928375244,
        "bert_f1": 0.8421600461006165,
        "meteor": 0.0,
        "sbert_similarity": 0.3829607665538788
      }
    },
    {
      "sample_id": 9,
      "question": "What is Calvin's biggest current goal?",
      "prediction": "Calvin's biggest current goal is to keep pursuing their music and never give up.",
      "reference": "expand his brand worldwide and grow his fanbase",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09523809523809523,
        "rouge1_f": 0.08695652173913045,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913045,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8630846738815308,
        "bert_recall": 0.8688149452209473,
        "bert_f1": 0.8659403324127197,
        "meteor": 0.058139534883720936,
        "sbert_similarity": 0.22474640607833862
      }
    },
    {
      "sample_id": 9,
      "question": "What is Dave's advice to Calvin regarding his dreams?",
      "prediction": "Dave advises Calvin to keep pursuing his dream and never give up.",
      "reference": "to never forget his dreams",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.35294117647058826,
        "rouge1_f": 0.47058823529411764,
        "rouge2_f": 0.13333333333333333,
        "rougeL_f": 0.35294117647058826,
        "bleu1": 0.23076923076923078,
        "bleu2": 0.04385290096535146,
        "bleu3": 0.02691902405124093,
        "bleu4": 0.020448007360218394,
        "bert_precision": 0.893661379814148,
        "bert_recall": 0.9089304208755493,
        "bert_f1": 0.9012312293052673,
        "meteor": 0.5537280701754386,
        "sbert_similarity": 0.4238196313381195
      }
    },
    {
      "sample_id": 9,
      "question": "What workshop did Dave get picked for on 11 August, 2023?",
      "prediction": "Dave got picked for the car mod workshop on 11 August, 2023.",
      "reference": "Car mod workshop",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.3076923076923077,
        "rougeL_f": 0.4,
        "bleu1": 0.21428571428571425,
        "bleu2": 0.1815682598006407,
        "bleu3": 0.1428356262969865,
        "bleu4": 0.07069301148938888,
        "bert_precision": 0.8709274530410767,
        "bert_recall": 0.9487952589988708,
        "bert_f1": 0.9081953763961792,
        "meteor": 0.7549857549857552,
        "sbert_similarity": 0.6203420162200928
      }
    },
    {
      "sample_id": 9,
      "question": "What kind of modifications has Dave been working on in the car mod workshop?",
      "prediction": "Dave was recently working on modifying his car by adding a custom exhaust and performance upgrades.",
      "reference": "engine swaps, suspension modifications, and body modifications",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.09090909090909091,
        "rouge1_f": 0.08695652173913043,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08695652173913043,
        "bleu1": 0.0588235294117647,
        "bleu2": 0.019174124721184262,
        "bleu3": 0.014076093604378503,
        "bleu4": 0.011502783619900045,
        "bert_precision": 0.8758201599121094,
        "bert_recall": 0.8860058784484863,
        "bert_f1": 0.8808835744857788,
        "meteor": 0.06329113924050632,
        "sbert_similarity": 0.47066599130630493
      }
    },
    {
      "sample_id": 9,
      "question": "What type of car did Dave work on during the workshop?",
      "prediction": "A red sports car",
      "reference": "classic muscle car",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8999699950218201,
        "bert_recall": 0.923336923122406,
        "bert_f1": 0.9115037322044373,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.49657684564590454
      }
    },
    {
      "sample_id": 9,
      "question": "What does Dave say is important for making his custom cars unique?",
      "prediction": "Dave says it's important to incorporate custom exhaust and performance upgrades.",
      "reference": "attention to small details",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.13333333333333333,
        "rouge1_f": 0.125,
        "rouge2_f": 0.0,
        "rougeL_f": 0.125,
        "bleu1": 0.07692307692307693,
        "bleu2": 0.025318484177091663,
        "bleu3": 0.018733084686676876,
        "bleu4": 0.01553712569276035,
        "bert_precision": 0.8815417289733887,
        "bert_recall": 0.8635215163230896,
        "bert_f1": 0.8724385499954224,
        "meteor": 0.10638297872340427,
        "sbert_similarity": 0.08828799426555634
      }
    },
    {
      "sample_id": 9,
      "question": "How did the audience in Tokyo react when Calvin sang one of his songs?",
      "prediction": "Everyone was so into it when I played one of my songs and sang along. It was a magical moment.",
      "reference": "Everyone was so into it and sang along",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6153846153846153,
        "rouge1_f": 0.5714285714285715,
        "rouge2_f": 0.46153846153846156,
        "rougeL_f": 0.5714285714285715,
        "bleu1": 0.36363636363636365,
        "bleu2": 0.3223291856101521,
        "bleu3": 0.2784951619551111,
        "bleu4": 0.21626010675918345,
        "bert_precision": 0.9224026203155518,
        "bert_recall": 0.9589487910270691,
        "bert_f1": 0.9403206706047058,
        "meteor": 0.6222271517302572,
        "sbert_similarity": 0.7536683082580566
      }
    },
    {
      "sample_id": 9,
      "question": "How did Calvin meet Frank Ocean?",
      "prediction": "Calvin met Frank Ocean at a music festival in Tokyo.",
      "reference": "At a music festival in Tokyo",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.7142857142857143,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.5454545454545454,
        "bleu2": 0.5222329678670935,
        "bleu3": 0.49839100664976926,
        "bleu4": 0.4617366309441026,
        "bert_precision": 0.9194940328598022,
        "bert_recall": 0.9570966958999634,
        "bert_f1": 0.9379186034202576,
        "meteor": 0.778125,
        "sbert_similarity": 0.4513257145881653
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Calvin and Frank Ocean record a song together?",
      "prediction": "at my mansion",
      "reference": "In the studio at Calvin's mansion",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4444444444444444,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.17573142541048448,
        "bleu2": 0.04812603287992944,
        "bleu3": 0.040131988279406595,
        "bleu4": 0.035617249382866124,
        "bert_precision": 0.9414204359054565,
        "bert_recall": 0.9129863977432251,
        "bert_f1": 0.9269854426383972,
        "meteor": 0.17543859649122806,
        "sbert_similarity": 0.641514241695404
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin and his friends record in August 2023?",
      "prediction": "a project on location",
      "reference": "a podcast discussing the rap industry",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.15163266492815836,
        "bleu2": 0.05536842069051654,
        "bleu3": 0.04649251600872798,
        "bleu4": 0.04873039689743777,
        "bert_precision": 0.8997437953948975,
        "bert_recall": 0.9039844274520874,
        "bert_f1": 0.9018591046333313,
        "meteor": 0.08620689655172413,
        "sbert_similarity": 0.04459823668003082
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Calvin start shooting a video for his new album?",
      "prediction": "Last weekend",
      "reference": "Miami",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8768419027328491,
        "bert_recall": 0.8942722082138062,
        "bert_f1": 0.8854712843894958,
        "meteor": 0.0,
        "sbert_similarity": 0.25965237617492676
      }
    },
    {
      "sample_id": 9,
      "question": "What design is featured on Calvin's guitar?",
      "prediction": "a unique and special design",
      "reference": "octopus",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8457732200622559,
        "bert_recall": 0.8652632236480713,
        "bert_f1": 0.8554072380065918,
        "meteor": 0.0,
        "sbert_similarity": 0.15468010306358337
      }
    },
    {
      "sample_id": 9,
      "question": "Why did Calvin get his guitar customized with a shiny finish?",
      "prediction": "Calvin got his guitar customized with a shiny finish because it gives it a unique look and goes well with his style.",
      "reference": "unique look",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.19999999999999998,
        "rouge1_f": 0.16666666666666669,
        "rouge2_f": 0.0909090909090909,
        "rougeL_f": 0.16666666666666669,
        "bleu1": 0.08695652173913045,
        "bleu2": 0.06286946134619315,
        "bleu3": 0.02758276890070265,
        "bleu4": 0.017514895362802613,
        "bert_precision": 0.8372067809104919,
        "bert_recall": 0.8761001825332642,
        "bert_f1": 0.8562120199203491,
        "meteor": 0.4687500000000001,
        "sbert_similarity": 0.33882975578308105
      }
    },
    {
      "sample_id": 9,
      "question": "What color glow did Calvin customize his guitar with?",
      "prediction": "purple",
      "reference": "purple",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Dave come back from with insights on car modification on 1st September 2023?",
      "prediction": "San Francisco",
      "reference": "San Francisco",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 9,
      "question": "What emotion does Dave mention feeling when he sees the relief of someone whose car he fixed?",
      "prediction": "power and happiness",
      "reference": "Proud",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8736451864242554,
        "bert_recall": 0.8694454431533813,
        "bert_f1": 0.8715402483940125,
        "meteor": 0.0,
        "sbert_similarity": 0.29175442457199097
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin book a flight ticket for on 1st September 2023?",
      "prediction": "Calvin booked a flight ticket to Boston.",
      "reference": "Boston",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.25,
        "rouge1_f": 0.25,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8576656579971313,
        "bert_recall": 0.8894903659820557,
        "bert_f1": 0.8732881546020508,
        "meteor": 0.0,
        "sbert_similarity": 0.42436519265174866
      }
    },
    {
      "sample_id": 9,
      "question": "What is Calvin excited about after the tour?",
      "prediction": "Calvin's excitement focuses on growing and evolving his artistic vision.",
      "reference": "exploring and growing his brand",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.37499999999999994,
        "rouge2_f": 0.0,
        "rougeL_f": 0.25000000000000006,
        "bleu1": 0.25,
        "bleu2": 0.047673129462279626,
        "bleu3": 0.02935354896563489,
        "bleu4": 0.022416933501922302,
        "bert_precision": 0.9007854461669922,
        "bert_recall": 0.9089866280555725,
        "bert_f1": 0.9048675298690796,
        "meteor": 0.2727272727272727,
        "sbert_similarity": 0.3961060047149658
      }
    },
    {
      "sample_id": 9,
      "question": "What plans do Calvin and Dave have for when Calvin visits Boston?",
      "prediction": "Dave will show Calvin around town, including popular music spots like Paradise Rock, House of Blues, and Fenway Park. They plan to attend the Frank Ocean tour and possibly watch Calvin perform on stage.",
      "reference": "Check out Dave's garage and maybe get some ideas for future projects",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.045454545454545456,
        "rouge1_f": 0.0851063829787234,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0851063829787234,
        "bleu1": 0.05128205128205127,
        "bleu2": 0.011616917255955381,
        "bleu3": 0.007506775405141993,
        "bleu4": 0.005641822153183602,
        "bert_precision": 0.845914900302887,
        "bert_recall": 0.8589047193527222,
        "bert_f1": 0.8523603081703186,
        "meteor": 0.14084507042253522,
        "sbert_similarity": 0.3659505248069763
      }
    },
    {
      "sample_id": 9,
      "question": "Which Disney movie did Dave mention as one of his favorites?",
      "prediction": "The Lion King",
      "reference": "Ratatouille",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8585082292556763,
        "bert_recall": 0.8430284261703491,
        "bert_f1": 0.850697934627533,
        "meteor": 0.0,
        "sbert_similarity": 0.15754874050617218
      }
    },
    {
      "sample_id": 9,
      "question": "How does Dave feel about the reactions of people when they see the finished restoration project?",
      "prediction": "Dave feels excited and motivated because seeing the project complete gives him a sense of satisfaction.",
      "reference": "satisfying and worth the hard work",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.11764705882352941,
        "bleu2": 0.027116307227332027,
        "bleu3": 0.017693837921724947,
        "bleu4": 0.013679192123121896,
        "bert_precision": 0.8794063329696655,
        "bert_recall": 0.8819573521614075,
        "bert_f1": 0.8806799650192261,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.3742745518684387
      }
    },
    {
      "sample_id": 9,
      "question": "What activity did Calvin enjoy during his summer drives?",
      "prediction": "summer drive with freedom",
      "reference": "feeling the wind blowing through his hair",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8435308337211609,
        "bert_recall": 0.8443931341171265,
        "bert_f1": 0.843961775302887,
        "meteor": 0.0,
        "sbert_similarity": 0.04511753097176552
      }
    },
    {
      "sample_id": 9,
      "question": "Which song from the childhood of Calvin brings back memories of a road trip with his dad?",
      "prediction": "California Love",
      "reference": "\"California Love\"",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.36787944117144233,
        "bleu3": 0.1720701423929801,
        "bleu4": 0.11633369384516798,
        "bert_precision": 0.9628615379333496,
        "bert_recall": 0.9224772453308105,
        "bert_f1": 0.9422369003295898,
        "meteor": 0.0,
        "sbert_similarity": 0.9066882133483887
      }
    },
    {
      "sample_id": 9,
      "question": "What project did Calvin work on to chill out?",
      "prediction": "Creating his first beat with a talented new artist",
      "reference": "A shiny orange car",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.15384615384615383,
        "rouge2_f": 0.0,
        "rougeL_f": 0.15384615384615383,
        "bleu1": 0.11111111111111109,
        "bleu2": 0.03726779962499649,
        "bleu3": 0.028067124621919234,
        "bleu4": 0.023980296761827107,
        "bert_precision": 0.8729705810546875,
        "bert_recall": 0.8628141283988953,
        "bert_f1": 0.8678626418113708,
        "meteor": 0.11111111111111112,
        "sbert_similarity": 0.13540056347846985
      }
    },
    {
      "sample_id": 9,
      "question": "What car did Dave work on in the junkyard?",
      "prediction": "classic cars",
      "reference": "Ford Mustang",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8756834268569946,
        "bert_recall": 0.8756834268569946,
        "bert_f1": 0.8756834268569946,
        "meteor": 0.0,
        "sbert_similarity": 0.48482078313827515
      }
    },
    {
      "sample_id": 9,
      "question": "What does Dave find satisfying about restoring old cars?",
      "prediction": "satisfaction from bringing broken things back to life.",
      "reference": "Transforming something old and beat-up into something beautiful",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8872002363204956,
        "bert_recall": 0.8937040567398071,
        "bert_f1": 0.8904402256011963,
        "meteor": 0.0,
        "sbert_similarity": 0.38307008147239685
      }
    },
    {
      "sample_id": 9,
      "question": "What do Calvin and Dave use to reach their goals?",
      "prediction": "calvin's skills and hard work",
      "reference": "Hard work and determination",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666665,
        "rouge1_f": 0.6,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.5,
        "bleu2": 0.316227766016838,
        "bleu3": 0.1384586850774637,
        "bleu4": 0.09554427922043669,
        "bert_precision": 0.8962016701698303,
        "bert_recall": 0.9273980259895325,
        "bert_f1": 0.911532998085022,
        "meteor": 0.6233062330623306,
        "sbert_similarity": 0.479789674282074
      }
    },
    {
      "sample_id": 9,
      "question": "What does working on cars represent for Dave?",
      "prediction": "working on cars represents passion and fulfillment for Dave as it provides him with an outlet to express himself, connect with his inner self, and offers a sense of calm through the use of tools.",
      "reference": "Therapy and a way to get away from everyday stress",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.14285714285714285,
        "rouge1_f": 0.13333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.08888888888888888,
        "bleu1": 0.07894736842105263,
        "bleu2": 0.014607233345258954,
        "bleu3": 0.00881119946928069,
        "bleu4": 0.006414921514603761,
        "bert_precision": 0.8537145853042603,
        "bert_recall": 0.8767638802528381,
        "bert_f1": 0.8650857210159302,
        "meteor": 0.12,
        "sbert_similarity": 0.05538993701338768
      }
    },
    {
      "sample_id": 9,
      "question": "What does Dave aim to do with his passion for cars?",
      "prediction": "create powerful vehicles",
      "reference": "Take something broken and make it into something awesome",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8724365234375,
        "bert_recall": 0.8397471904754639,
        "bert_f1": 0.8557798266410828,
        "meteor": 0.0,
        "sbert_similarity": 0.22853514552116394
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin recently get that is a \"masterpiece on wheels\"?",
      "prediction": "A restored classic car",
      "reference": "Ferrari",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8589990139007568,
        "bert_recall": 0.8427926301956177,
        "bert_f1": 0.8508186340332031,
        "meteor": 0.0,
        "sbert_similarity": 0.41528457403182983
      }
    },
    {
      "sample_id": 9,
      "question": "Who headlined the music festival that Dave attended in October?",
      "prediction": "Fireworks",
      "reference": "The Fireworks",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.6666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.6666666666666666,
        "bleu1": 0.36787944117144233,
        "bleu2": 0.11633369384516798,
        "bleu3": 0.08048325236348887,
        "bleu4": 0.06541924356118012,
        "bert_precision": 0.9433984160423279,
        "bert_recall": 0.941599428653717,
        "bert_f1": 0.9424980282783508,
        "meteor": 0.2631578947368421,
        "sbert_similarity": 0.9720538854598999
      }
    },
    {
      "sample_id": 9,
      "question": "How does Calvin stay motivated when faced with setbacks?",
      "prediction": "Calvin stays motivated by relying on his passion for goals, using supportive individuals as sources of encouragement, and taking breaks to recharge through enjoyable activities.",
      "reference": "Reminds himself of his passion for goals, gets help from others, and takes a break to recharge",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.38095238095238104,
        "rouge1_f": 0.4761904761904762,
        "rouge2_f": 0.3,
        "rougeL_f": 0.42857142857142855,
        "bleu1": 0.3571428571428572,
        "bleu2": 0.28171808490950556,
        "bleu3": 0.2125135026205556,
        "bleu4": 0.16451929399933107,
        "bert_precision": 0.9018901586532593,
        "bert_recall": 0.9199120402336121,
        "bert_f1": 0.9108120203018188,
        "meteor": 0.5889590491224813,
        "sbert_similarity": 0.5702906250953674
      }
    },
    {
      "sample_id": 9,
      "question": "What activity does Dave find fulfilling, similar to Calvin's passion for music festivals?",
      "prediction": "Attending and being part of music festivals",
      "reference": "fixing things",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8707150220870972,
        "bert_recall": 0.9011717438697815,
        "bert_f1": 0.8856816291809082,
        "meteor": 0.0,
        "sbert_similarity": 0.022959427908062935
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Calvin and Dave meet Frank Ocean to start collaborating?",
      "prediction": "Calvin met Frank Ocean at a music festival in Tokyo.",
      "reference": "at a festival",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4615384615384615,
        "rouge1_f": 0.4615384615384615,
        "rouge2_f": 0.1818181818181818,
        "rougeL_f": 0.4615384615384615,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.1651445647689541,
        "bleu3": 0.06900656343516053,
        "bleu4": 0.04411629359322707,
        "bert_precision": 0.8893787860870361,
        "bert_recall": 0.9113620519638062,
        "bert_f1": 0.9002362489700317,
        "meteor": 0.6906906906906907,
        "sbert_similarity": 0.2973744869232178
      }
    },
    {
      "sample_id": 9,
      "question": "Which part of Tokyo is described as Tokyo's Times Square by Calvin?",
      "prediction": "Shibuya Crossing",
      "reference": "Shibuya Crossing",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What specific location in Tokyo does Calvin mention being excited to explore?",
      "prediction": "Tokyo",
      "reference": "Shinjuku",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9201675057411194,
        "bert_recall": 0.9061022996902466,
        "bert_f1": 0.9130806922912598,
        "meteor": 0.0,
        "sbert_similarity": 0.5100042819976807
      }
    },
    {
      "sample_id": 9,
      "question": "What dish does Dave recommend Calvin to try in Tokyo?",
      "prediction": "california roll",
      "reference": "ramen",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.788480281829834,
        "bert_recall": 0.8253087997436523,
        "bert_f1": 0.8064743280410767,
        "meteor": 0.0,
        "sbert_similarity": 0.1552816927433014
      }
    },
    {
      "sample_id": 9,
      "question": "What does Calvin find energizing during the tour?",
      "prediction": "energy",
      "reference": "Performing and connecting with the crowd",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8803058862686157,
        "bert_recall": 0.8517369627952576,
        "bert_f1": 0.8657858371734619,
        "meteor": 0.0,
        "sbert_similarity": 0.12782169878482819
      }
    },
    {
      "sample_id": 9,
      "question": "How does Calvin balance his job and personal life?",
      "prediction": "One day at a time",
      "reference": "Takes it one day at a time",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.8333333333333333,
        "rouge1_f": 0.8333333333333333,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8333333333333333,
        "bleu1": 0.6703200460356393,
        "bleu2": 0.6703200460356393,
        "bleu3": 0.6703200460356393,
        "bleu4": 0.6703200460356393,
        "bert_precision": 0.975034236907959,
        "bert_recall": 0.9573968648910522,
        "bert_f1": 0.9661350846290588,
        "meteor": 0.7323529411764707,
        "sbert_similarity": 0.7078113555908203
      }
    },
    {
      "sample_id": 9,
      "question": "What inspired Calvin's recent music?",
      "prediction": "Touring with Frank Ocean last week",
      "reference": "Struggles that people go through",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8584416508674622,
        "bert_recall": 0.8447048664093018,
        "bert_f1": 0.851517915725708,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.0957401692867279
      }
    },
    {
      "sample_id": 9,
      "question": "How does Calvin describe his music in relation to capturing feelings?",
      "prediction": "Calvin's music is described as moving and conveying deep emotions.",
      "reference": "Express himself and work through his emotions",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.23529411764705882,
        "rouge1_f": 0.2222222222222222,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2222222222222222,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.03892494720807617,
        "bleu3": 0.025677367760996977,
        "bleu4": 0.020255986027125642,
        "bert_precision": 0.8799601197242737,
        "bert_recall": 0.8892906904220581,
        "bert_f1": 0.8846007585525513,
        "meteor": 0.136986301369863,
        "sbert_similarity": 0.2777494788169861
      }
    },
    {
      "sample_id": 9,
      "question": "Why did Dave start working on cars?",
      "prediction": "Dave started working on cars because it was like therapy for him; growing up with his dad, refurbishing classic cars gave him a sense of fulfillment.",
      "reference": "Fascinated with how machines work",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06666666666666667,
        "rouge1_f": 0.12903225806451613,
        "rouge2_f": 0.0,
        "rougeL_f": 0.06451612903225806,
        "bleu1": 0.034482758620689655,
        "bleu2": 0.011097419040461881,
        "bleu3": 0.008081583568827886,
        "bleu4": 0.006471824245088331,
        "bert_precision": 0.853582501411438,
        "bert_recall": 0.8685314655303955,
        "bert_f1": 0.8609921336174011,
        "meteor": 0.14084507042253522,
        "sbert_similarity": 0.35942184925079346
      }
    },
    {
      "sample_id": 9,
      "question": "What is the toughest part of car restoration according to Dave?",
      "prediction": "Bringing broken parts back to life",
      "reference": "Paying extra attention to detail",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1818181818181818,
        "rouge1_f": 0.1818181818181818,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1818181818181818,
        "bleu1": 0.16666666666666669,
        "bleu2": 0.05773502691896258,
        "bleu3": 0.04506819749679463,
        "bleu4": 0.040824829046386304,
        "bert_precision": 0.892287015914917,
        "bert_recall": 0.9033498764038086,
        "bert_f1": 0.897784411907196,
        "meteor": 0.09803921568627452,
        "sbert_similarity": 0.13352105021476746
      }
    },
    {
      "sample_id": 9,
      "question": "What does Calvin believe makes an artist create something extraordinary?",
      "prediction": "Calvin believes that it is by creating something extraordinary, making a difference in someone's life, experimenting and pushing boundaries, and supporting young musicians.",
      "reference": "Paying attention to small details",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8500171899795532,
        "bert_recall": 0.8647516965866089,
        "bert_f1": 0.8573210835456848,
        "meteor": 0.0,
        "sbert_similarity": 0.06598109751939774
      }
    },
    {
      "sample_id": 9,
      "question": "When did Dave sell the car he restored last year?",
      "prediction": "Thanks Calvin! It was a fun project. I sold it to a collector.",
      "reference": "Last year",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8617432713508606,
        "bert_recall": 0.8644058704376221,
        "bert_f1": 0.8630725145339966,
        "meteor": 0.0,
        "sbert_similarity": 0.03480227664113045
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin first get interested in cars?",
      "prediction": "Calvin expressed interest in working on and sharing knowledge about cars when Davey mentioned he was opening his own car maintenance shop.",
      "reference": "at an early age",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8404767513275146,
        "bert_recall": 0.8679019212722778,
        "bert_f1": 0.853969156742096,
        "meteor": 0.0,
        "sbert_similarity": 0.0667596235871315
      }
    },
    {
      "sample_id": 9,
      "question": "How did Calvin feel about performing with someone he admires?",
      "prediction": "Calvin felt energized and moved by performing, as evidenced by his enthusiasm and ability to express himself through live performances.",
      "reference": "Unreal, like a dream come true",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0.04545454545454546,
        "bleu2": 0.014712247158412498,
        "bleu3": 0.010748014644407517,
        "bleu4": 0.008687475782716616,
        "bert_precision": 0.8415194749832153,
        "bert_recall": 0.8608577847480774,
        "bert_f1": 0.8510787487030029,
        "meteor": 0.06756756756756757,
        "sbert_similarity": 0.050805266946554184
      }
    },
    {
      "sample_id": 9,
      "question": "What realization did the nightclub experience bring to Calvin?",
      "prediction": "A big life change",
      "reference": "how much music means to him, it's like his passion and purpose",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8992527723312378,
        "bert_recall": 0.8536363840103149,
        "bert_f1": 0.875851035118103,
        "meteor": 0.0,
        "sbert_similarity": 0.16373631358146667
      }
    },
    {
      "sample_id": 9,
      "question": "What do Dave and Calvin agree on regarding their pursuits?",
      "prediction": "They both believe in pursuing hard work and success.",
      "reference": "It's fulfilling and motivating",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.15384615384615383,
        "rouge1_f": 0.14285714285714285,
        "rouge2_f": 0.0,
        "rougeL_f": 0.14285714285714285,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.9015583992004395,
        "bert_recall": 0.9050309658050537,
        "bert_f1": 0.9032913446426392,
        "meteor": 0.11111111111111112,
        "sbert_similarity": 0.37375855445861816
      }
    },
    {
      "sample_id": 9,
      "question": "Which city is featured in the photograph Dave showed Calvin?",
      "prediction": "Boston",
      "reference": "Boston",
      "category": 4,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin do recently at his Japanese house?",
      "prediction": "Calvin had a big life change and moved into a new mansion.",
      "reference": "Threw a small party for his new album",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2105263157894737,
        "rouge1_f": 0.2,
        "rouge2_f": 0.0,
        "rougeL_f": 0.2,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.8862435817718506,
        "bert_recall": 0.8830875158309937,
        "bert_f1": 0.8846626877784729,
        "meteor": 0.11904761904761905,
        "sbert_similarity": 0.18953004479408264
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave recently start a blog about?",
      "prediction": "Dave recently started a blog on car mods.",
      "reference": "Car mods",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.25,
        "rougeL_f": 0.4,
        "bleu1": 0.2222222222222222,
        "bleu2": 0.16666666666666669,
        "bleu3": 0.07542890820012248,
        "bleu4": 0.050712153369465586,
        "bert_precision": 0.9047865867614746,
        "bert_recall": 0.9527417421340942,
        "bert_f1": 0.9281451106071472,
        "meteor": 0.19230769230769235,
        "sbert_similarity": 0.6294012069702148
      }
    },
    {
      "sample_id": 9,
      "question": "What is Dave's way to share his passion with others?",
      "prediction": "Dave shares his passion through blogging.",
      "reference": "Through a blog on car mods",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.16666666666666666,
        "rouge1_f": 0.3333333333333333,
        "rouge2_f": 0.0,
        "rougeL_f": 0.3333333333333333,
        "bleu1": 0.14285714285714285,
        "bleu2": 0.048795003647426664,
        "bleu3": 0.037468614372419724,
        "bleu4": 0.033031643180138064,
        "bert_precision": 0.8903403878211975,
        "bert_recall": 0.8994516134262085,
        "bert_f1": 0.894872784614563,
        "meteor": 0.08333333333333333,
        "sbert_similarity": 0.29548168182373047
      }
    },
    {
      "sample_id": 9,
      "question": "What type of videos does Calvin usually watch on his television?",
      "prediction": "music videos",
      "reference": "Music videos, concerts, documentaries about artists and their creative process",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.33333333333333337,
        "rouge2_f": 0.19999999999999998,
        "rougeL_f": 0.33333333333333337,
        "bleu1": 0.006737946999085467,
        "bleu2": 0.006737946999085467,
        "bleu3": 0.003151574591602894,
        "bleu4": 0.002130725927060655,
        "bert_precision": 0.9235300421714783,
        "bert_recall": 0.8545820116996765,
        "bert_f1": 0.8877192139625549,
        "meteor": 0.05434782608695652,
        "sbert_similarity": 0.5943616628646851
      }
    },
    {
      "sample_id": 9,
      "question": "What type of music has Dave been getting into lately?",
      "prediction": "Dave is currently exploring different genres and pushing himself out of his comfort zone in music.",
      "reference": "Classic rock",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8385059833526611,
        "bert_recall": 0.8808491230010986,
        "bert_f1": 0.8591561913490295,
        "meteor": 0.0,
        "sbert_similarity": 0.2820155620574951
      }
    },
    {
      "sample_id": 9,
      "question": "What tools does Calvin use to boost his motivation for music?",
      "prediction": "Calvin uses methods such as surrounding himself with positive energy and passion, collaborating with others who inspire him, and experimenting with different genres and pushing boundaries in his music.",
      "reference": "Writing lyrics and notes",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.06896551724137932,
        "rouge1_f": 0.0606060606060606,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0606060606060606,
        "bleu1": 0.03125,
        "bleu2": 0.010040241611281232,
        "bleu3": 0.007306361129019465,
        "bleu4": 0.005834347180338519,
        "bert_precision": 0.8428723812103271,
        "bert_recall": 0.8851277828216553,
        "bert_f1": 0.8634834289550781,
        "meteor": 0.07692307692307693,
        "sbert_similarity": 0.24680694937705994
      }
    },
    {
      "sample_id": 9,
      "question": "What type of content does Dave post on his blog that inspired others to start their own DIY projects?",
      "prediction": "how I made this car look like a beast",
      "reference": "How he made his car look like a beast",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7777777777777778,
        "rouge1_f": 0.7777777777777778,
        "rouge2_f": 0.5,
        "rougeL_f": 0.7777777777777778,
        "bleu1": 0.7777777777777778,
        "bleu2": 0.6236095644623236,
        "bleu3": 0.5536178537956702,
        "bleu4": 0.4854917717073234,
        "bert_precision": 0.9748137593269348,
        "bert_recall": 0.9758145809173584,
        "bert_f1": 0.9753139615058899,
        "meteor": 0.7471655328798186,
        "sbert_similarity": 0.8239293098449707
      }
    },
    {
      "sample_id": 9,
      "question": "What kind of impact does Dave's blog on car mods have on people?",
      "prediction": "Inspires creativity and encourages others to start their own DIY projects.",
      "reference": "It inspires others to start their DIY projects",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7368421052631579,
        "rouge1_f": 0.7368421052631579,
        "rouge2_f": 0.47058823529411764,
        "rougeL_f": 0.7368421052631579,
        "bleu1": 0.5833333333333334,
        "bleu2": 0.46056618647183833,
        "bleu3": 0.35246228435246557,
        "bleu4": 0.2620251007173262,
        "bert_precision": 0.9457343220710754,
        "bert_recall": 0.9628031253814697,
        "bert_f1": 0.9541923999786377,
        "meteor": 0.6777108433734939,
        "sbert_similarity": 0.9097738265991211
      }
    },
    {
      "sample_id": 9,
      "question": "Who did Calvin invite to see him perform in Boston on 13 November, 2023?",
      "prediction": "Calvin's support is making a significant impact on the next generation of musicians by nurturing their passion and helping them succeed.",
      "reference": "his old high school buddy",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8352875709533691,
        "bert_recall": 0.8457722663879395,
        "bert_f1": 0.8404971957206726,
        "meteor": 0.0,
        "sbert_similarity": 0.09725604951381683
      }
    },
    {
      "sample_id": 9,
      "question": "What hobby did Calvin take up recently?",
      "prediction": "car repair",
      "reference": "Photography",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8694310188293457,
        "bert_recall": 0.8696171045303345,
        "bert_f1": 0.8695240616798401,
        "meteor": 0.0,
        "sbert_similarity": 0.25759145617485046
      }
    },
    {
      "sample_id": 9,
      "question": "What new item did Dave buy recently?",
      "prediction": "Dave bought a gold necklace with a diamond pendant.",
      "reference": "A vintage camera",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.18181818181818182,
        "rouge1_f": 0.16666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.16666666666666666,
        "bleu1": 0.10000000000000002,
        "bleu2": 0.03333333333333335,
        "bleu3": 0.024950558772614845,
        "bleu4": 0.021105340631872645,
        "bert_precision": 0.8776347637176514,
        "bert_recall": 0.8807603716850281,
        "bert_f1": 0.8791947960853577,
        "meteor": 0.13888888888888892,
        "sbert_similarity": 0.1492486149072647
      }
    },
    {
      "sample_id": 9,
      "question": "What type of photos does Dave like to capture with his new camera?",
      "prediction": "nature such as sunsets, beaches, and waves",
      "reference": "Nature - sunsets, beaches, waves",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.6666666666666666,
        "rouge1_f": 0.7272727272727273,
        "rouge2_f": 0.2222222222222222,
        "rougeL_f": 0.7272727272727273,
        "bleu1": 0.6666666666666666,
        "bleu2": 0.5,
        "bleu3": 0.41857930082433814,
        "bleu4": 0.3303164318013807,
        "bert_precision": 0.9465790390968323,
        "bert_recall": 0.9594631791114807,
        "bert_f1": 0.9529775381088257,
        "meteor": 0.6069711538461539,
        "sbert_similarity": 0.9125984907150269
      }
    },
    {
      "sample_id": 9,
      "question": "What event did Calvin attend in Boston?",
      "prediction": "a conference",
      "reference": "Fancy gala",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9013809561729431,
        "bert_recall": 0.865780770778656,
        "bert_f1": 0.8832222819328308,
        "meteor": 0.0,
        "sbert_similarity": 0.20110231637954712
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin discuss with the cool artist he met at the gala?",
      "prediction": "Calvin discussed his new album and the collaboration with the music industry.",
      "reference": "Music and art",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.26666666666666666,
        "rouge2_f": 0.0,
        "rougeL_f": 0.13333333333333333,
        "bleu1": 0.15384615384615385,
        "bleu2": 0.03580574370197164,
        "bleu3": 0.023547737997202885,
        "bleu4": 0.018476860420522198,
        "bert_precision": 0.873676061630249,
        "bert_recall": 0.8955896496772766,
        "bert_f1": 0.8844972252845764,
        "meteor": 0.25641025641025644,
        "sbert_similarity": 0.2995337247848511
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Dave take a stunning photo of a waterfall?",
      "prediction": "Dave took a stunning photo of a waterfall next to rocks and rivers.",
      "reference": "Nearby park",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8547743558883667,
        "bert_recall": 0.8652783632278442,
        "bert_f1": 0.8599942922592163,
        "meteor": 0.0,
        "sbert_similarity": 0.22210291028022766
      }
    },
    {
      "sample_id": 9,
      "question": "What positive impact does Calvin mention nature has on tough times?",
      "prediction": "Calvin mentions that nature reminds him to stay focused and keep going during challenging times.",
      "reference": "Nature helps us appreciate life",
      "category": 4,
      "metrics": {
        "exact_match": 0,
        "f1": 0.1,
        "rouge1_f": 0.1,
        "rouge2_f": 0.0,
        "rougeL_f": 0.1,
        "bleu1": 0.0625,
        "bleu2": 0.02041241452319315,
        "bleu3": 0.015007489164383502,
        "bleu4": 0.012300686288463768,
        "bert_precision": 0.8783470988273621,
        "bert_recall": 0.9001656174659729,
        "bert_f1": 0.8891225457191467,
        "meteor": 0.08333333333333336,
        "sbert_similarity": 0.40012359619140625
      }
    },
    {
      "sample_id": 9,
      "question": "Which DJ was Dave's favorite at the music festival in April 2023?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Aerosmith",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8860553503036499,
        "bert_recall": 0.8427956104278564,
        "bert_f1": 0.8638842701911926,
        "meteor": 0.0,
        "sbert_similarity": 0.05664724484086037
      }
    },
    {
      "sample_id": 9,
      "question": "What advice did Calvin receive from the chef at the music festival?",
      "prediction": "to stay true to himself and sound unique",
      "reference": "to stay true to himself and sound unique",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What is Calvin's new business venture as of 1 May, 2023?",
      "prediction": "A photo of a car project in Boston",
      "reference": "Car maintenance shop",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.2,
        "rouge1_f": 0.18181818181818182,
        "rouge2_f": 0.0,
        "rougeL_f": 0.18181818181818182,
        "bleu1": 0.12500000000000003,
        "bleu2": 0.04225771273642583,
        "bleu3": 0.032085442892067474,
        "bleu4": 0.02777619034011792,
        "bert_precision": 0.8713650703430176,
        "bert_recall": 0.8695790767669678,
        "bert_f1": 0.870471179485321,
        "meteor": 0.14285714285714288,
        "sbert_similarity": 0.39990633726119995
      }
    },
    {
      "sample_id": 9,
      "question": "What type of cars does Calvin work on at his shop?",
      "prediction": "From regular maintenance to full restorations of classic cars",
      "reference": "all kinds of cars, from regular maintenance to full restorations of classic cars",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.9,
        "rouge1_f": 0.8181818181818181,
        "rouge2_f": 0.8,
        "rougeL_f": 0.8181818181818181,
        "bleu1": 0.5737534207374327,
        "bleu2": 0.5737534207374327,
        "bleu3": 0.5737534207374327,
        "bleu4": 0.5737534207374327,
        "bert_precision": 0.9662272930145264,
        "bert_recall": 0.9463903903961182,
        "bert_f1": 0.9562059640884399,
        "meteor": 0.713795806388399,
        "sbert_similarity": 0.9301742911338806
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave receive as a gift from another artist?",
      "prediction": "Not mentioned in the conversation",
      "reference": "a gold necklace with a diamond pendant",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8921197652816772,
        "bert_recall": 0.8522205352783203,
        "bert_f1": 0.8717138171195984,
        "meteor": 0.0,
        "sbert_similarity": 0.13617947697639465
      }
    },
    {
      "sample_id": 9,
      "question": "What was the necklace Dave received meant to remind him of?",
      "prediction": "why he keeps hustling as a musician",
      "reference": "why he keeps hustling as a musician",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9985422740524781,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin open in May 2023?",
      "prediction": "A mansion",
      "reference": "a car shop",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.4,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.3032653298563167,
        "bleu2": 0.13562437855552414,
        "bleu3": 0.10556327625176448,
        "bleu4": 0.09069748827745895,
        "bert_precision": 0.9217522740364075,
        "bert_recall": 0.9102884531021118,
        "bert_f1": 0.9159844517707825,
        "meteor": 0.17241379310344826,
        "sbert_similarity": 0.32433009147644043
      }
    },
    {
      "sample_id": 9,
      "question": "What gives Calvin a sense of achievement and purpose?",
      "prediction": "Fixing up things",
      "reference": "Fixing up things",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 0.9999999403953552,
        "bert_recall": 0.9999999403953552,
        "bert_f1": 0.9999999403953552,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What sports activity is Dave planning to try after the tour with Frank Ocean?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Skiing",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8871620297431946,
        "bert_recall": 0.8482587933540344,
        "bert_f1": 0.8672743439674377,
        "meteor": 0.0,
        "sbert_similarity": 0.09310255944728851
      }
    },
    {
      "sample_id": 9,
      "question": "How does Calvin describe his process of adding acoustic elements to his songs?",
      "prediction": "Not mentioned in the conversation",
      "reference": "gives them a fresh vibe",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8852189183235168,
        "bert_recall": 0.856185793876648,
        "bert_f1": 0.8704602718353271,
        "meteor": 0.0,
        "sbert_similarity": 0.06861761212348938
      }
    },
    {
      "sample_id": 9,
      "question": "What clothing brand does Calvin own that he is proud of?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ferrari",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8785410523414612,
        "bert_recall": 0.8062957525253296,
        "bert_f1": 0.8408694267272949,
        "meteor": 0.0,
        "sbert_similarity": 0.13934244215488434
      }
    },
    {
      "sample_id": 9,
      "question": "What workshop did Calvin get picked for on 11 August, 2023?",
      "prediction": "Car mod workshop",
      "reference": "Car mod workshop",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 9,
      "question": "What kind of modifications has Calvin been working on in the car mod workshop?",
      "prediction": "Not mentioned in the conversation",
      "reference": "engine swaps, suspension modifications, and body modifications",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8862070441246033,
        "bert_recall": 0.8445767164230347,
        "bert_f1": 0.8648912310600281,
        "meteor": 0.0,
        "sbert_similarity": 0.08233410120010376
      }
    },
    {
      "sample_id": 9,
      "question": "What type of car did Calvin work on during the workshop?",
      "prediction": "classic muscle car",
      "reference": "classic muscle car",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 0.5623413251903491,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9814814814814815,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave and his friends record in August 2023?",
      "prediction": "A recording of their recent podcast where they discussed rap culture",
      "reference": "a podcast discussing the rap industry",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.3529411764705882,
        "rouge1_f": 0.4705882352941177,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4705882352941177,
        "bleu1": 0.2727272727272727,
        "bleu2": 0.05222329678670937,
        "bleu3": 0.03227679469808881,
        "bleu4": 0.024808415001701823,
        "bert_precision": 0.8924567103385925,
        "bert_recall": 0.9410553574562073,
        "bert_f1": 0.916111946105957,
        "meteor": 0.3076923076923077,
        "sbert_similarity": 0.8558792471885681
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Dave start shooting a video for his new album?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Miami",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8654989004135132,
        "bert_recall": 0.8664630651473999,
        "bert_f1": 0.8659806847572327,
        "meteor": 0.0,
        "sbert_similarity": 0.12402265518903732
      }
    },
    {
      "sample_id": 9,
      "question": "What design is featured on Dave's guitar?",
      "prediction": "Not mentioned in the conversation",
      "reference": "octopus",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8656320571899414,
        "bert_recall": 0.8373445272445679,
        "bert_f1": 0.85125333070755,
        "meteor": 0.0,
        "sbert_similarity": 0.08238226175308228
      }
    },
    {
      "sample_id": 9,
      "question": "Why did Dave get his guitar customized with a shiny finish?",
      "prediction": "Unique Look",
      "reference": "unique look",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9570313692092896,
        "bert_recall": 0.9570313692092896,
        "bert_f1": 0.9570313692092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 9,
      "question": "What color glow did Dave customize his guitar with?",
      "prediction": "purple",
      "reference": "purple",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Calvin come back from with insights on car modification on 1st September 2023?",
      "prediction": "San Francisco",
      "reference": "San Francisco",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9999998807907104,
        "bert_recall": 0.9999998807907104,
        "bert_f1": 0.9999998807907104,
        "meteor": 0.9375,
        "sbert_similarity": 0.9999998807907104
      }
    },
    {
      "sample_id": 9,
      "question": "What emotion does Calvin mention feeling when he sees the relief of someone whose car he fixed?",
      "prediction": "Proud",
      "reference": "Proud",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 0.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 0.316227766016838,
        "bleu3": 0.2187761623949553,
        "bleu4": 0.1778279410038923,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.5,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave book a flight ticket for on 1st September 2023?",
      "prediction": "Dave booked a flight ticket to see Calvin",
      "reference": "Boston",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8353517651557922,
        "bert_recall": 0.8428490161895752,
        "bert_f1": 0.8390836715698242,
        "meteor": 0.0,
        "sbert_similarity": 0.024714753031730652
      }
    },
    {
      "sample_id": 9,
      "question": "Which horror movie did Dave mention as one of his favorites?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ratatouille",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8884799480438232,
        "bert_recall": 0.8307465314865112,
        "bert_f1": 0.8586438894271851,
        "meteor": 0.0,
        "sbert_similarity": 0.12494362145662308
      }
    },
    {
      "sample_id": 9,
      "question": "Which song from the childhood of Dave brings back memories of a road trip with his dad?",
      "prediction": "Not mentioned in the conversation",
      "reference": "\"California Love\"",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8864659070968628,
        "bert_recall": 0.8712619543075562,
        "bert_f1": 0.8787981271743774,
        "meteor": 0.0,
        "sbert_similarity": 0.10623804479837418
      }
    },
    {
      "sample_id": 9,
      "question": "What car did Calvin work on in the junkyard?",
      "prediction": "Ford Mustang",
      "reference": "Ford Mustang",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0000001192092896,
        "bert_recall": 1.0000001192092896,
        "bert_f1": 1.0000001192092896,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What does Dave find satisfying about destroying old cars?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Transforming something old and beat-up into something beautiful",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8922281265258789,
        "bert_recall": 0.8555271625518799,
        "bert_f1": 0.8734923005104065,
        "meteor": 0.0,
        "sbert_similarity": -0.0958837941288948
      }
    },
    {
      "sample_id": 9,
      "question": "What does working on boats represent for Dave?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Therapy and a way to get away from everyday stress",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8988227248191833,
        "bert_recall": 0.8723077178001404,
        "bert_f1": 0.8853667378425598,
        "meteor": 0.0,
        "sbert_similarity": -0.017252512276172638
      }
    },
    {
      "sample_id": 9,
      "question": "What does Dave aim to do with his passion for cooking?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Take something broken and make it into something awesome",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8812065720558167,
        "bert_recall": 0.8406394720077515,
        "bert_f1": 0.8604450821876526,
        "meteor": 0.0,
        "sbert_similarity": -0.03803138807415962
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin recently get that is a \"masterpiece on canvas\"?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Ferrari",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8785410523414612,
        "bert_recall": 0.8062957525253296,
        "bert_f1": 0.8408694267272949,
        "meteor": 0.0,
        "sbert_similarity": 0.13934244215488434
      }
    },
    {
      "sample_id": 9,
      "question": "Who headlined the music festival that Calvin attended in October?",
      "prediction": "Not mentioned in the conversation",
      "reference": "The Fireworks",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.2,
        "bleu2": 0.07071067811865477,
        "bleu3": 0.05665132701906204,
        "bleu4": 0.05372849659117709,
        "bert_precision": 0.895659327507019,
        "bert_recall": 0.8753659725189209,
        "bert_f1": 0.885396420955658,
        "meteor": 0.21739130434782608,
        "sbert_similarity": 0.04451312869787216
      }
    },
    {
      "sample_id": 9,
      "question": "Which part of Tokyo is described as Tokyo's Times Square by Dave?",
      "prediction": "Shibuya Crossing",
      "reference": "Shibuya Crossing",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What specific location in Tokyo does Calvin mention being excited to avoid?",
      "prediction": "Shibuya",
      "reference": "Shinjuku",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.9159332513809204,
        "bert_recall": 0.9179298877716064,
        "bert_f1": 0.9169304966926575,
        "meteor": 0.0,
        "sbert_similarity": 0.5881503224372864
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin sell the car he restored last year?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Last year",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8994794487953186,
        "bert_recall": 0.9115267395973206,
        "bert_f1": 0.9054630398750305,
        "meteor": 0.0,
        "sbert_similarity": 0.2383238822221756
      }
    },
    {
      "sample_id": 9,
      "question": "When did Calvin first get interested in motorcycles?",
      "prediction": "Not mentioned in the conversation",
      "reference": "at an early age",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8959712982177734,
        "bert_recall": 0.8885107040405273,
        "bert_f1": 0.892225444316864,
        "meteor": 0.0,
        "sbert_similarity": 0.1688837707042694
      }
    },
    {
      "sample_id": 9,
      "question": "What realization did the nightclub experience bring to Dave?",
      "prediction": "It is like his passion and purpose.",
      "reference": "how much music means to him, it's like his passion and purpose",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.5263157894736842,
        "rouge1_f": 0.6,
        "rouge2_f": 0.4444444444444444,
        "rougeL_f": 0.6,
        "bleu1": 0.354274914555761,
        "bleu2": 0.3092364976715785,
        "bleu3": 0.28412333312786286,
        "bleu4": 0.25558916618229566,
        "bert_precision": 0.9666986465454102,
        "bert_recall": 0.9167498350143433,
        "bert_f1": 0.9410619139671326,
        "meteor": 0.3451086956521739,
        "sbert_similarity": 0.7068841457366943
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave do recently at his Japanese house?",
      "prediction": "Dave shared that he just got back from Japan and is telling Calvin about some of the experiences he had there, including seeing beautiful views, meeting people with similar interests, and enjoying great food. ",
      "reference": "Threw a small party for his new album",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8470109701156616,
        "bert_recall": 0.861648440361023,
        "bert_f1": 0.8542670011520386,
        "meteor": 0.0,
        "sbert_similarity": 0.21491394937038422
      }
    },
    {
      "sample_id": 9,
      "question": "What did Calvin recently start a blog about?",
      "prediction": "Car Mods",
      "reference": "Car mods",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 0.46773514128719823,
        "bleu4": 0.316227766016838,
        "bert_precision": 0.9836099147796631,
        "bert_recall": 0.9836099147796631,
        "bert_f1": 0.9836099147796631,
        "meteor": 0.9375,
        "sbert_similarity": 1.0
      }
    },
    {
      "sample_id": 9,
      "question": "What type of videos does Dave usually watch on his television?",
      "prediction": "Music Videos, Concerts, Documentaries about Artists",
      "reference": "Music videos, concerts, documentaries about artists and their creative process",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.7499999999999999,
        "rouge1_f": 0.7499999999999999,
        "rouge2_f": 0.7142857142857143,
        "rougeL_f": 0.7499999999999999,
        "bleu1": 0.6065306597126334,
        "bleu2": 0.6065306597126334,
        "bleu3": 0.6065306597126334,
        "bleu4": 0.6065306597126334,
        "bert_precision": 0.9571365118026733,
        "bert_recall": 0.9335247874259949,
        "bert_f1": 0.9451832175254822,
        "meteor": 0.6235532407407408,
        "sbert_similarity": 0.9060804843902588
      }
    },
    {
      "sample_id": 9,
      "question": "What type of art has Dave been getting into lately?",
      "prediction": "car",
      "reference": "Classic rock",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8677437901496887,
        "bert_recall": 0.8478270173072815,
        "bert_f1": 0.8576697707176208,
        "meteor": 0.0,
        "sbert_similarity": 0.21080894768238068
      }
    },
    {
      "sample_id": 9,
      "question": "What type of content does Dave post on his blog that inspired others to start their own cooking projects?",
      "prediction": "Not mentioned in the conversation",
      "reference": "How he made his car look like a beast",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.0,
        "bert_recall": 0.0,
        "bert_f1": 0.0,
        "meteor": 0.0,
        "sbert_similarity": -0.000228063203394413
      }
    },
    {
      "sample_id": 9,
      "question": "What kind of impact does Dave's blog on vegan recipes have on people?",
      "prediction": "It inspires others to start their DIY projects",
      "reference": "It inspires others to start their DIY projects",
      "category": 5,
      "metrics": {
        "exact_match": 1,
        "f1": 1.0,
        "rouge1_f": 1.0,
        "rouge2_f": 1.0,
        "rougeL_f": 1.0,
        "bleu1": 1.0,
        "bleu2": 1.0,
        "bleu3": 1.0,
        "bleu4": 1.0,
        "bert_precision": 1.0,
        "bert_recall": 1.0,
        "bert_f1": 1.0,
        "meteor": 0.9990234375,
        "sbert_similarity": 1.0000001192092896
      }
    },
    {
      "sample_id": 9,
      "question": "Who did Dave invite to see him perform in Boston on 13 November, 2023?",
      "prediction": "Frank Ocean",
      "reference": "his old high school buddy",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8264706134796143,
        "bert_recall": 0.8331855535507202,
        "bert_f1": 0.8298145532608032,
        "meteor": 0.0,
        "sbert_similarity": 0.24584442377090454
      }
    },
    {
      "sample_id": 9,
      "question": "What new item did Calvin buy recently?",
      "prediction": "Not mentioned in the conversation",
      "reference": "A vintage camera",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8834429383277893,
        "bert_recall": 0.8809034824371338,
        "bert_f1": 0.8821714520454407,
        "meteor": 0.0,
        "sbert_similarity": 0.023414116352796555
      }
    },
    {
      "sample_id": 9,
      "question": "What type of photos does Calvin like to capture with his new camera?",
      "prediction": "Nature",
      "reference": "Nature - sunsets, beaches, waves",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.33333333333333337,
        "rouge1_f": 0.4,
        "rouge2_f": 0.0,
        "rougeL_f": 0.4,
        "bleu1": 0.0024787521766663585,
        "bleu2": 0.000783850263336577,
        "bleu3": 0.0005422918887392082,
        "bleu4": 0.0004407913958354948,
        "bert_precision": 0.9275166392326355,
        "bert_recall": 0.8404079675674438,
        "bert_f1": 0.8818163275718689,
        "meteor": 0.10869565217391304,
        "sbert_similarity": 0.5190649628639221
      }
    },
    {
      "sample_id": 9,
      "question": "What did Dave discuss with the cool artist he met at the gala?",
      "prediction": "Collaboration on new music",
      "reference": "Music and art",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.28571428571428575,
        "rouge1_f": 0.28571428571428575,
        "rouge2_f": 0.0,
        "rougeL_f": 0.28571428571428575,
        "bleu1": 0.25,
        "bleu2": 0.09128709291752769,
        "bleu3": 0.07665320007195604,
        "bleu4": 0.08034284189446518,
        "bert_precision": 0.8794258832931519,
        "bert_recall": 0.9055171608924866,
        "bert_f1": 0.8922808170318604,
        "meteor": 0.16129032258064518,
        "sbert_similarity": 0.5512104034423828
      }
    },
    {
      "sample_id": 9,
      "question": "Where did Calvin take a stunning photo of a waterfall?",
      "prediction": "Not mentioned in the conversation",
      "reference": "Nearby park",
      "category": 5,
      "metrics": {
        "exact_match": 0,
        "f1": 0.0,
        "rouge1_f": 0.0,
        "rouge2_f": 0.0,
        "rougeL_f": 0.0,
        "bleu1": 0,
        "bleu2": 0,
        "bleu3": 0,
        "bleu4": 0,
        "bert_precision": 0.8781157732009888,
        "bert_recall": 0.8758237361907959,
        "bert_f1": 0.876968264579773,
        "meteor": 0.0,
        "sbert_similarity": 0.1495600789785385
      }
    }
  ]
}