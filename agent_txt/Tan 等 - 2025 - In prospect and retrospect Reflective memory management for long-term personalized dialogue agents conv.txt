

In Prospect and Retrospect: Reﬂective Memory Management for Long-term Personalized Dialogue Agents

Zhen Tan1*	Jun Yan2	I-Hung Hsu2	Rujun Han2	Zifeng Wang2 Long T. Le2	Yiwen Song2        Yanfei Chen2        Hamid Palangi2	George Lee2
Anand Iyer3        Tianlong Chen4        Huan Liu1        Chen-Yu Lee2        Tomas Pﬁster2 1Arizona State University     2Google Cloud AI Research     3Google Cloud AI 4University of North Carolina at Chapel Hill



Abstract

Large Language Models (LLMs) have made signiﬁcant  progress  in  open-ended  dialogue, yet their inability to retain and retrieve rele-vant information from long-term interactions limits their effectiveness in applications requir-ing sustained personalization.  External mem-ory  mechanisms  have  been  proposed  to  ad-dress this limitation, enabling LLMs to main-tain conversational continuity.   However, ex-isting approaches struggle with two key chal-lenges.   First, rigid memory granularity fails to  capture  the  natural  semantic  structure  of conversations,  leading to fragmented and in-complete  representations.    Second,  ﬁxed  re-trieval mechanisms cannot adapt to diverse di-alogue contexts and user interaction patterns. In this work, we propose Reﬂective Memory Management  (RMM),  a  novel  mechanism for  long-term  dialogue  agents,   integrating forward-  and  backward-looking  reﬂections: (1)  Prospective  Reﬂection,  which  dynami-cally summarizes interactions across granular-ities—utterances, turns, and sessions—into a personalized  memory  bank  for  effective  fu-ture retrieval,  and (2) Retrospective Reﬂec-tion, which iteratively reﬁnes the retrieval in an  online  reinforcement  learning  (RL)  man-ner  based  on  LLMs’  cited  evidence.	Ex-periments show that RMM demonstrates con-sistent  improvement  across  various  metrics and benchmarks.  For example, RMM shows more  than  10%  accuracy  improvement  over the baseline without memory management on the LongMemEval dataset.

1	Introduction

Large  Language  Models  (LLMs)  have  demon-strated remarkable capabilities in engaging in open-ended dialogue (Lee et al., 2023; Mendonça et al.,

*This work was done while Zhen Tan was a Research Intern at Google Cloud AI Research. Corresponding authors: ztan36@asu.edu, {junyann, chenyulee}@google.com


Current Dialogue Session (Today)
I now have a headache, and the fever is gone. User
So the fever subsided, the cough persists,
and a headache started. Considering your     Agent
allergy, let's explore …


Relevant History (Yesterday)	Relevant History (A Week Ago)
I have a persistent	…	I am allergic to penicillin.     …
cough and a fever.
Sorry to hear that …	Noted: Penicillin allergy.

User’s Full Dialogue History

Figure 1:  An illustration of a personalized healthcare dialog agent.   Key information about a user’s allergy and previous symptoms mentioned in the past sessions is needed to provide a more informed response in the current session.

2024), yet their inherent statelessness poses a sig-niﬁcant challenge for maintaining coherent, per-sonalized conversations over time (Chen et al., 2024; Li et al., 2024c; Tseng et al., 2024), which are crucial across various real-world applications (e.g., customer service (Kolasani, 2023), virtual assistants (Guan et al., 2024), and education plat-forms (Zhang et al., 2024d; Wen et al., 2024)). As illustrated in Figure 1, effective personaliza-tion requires not only understanding the immedi-ate context but also recalling relevant information from the user’s previous interactions (Williams and Hollan, 1981; Whittaker et al., 2002; Dong et al., 2024). The limitations with current LLMs to natu-rally retain and recall information from past inter-actions beyond their context windows sparked the development of external memory mechanisms for LLMs (Zhang et al., 2024c; Li et al., 2024a; Kim et al., 2024). These memory systems serve as cru-cial components in personalized dialogue agents, enabling them to maintain consistent personality traits, remember user preferences, and build upon previous interactions.
While external memory mechanisms represent a signiﬁcant step towards enabling persistent dia-

8416
Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8416–8439 July 27 - August 1, 2025 ©2025 Association for Computational Linguistics


logue, current approaches suffer from two critical limitations. Firstly, existing systems digest infor-mation at a pre-deﬁned granularity, such as turn, session, or time interval boundaries, which may not align with the inherent semantic structure of the conversation (e.g., topic shifts). This rigid ap-proach can lead to fragmented or incomplete mem-ory representations, hindering the LLM’s ability to retrieve, utilize, and update relevant information effectively (Wu et al., 2024; Pan et al., 2025). Sec-ondly, these systems rely on ﬁxed retrievers (Zhong et al., 2024; Li et al., 2024a), which struggle to adapt to the diverse retrieval demands of varying dialogue domains and individual user interaction patterns.  Moreover, the expense associated with collecting labeled data for training personalized re-trievers presents a substantial barrier to widespread adoption and scalability.
To address these limitations, we propose a novel Reﬂective Memory Management (RMM) mech-anism to provide a more adaptable and granular ap-proach to long-term dialogue memory. Our frame-work incorporates two key innovations. Prospec-tive Reﬂection tackles the issue of ﬁxed granular-ity by summarizing dialogue histories into decom-posed topics, effectively integrating fragmented conversational  segments  into  cohesive  memory structures.  This approach optimizes memory or-ganization for future retrieval, allowing the LLM to access relevant information more effectively re-gardless of the original turn or session boundaries. Complementing this, Retrospective Reﬂection ad-dresses the challenge of ﬁxed retrievers by lever-aging unsupervised attribution signals generated during the LLM’s response generation to reﬂect on past retrieval.   This allows for online reﬁne-ment of the retriever as the conversation progresses, enabling the system to adapt to diverse dialogue domains and individual user interaction patterns without the need for costly labeled data.
By integrating these two reﬂective mechanisms, our approach enables LLMs to maintain a more nu-anced and adaptable memory, leading to more co-herent, personalized, and engaging dialogues. Ex-periments on MSC and LongMemEval benchmarks show that RMM achieves more than 5% improve-ment over the strongest baseline across memory retrieval and response generation metrics.
Our contributions are as follows: (1) We propose RMM as a novel memory management mechanism that employs topic-based memory management op-timized for future retrieval and leverages attribution

signal to reﬂect on past retrieval for unsupervised online retrieval reﬁnement. (2) We conduct exten-sive experiments on two long-term personalized dialogue benchmarks to demonstrate the effective-ness of RMM over strong baselines.  (3) We per-form detailed analysis on the impact of various design choices to pinpoint the limitations of exist-ing memory management mechanisms with ﬁxed granularity and retrievers, shedding light on the room for future improvement.

2	Related Work

Long-term  Conversations  for  LLMs.	LLMs have demonstrated the ability to engage in ex-tended, coherent dialogues, yet maintaining con-text and consistency over long-term interactions remains a challenge. Maharana et al. (2024) intro-duced the LoCoMo dataset to assess LLMs’ perfor-mance in sustained dialogues, showing their strug-gles with long-range temporal and causal under-standing.  Existing solutions can be broadly cat-egorized into two approaches:  (1) Architectural modiﬁcations, such as enhancing attention mecha-nisms (Liu et al., 2024a; Zhang et al., 2024a), op-timizing KV caches (Li et al., 2024d; Liu et al., 2025), and reﬁning position embeddings (Zhao et al., 2024; Zheng et al., 2024).  These methods require white-box access to model internals, mak-ing them infeasible for proprietary or API-based LLMs. (2) Summarization-based methods, which condense long contexts into structured events or topics for direct conditioning or retrieval  (Lu et al., 2023; Wang et al., 2023; Jiang et al., 2024; Li et al., 2024b). RMM falls into this category but explicitly addresses the issue of fragmented topics arising from ﬁxed granularity and incorporates retrospec-tive reﬂection to reﬁne the retrieval process, encour-aging more coherent and contextual responses. Memory-based Personalized Dialogue Agents. The development of memory-based personalized dialogue agents has further enhanced long-term in-teractions by enabling systems to retain and utilize information from past conversations (Bae et al., 2022). Traditional methods (Weizenbaum, 1966; Walker et al., 1997) laid the groundwork for un-derstanding how systems can model user prefer-ences, intentions, and behaviors across sessions, often using handcrafted rules, heuristics, or sym-bolic representations.  Early approaches, such as CoMemNN (Pei et al., 2021), introduce mecha-nisms to incrementally enrich user proﬁles dur-

8417


ing dialogues. However, collecting substantial an-notations for training a personalized system for long-term use is hard (Tseng et al., 2024).   Re-cent advancements focus on integrating LLMs with memory modules (Packer et al., 2024; Chhikara et al., 2025; Wang et al., 2024; Xu et al., 2025; Rasmussen et al., 2025).   For instance, the LD-Agent framework (Li et al., 2024a) employs long-, short-term memory banks to manage conversa-tional history for retrieval. MemoryBank (Zhong et al., 2024) incorporates a memory updating mech-anism inspired by the Ebbinghaus Forgetting Curve, enabling models to retrieve relevant memories con-sidering recency. Theanine (Kim et al., 2024) in-troduces timeline-based retrieval and utilizes an additional LLM for reﬁnement.  These methods typically deploy ﬁxed retrievers with a pre-deﬁned granularity.  In contrast, the proposed RMM ap-proach facilitates adaptive retrieval with a revised retrieval granularity.

3	Problem Formulation

We consider the task of building a personalized dialogue agent in a multi-session conversational setting.  In this setting, an agent interacts with a user across multiple distinct sessions.  A session represents a distinct interaction period, often delim-ited by user inactivity, explicit user conﬁrmation of conversation completion, or the initiation of a new dialogue thread. Within each session, the con-versation unfolds as a sequence of turns, where a turn consists of a user query and the agent’s cor-responding response. The agent is equipped with an external memory, serving as the sole repository for information gathered from previous sessions. The agent’s objective is to generate contextually relevant and personalized responses to user queries, leveraging both the immediate conversational con-text within the current session and the relevant in-formation retrieved from the memory.
This task presents two key challenges: ﬁrst, the agent must proactively identify and store salient information from each session, anticipating future retrieval needs. Second, the agent must accurately retrieve relevant past information from the mem-ory, as incorporating irrelevant context can distract the LLM and degrade response quality (Shi et al., 2023; Liu et al., 2024b). Effectively managing this balance between comprehensive storage and pre-cise retrieval is critical for achieving personalized and coherent multi-session dialogues.

Algorithm 1 Reﬂective Memory Management (RMM) for Dialogue Agents
Input: query q, past messages in current session S, memory bank B, retriever fθ, reranker gφ, LLM
Output: response a, updated S, gφ, B
1:  Retrieve: MK  ← fθ(q,B)
M
2:  Rerank: MM  ← gφ(q,MK), where MM  = {mi}i=1
3:  //  Retrospective  Reflection
4:  Generate:  a,RM   ←  LLM(q,S,MM ) where RM   =
M
{ri}i=1
5:  gφ  ← RL_Update(gφ,RM ) 6:  S.append((q,a))
7:  //  Prospective  Reflection 8:  if session S ends then
9:	M ← ExtractMemory(S) 10:	for m ∈ M do
11:	B ← UpdateMemory(B,m) 12:        end for
13:	S ← [] 14:  end if


4	Framework Overview
To tackle the challenges, we introduce Reﬂective Memory Management (RMM), a framework that integrates two mechanisms.  Prospective Reﬂec-tion proactively decomposes dialogue history into topic-based memory representations, optimized for future retrieval, while Retrospective Reﬂection dy-namically reﬁnes the retrieval mechanism through online feedback signals generated during response generation.  They improve the quality of the re-trieved memories, contributing to personalization.
Our framework comprises four key components. The memory bank stores dialogue history as a collection of memory entries, each represented as a pair (topic summary, raw dialogue), where the “topic summary” serves as the search key for re-trieving the conversational segment. The retriever identiﬁes relevant memories based on the current user query.  To enable lightweight adaptation of the retrieval process, we incorporate a reranker, which reﬁnes the retriever’s initial output by pri-oritizing the most pertinent memories. Finally, an LLM synthesizes the relevant memories with the current context to produce a personalized response. Crucially, the LLM also provides feedback sig-nals based on its utilization of retrieved memories, which are used to reﬁne the reranker through Ret-rospective Reﬂection. Our complete workﬂow is detailed in Algorithm 1.

5	Prospective Reﬂection: Topic-Based Memory Organization

Traditional memory management systems often rely on ﬁxed boundaries, such as session or turn delimiters, to structure dialogue history. However,

8418


Memory Merge	Memory Addition
Finished Dialogue Session


…



Decompose & Summarize

Current Memory Bank
Topic Summary	    Raw Dialogue
User is an undergrad.	… User enjoys hiking.	… User  is  19  years old.	…
Retrieve and update relevant memory (if exists)
Updated Memory Bank Topic Summary                    Raw
Dialogue


a new topic) or merged with an existing memory into an updated one (e.g., when the extracted mem-ory provides updated information to a previously discussed topic).
Through Prospective Reﬂection,  the memory bank maintains a coherent and consolidated rep-resentation of the evolving dialogue history, orga-nized around meaningful topic structures.



Topic Summary	    Raw Dialogue
User likes running.	…
User is allergic to eggs.	…

User is an undergrad.                 … User likes hiking and running.          … User is 19 years old.                   …
User is allergic to eggs.	…


6	Retrospective Reﬂection: Retrieval Reﬁnement via LLM Attribution




Figure 2:  Illustration of Prospective Reﬂection.  After each session, the agent decomposes and summarizes the session into speciﬁc topics. These newly generated memories are compared with existing memories in the memory bank. Relevant memories are  merged , while
others are directly  added .  Prospective reﬂection en-sures efﬁcient organization of personal knowledge for future retrieval.


these pre-deﬁned boundaries may not align with the underlying semantic units of conversation. As a result, critical information may be fragmented across multiple memory entries, hindering effective retrieval. To address this, we introduce Prospective Reﬂection, a mechanism for organizing memory based on coherent topics, enabling more granular and semantically relevant future retrieval.  Here “topic”  refers  to  a  semantically  coherent  unit  of discussion that may span across one or multiple turns in a session. Each topic is associated with the raw dialogue segment(s) in which it was discussed. These topics can range from ﬁne-grained user in-tents (e.g., asking about vegan recipes) to broader themes (e.g., travel planning).   As illustrated in Figure 2, this process occurs at the conclusion of each session and consists of two key steps: memory extraction and memory update.
First,  memory extraction is achieved by us-ing an LLM (prompt in Appendix D.1.1) to ex-tract dialogue snippets from the session with their corresponding summaries based on the distinct mentioned topics.   Second, memory update in-volves integrating the extracted topic-based memo-ries into the memory bank. Speciﬁcally, for each extracted memory, we retrieve the Top-K most se-mantically similar memories already present in the memory bank. Subsequently, an LLM (prompt in Appendix D.1.2) determines whether the extracted memory should be directly added into the memory bank (e.g., when the extracted memory discusses

6.1	Reranker Design
While   an   off-the-shelf   retriever   can   identify semantically-relevant memories, its performance can degrade across diverse dialogue domains and user interaction patterns.  Instead of resorting to computationally expensive ﬁne-tuning of the re-triever, which requires extensive labeled data, we introduce a lightweight reranker to reﬁne the re-trieved memory list. This reranker allows for efﬁ-cient adaptation to the nuances of speciﬁc dialogue domains and user preferences, enabling the system to dynamically adjust its retrieval strategy.
To be speciﬁc, the reranker processes the Top-K memory embeddings retrieved by the retriever, reﬁning their relevance with respect to the user query and selecting the Top-M candidates.  The whole process includes the following steps. Embedding Adaptation. Let q represent the em-bedding of the query and mi represent the embed-ding of the i-th memory entry retrieved by retriever. The embeddings are fed into the reranker to be re-ﬁned via a linear layer with residual connections:


q0 = q + Wqq,	m0  = mi + Wmmi,	(1)
i
where Wq and Wm are linear transformation ma-trices for the query and memory, respectively. Stochastic Sampling with Gumbel Trick.  The adapted query embedding q0 and memory embed-dings m0  are adopted to compute relevance scores via dot product:  si   =  q0>m0.   To select mem-ory entries based on relevance scores, we employ the Gumbel Trick (Gumbel, 1954), which enables stochastic sampling from a discrete probability dis-tribution while preserving gradients, making it par-ticularly useful in reinforcement learning and dif-ferentiable ranking tasks (Jang et al., 2017).  We add Gumbel noise gi (Maddison et al., 2014) to the relevance scores si for each memory entry:
i
i

s˜  = si + gi,	gi = −log(−log(ui)),	(2)
i

8419


where ui  ∼ Uniform(0,1). The perturbed scores s˜  are then normalized using the softmax func-
tion  to  compute  sampling  probabilities:   pi    =
i
P exp(s˜ /τ)/τ), where τ  >  0 is the temperature parameter controlling the sharpness of the distribu-tion.  Lower τ results in more deterministic sam-pling (approaching the maximum of si),  while higher τ increases stochasticity, encouraging ex-ploration.
i
K
j=1
exp(s˜
j
By introducing a reranker, RMM ensures efﬁ-cient retrieval reﬁnement without modifying the re-triever itself, making it adaptable to any pre-trained retrieval model while allowing task-speciﬁc opti-mizations through Reinforcement Learning (RL).

6.2	LLM Attribution as Rewards

Obtaining high-quality user-speciﬁc labeled data for reﬁning the retrieval process is prohibitively expensive. To overcome this challenge, we propose leveraging the inherent capabilities of the LLM generator itself to provide automated feedback on the quality of retrieved memories. Given the user query with context in the current session, and the retrieved memories, we prompt the LLM (prompt in Appendix D.2) to generate both the response and the associated citations to each individual memory in the context (Kenthapadi et al., 2024). This design uses a single LLM call for generating response and LLM attribution, reducing computational overhead. Moreover, the citations are generated conditioned on the response, which has been shown to be more effective compared to prior or post-hoc citations (Buchmann et al., 2024).
Rewards.  As shown in Figure 3, each retrieved memory entry receives either a positive or nega-tive reward based on its citation in the generated response. Speciﬁcally, we assign a reward of +1 (Useful) if the generator cites the memory in the ﬁ-nal response, and −1 (Not Useful) otherwise. This reward assignment reﬂects the utility of each mem-ory entry and allows the reranker to learn better retrieval strategies over time, aligning future se-lections with the generator’s actual usage of re-trieved evidence. We validate its effectiveness in Section 8.3.

6.3	Reranker Update

The reranker is ﬁne-tuned using the REINFORCE algorithm (Williams, 1992) to optimize its rele-vance predictions based on these binary rewards


Memory	Query ⨁
Bank
-1
+1
Retriever                                                Reranker             Top-M    +1 Memory Entries                                               Memory Entries
Top-K
Citation
LLM
Frozen module	Scores Learnable module              RL Update             Response

Figure 3: Illustration of Retrospective Reﬂection.  The Retriever  fetches  Top-K  memory  entries  from  the memory  bank,  which  are  reﬁned  by  the  learnable Reranker  to  select  the  Top-M  most  relevant  entries. These entries are passed to the LLM along with the query to generate the ﬁnal response. The LLM assigns binary citation scores (+1 for useful and −1 for not useful) to the retrieved memory entries based on their utility in the response. These scores are used as reward signals to update the reranker via an RL update, adapt-ing the selection of relevant memory over time.


with the following formulation:

Δφ = η(R−b)∇φ logP(MM|q,MK;φ),  (3)

where R is the reward (+1 or −1), b is a baseline value set as a hyperparameter, and φ denotes the weights of the reranker.

7	Experimental Setup

7.1	Implementation Details
In our experiments, we use Gemini-1.5-Flash as the generator and evaluate Gemini-1.5-Pro in Sec-tion 8.4. We equip RMM with the following dense retrievers with strong semantic representation ca-pabilities and widespread adoption in personalized dialogue systems (Wu et al., 2024).

•  Contriever  (facebook/contriever)  (Izacard et al., 2022):  A dense retriever optimized for semantic search leveraging contrastive learning.

•  Stella (dunzhang/stella_en_1.5B_v5) (Zhang et al., 2024b): A large embedding-based retriever, which is developed based on language models.

•  GTE (Alibaba-NLP/gte-Qwen2-7B-instruct) (Li  et  al.,  2023):	A  retriever  designed  for instruction-following queries, which is trained across a vast, multilingual text corpus spanning diverse domains.

Contriever is used as the default retriever.   Fol-lowing Wu et al. (2024), for experiments without

8420


a reranker, the Top-K  is 5.   Otherwise, the de-fault Top-K is 20 and Top-M is 5.  We explore the impact of retrieval parameters in Appendix 8.7. For LongMemEval, we also consider the results of using an “Oracle” retriver which retrieves the ground-truth turns annotated in the dataset with the necessary personal knowledge to respond to a ques-tion. More implementation and training details are elaborated in Appendix A.

7.2	Datasets and Evaluation Metrics
We experiment on two publicly available bench-mark datasets commonly used for personalized dialogue evaluation:  MSC (Xu et al., 2022) and LongMemEval (Wu et al., 2024). Additional details about datasets can be found in Appendix B.
For MSC, the evaluation measures if the gener-ated response matches the human-provided ground truth.   We follow Li et al. (2024a) to use ME-TEOR (Banerjee and Lavie, 2005) for measuring lexical similarity and BERTScore (Zhang et al., 2020) for measuring semantic similarity. We also provide LLM judge results in Appendix 8.8.
For LongMemEval, we follow the original paper to use Recall@K to evaluate the model’s ability to retrieve relevant information for the query from conversation histories and use an LLM judge to measure the Accuracy of the generated answer by comparing it to the human-provided ground truth using Gemini-1.5-Pro. The prompt is presented in Appendix D.3.

7.3	Compared Methods
To benchmark the performance of RMM, we com-pare it against the following baselines which repre-sent different strategies for managing and retrieving long-term conversational memories, allowing for a comprehensive comparison with RMM.

•  No History: No history session is used.
•  Long Context: This method directly incorporate as much conversation history as possible into the context window. Older turns are truncated.
•  RAG: These models retrieve relevant turns or sessions for a given user query, concatenate them with the query, and feed the resulting input to the LLM for response generation. We use turns as the default granularity for better performance.
•  Personalized Dialogue Agents:   We consider two agent systems:  (1) MemoryBank (Zhong et al., 2024) treats conversation history as a ﬁxed

database and modulates retrieval using heuris-tics  based  on  the  forgetting  curve.	(2)  LD-Agent (Li et al., 2024a) employs ﬁxed conver-sation databases with additional retrieval modula-tion using strategies such as keywords matching.

8	Experimental Results

8.1	Main Results
We present the main results shown in Table 1 and analyze each method’s performance as follow. History matters: Without any history, the LLM performs poorly, achieving a METEOR score of 5.2% on MSC and 0.0% accuracy on LongMemEval, showing the necessity of historical context.
Long context is not enough: Long-context mod-els struggle due to ﬁxed context windows and the inclusion of noisy context. On MSC, scores remain low (e.g., METEOR below 20%, BERT score un-der 40%), and on LongMemEval, accuracy is lower than 58%. This limitation highlights their inability to retain and utilize long-term knowledge.
RAG Models:  RAG models outperform Long-Context LLMs by only incorporating relevant his-tories.	With  strong  retrievers  like  GTE,  RAG achieves  27.5%  METEOR  and  52.1%  BERT Scores on MSC and 62.4% recall and 63.6% accu-racy on LongMemEval.  We also observe that the performance is retriever-dependent, where stronger retrievers boost the performance.
Personalized  Dialogue  Agents:   MemoryBank and LD-Agent show more moderate improvements over Long-Context LLMs. For instance, LD-Agent achieves 25.4% METEOR and 51.5% BERT score on MSC, but these models fall short of RAG and RMM. Their reliance on heuristic-based retrieval potentially limits adaptability to complex tasks. Proposed RMM Framework: RMM consistently achieves the best results across datasets and metrics. With GTE, RMM achieves 33.4% METEOR and 57.1% BERT on MSC, and 69.8% recall and 70.4% accuracy on LongMemEval. Even with weaker re-trievers like Contriever, RMM maintains competi-tive performance, demonstrating robustness. The improvements stem from RMM’s ability to inte-grate dynamic memory management with adaptive retrieval optimization enables it to retrieve and uti-lize relevant knowledge effectively, outperforming all baselines.
To further assess the impact of memory integra-tion, we calculate the proportion of test examples where memory improves response quality. On MSC,

8421


Method	Retriever	MSC
METEOR (%) ↑    BERT (%) ↑

LongMemEval
Recall@5 (%) ↑    Acc. (%) ↑



No History Long Context

RAG

MemoryBank LD-Agent

RMM (Ours)

RAG

-                           5.2 -                         14.8
Contriever	24.8 Stella	26.2 GTE	27.5
Speciﬁc1                  20.1 Speciﬁc2                  25.4
Contriever	30.8 Stella	31.9 GTE	33.4

Oracle	-

10.6                           -                           0.0 31.9                           -                         57.4
50.8	54.3	58.8 51.6	59.2	61.4 52.1	62.4	63.6
40.3                        58.6                      59.6 51.5                        56.8                      59.2
55.4	60.4	61.2 56.3	65.9	64.8 57.1	69.8	70.4

-	100.0	90.2


Table 1: Performance comparison of RMM with baseline methods on the MSC and LongMemEval datasets. Metrics include METEOR and BERT Scores for MSC, and Recall@5 and Accuracy (Acc.) scores for LongMemEval. RMM demonstrates superior performance across all metrics, highlighting its effectiveness in retrieval relevance and per-sonalized response generation.  No oracle retrieval is available for the MSC dataset.  MemoryBank and LD-Agent utilize their speciﬁc methods for retrieval. Scores are averaged over 3 runs and are reported in percentage (%).



Variant		MSC	LongMemEval METEOR     BERT     Recall@5     Acc.
RAG	24.8	50.8	54.3	58.8
+ PR	28.6	53.3	57.4	59.6 + RR (W/O reranker)	20.3	31.8	34.2	31.0 + RR	27.5	52.2	58.8	60.2
RMM	30.8	55.4	60.4	61.2

Table 2: Ablation study on the datasets. Variants evalu-ate the impact of key components in RMM: Prospective Reﬂection (PR), Retrospective Reﬂection (RR), and the reranker. RR (W/O reranker) means the retriever is ﬁne-tuned instead. Scores are obtained with Contriever and Gemini-1.5-Flash and in percentage (%).


memory improves on 86% of responses, as the dataset frequently requires recalling prior discus-sion topics. On LongMemEval, where questions are deliberately designed to test historical recall, mem-ory contributes to quality improvements in 100% of cases. These results show the necessity of memory mechanisms in maintaining long-term coherence. We provide case studies of the memory usage in Appendix C.

8.2	Ablation Study

We conduct ablation study to evaluate the contribu-tions of key components in the RMM framework. we present the results in Table 2 and list our obser-vations as below.
(i) Adding Prospective Reﬂection boosts perfor-mance by organizing the memory into structured

topics, which reduces redundancy and improves rel-evance. (ii) Retrospective Reﬂection alone without a reranker misaligns retrieved content, leading to suboptimal results. Directly updating the retriever using RL rewards requires extensive amounts of training data for effective full ﬁne-tuning, which is often difﬁcult to obtain in real-world scenar-ios.  Without sufﬁcient data, it can lead to issues like catastrophic forgetting (McCloskey and Cohen, 1989). (iii) The addition of the reranker alongside RR signiﬁcantly enhances alignment, achieving 27.5% METEOR and 58.8% Recall@5, demon-strating its effectiveness in reﬁning retrieval quality. (iv) Finally, the complete RMM framework, which integrates Prospective Reﬂection, Retrospective Re-ﬂection, and the reranker, achieves the best results across all metrics, with a METEOR score of 30.8% on  MSC  and  60.4%  Recall@5  on  LongMemEval. This conﬁrms that RMM enables more accurate and efﬁcient future retrieval.

8.3	Validation of Citation Scores
Our framework leverages LLM-generated citations to determine reward scores, guiding the retrieval reﬁnement  process.    To  assess  the  validity  of the citation scores, we conduct evaluation on the LongMemEval dataset, using the Gemini-1.5-Pro model as the judge. The experiment tasks the LLM with determining whether cited memories were use-ful for response generation. The results, presented in Table 3, demonstrate high precision, recall, and F1, conﬁrming the effectiveness of citation-based

8422


Metric
Useful memory Not useful memory

Precision
89.4 87.2

Recall	F1
91.1       90.2 84.6       85.9


turn                     session Passage Retrieval
90	86 80                                                  78
50
Recall@5 (%)
70	69


mix		PR	best Question Answering
60	58
50	49



Overall	87.6	85.8	86.7

Table 3: Evaluation of citation-based scoring in RR for useful memory identiﬁcation on LongMemEval (results in %).
MSC	LongMemEval METEOR    BERT            Acc.
Method	LLM
Long       Gemini-1.5-Flash          14.8            31.9               57.4 Context      Gemini-1.5-Pro            17.4            36.1               56.6
Gemini-1.5-Flash          30.8            55.4               61.2 Gemini-1.5-Pro            24.6            50.6               58.6
RMM

Table  4:	Effect  of  different  LLMs  on  MSC  and LongMemEval.  Results (in %) compare Long-Context LLMs and RMM using the Contriever retriever with Gemini models as generators.


scoring in our framework.

8.4	Effect of Different LLMs
To examine the effect of different LLMs as gen-erators, we evaluate both Gemini-1.5-Flash and Gemini-1.5-Pro in Long-Context LLMs and RMM. As  shown  in  Table  4,  for  Long-Context  mod-els, Gemini-1.5-Pro achieves slightly better per-formance than Gemini-1.5-Flash across all metrics, suggesting that a stronger model improves response quality when relying solely on extended context windows. However, for RMM, Gemini-1.5-Flash outperforms Gemini-1.5-Pro, achieving higher ME-TEOR and BERT scores on MSC and better accu-racy on LongMemEval.  Similar observations are reported by Wu et al. (2024), where GPT-4o-mini performs better than GPT-4o in personal knowl-edge QA. This trend can be attributed to stronger LLMs, such as Gemini-1.5-Pro, being more likely to abstain from answering queries involving per-sonal information, possibly due to stronger align-ment tuning aimed at enhancing privacy protection.

8.5	Effect of Different Granularities
We conduct experiments to show the advantage of the ﬂexible granularity resulting from the pro-posed Prospective Reﬂection (PR) over pre-deﬁned ﬁxed granularities as baselines.   Results in Fig-ure 4 show that ﬁxed granularities, such as “turn” and “session”, achieve moderate performance, with session-level retrieval outperforming turn-level due to richer contexts.  The “mixed” granularity un-derperforms, likely due to increased noise from

60	40	34
40      47	38	30      29
Accuracy (%)
30	20	17
20    turn session  mix       PR      best	10    turn session  mix       PR      best

Figure 4:  Granularity analysis on randomly sampled 100  instances  from  LongMemEval  with  the  GTE  re-triever and Gemini-1.5-Flash generator.   “Turn” and “Session” indicate retrieval at a ﬁxed granularity. “Mix” represents retrieving from a pool combining both turns and sessions.  “PR” refers to the granularity resulting from the proposed Prospective Reﬂection, while “Best” corresponds to selecting the optimal granularity (either turn or session) for each instance.

W/O Offline	W/ Offline
Passage Retrieval	Question Answering
90	+7	+5	60	+6	+2
80	+8	50
Recall@5 (%)
Accuracy (%)
70
60	+7	40	+4
+3
50	+11	30
40	+6
30	20
20	10
turn session mix       PR      best	turn session mix       PR      best

Figure 5: Impact of ofﬂine pretraining on retriever per-formance for LongMemEval dataset with the same 100 random samples as Figure 4.  Results without ofﬂine pretraining are shown in blue, while results with ofﬂine pretraining are shown in orange. Ofﬂine pretraining im-proves recall and accuracy across all settings.


a  larger  search  space.	The  best  conﬁguration, which selects the optimal granularity per instance, achieves the highest scores, demonstrating the im-portance of adaptive memory organization.   In contrast, PR improves performance by integrating fragmented conversational segments into cohesive memory structure, exhibiting an approaching per-formance with the best oracle granularity.

8.6	Ofﬂine Supervised Training
We further investigate the applicability of RMM in scenarios where a handful of labelled retrieval data is available, allowing for ofﬂine supervised pretraining (based on the off-the-shelf retriever) before online reﬁnement. Figure 5 illustrates the impact of ofﬂine pretraining on retriever perfor-mance on the LongMemEval dataset. We randomly select 100 samples as test data with the rest as training and validation sets and apply vanilla super-vised contrastive learning for the GTE retriever (Li et al., 2023). As the results show, across all settings, RMM consistently beneﬁts from ofﬂine pretraining (orange bars) by outperforming retrievers without

8423


pretraining (blue bars). These results show that of-ﬂine pretraining can enhance the retriever’s ability to identify relevant information, providing a robust foundation for subsequent ﬁne-tuning via RL.

8.7	The Impact of Top-K and Top-M for RMM
The  results  in  Table  6  evaluate  the  impact  of the number of retrieved memories (Top-K) and the  number of  reranked  memories  used  for re-sponse generation (Top-M) in the RMM frame-work. Speciﬁcally, we analyze the performance on LongMemEval using Recall@5 (Top-K = 20, Top-M = 5), Recall@10 (Top-K = 50, Top-M = 10), and their corresponding QA accuracy scores.
The results demonstrate two key ﬁndings. First, increasing the number of memories (M) from 5 to 10 consistently improves both retrieval and accu-racy metrics across all retrievers. For example, with the GTE retriever, Recall improves from 69.8% to 74.4%, and Accuracy increases from 70.4% to 73.8%. Second, the performance gain is most sig-niﬁcant for stronger retrievers like GTE and Stella, highlighting the importance of retrieval quality. RMM with GTE achieves the best results of 70.4% Accuracy with Top-K = 20, Top-M = 5 and 73.8% Accuracy with Top-K = 50, Top-M = 10.
These observations emphasize that careful se-lection of Top-K and Top-M values can enhance both retrieval relevance and downstream QA per-formance. The combination of effective retrieval and reranking ensures that RMM efﬁciently lever-ages the most relevant information for long-term dialogue tasks.

8.8	Results for MSC with LLM-as-a-judge
For fair comparison, we follow prior work (Xu et al., 2022; Wu et al., 2024) and use METEOR and BERTScore.  Here we include additional re-sults using LLM-as-a-judge. Following Wu et al. (2024), we use Gemini-1.5-Pro to decide whether the generated answer matches the ground-truth as a binary annotation. The prompt we used in given in Appendix D.3. LLM-as-a-judge results also show the effectiveness of the proposed RMM.
Furthermore, we conducted a human evaluation on 100 randomly sampled instances from the MSC dataset. For each instance, annotators were shown the user query, the ground-truth response, and the model-generated response—mirroring the setup used for the LLM-based evaluations. We adopted the same instruction used in our LLM judge prompt,


Method	LLM	METEOR    BERT    LLM-as-a-Judge
(Yes%)

Long       Gemini-1.5-Flash          14.8            31.9                  25.4 Context      Gemini-1.5-Pro            17.4            36.1                  22.8
Gemini-1.5-Flash          30.8            55.4                  69.7 Gemini-1.5-Pro            24.6            50.6                  65.4
RMM

Table 5: Results for MSC with LLM-as-a-judge. RMM shows  consistent  advantages  with  more  ﬁne-grained evaluation.

Model	Retriever	LongMemEval
Recall@5    Acc.     Recall@10    Acc.
Contriever	60.4	61.2	67.2	66.8 RMM	Stella	65.9	64.8	70.6	71.0
GTE	69.8	70.4	74.4	73.8

Table 6:  Impact of Top-K  (retrieved memories) and Top-M  (reranked memories) on LongMemEval perfor-mance.  Results include Recall@5 (Top-K = 20, Top-M = 5) and Recall@10 (Top-K = 50, Top-M = 10), and corresponding Accuracy across different retrievers. Results show that increasing the number of retrieved and reranked memories improves retrieval and QA per-formance on the LongMemEval dataset.

Pair	Cohen’s Kappa (κ)	Interpretation
Human A vs. Human B	0.82	Substantial agreement LLM vs. Human A	0.71	Substantial agreement LLM vs. Human B	0.69	Substantial agreement

Table 7: Cohen’s Kappa Agreement Between Annota-tors and LLM

and asked human annotators to answer Yes/No to evaluate each instance.   The annotation was in-dependently performed by two NLP researchers. Both are PhD students working in NLP and not on the author list. We report inter-annotator agreement and agreement between human judgments and the LLM judge in Table 7.

9	Conclusion

We present RMM, a framework that integrates Prospective Reﬂection for structured, topic-based memory organization and Retrospective Reﬂec-tion for dynamic memory reranking via reinforce-ment learning. Experimental results on benchmark datasets demonstrate that RMM outperforms state-of-the-art baselines in retrieval relevance and re-sponse quality for personalized dialogue tasks. By identifying limitations in existing memory man-agement approaches—particularly those relying on ﬁxed granularity and static retrievers, we highlight key challenges and avenues for future research in long-term dialogue memory modeling.

8424


Limitations

While the proposed RMM framework demonstrates signiﬁcant improvements in retrieval relevance and response quality, it is not without limitations. First, RMM relies on reinforcement learning for memory reranking, which can be computationally expensive, especially for large-scale datasets or real-time appli-cations. Second, the current framework primarily focuses on textual data, limiting its applicability to multi-modal dialogue systems that incorporate images, audio, or video. Additionally, the memory updating mechanism may require further optimiza-tion to handle dynamically evolving long-term user interactions efﬁciently.
For  future  work,  we  plan  to  address  these limitations by exploring more efﬁcient reinforce-ment learning techniques and lightweight memory reranking strategies. We also aim to extend RMM to multi-modal dialogue systems to accommodate diverse user interactions. Furthermore, we will in-vestigate privacy-preserving techniques to ensure safe deployment of RMM in real-world personal-ized dialogue applications where sensitive user data is involved.


Ethical Statement

This work focuses on developing a framework for long-term personalized dialogue systems to im-prove user experiences. However, we acknowledge the potential ethical implications of handling per-sonal data in such systems. The RMM framework relies on historical conversations, which may con-tain sensitive or private information. To mitigate privacy risks, we recommend adopting robust en-cryption and privacy-preserving methods, such as differential privacy or federated learning, during data collection and model training.
Additionally, we emphasize the importance of transparent data usage policies and obtaining user consent when deploying personalized dialogue sys-tems.  Efforts should also be made to minimize biases in memory retrieval and response generation to ensure fairness and inclusivity across diverse user groups. Future work will continue to prioritize ethical considerations to promote the responsible development and deployment of personalized dia-logue technologies.

References
Sanghwan  Bae,   Donghyun  Kwak,   Soyoung  Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, Sang-Woo Lee, Woomyoung Park, and Nako Sung. 2022.   Keep me updated!  memory manage-ment in long-term conversations. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3769–3787, Abu Dhabi, United Arab Emirates.  Association  for  Computational  Linguis-tics.

Satanjeev Banerjee and Alon Lavie. 2005.  METEOR: An  automatic  metric  for  MT  evaluation  with  im-proved correlation with human judgments.  In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65–72, Ann Ar-bor, Michigan. Association for Computational Lin-guistics.

Jan Buchmann, Xiao Liu, and Iryna Gurevych. 2024. Attribute  or  abstain:    Large  language  models  as long document  assistants.    In Proceedings of the 2024  Conference  on  Empirical  Methods  in  Natu-ral Language Processing, pages 8113–8140, Miami, Florida, USA. Association for Computational Lin-guistics.

Jin  Chen,  Zheng  Liu,  Xu  Huang,  Chenwang  Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xi-aolong Chen, Xingmei Wang, Kai Zheng, Defu Lian, and Enhong Chen. 2024. When large language mod-els meet personalization: perspectives of challenges and opportunities. World Wide Web, 27(4).

Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025.  Mem0:  Building production-ready ai agents with scalable long-term memory. Preprint, arXiv:2504.19413.

Yijiang River Dong, Tiancheng Hu, and Nigel Collier. 2024.  Can LLM be a personalized judge?  In Find-ings of the Association for Computational Linguis-tics:   EMNLP  2024,  pages  10126–10141,  Miami, Florida, USA. Association for Computational Lin-guistics.

Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, and Chenyi Zhuang. 2024. Intelligent  agents  with  llm-based  process  automa-tion. In Proceedings of the 30th ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining, KDD ’24, page 5018–5027, New York, NY, USA. Association for Computing Machinery.

E.J. Gumbel. 1954.  Statistical Theory of Extreme Val-ues and Some Practical Applications:  A Series of Lectures. Applied mathematics series. U.S. Govern-ment Printing Ofﬁce.

Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-bastian Riedel,  Piotr Bojanowski,  Armand Joulin, and Edouard Grave. 2022.  Unsupervised dense in-formation retrieval with contrastive learning. Trans-actions on Machine Learning Research.


8425


Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categor-ical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representa-tions, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.

Zhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang Zhang. 2024.  Retrieve, summarize, plan:  Advanc-ing multi-hop question answering with an iterative approach. ArXiv preprint, abs/2407.13101.

Krishnaram   Kenthapadi,   Mehrnoosh   Sameki,   and Ankur Taly. 2024.    Grounding and evaluation for large  language  models:   Practical  challenges  and lessons learned (survey). In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’24, page 6523–6533, New York, NY, USA. Association for Computing Machin-ery.

Seo Hyun Kim,  Kai Tzu-iunn Ong,  Taeyoon Kwon, Namyoung Kim, Keummin Ka, SeongHyeon Bae, Yohan  Jo,  Seung-won  Hwang,  Dongha  Lee,  and Jinyoung Yeo. 2024.    Theanine:  Revisiting mem-ory  management  in  long-term  conversations  with timeline-augmented  response  generation.      ArXiv preprint, abs/2406.10996.

Saydulu Kolasani. 2023.  Optimizing natural language processing,  large  language  models  (llms)  for  efﬁ-cient customer service, and hyper-personalization to enable sustainable growth and revenue.    Transac-tions on Latest Trends in Artiﬁcial Intelligence, 4(4).

Gibbeum Lee,  Volker Hartmann,  Jongho Park,  Dim-itris  Papailiopoulos,   and  Kangwook  Lee.  2023. Prompted LLMs as chatbot modules for long open-domain conversation. In Findings of the Association for  Computational  Linguistics:   ACL  2023,  pages 4536–4554, Toronto, Canada. Association for Com-putational Linguistics.

Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2024a. Hello again! llm-powered personalized agent for long-term dialogue. ArXiv preprint, abs/2406.05925.

Huayang Li,  Pat Verga,  Priyanka Sen,  Bowen Yang,
Vijay Viswanathan, Patrick Lewis, Taro Watanabe,
and  Yixuan  Su.  2024b.	Alr2:   A  retrieve-then-reason framework for long-context question answer-ing. ArXiv preprint, abs/2410.03227.

Yuanchun Li,  Hao Wen,  Weijun Wang,  Xiangyu Li, Yizhen Yuan,  Guohong Liu,  Jiacheng Liu,  Wenx-ing Xu, Xiang Wang, Yi Sun, et al. 2024c.   Per-sonal llm agents:  Insights and survey about the ca-pability,  efﬁciency  and  security.    ArXiv preprint, abs/2401.05459.

Yucheng  Li,  Huiqiang  Jiang,  Qianhui  Wu,  Xufang Luo,  Surin  Ahn,  Chengruidong  Zhang,  Amir  H Abdi,  Dongsheng Li,  Jianfeng Gao,  Yuqing Yang, et al. 2024d.    Scbench:   A kv cache-centric anal-ysis  of  long-context  methods.      ArXiv  preprint, abs/2412.10319.

Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun  Xie,   and  Meishan  Zhang.  2023.	To-wards general text embeddings with multi-stage con-trastive learning. ArXiv preprint, abs/2308.03281.

Hao  Liu,  Matei  Zaharia,  and  Pieter  Abbeel.  2024a. Ringattention with blockwise transformers for near-inﬁnite context.  In The Twelfth International Con-ference on Learning Representations.

Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b.   Lost in the middle:  How language models use long contexts. Transactions of the Asso-ciation for Computational Linguistics, 12:157–173.

Xiang  Liu,  Zhenheng  Tang,  Peijie  Dong,  Zeyu  Li, Bo  Li,   Xuming  Hu,   and  Xiaowen  Chu.  2025. Chunkkv:  Semantic-preserving kv cache compres-sion for efﬁcient long-context llm inference.  ArXiv preprint, abs/2502.00299.

Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yu-lan He, Di Yin, Xing Sun, and Yunsheng Wu. 2023. Memochat:  Tuning llms to use memos for consis-tent long-range open-domain conversation.   ArXiv preprint, abs/2308.08239.

Chris  J  Maddison,  Daniel  Tarlow,  and  Tom  Minka. 2014.  A* sampling.  In Advances in Neural Infor-mation Processing Systems, volume 27. Curran As-sociates, Inc.

Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024.	Evaluating  very  long-term  conversational memory  of  LLM  agents.    In  Proceedings  of  the 62nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages 13851–13870,  Bangkok,  Thailand.  Association  for Computational Linguistics.

Michael McCloskey and Neal J. Cohen. 1989.  Catas-trophic interference in connectionist networks: The sequential  learning  problem.    volume  24  of  Psy-chology of Learning and Motivation, pages 109–165. Academic Press.

John  Mendonça,  Alon  Lavie,  and  Isabel  Trancoso. 2024.	On  the  benchmarking  of  LLMs  for  open-domain  dialogue  evaluation.     In  Proceedings  of the  6th  Workshop  on  NLP  for  Conversational  AI (NLP4ConvAI  2024),  pages  1–12,  Bangkok,  Thai-land. Association for Computational Linguistics.

Charles  Packer,  Sarah  Wooders,  Kevin  Lin,  Vivian Fang,  Shishir G. Patil,  Ion Stoica,  and Joseph E. Gonzalez. 2024.   Memgpt:  Towards llms as oper-ating systems. Preprint, arXiv:2310.08560.

Zhuoshi  Pan,  Qianhui  Wu,  Huiqiang  Jiang,  Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew  Lin,  H.  Vicky  Zhao,  Lili  Qiu,  and  Jianfeng Gao. 2025.   Secom:  On memory construction and retrieval for personalized conversational agents.  In The Thirteenth International Conference on Learn-ing Representations.


8426


Jiahuan Pei, Pengjie Ren, and Maarten de Rijke. 2021. A  cooperative  memory  network  for  personalized task-oriented dialogue systems with incomplete user proﬁles.  In WWW ’21: The Web Conference 2021, Virtual  Event  /  Ljubljana,  Slovenia,  April  19-23, 2021, pages 1552–1561. ACM / IW3C2.

Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. 2025. Zep: A tempo-ral knowledge graph architecture for agent memory. Preprint, arXiv:2501.13956.

Freda  Shi,  Xinyun  Chen,  Kanishka  Misra,  Nathan Scales, David Dohan, Ed H. Chi, Nathanael Schärli, and Denny Zhou. 2023. Large language models can be easily distracted by irrelevant context.  In Inter-national Conference on Machine Learning,  ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, vol-ume 202 of Proceedings of Machine Learning Re-search, pages 31210–31227. PMLR.

Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen,  Chao-Wei Huang,  Yu Meng,  and Yun-Nung Chen. 2024.  Two tales of persona in LLMs: A survey of role-playing and personalization.    In Findings of the Association for Computational Lin-guistics: EMNLP 2024, pages 16612–16631, Miami, Florida, USA. Association for Computational Lin-guistics.

Marilyn  A.  Walker,  Diane  J.  Litman,  Candace  A. Kamm,  and Alicia Abella. 1997.   PARADISE: A framework  for  evaluating  spoken  dialogue  agents. In 35th Annual Meeting of the Association for Com-putational  Linguistics  and  8th  Conference  of  the European Chapter of the Association for Computa-tional Linguistics,  pages 271–280,  Madrid,  Spain. Association for Computational Linguistics.

Qingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. 2023.   Re-cursively summarizing enables long-term dialogue memory in large language models.  ArXiv preprint, abs/2308.15022.

Zora Zhiruo Wang,  Jiayuan Mao,  Daniel Fried,  and Graham Neubig. 2024.   Agent workﬂow memory. Preprint, arXiv:2409.07429.

Joseph Weizenbaum. 1966.    Eliza—a computer pro-gram for the study of natural language communi-cation between man and machine.  Commun. ACM, 9(1):36–45.

Qingsong Wen, Jing Liang, Carles Sierra, Rose Luckin, Richard Tong, Zitao Liu, Peng Cui, and Jiliang Tang. 2024. Ai for education (ai4edu): Advancing person-alized education with llm and adaptive learning.  In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’24,  page  6743–6744,  New  York,  NY,  USA.  Asso-ciation for Computing Machinery.

S. Whittaker, Q. Jones, and L. Terveen. 2002.   Man-aging long term communications: conversation and contact management.   In Proceedings of the 35th

Annual Hawaii International Conference on System Sciences, pages 1070–1079.

Michael David Williams and James D. Hollan. 1981. The process of retrieval from very long-term mem-ory. Cognitive Science, 5(2):87–119.

Ronald J. Williams. 1992.  Simple statistical gradient-following  algorithms  for  connectionist  reinforce-ment learning. Mach. Learn., 8(3–4):229–256.

Di Wu,  Hongwei Wang,  Wenhao Yu,  Yuwei Zhang, Kai-Wei Chang, and Dong Yu. 2024. Longmemeval: Benchmarking chat assistants on long-term interac-tive memory. ArXiv preprint, abs/2410.10813.

Jing Xu, Arthur Szlam, and Jason Weston. 2022.  Be-yond  goldﬁsh  memory:   Long-term  open-domain conversation.    In Proceedings of the 60th Annual Meeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 5180–5197, Dublin, Ireland. Association for Computational Lin-guistics.

Wujiang   Xu,   Kai   Mei,   Hang   Gao,   Juntao   Tan, Zujie  Liang,   and  Yongfeng  Zhang.  2025.	A-mem:  Agentic memory for llm agents.    Preprint, arXiv:2502.12110.

Chiyu Zhang, Yifei Sun, Jun Chen, Jie Lei, Muhammad Abdul-Mageed, Sinong Wang, Rong Jin, Sem Park, Ning Yao, and Bo Long. 2024a. Spar: Personalized content-based  recommendation  via  long  engage-ment attention. ArXiv preprint, abs/2402.10555.

Dun  Zhang,   Jiacheng  Li,   Ziyang  Zeng,   and  Fu-long  Wang.  2024b.	Jasper  and  stella:    distilla-tion  of  sota  embedding  models.    ArXiv  preprint, abs/2412.19048.

Tianyi  Zhang,  Varsha  Kishore,  Felix  Wu,  Kilian  Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-uating  text  generation  with  BERT.    In 8th Inter-national Conference on Learning Representations, ICLR  2020,  Addis  Ababa,  Ethiopia,  April  26-30, 2020. OpenReview.net.

Zhehao Zhang, Ryan A Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernon-court, Joe Barrow, Tong Yu, Sungchul Kim, et al. 2024c. Personalization of large language models: A survey. ArXiv preprint, abs/2411.00027.

Zheyuan  Zhang,  Daniel  Zhang-Li,  Jifan  Yu,  Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, and Juanzi  Li.  2024d.	Simulating  classroom  educa-tion with llm-empowered agents.    ArXiv preprint, abs/2406.19226.

Liang Zhao,  Xiachong Feng,  Xiaocheng Feng,  Wei-hong Zhong, Dongliang Xu, Qing Yang, Hongtao Liu, Bing Qin, and Ting Liu. 2024.  Length extrap-olation of transformers: A survey from the perspec-tive of positional encoding.   In Findings of the As-sociation for Computational Linguistics:  EMNLP 2024, pages 9959–9977, Miami, Florida, USA. As-sociation for Computational Linguistics.


8427

Chuanyang  Zheng,  Yihang  Gao,  Han  Shi,  Minbin Huang,   Jingyao  Li,   Jing  Xiong,   Xiaozhe  Ren, Michael  Ng,  Xin  Jiang,  Zhenguo  Li,  and  Yu  Li. 2024.	Dape:   Data-adaptive  positional  encoding for  length  extrapolation.    In  Advances  in  Neural Information Processing Systems, volume 37, pages 26659–26700. Curran Associates, Inc.

Wanjun  Zhong,  Lianghong  Guo,  Qiqi  Gao,  He  Ye, and Yanlin Wang. 2024.  Memorybank: Enhancing large language models with long-term memory.  In Thirty-Eighth AAAI Conference on Artiﬁcial Intelli-gence, AAAI 2024, Thirty-Sixth Conference on In-novative Applications of Artiﬁcial Intelligence, IAAI 2024,  Fourteenth  Symposium  on  Educational  Ad-vances in Artiﬁcial Intelligence, EAAI 2014, Febru-ary 20-27, 2024, Vancouver, Canada, pages 19724– 19731. AAAI Press.











































8428

A	Implementation and Training Details	0.5	Increase in Usefulness Score Over Time Usefulness Score
Usefulness Score (Ratio of Useful Memories)
A.1	Parameter Setup	0.4

We use the following hyper-parameters for all ex-periments:
•  Reranker: The reranker is an MLP with a resid-ual connection. The training setup is:
–  Batch size: 4 –  Top-M: 5


0.3

0.2

0.1

0.0       0	200







400		600	800	1000 Steps



–  Top-K: 20
•  Reinforcement  Learning:   Retrospective  Re-ﬂection uses REINFORCE with:
–  Batch size: 4
–  Gumbel temperature (τ): 0.5


Figure 6:  Convergence of usefulness scores (ratio of useful memories cited) over RL training steps.   The score improves as the reranker is updated based on Ret-rospective Reﬂection,  indicating enhanced alignment between retrieved memory and generated responses.



–  Reward (R): +1 for cited entries, −1 for non-cited entries
–  Baseline value (b): 0.5


A.4	 Details for MemoryBank and LD-Agent Baselines



–  Learning rate for policy gradient updates (η): 1 × 10−3

•  LLM:  Gemini-1.5-Flash/-Pro  is  used  for  re-sponse generation with:
–  Context window size: 128k tokens –  Temperature: 0.0
•  Retriever: GTE for experiments in Section 8.6 is pretrained with supervised contrastive learning using the following conﬁguration:
–  Learning rate: 1 × 10−4 –  Training epochs: 10
–  Batch size: 32 –  Top-K: 5
A.2	Dependencies
Our implementation relies on the following tools and libraries:
•  Programming Language: Python 3.10.13
•  Core Libraries: PyTorch 2.4.1+cu121, Hugging Face Transformers 4.44.2
•  Utilities:   NumPy,  Pandas,  Sklearn  and  Mat-plotlib for data processing and visualization
A.3	Hardware and Reproducibility
All experiments are conducted on a server with the following hardware conﬁguration:
•  GPUs: 16 NVIDIA A100 GPUs •  RAM: 40 GB
•  CUDA Version: 12.2


We integrate MemoryBank and LD-Agent as base-lines,  with key features implemented using the LongMemEval codebase1. We use Contriever as the default retriever. Particularly, they differ in the way for structuring and accessing stored informa-tion.
MemoryBank (Zhong et al., 2024) retrieves his-torical context by maintaining a structured memory where both conversational summaries and round-level utterances are stored as key-value pairs. The retrieval process involves directly matching user queries to the most relevant stored information, en-suring efﬁcient context retrieval for response gen-eration.
LD-Agent (Li et al., 2024a), on the other hand, enhances  retrieval  by  incorporating  keyphrase-based queries.  In addition to storing factual and summarized information, its retrieval is based on queries with key phrases extracted from past in-teractions. This enables the model to adapt more effectively to diverse query formulations, retrieving context that aligns with the underlying semantic meaning of the user input.
For both methods, retrieval operates in a non-hierarchical manner, meaning that all stored data is accessed through a uniform search mechanism without  additional  interaction-based  reﬁnement. The retrieved content is then used to provide histor-ical grounding for response generation.


1https://github.com/xiaowu0162/LongMemEval

8429

A.5	 The Convergence of Citation Scores in	from long-term memory. RL
Figure 6 illustrates the convergence of citation scores (usefulness scores) during reinforcement learning.  The x-axis represents the RL training steps, while the y-axis measures the ratio of useful memories cited by the LLM generator.  Initially, the usefulness score starts at a low value around 0.2, reﬂecting the misalignment between retrieved memories and response generation.  As training progresses, the score steadily increases, converging to approximately 0.4 by step 1000. This trend high-lights the effectiveness of Retrospective Reﬂection in updating the reranker, allowing the retrieval pro-cess to better align with the generator’s citation behavior. The gradual convergence indicates stable learning and suggests that RL ﬁne-tuning improves retrieval quality without overﬁtting.

B	Dataset Description

We conduct experiments on two publicly avail-able   datasets:	MSC   (Xu   et   al.,   2022)   and LongMemEval (Wu et al., 2024).  MSC is a bench-mark dataset for multi-session conversations, pro-viding turn-level and session-level conversational data with annotations for relevance and response quality. On this dataset, following Li et al. (2024a), we evaluate the ability of an LLM agent to produce human-like personalized responses. Each response can be grounded in historical context across multi-ple previous sessions. The focus is on accurately generating personalized responses by leveraging relevant user preferences and conversation patterns. We followed the methodology outlined by Li et al. (2024a) to construct the data for our experiments. Speciﬁcally, we use the ﬁrst 1000 sessions as chat history and the rest for evaluation.
LongMemEval is designed for long-term conver-sational evaluation. It includes extended histories across turn, session, and mixed granularities. For experiments in Section 8.6, we randomly sample 100 test instances and use the remaining data for training and validation.  On this dataset, follow-ing Li et al. (2024a), we evaluate the system’s ability to answer human-designed questions about speciﬁc personal knowledge described in the his-torical sessions. For example, given a query like, “What car did Mary buy last summer?”, the system must retrieve and synthesize information scattered across multiple sessions. The task emphasizes ac-curately identifying and leveraging relevant details
8430

C	Case Studies

We present case studies to illustrate how RMM effectively integrates relevant memory fragments to enhance response quality. The following examples highlight scenarios where historical context is essential for maintaining coherence and accuracy in long-term dialogue.

C.1	Case 1: Revisiting Fitness Choices (MSC)
Tracking personal preferences and habits across multiple conversations is essential for maintaining coherent and personalized dialogue.  In this case, the user initially considers purchasing a treadmill (Session A), later expresses a preference for using the gym treadmill due to weather constraints (Session B), and ﬁnally conﬁrms their gym-going routine (Session C). An effective memory mechanism should correctly track this evolving decision and retrieve the most up-to-date preference.
Case 1: Revisiting Fitness Choices (MSC)

Session  A,  Turn  3:

•  Speaker_1:  Ah,  got  it.  Well,  maybe  one  of  the  older  gyms  will  work  out  better  for  you  –  or I  guess  you  could  get  that  treadmill  you  were  talking  about  before.

•  Speaker_2:  I’m  leaning  towards  the  treadmill.  I  think  it  will  work  better  for  my  lifestyle.

Session  B,  Turn  2:

•  Speaker_1:  I  go  to  the  gym  at  least  five  times  a  week,  and  I  lift  weights  at  least  three of  those  days.  When  I  need  to  give  my  arms  a  break,  I  work  on  my  leg  muscles.  I  run  around the  track  or  just  ride  the  stationary  exercise  bicycle.

•  Speaker_2: That sounds like a good plan.  I definitely need to add some weights to my routine. I  will  be  on  the  treadmills  a  lot,  especially  since  it  is  hard  for  me  to  run  outdoors  daily due  to  the  weather.

Session  C,  Turn  2:

•  [Question]  Speaker_1:  They  are  great  also,  thanks  for  asking.  Are  you  still  going  to  the gym?

•  [Answer]  Speaker_2:
–  Ground-truth:  Yes,  every  night.  I  run  on  a  treadmill.
–  Output  (RMM):  Yes,  I  go  to  the  gym  and  run  on  the  treadmill.  It  has  become  a  key  part of  my  routine.
–  Output  (Long  Context):  I  have  been  considering  getting  a  treadmill  for  home,  but  I  am still  unsure.  I  haven’t  decided  yet.

Analysis: The user’s decision about treadmill usage shifts across sessions. Initially, in Session A, they express interest in buying a treadmill.  By Session B, they reconsider and decide that using the gym treadmill would be sufﬁcient and conﬁrm that they run on the treadmill at the gym. Without memory management, the model generates an outdated response, assuming the user is still undecided about purchasing a treadmill.

C.2	Case 2: Tracking Chronological Order of Events (LongMemEval)
In long-term interactions, correctly recalling the sequence of past events is essential for maintaining factual consistency. This case examines whether the model can track the order in which the user attended two different events.
Case 2: Tracking Chronological Order of Events (LongMemEval)

Session  A,  Turn  1:

•  Speaker_1:    I   recently   attended   a   workshop   on   Effective   Time   Management   at   the   local community  center.   It  was  incredibly  insightful  and  gave  me  some  strategies  for  managing tasks  efficiently.  What  are  your  thoughts  on  time  management  workshops?


8431


•  Speaker_2:  They’re  quite  helpful  for  understanding  basic  techniques,  but  it  also  depends on  how  you  apply  the  strategies  in  daily  life.

Session  B,  Turn  3:

•  Speaker_1:   I  also  attended  a  webinar  on  Data  Analysis  using  Python  two  months  ago.   The instructor   shared   some   incredible   resources   for   data   visualization.    Have   you   explored advanced  visualization  techniques?

•  Speaker_2: Yes, they can greatly enhance how you present your data. Libraries like Matplotlib and  Seaborn  are  good  starting  points  for  creating  professional  visuals.

[Question] Which event did I attend first, the “Effective Time Management” workshop or the “Data Analysis  using  Python”  webinar?
[Answer]

•  Ground-truth:  “Data  Analysis  using  Python”  webinar.

•  Output  (RMM):  You  attended  the  Data  Analysis  using  Python  webinar  two  months  ago.    The Effective  Time  Management  workshop  happened  later  at  the  local  community  center.

•  Output (Long Context): I’m not sure, but you mentioned both events in previous conversations.

Analysis: The correct response requires linking the time reference (“two months ago”) with the cor-responding event. Without RMM, the model fails to retrieve this detail, resulting in an uncertain and incomplete answer. With RMM, the model correctly recalls the chronological order, demonstrating the advantage of structured memory retrieval in tracking event sequences.

D	Prompts

D.1	Prospective Reﬂection
D.1.1	Memory Extraction
Function:  Memory extraction for SPEAKER_1

Task  Description:   Given  a  session  of  dialogue  between  SPEAKER_1  and  SPEAKER_2,   extract  the personal  summaries  of  SPEAKER_1,   with  references  to  the  corresponding  turn  IDs.    Ensure  the output  adheres  to  the  following  rules:

•  Output  results  in  JSON  format.  The  top-level  key  is  “extracted_memories”.  The  value  should be  a  list  of  dictionaries,  where  each  dictionary  has  the  keys  “summary”  and  “reference”:

–  summary:	A   concise   personal   summary,   which   captures   relevant   information   about SPEAKER_1’s  experiences,  preferences,  and  background,  across  multiple  turns.
–  reference:  A  list  of  references,  each  in  the  format  of  [turn_id]  indicating  where  the information  appears.

•  If  no  personal  summary  can  be  extracted,  return  NO_TRAIT.

Example: INPUT:

•  Turn  0:
–  SPEAKER_1:  Did  you  check  out  that  new  gym  in  town?
–  SPEAKER_2:  Yeah,  I  did.  I’m  not  sure  I  like  the  vibe  there,  though.

•  Turn  1:
–  SPEAKER_1:  What  was  wrong  with  it?
–  SPEAKER_2:  The  folks  there  seemed  to  care  more  about  how  they  looked  than  working  out. It  was  a  little  too  trendy  for  me.  I’m  pretty  plain.

•  Turn  2:
–  SPEAKER_1:   Ah,  got  it.   Well,  maybe  one  of  the  older  gyms  will  work  out  better  for you—or  I  guess  you  could  get  that  treadmill  you  were  talking  about  before.   Are  you leaning  one  way  or  the  other  yet?


8432


–  SPEAKER_2:   I’m  leaning  towards  the  treadmill.    I  think  it  will  work  better  for  my lifestyle.  I  just  don’t  know  which  type  to  get.  There  are  so  many  choices  out  there. Do  you  use  a  treadmill  at  your  gym?  Do  you  have  a  suggestion  for  a  home  one?

•  Turn  3:
–  SPEAKER_1:  I  usually  just  lift  weights  there,  to  be  honest.   But  I  think  I’ve  heard good  things  about  the  NordicTrack?
–  SPEAKER_2:    Yeah,   I’ve   heard   good   things   about   that,   too.	I   like   the   idea   of   a multi-exercise piece of equipment.  As long as the weather isn’t too bad, then I prefer to  go  for  a  run.   But  since  it  rains  quite  a  bit  here,  I  like  the  idea  of  an  inside option.  How  is  the  weather  in  New  England?

•  Turn  4:
–  SPEAKER_1:   Oh,  it  can  get  pretty  foggy  and  rainy  here  too,  I’m  afraid.   But  as  I’m sure  you’ve  heard,  it’s  really  beautiful  in  the  fall!  Are  there  four  distinct  seasons where  you  are,  too?
–  SPEAKER_2:  Yes,  I’ve  heard  about  the  fall  colors.   I  may  get  there  one  day.   Yes,  we have  seasons—rain,  lighter  rain,  summer,  and  more  rain!  Ha!

•  Turn  5:
–  SPEAKER_1:  Haha!  I  lived  overseas  in  the  tropics  once.  Sounds  just  like  it!
–  SPEAKER_2:  The  tropics  sound  great.  It’s  not  as  warm  as  the  tropics,  but  I  like  it. I’m  from  Alaska,  so  I’m  pretty  weather-tough.

OUTPUT:
{
"extracted_memories":   [ {
"summary":   "SPEAKER_1   asked   about   a   new   gym   in   town   and   suggested older   gyms   or   a   treadmill   as   alternatives.",
"reference":   [0,   2] },
{
"summary":   "SPEAKER_1   usually   lifts   weights   at   the   gym   rather   than using   a   treadmill.",
"reference":   [3] },
{
"summary":   "SPEAKER_1   has   heard   good   things   about   the   NordicTrack treadmill.",
"reference":   [3] },
{
"summary":   "SPEAKER_1   lives   in   New   England   and   experiences   foggy and   rainy   weather   but   enjoys   the   fall   season.",
"reference":   [4] },
{
"summary":   "SPEAKER_1   has   lived   overseas   in   the   tropics   before.", "reference":   [5]
} ]
}

Task: Follow the JSON format demonstrated in the example above and extract the personal summaries for  SPEAKER_1  from  the  following  dialogue  session.
Input:  {} Output:

Function:  Memory extraction for SPEAKER_2

Task  Description:   Given  a  session  of  dialogue  between  SPEAKER_1  and  SPEAKER_2,   extract  the personal  summaries  of  SPEAKER_2,   with  references  to  the  corresponding  turn  IDs.    Ensure  the output  adheres  to  the  following  rules:
•  Output  results  in  JSON  format.  The  top-level  key  is  “extracted_memories”.  The  value  should

8433


be  a  list  of  dictionaries,  where  each  dictionary  has  the  keys  “summary”  and  “reference”:
–  summary:	A   concise   personal   summary,   which   captures   relevant   information   about SPEAKER_2’s  experiences,  preferences,  and  background,  across  multiple  turns.
–  reference:  A  list  of  references,  each  in  the  format  of  [turn_id]  indicating  where  the information  appears.

•  If  no  personal  summary  can  be  extracted,  return  NO_TRAIT.

Example: INPUT:

•  Turn  0:
–  SPEAKER_1:  Did  you  manage  to  go  out  on  a  run  today?
–  SPEAKER_2:  Yes,  I  actually  was  able  to.   I  am  considering  joining  the  local  gym.   Do you  prefer  going  to  the  gym?

•  Turn  1:
–  SPEAKER_1:  I  do  actually.  I  like  the  controlled  environment.  I  don’t  want  to  have  to depend  on  the  weather  considering  where  I  live.
–  SPEAKER_2:  That’s  why  I  am  thinking  about  it.  I  hate  to  have  to  run  when  it’s  raining, and  I  feel  like  it  rains  here  all  the  time.

•  Turn  2:
–  SPEAKER_1:  A lot of gyms have tracks so that you can run indoors.  Hey, have you thought about  maybe  buying  a  treadmill  and  using  that  at  home?
–  SPEAKER_2:  I am definitely considering getting one.  I’m just trying to figure out what I  would  do  more—go  to  the  gym  and  actually  do  more  than  just  running,  or  stick  to  what I  know  and  get  a  treadmill.

•  Turn  3:
–  SPEAKER_1:  Oh,  that’s  true.  I  hadn’t  thought  about  all  of  that.  You’re  right.  With  a gym,  there  are  a  whole  lot  of  options  for  what  you  can  do.  Do  you  have  some  good  gyms near  you?
–  SPEAKER_2:   They  just  built  one  in  the  small  town  really  close  to  me,  and  it  looks pretty  decent.  Before  that,  it  was  like  an  hour  drive.

•  Turn  4:
–  SPEAKER_1:  With  you  not  owning  a  car,  going  to  any  others  would  probably  be  difficult. Well,  do  you  have  any  good  parks  and  running  trails  nearby?
–  SPEAKER_2:  Yeah,  exactly.  There  is  a  super  nice  little  running  trail  that  is  pretty decent.

•  Turn  5:
–  SPEAKER_1:  Hey,  do  you  run  with  anyone?  I  mean,  have  you  joined  a  club,  or  will  you if  you  haven’t?
–  SPEAKER_2:  There  isn’t  any  around  here;  maybe  I  could  start  one.  Thank  you  for  that idea.

OUTPUT:
{
"extracted_memories":   [ {
"summary":   "SPEAKER_2   is   considering   joining   a   local   gym   due   to frequent   rain   affecting   outdoor   runs.",
"reference":   [0,   1] },
{
"summary":   "SPEAKER_2   is   debating   between   buying   a   treadmill   for home   use   or   going   to   the   gym   for   more   workout   variety.",
"reference":   [2] },
{


8434


"summary":   "A   new   gym   was   recently   built   nearby   SPEAKER_2 , replacing   a   previous   one   that   was   an   hour   away.",
"reference":   [3] },
{
"summary":   "SPEAKER_2   has   access   to   a   nice   local   running   trail.", "reference":   [4]
}, {
"summary":   "SPEAKER_2   notices   there   is   no   local   running   club   but is   considering   starting   one.",
"reference":   [5] }
] }
Task: Follow the JSON format demonstrated in the example above and extract the personal summaries for  SPEAKER_2  from  the  following  dialogue  session.
Input:  {} Output:










































8435

D.1.2	Memory Update

Task  Description:  Given  a  list  of  history  personal  summaries  for  a  specific  user  and  a  new  and similar  personal  summary  from  the  same  user,  update  the  personal  history  summaries  following the  instructions  below:

•  Input format:  Both the history personal summaries and the new personal summary are provided in  JSON  format,  with  the  top-level  keys  of  “history_summaries”  and  “new_summary”.

•  Possible  update  actions:
–  Add:  If  the  new  personal  summary  is  not  relevant  to  any  history  personal  summary,  add it.
Format:  Add()
–  Merge:  If  the  new  personal  summary  is  relevant  to  a  history  personal  summary,  merge them  as  an  updated  summary.
Format:  Merge(index,  merged_summary)
Note: index is the position of the relevant history summary in the list.  merged_summary is   the   merged   summary   of   the   new   summary   and   the   relevant   history   summary.	Two summaries   are   considered   relevant   if   they   discuss   the   same   aspect   of   the   user’s personal  information  or  experiences.

•  If  multiple  actions  need  to  be  executed,  output  each  action  in  a  single  line,  and  separate them  with  a  newline  character  ("\n").

•  Do  not  include  additional  explanations  or  examples  in  the  output—only  return  the  required action  functions.

Example: INPUT:

•  History  Personal  Summaries:
–  {"history_summaries":   ["SPEAKER_1  works  out  although  he  doesn’t  particularly  enjoy it."]}

•  New  Personal  Summary:
–  {"new_summary":  "SPEAKER_1  exercises  every  Monday  and  Thursday."}

OUTPUT  ACTION:
Merge(0,  SPEAKER_1  exercises  every  Monday  and  Thursday,  although  he  doesn’t  particularly  enjoy it.)
Task:  Follow  the  example  format  above  to  update  the  personal  history  for  the  given  case. INPUT:

•  History  Personal  Summaries:  {}

•  New  Personal  Summary:  {}

OUTPUT  ACTION:

















8436

D.2	Retrospective Reﬂection

Task  Description:  Given  a  user  query  and  a  list  of  memories  consisting  of  personal  summaries with  their  corresponding  original  turns,  generate  a  natural  and  fluent  response  while  adhering to  the  following  guidelines:

•  Cite  useful  memories  using  [i],  where  i  corresponds  to  the  index  of  the  cited  memory.

•  Do  not  cite  memories  that  are  not  useful.  If  no  useful  memory  exist,  output  [NO_CITE].

•  Each  memory  is  independent  and  may  repeat  or  contradict  others.    The  response  must  be directly  supported  by  cited  memories.

•  If  the  response  relies  on  multiple  memories,  list  all  corresponding  indices,  e.g.,  [i,j,k].

•  The  citation  is  evaluated  based  on  whether  the  response  references  the  original  turns,  not the  summaries.

Examples:
Case  1:  Useful  Memories  Found INPUT:

•  User  Query:  SPEAKER_1:  What  hobbies  do  I  enjoy?

•  Memories:
–  Memory  [0]:  SPEAKER_1  enjoys  hiking  and  often  goes  on  weekend  trips.
*  Speaker  1:  I  love  spending  my  weekends  hiking  in  the  mountains. Speaker  2:  That  sounds  amazing!  Do  you  go  alone  or  with  friends?
*  Speaker  1:  Last  month,  I  hiked  a  new  trail  and  it  was  amazing. Speaker  2:  Nice!  Which  trail  was  it?
–  Memory  [1]:  SPEAKER_1  plays  the  guitar  and  occasionally  performs  at  open  mics.
*  Speaker  1:  I’ve  been  practicing  guitar  for  years  and  love  playing  at  open  mics. Speaker  2:  That’s  awesome!  What  songs  do  you  usually  play?
*  Speaker  1:  I  performed  at  a  local  cafe  last  week  and  had  a  great  time. Speaker  2:  That  must  have  been  fun!  Were  there  a  lot  of  people?
–  Memory  [2]:  SPEAKER_1  is  interested  in  astronomy  and  enjoys  stargazing.
*  Speaker  1:  I  recently  bought  a  telescope  to  get  a  closer  look  at  planets. Speaker  2:  That’s  so  cool!  What  have  you  seen  so  far?
*  Speaker  1:  I  love  stargazing,  especially  when  there’s  a  meteor  shower. Speaker  2:  I’d  love  to  do  that  sometime.  When’s  the  next  one?

Output:  You  enjoy  hiking,  playing  the  guitar,  and  stargazing.  [0,  1,  2]

Case  2:  No  Useful  Memories INPUT:

•  User  Query:  SPEAKER_1:  What  countries  did  I  go  to  last  summer?

•  Memories:
–  Memory  [0]:  SPEAKER_1  enjoys  hiking  and  often  goes  on  weekend  trips.
*  Speaker  1:  I  love  spending  my  weekends  hiking  in  the  mountains. Speaker  2:  That  sounds  amazing!  Do  you  go  alone  or  with  friends?
*  Speaker  1:  Last  month,  I  hiked  a  new  trail  and  it  was  amazing. Speaker  2:  Nice!  Which  trail  was  it?
–  Memory  [1]:  SPEAKER_1  plays  the  guitar  and  occasionally  performs  at  open  mics.
*  Speaker  1:  I’ve  been  practicing  guitar  for  years  and  love  playing  at  open  mics. Speaker  2:  That’s  awesome!  What  songs  do  you  usually  play?
*  Speaker  1:  I  performed  at  a  local  cafe  last  week  and  had  a  great  time. Speaker  2:  That  must  have  been  fun!  Were  there  a  lot  of  people?
–  Memory  [2]:  SPEAKER_1  is  interested  in  astronomy  and  enjoys  stargazing.
*  Speaker  1:  I  recently  bought  a  telescope  to  get  a  closer  look  at  planets. Speaker  2:  That’s  so  cool!  What  have  you  seen  so  far?
*  Speaker  1:  I  love  stargazing,  especially  when  there’s  a  meteor  shower. Speaker  2:  I’d  love  to  do  that  sometime.  When’s  the  next  one?

Output:  I  don’t  have  enough  information  to  answer  that.  [NO_CITE] Additional  Instructions:

8437


•  Ensure  the  response  is  fluent  and  directly  answers  the  user’s  query.

•  Always  cite  the  useful  memory  indices  explicitly.

•  The  citation  is  evaluated  based  on  whether  the  response  references  the  original  turns,  not the  summaries.

•  Follow  the  format  of  the  examples  provided  above.

Input:

•  User  Query:  {}

•  Memories:  {}

Output:













































8438

D.3	LLM-as-a-Judge

You  are  an  expert  language  model  evaluator.  I  will  provide  you  with  a  question,  a  ground-truth answer, and a model-generated response.  Your task is to determine whether the response correctly answers  the  question  by  following  these  evaluation  rules:

•  Answer  Yes  if  the  response  contains  or  directly  matches  the  correct  answer.

•  Answer Yes if the response includes all necessary intermediate steps leading to the correct answer.

•  Answer  No  if  the  response  provides  only  a  partial  answer  or  omits  essential  information.

•  Answer  No  if  the  response  does  not  sufficiently  address  the  question.

Examples:
Example  1:  Correct  Response

•  Question:  What  is  the  capital  of  France?

•  Ground-truth  Answer:  Paris

•  Response:  The  capital  of  France  is  Paris.

Evaluation:

•  Output:  Yes

Example  2:  Incorrect  Response

•  Question:  What  is  the  capital  of  France?

•  Ground-truth  Answer:  Paris

•  Response:  France  is  a  country  in  Europe.

Evaluation:

•  Output:  No

Additional  Instructions:

•  Apply  the  evaluation  criteria  consistently.

•  Base  your  decision  strictly  on  the  information  in  the  response.

•  Avoid  subjective  interpretations  and  adhere  to  the  provided  examples.

Input:

•  Question:  {}

•  Ground-truth  Answer:  {}

•  Response:  {}

Output:











8439
