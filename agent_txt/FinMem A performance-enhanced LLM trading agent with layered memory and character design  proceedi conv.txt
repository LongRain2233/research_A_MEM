IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025	3443

FINMEM: A Performance-Enhanced LLM Trading Agent With Layered Memory and Character Design

Yangyang Yu∗	, Haohang Li∗	, Zhi Chen∗	, Yuechen Jiang∗	, Yang Li∗	, Jordan W. Suchow   , Denghui Zhang   , and Khaldoun Khashanah




Abstract—WeintroduceFINMEM,anovelLargeLanguageMod-els (LLM)-based agent framework for ﬁnancial trading, designed toaddresstheneedforautomatedsystemsthatcantransformreal-time data into executable decisions. FINMEM comprises three core modules:Proﬁleforcustomizingagentcharacteristics,Memoryfor hierarchical ﬁnancial data assimilation, and Decision-making for converting insights into investment choices. The Memory module, which mimics human traders’ cognitive structure, offers inter-pretability and real-time tuning while handling the critical timing of various information types. It employs a layered approach to process and prioritize data based on its timeliness and relevance, ensuring that the most recent and impactful information is given appropriate weight in decision-making. FINMEM’s adjustable cog-nitive span allows retention of critical information beyond human limits, enabling it to balance historical patterns with current mar-ket dynamics. This framework facilitates self-evolution of profes-sional knowledge, agile reactions to investment cues, and contin-uous reﬁnement of trading decisions in ﬁnancial environments. Whencomparedagainstadvancedalgorithmicagentsusingalarge-scale real-world ﬁnancial dataset, FINMEM demonstrates superior performance across classic metrics like Cumulative Return and Sharpe ratio. Further tuning of the agent’s perceptual span and character setting enhances its trading performance, positioning FINMEM as a cutting-edge solution for automated trading.

Index  Terms—Financial  AI,  large  language  models,  trading algorithms, deep learning, ﬁnancial technology.


I.  INTRODUCTION
ITHtheinﬂuxofdiverseﬁnancialdatastreamsfromthe web, traders face a deluge of information from various
W
sources. This requires them rapidly to understand, memorize, and ﬁltrate crucial events for investment decisions. However, innatecognitivelimitationsrestricthumantradersfromprocess-ing information within their perception and memory capacity, a span much narrower than the actual volume of available in-formation [1]. Consequently, insufﬁciently considering or even dismissing critical events affecting trading decisions becomes

Received 1 December 2023; revised 26 September 2024; accepted 17 July
2025. Date of publication 4 August 2025; date of current version 13 November
2025. Recommended for acceptance by G. Yang. (∗Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, and Yang Li contributed equally to this work.) (Corresponding author: Yangyang Yu.)
TheauthorsarewiththeStevensInstituteofTechnology,Hoboken,NJ07030 USA (e-mail: yyu44@stevens.edu; hli113@stevens.edu; zchen100@stevens. edu;  yjiang52@stevens.edu;  yli269@stevens.edu;  jws@stevens.edu;  dzhang 42@stevens.edu; kkhashan@stevens.edu).
This   article   has   supplementary   downloadable   material   available   at https://doi.org/10.1109/TBDATA.2025.3593370, provided by the authors.
Digital Object Identiﬁer 10.1109/TBDATA.2025.3593370

increasingly concerning as data availability expands. To over-come the physical limitations in the memory systems of human traders, researchers have been consistently working on design-ing autonomous trading agent systems. These systems need to thoroughly integrate all available information and possess a sophisticateddesignintheagent’sbackbonealgorithmtodeliver enhanced trading performance.
Theevolutionofautonomoustradingsystemshastransitioned from the initial rule-based trading strategies [2] to more ad-vanced machine-learning-based algorithms [3]. In recent years, ReinforcementLearning(RL)-basedagents[4],especiallythose employing Deep Reinforcement Learning (DRL) [5] as back-bone algorithms, garner joint attention of both academia and industry.LeveragingbothRLprinciplesanddeeplearning,DRL agents effectively handle and learn from scalable and diverse ﬁnancial data, including stock prices, key ﬁnancial indicators, and market sentiments. They utilize deep neural networks to extract expressive features from input data, representing the complex ﬁnancial market environment, which enhances their comprehension ability. Retaining the key features of RL agents, they learn through interaction with a predeﬁned environment to maximize investment gain over time. Research suggests DRLs canmeetthecrucialneedsoftradingagentstoprocessandmake informeddecisionsfromlargevolumesofdata.However,certain inherentfeaturesofDRLalgorithmsexhibitnotabledeﬁciencies inﬁnancialapplications.Firstly,DRLagentsexhibitalackofin-terpretabilityconcerningtherationalebehindtheirdecisions[6]. They are often described as “Closed system,” where the internal processesandcomputationallayersleadingtoaspeciﬁcdecision are neither easily understandable nor transparent. Secondly, DRL agents ﬁnd it challenging to effectively integrate textual data with numerical features. Text data plays a vital role in ﬁnance, since the majority of market information is conveyed through news articles and ﬁnancial reports. However, trans-forming text data to embeddings considerably increases input space dimensionality, making learning more computationally demanding. Plus, empirical studies have shown that combining textual representations with numerical ﬁnancial indicators often leads to convergence challenges [7]. Therefore, a backbone algorithm with transparent reasoning and the enhanced ability to capture investment-related textual insights comprehensively is essential.
Recent advancements in Large Language Models (LLMs), like Generative Pre-trained Transformers (GPTs) [8], offer vi-ablesolutionsfordevelopingtradingagents,alleviatingprevious


2332-7790 © 2025 IEEE. All rights reserved, including rights for text and data mining, and training of artiﬁcial intelligence and similar technologies. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information.


Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3444	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025


concerns. With carefully designed prompts, the LLM-based agent is able to provide reasons and outcomes in plain text. This allows for immediate observation and prompt adjustment of its reasoning process. Employing LLMs as the backbone algorithms for agents also overcomes the constraint of isolated environments. This is achieved through their vast pre-existing knowledgeandtheeffectiveintegrationofvaluableinsightsfrom a variety of data sources, including both textual and numerical. When equipped with suitable prompt templates, this approach signiﬁcantly enhances decision-making capabilities [9]. Studies indicate that prompt-guided reasoning signiﬁcantly improves problem-solving rates across various domains [10]. Notably, a growing body of research has focused on utilizing LLMs to makeinformedtradingdecisionsforstocksandfundsbycontin-uously interacting with ﬁnancial environment information [11], [12]. However, in these approaches, LLMs primarily serve in a question-answering (QA) capacity rather than functioning as autonomous agents. Despite outperforming traditional trading benchmarks,  they  generally  process  information  indiscrimi-natelythroughQAiterations,lackingtheabilitytoprioritizeand retain crucial information. Moreover, these approaches heavily rely on the uncertain and resource-intensive process of LLM ﬁne-tuning. These LLM-based methods often oversimplify the complex cognitive functions essential to traders. For instance, they struggle to recognize the varying relevance and timeliness of different data types or to dynamically adjust risk tolerance based on changing market conditions. Consequently, they may fail to effectively prioritize and retain critical information or events in their decision-making process. In scenarios requiring sequential decision-making, such as stock trading in a volatile environment, these limitations can lead to suboptimal perfor-mance. The inability to maintain a nuanced understanding of market dynamics and adapt strategies accordingly hampers the LLMs’ effectiveness in replicating the sophisticated decision-making processes of human traders.
WeintroduceFINMEM,anovelLLM-basedautonomoustrad-ing agent. We claim that it bridges these existing research gaps in the following aspects:
r
FINMEM provides a language agent whose reasoning and decision-making processes are completely transparent, in-terpreted through plain texts. This signiﬁcantly enhances theinterpretabilityoftherationalebehinditsrecommended investment actions.
r
FINMEMefﬁcientlyproduceshigh-qualitystocktradingde-cisions by effectively integrating numeric and textual data, requiring signiﬁcantly less training time and data volume. This method provides a robust solution for the automated trading of stocks from recently IPOed companies. FINMEM ﬂexibly adapts to market volatility by offering a self-adaptive character setting in its proﬁle module. FIN-MEM’sproﬁlemoduleincludesadynamiccharactersetting feature,offeringseedinformationaboutprofessionalback-grounds and adjustable risk inclinations. Additionally, it continuouslyupdatesdomainknowledgeasthetradingex-perience grows, which reinforces FINMEM’s professional experience to further augment its decision-making quality.
r

r  FINMEM ﬁrstlyintroducesacutting-edgeLLM-basedtrad-ingagentframeworkequippedwithahuman-alignedmem-ory  mechanism.  This  innovative  approach  mimics  the cognitive architecture of human traders, featuring work-ing and layered long-term memory mechanisms. This de-sign excels at capturing essential investment insights from diverse ﬁnancial data characterized by multi-timeliness, thereby enhancing trading performance. Beyond a simple similarity-based search, FINMEM retrieves information us-ing three key metrics: similarity, recency, and importance. The latter two metrics are maintained using distinct decay ratios for each layer, enhancing the system’s ability to prioritize and access relevant information across various time scales.
FINMEM leverages its distinctive features to expand the agent’s perceptual range beyond the human limitation to make well-informed trading decisions. Cognitive research shows that human working memory can recall only ﬁve to nineeventsatatime[13].Thislimitation,whilepreventing information overload, can yield insufﬁcient insight for precise decision-making. In contrast, FINMEM’s memory module transcends this constraint. It allows adjusting cog-nitive load by selecting a ﬂexible number of top-ranked events from each layer of its hierarchical long-term mem-ory, allowing FINMEM to maintain agility and deliver su-perior trading decisions in data-rich contexts.
r
Inspiredbythemodulardesignoftheclassicgenerativeagent framework by Park et al. [14], FINMEM introduces innovative proﬁle and memory modules. The proﬁle module provides FIN-MEM with a trade-speciﬁc professional background and a self-adaptive risk option, sharpening its trading focus and enhancing resilience to market ﬂuctuations. Moreover, FINMEM’s memory module, featuring working and layered long-term memory, is tailored for processing information based on its importance and timeliness. These innovations address the challenges in Park et al.’s framework regarding the accurate interpretation of time-sensitive and critical data. They enhance FINMEM’s ability to distinguish data timeliness, optimize information retrieval, and offer insightful feedback for reﬁning future decisions, making it exceptionally suitable for ﬁnancial decision-making contexts. FINMEMs´ working memory acts as a dynamic “workspace,” enablingoperationslikesummarization,observation,andreﬂec-tion on multi-source information to facilitate trading decisions. Its long-term memory, structured into shallow, intermediate, and deep layers [15], manages varied decay rates to satisfy the need to retain distinct types of ﬁnancial information within different time scales based on their corresponding timeliness. For instance, daily news, with its immediate effects on stock markets, is channeled into the shallow processing layer. Mean-while, annual company reports, exerting a more prolonged im-pact, are processed in the deep layer by FINMEM. Each layer in FINMEM prioritizes memory events based on the assemble of recency, relevancy, and importance close to Park et al.’s method. However, it introduces new measurements for recency andimportance,speciﬁcallytailoredtobetterrankﬁnancialdata according to their unique time sensitivity. FINMEM’s memory




Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3445



mechanism can also transit signiﬁcantly impactful investment memoryeventstodeeperprocessinglayers,ensuringtheirreten-tionforextendedperiods.FINMEM’smemorymodulecanmirror the human cognitive system [16] and facilitate agile, real-time decisions [17]. It enables continuous evolution in professional knowledge through structured summarizing, retrospecting past experiences,andreactingtonewtradingscenarios.Additionally, FINMEMincludesadecision-makingmodulecapableofderiving investment decisions by considering top-ranked memory events and current market conditions.
In this paper, we begin by explaining the three core modules of FINMEM. Subsequently, we emphasize its superior trading performance compared to a range of representative algorithmic agents. We further highlight FINMEM’s optimal performance by comparing it with advanced algorithmic-based agent frame-works using widely adapted ﬁnancial evaluation metrics like cumulative return and Sharpe ratio. We last examine the effec-tivenessofthreecriticalcomponentsofFINMEM—backboneal-gorithms, working memory capacity, and character settings—in enhancingitsdecisionqualitythroughablationstudies.Through experiments, we show that FINMEM  exhibits an outstanding ability to stratify and leverage the various levels of market insights,signiﬁcantlyimprovingthequalityoftradingdecisions.


II.  RELATED WORK

A.  Deep Learning and Reinforcement Learning for Trading
The development of trading agents has evolved signiﬁcantly over decades, driven by advancements in technology, ﬁnance, and computational methodologies. This evolution progressed from traditional rule-based algorithms operating on predeﬁned sets of rules [18], [19], [20], to statistical learning methods like time series forecasting [21], [22], [23], [24], [25], [26], [27], [28]. It has further advanced to deep learning techniques [29], [30] that enhance predictive decision-making by leveraging ﬁne-grained ﬁnancial features. Compared to prior approaches, deep learning algorithms more effectively identify nuanced his-toricalmarketpatternscriticalfortradingdecisions,excellingin generating sophisticated, higher-dimensional representations of market features. The implementation of Reinforcement Learn-ing (RL) offers a robust method for developing agent sys-tems tailored for sequential ﬁnancial decision-making tasks. This approach transcends mere predictive modeling, facilitat-ing a dynamic learning environment where the system con-tinuously adapts to market dynamics through interactions and feedback [31], [32]. Advanced forms of RL, such as Deep Reinforcement Learning (DRL) - including Deep Q-Network (DQN) [33], Advantage Actor-Critic (A2C) [34], and Proximal Policy Optimization (PPO) [35] - further enhance these systems through feedback-driven learning [31], [32]. However, DRL faces challenges such as limited interpretability of decisions [6] and difﬁculties in incorporating high-dimensional textual infor-mation directly. DRL agents often rely on extracting textual sentiment [36], [37], [38], potentially missing other essential market information embedded in news and macroeconomic texts [39], [40].

B.  LLM for Financial Decision-Making
With the emergence of large language models (LLMs), a signiﬁcant body of research has explored their capabilities for textual  summarization  and  reasoning  in  complex  tasks  like ﬁnancial sentiment analysis and professional knowledge as-sessment in the ﬁnancial sector [12], [41], [42], [43], [44]. Subsequently, [11] examined the use of LLMs to summarize investment-related insights and extract ﬁnancial sentiment from extensive media news through a question-answering (Q&A) framework. This work initially demonstrates the potential of LLMs to generate trading recommendations, showcasing their analytical proﬁciency. The subsequent study by [45] further explores various cognitive biases that naturally affect human investors in ﬁnancial decision-making, and shows that these bi-asesalsoinﬂuencethedecision-makingprocessesofLLMs[45]. Speciﬁcally, risk preference is a critical factor affecting the decisionoutcomesofLLMs,necessitatingcarefulmanagement.

C.  LLM Agent for Financial Decision-Making
There  have  been  extensive  studies  demonstrating  LLMs agents’impressivecapabilitiesinvariousdecision-makingtasks acrossvariousdomains[9],[46],[47],[48],[49],[50],[51]such ashealthcareandtechnologyservice.Theseworkshavedemon-strated the necessity and advantage of accomplishing complex decision-making by having agent frameworks on top of LLMs to accomplish more complex and sequential decision-making tasks. A few recent studies [44], [52], [53] have tried to design language agent frameworks crafted for decision-making cases intheﬁnancialdomain,featuringanopen-endedandmuchmore volatile environment. Ref. [53]’s approach uses multiple simple LLM agents to generate trading decisions, but this incurs high costs due to extensive coordination. Ref. [52] propose a more efﬁcient single-agent framework with memory and reﬂection capabilities. However, their oversimpliﬁed memory retrieval process mayleadtodecisions based onoutdated market signals. Meanwhile,  [44]  generate  trading  recommendations  as  part of  their  ﬁnancial  report  generation  process.  However,  their framework and evaluation are designed primarily for textual documentation. That is, they lack a specialized agent structure for decision-making tasks. Additionally, their recommended decisions share the frequency with their ﬁnancial reports, which are  typically  issued  monthly  at  most.  This  frequency  does not  support  higher-frequency  decision-making  needs,  such as daily updates. Due to these constraints, they do not offer performance evaluations for their investment recommendation. In summary, given the potential issues to be improved, there is a need to develop a more nuanced framework that better handles multi-timeliness  ﬁnancial  data  to  support  more  intelligent decision-making.

III.  ARCHITECTURE OF FINMEM
Inthissection,wecomprehensivelydetailthethreecorecom-ponents of FINMEM, namely the proﬁle, memory, and decision-making modules. The proﬁle module empowers FINMEM  to adaptively tailor character setting for speciﬁc trading tasks. Memory module leverages diverse time-efﬁciency attributes of



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3446	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025



ﬁnancial data, enhancing its trading efﬁcacy. The decision-making  module  enables  FINMEM  to  synchronize  its  mem-ory streams with market facts, facilitating high-quality trading decisions. The details and notations associated with these three modules are provided in the subsequent sections.

A.  Proﬁle Module
The proﬁle module empowers FINMEM to develop a dynamic agent character speciﬁcally designed to navigate the complex dynamics of ﬁnancial markets effectively.
The dynamic character of FINMEM comprises two principal components:ﬁrstly,afoundationalprofessionalknowledgebase akintoatradingexpert,andsecondly,anagentwiththreedistinct investment risk inclinations. The ﬁrst component includes two types of information: an introduction to the primary trading sectorsrelevanttothecompanystockFINMEM willtradein,and a concise overview of the historical ﬁnancial performance of the speciﬁed ticker, spanning from the beginning to the end of the training period. Before initiating trades in a new company’s stock,  FINMEM   accesses  and  updates  this  sector-speciﬁc and historical ﬁnancial data from a backend database. This professional background setting narrows down information and memory events pertinent to speciﬁc trading tasks. The detailed prompttemplateforconstructingtheproﬁlemoduleispresented in Fig. 9 in Section VI-D, using TSLA trading as an example.
The second component of FINMEM’s design encompasses three distinct risk inclination options: risk-seeking, risk-averse, and a self-adaptive riskcharacter. The risk-seeking setting gears FINMEM towards an aggressive, high-reward approach, while therisk-aversesettinggearsittowardsaconservative,lower-risk strategy.AdistinctiveaspectofFINMEM isitsabilitytodynami-cally alternate between these risk settings in response to current market conditions. Speciﬁcally, it shifts risk preferences when the Cumulative Return falls to below zero within a brief period, such as three days, and reversely. This ﬂexible design functions as a protective mechanism, mitigating prolonged downturns in turbulent market environments. During the initial stage of the training phase, FINMEM is conﬁgured with a chosen risk preference, each supplemented with comprehensive textual ex-planations through LLM prompts. These guidelines shape how FINMEM processes incoming messages and determines its sub-sequentactionsinalignmentwithitsdesignatedriskinclination. The system maintains a catalog of all risk inclinations and their detailedexplanationsinabacklog,enablingseamlessadaptation to different stocks by switching among these risk proﬁles as needed.
The dynamic character setting in FINME’s proﬁle module providessubjectiveandprofessionalbackgroundknowledgeand ﬂexiblechoiceofriskinclinations.Itprovidescrucialcontextfor ﬁlteringandretrievingtrading-relevantinformationandmemory events, thus improving accurate inferencing and adaptability to ﬂuctuating market conditions.

B.  Memory Module
The memory module of FINMEM emulates a human trader’s cognitive system so that it can efﬁciently process hierarchical

ﬁnancial information and prioritize the critical messages for high-quality investment decisions. Furthermore, it adjusts the memory span ﬂexibly, enabling the agent to operate on a wider rangeofeventsoveralongerretrievalperiod.FINMEM’smemory module, illustrated in Fig. 1, comprises working and long-term memory with layered processing capability and is initiated by a speciﬁc investment inquiry.
1)  Working Memory: Working memory refers to the hu-man cognitive system’s functions for temporary storage and diverse operations. We incorporate this concept into FINMEM’s memory module development, creating a central workspace for informed decision-making. Unlike human working memory, havingamaximumcapacityofsevenplusorminustwomemory events[13],FINMEM hastheabilitytoexpandthecapacitybased on speciﬁc requirements. Tailored for converting ﬁnancial data into trading actions, FINMEM’s working memory encompasses three key operations: summarization, observation, and reﬂec-tion. The mechanisms by which they interact and operate as an integrated decision-making workﬂow are detailed in the middle box of Fig. 1. Additionally, the LLM prompt template that supports these processes is detailed in the case study presented in Figs. 9 and 10 in Section VI-D below.
Summarization: FINMEM leverages external market data to derive critical investment insights and sentiments tailored to speciﬁc stock trading queries, such as “Can you make an in-vestment decision on TSLA on 10/25/2022?”. As illustrated in Fig. 1(1), this system condenses the original text into a com-pact yet informative paragraph, thereby enhancing FINMEM’s processing efﬁciency. It efﬁciently extracts and summarizes pertinent data and sentiments for stock investment decisions, demonstratedhereusingTeslaInc.asanexample.Subsequently, FINMEM directs these insights to an appropriate layer within its long-term memory architecture, selecting the layer based on the time sensitivity of the information.
Observation:Triggeredthesameinquiry,FINMEM initiatesan observation operation to gather market facts. The information available to FINMEM  varies between the training and testing phases.
Duringthetrainingphase,FINMEM hasaccesstocomprehen-sivestockpricedatawithinthespeciﬁed period. Upon receiving trading inquiries that specify a stock ticker and date, FINMEM focuses on the daily adjusted closing price differences, compar-ing the following day’s price with the current day’s. These price differences are utilized as market ground labels. Speciﬁcally, a decrease in price suggests a “Sell” action, while an increase or no change in price indicates a “Buy” action.
During the testing phase, at a speciﬁc time point, FINMEM loses the ability to access future price data. Its focus shifts to the analysis of historical stock price movements, depending on a retrospective evaluation of the cumulative return from the last M  trading days to infer future market trends. This phase,characterizedbytheabsenceofforeseenmarketgrounds, serves as a critical assessment of FINMEM’s development. It tests whether the system has adequately established logical connections between stock price trends and various ﬁnancial information sources, such as news, reports, and indicators. This stageiskeyinevaluatingFINMEM’scapabilityofindependently



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3447






















Fig. 1.    (1) FINMEM’s memory module interacts with the market environment to distill multi-source ﬁnancial information and facilitate investment decisions. It contains two core components – Working Memory and Layered Long-term Memory. (2) The outline of FINMEM’s decision-making workﬂow for retrieving critical memory events and market observations to inform speciﬁc investment decisions.



evolving its trading strategies for subsequent tasks, leveraging its analysis and interpretation of historical data patterns.
Reﬂection: Two types of reﬂections exist, immediate and extended reﬂection. (a) Immediate reﬂection is activated upon receiving a daily trading inquiry for a speciﬁc ticker. Using LLM and speciﬁc prompts exempliﬁed in Fig. 1(1), the agent merges market indications and top-K-ranked events from each long-term memory layer. Market indications are derived from theoutcomesoftheobservationoperationanddifferbetweenthe training and testing phases. During testing, this process yields three types of outputs: the trading direction (“Buy”, “Sell”, or “Hold”), the underlying rationale for this decision, and the most inﬂuential memory events, along with their IDs from each layer that informed the decision. In the training phase, specifying the tradingdirectionisunnecessary,asFINMEM isalreadyinformed offuturestockmovementdirections.Thetop-K-rankedmemory events encapsulate key insights and sentiments derived from critical investment-related incoming messages, all distilled by FINMEM’s advanced summarization capabilities.
(b) Extended reﬂection reevaluates immediate reﬂection out-comes for a ticker over a speciﬁed M-day trace period. It encompasses data like stock price trends, trading returns, and actionrationalesfrommultipleimmediatereﬂections.Whileim-mediate reﬂection enables direct trading execution and records currentfeedback,extendedreﬂectionsummarizesmarkettrends and reassesses recent Cumulative Return on investment. Ex-tended reﬂection is eventually transmitted and stored in the deep processing layer to emphasize its criticality (detailed in-troduced in Section III-B2) of long-term memory. K and M are hyperparameters to adjust FINMEM’s working memory capacity and information retrieval ability. FINMEM gains the ﬂexibility of integrating comprehensive information into well-informed decisions by ﬁne-tuning them.

2)  LAYERED   LONG-TERM   MEMORY: FINMEM’s  long-term memory  organizes  hierarchical  ﬁnancial  data  insights  in  a stratiﬁed structure, as illustrated in the lower section of Fig. 1. Drawinginspirationfromthevaryingdecayspeedsinthehuman cognitive system’s information processing layers [15], FINMEM employs a layered structure to accommodate the diverse time sensitivities inherent to different types of ﬁnancial data. This structure categorizes summarized insights by their timeliness and decay rates. Insights are derived by the working memory’s summarization operation. Those directed to deeper layers re-ceive smaller decay rates, indicating longer retention, while those in shallower layers are assigned larger decay rates for shorter retention.

γl    = SRecencyl  + SRelevancyl  + SImportancel,	(1)
E	E	E	E
where each memory event is only associated with one score and can only belong to a single layer.
Upon receiving an investment inquiry, FINMEM retrieves the top-K  pivotal memory events from each layer and channels them to the immediate reﬂection component of the working memory. These events are chosen according to the descending
E
order of their information retrieval score, denoted as γl   , where
l belongs to the set shallow, intermediate, deep, as speciﬁed in (1). E denotes a given memory event. This score, adapted from Park et al. [14] but with modiﬁed recency and importance computations, especially tailoring to handle data with various timelines. It encapsulates three metrics: recency, relevancy, and importance. Individual metric scores exceeding 1.0 are scaled to the [0,1] range before being summed. The modiﬁcation is to achievethelayeredprocessingfunctionandrepresentthevarious periodicity of the ﬁnancial environment.

SRecencyl   = e− Ql  ,	δE  = tP − tE,	(2)
δ
E
E


Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3448	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025



where δE  refers to the time difference between the memory event occurrence and the trading inquiry arrival. Qshallow = 14, Qintermediate = 90, and Qdeep = 365 correspond to day counts of two weeks, a quarter, and a year for shallow, intermediate, and deep processing layers, respectively.
Upon a trade inquiry P arrival in processing layer l via LLM
E
prompt, the agent computes the recency score SRecencyl  per (2). SRecency    inversely correlates with the time gap between the inquiry and the event’s memory timestamp, mirroring Ebbing-haus’sforgettingcurve[54].ThestabilitytermQl in(2)partially controls memory decay rates across layers, indicating longer
E
l
memorypersistenceinthelong-termlayerwithahigherstability value.Inthecontextoftrading,companyannualreports,suchas Form 10-Ks, are considered to have more extended timeliness compared to daily ﬁnancial news. Therefore, they are assigned a higher stability value and are categorized within the deeper processing layer. This classiﬁcation reﬂects their extended rele-vance and impact in ﬁnancial decision-making scenarios.

SRelevancyl   = kmEk2 × kmPk2	(3)
m
· m
E	P
E

The relevancy score, denoted as Srelevancy , quantiﬁes the co-sinesimilaritybetweentheembeddingvectors.Thesevectorsare derived from the textual content of the memory event, mE, and the LLM prompt query, mP, using OpenAI’s “text-embedding-ada-002” model, as depicted in (3). The LLM prompt query
E
l
incorporates inputs related to trading inquiries and the trading agent’s character setting.
E
The importance score SImportancel  is computed using the value
E
vl     from a uniform piecewise scoring function (Formula 4),
multiplied by a degrading ratio θl  (Formula 5) as per (6). The likelihood of higher vE  values increases from shallow to deep layers. θl measures the diminishing importance of an event over time, which has a close form design of [14]. But our approach tailors θl   to the stratiﬁed structure of long-term memory. It adopts unique exponential functions for each layer. The base αl for each layer is a hyperparameter, set to follow the sequence: αshallow  < αintermediate  < αdeep. These values correlate with the rate at which their importance degrades after a certain pe-
l
riod, providing another angle to measure importance variances across different memory types. Through experimentation, we set αshallow  = 0.9, αintermediate  = 0.967 and αdeep  = 0.988. This ensures that θl  decreases to a threshold score of 5 after intervals of 30,90,  and 365 days for shallow, intermediate, and deep layers, respectively. The three-piece-wise functions
E	E
l	l
for SImportance    and SRecency    enable FINMEM  to have layered processinginthelong-termmemorycomponent.Memoryevents are purged when SRecency   is below 0.05 or SImportance   is under 5 (pre-scaling).
E	E
l	l

⎧40   with probability p1
⎨
vE  =	60   with probability p2	(4) 80   with probability p3
l
⎩

θl  = (αl)δE ,    l = shallow,intermediate,deep,	(5)

where  p1 + p2 + p3  = 1,  but  their  values  vary  by  shal-low,  intermediate,  and  deep  processing.  when  shallow  pro-cessing p1,p2,p3  = {0.8,0.15,0.05}, intermediate processing, p1,p2,p3  = {0.05,0.8,0.15} anddeepprocessing,p1,p2,p3  = {0.05,0.15,0.8}.

SImportancel   = vl    ∗ θl,	(6)
E	E
Furthermore, an access counter function oversees the trans-fer of memory events among layers, ensuring that signiﬁcant events inﬂuencing trading decisions ascend from shallower to deeper layers for extended retention and recurrent access by FINMEM. Conversely, less pertinent events gradually diminish. ThisprocessisfacilitatedbytheLLMvalidationtoolGuardrails AI [55], which monitors critical memory IDs across different layers. An event identiﬁed as pivotal for investment success
E
receivesanadditional5pointsinitsimportancescoreSImportance . Upon meeting the criteria for upgrading to a deeper layer, an event’s recency score SRecency   is reset to 1.0, emphasizing its importance and preventing rapid decay. By implementing this
l
E
l
access counter, FINMEM  effectively identiﬁes and prioritizes key events, taking into account their nature and frequency of retrieval.

C.  Decision-Making Module
The decision-making module of FINMEM  efﬁciently inte-grates operational outcomes from the proﬁle and memory mod-ules to support well-informed investment decisions, as depicted in Fig. 1(1). In its daily trading decisions, FINMEM is asked to select from three distinct actions for a single share of a speciﬁc stock by Guardrails AI text validation function: “Buy”, “Sell”, or “Hold”. Additionally, the inputs and results required by FIN-MEM’s decision-making module vary between its training and testing phases, with each phase’s speciﬁcs detailed as follows:
During the training phase, FINMEM accesses a wide array of multi-source information relevant to the entire time period. When FINMEM is prompted with trading inquiries containing stock ticker and date, as well as trader character-related texts, it concurrentlyinitiatesobservationandsummarizationoperations in its working memory. FINMEM observes the market ground labels mentioned in the description about the observation op-eration in Section III-B1, which involve daily adjusted price differences between consecutive days, indicative of “Buy” or “Sell” actions. Utilizing these price change signals, FINMEM identiﬁes and prioritizes the top-K  memories, ranking them based on retrieval scores from each long-term memory layer. This procedure enables FINMEM  to produce comprehensive reﬂections that provide a well-founded rationale and in-depth inference of the correlation between market ground labels and the memories retrieved. Through repeated trading operations, reﬂections, and memory events with signiﬁcant impact, transi-tion to a deeper memory processing layer, getting preserved for guiding future investment decisions during the testing phase.
Inthetestingphase,whereFINMEMcannotaccessfutureprice data, it relies on the Cumulative Return over the previous M trading days to anticipate future market trends. To compensate for the absence of future market price information, FINMEM



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3449



utilizesenhancedreﬂectionsderivedfromimmediatereﬂections spanninganM-trading-dayperiodassupplementaryreferences. When faced with a speciﬁc trading inquiry, FINMEM integrates insights from various sources, including historical Cumulative Return, outcomes from extended reﬂection, and the Top-K retrieved memories. This comprehensive approach enables FIN-MEM to execute well-informed trading decisions.
It should be noted that FINMEM generates executable actions exclusively in the immediate reﬂection operation of the testing phase. Since the trading direction is guided by the actual price trend, the training phase of FINMEM does not make investment decisions. Instead, this phase is dedicated to accumulating trad-ing experience through comparing market trends with incom-ing multi-source ﬁnancial messages. Additionally, during this phase, FINMEM  develops a memory module enriched with a comprehensive knowledge base, thereby evolving its capability for independent decision-making in future trading activities.

IV.  EXPERIMENTS SETUPS
We aim to evaluate the trading performance of FINMEM. And we further illustrate its unique advantages of requiring signiﬁcantly less historical trading time window to train and take full use of key ﬁnancial data time series as well as textual information. Speciﬁcally, we conducted several experiments to study the following research questions (RQs):
r
RQ1: Is FINMEM capable of outperforming contemporary state-of-the-art algorithmic trading agents?
r
RQ2: Is FINMEM able to provide reliable performance on stocks with limited training data?
r
RQ3: Which LLM is best suited to form the backbone framework of FINMEM?
r
RQ4: Does the various risk inclination options that FIN-MEM’s proﬁle module offers truly differentiate its trading performance?
r
RQ5: Does FINMEM’s unique feature of an adjustable cognitive span effectively facilitate informed trading de-cisions?
In  the  rest  of  the  section,  we  begin  by  introducing  the real-world ﬁnancial dataset used in our experiments. We then describe the comparative algorithmic agents and list several widely used ﬁnancial metrics. Our experiments fall into two categories: 1) The comparative experiments of FINMEM versus other algorithmic trading agents, and FINMEM using different LLMs as backbone algorithms. 2) The ablation studies evaluate the effects of FINMEM’s adjustable cognitive span and the role of the trader’s dynamic character settings, particularly the risk inclinations, on its trading performance. Through experiments, FINMEM demonstrates to outperform other comparative algo-rithmic agents. Furthermore, we are able to show that its proﬁle and memory modules are sophisticated and tailored to effec-tivelyaddresstheintricaciesoftheﬁnanciallandscape,resulting in superior trading performance.

A.  Datasets and Database Structure:
We  assessed  FINMEM’s  performance  using  a  backtesting strategy running on multi-source ﬁnancial data from August













Fig. 2.    FINMEM’s data warehouse architecture and data pipelines. Multi-source data were collected from various APIs and organized into corresponding data warehouses to construct its layered long-term memory module.


15, 2021, to April 25, 2023. These data are collected through reputable ﬁnancial databases and APIs like Yahoo Finance (via yﬁnance) and Alpaca News API, detailed explained in Table 2. The stock tickers used in our comparative experiments are detailed in Fig. 11 of Appendix A, online available. These were selectedbecause they areamong thosewiththehighest volumes of accessible news text data, and they are spread across various trading sectors. This selection provides ample data to evaluate FINMEM’s generalization capabilities. Additionally, Tesla, Inc. (TSLA)wasselectedforablationstudiesbecauseitisassociated withasubstantialamountoftextualdata,providingampleinfor-mation to thoroughly assess the robustness of FINMEM’s perfor-mance. We conducted a sensitivity analysis on TSLA to demon-stratetheeffectiveness ofitsadjustablecognitivespanfunctions and the reinforcement of critical memory insights through an access count mechanism for the same reasons. Note that while we conducted backtesting—a standard and practical method for evaluating trading strategies—the FINMEM  framework is fully equipped to execute real-time trading once it integrates streaming ﬁnancial data API calls. Furthermore, the results of our ablation studies and sensitivity analysis demonstrate that FINMEM’s mechanism can promptly reinforce critical market trendsignals,facilitatinginformeddecision-making,whichshed light on its adaptability to sudden market changes in real-time investment.
The multi-source ﬁnancial data, derived from various data sources and APIs, collectively form the ”Market Environment Data Warehouse”. While the daily stock open-high-low-close-volume (OHLCV) is channeled to FINMEM’s working mem-ory to convert into texts interpreted market facts through its observation operation, the rest of the data are then funneled intoFINMEM’s”LayeredLong-termMemoryDataWarehouse,” sortedbytimeliness.Thisisachievedthroughthesummarization operation of the working memory, utilizing LLM prompts, as illustrated in Fig. 2. The deep processing layer holds annual reports (Form 10 K’s) insights, the intermediate layer contains quarterly reports (Form 10Q’s) insights, and the shallow layer accommodates daily ﬁnancial news insights. Given the daily frequency of this ﬁnancial news dataset, we evaluated FIN-MEM’s performance on a daily basis. This is a frequency is tent with prevalent trading practices, and it can complement



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3450	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025



high-frequency trading that operates at microsecond intervals in real-world fund ﬁrms, thereby enhancing investment diversity. But considering the typical response time of LLM models, the FINMEM mechanism is capable of generating sequential trading decisions at the granularity of minute-level intervals.
We leveraged the open-source vector database FAISS [56] for constructing the memory warehouse of FINMEM, beneﬁt-ing from its rapid querying in high-dimensional vectors and compatibilitywithOpenAIforcosine-similarity-basedsemantic searches on speciﬁc tickers. This setup facilitates efﬁcient top-ranked event retrieval. Data categorization and memory module workﬂow are also illustrated in Fig. 1.

B.  Baseline and Comparative Models:
We assess FINMEM’s trading performance in comparison to ﬁve advanced algorithmic agents and a commonly accepted baseline trading strategy. Among these, three models employ Deep Reinforcement Learning (DRL) approaches, while the re-mainingtwoarebasedonLargeLanguageLLMs.Briefdescrip-tions of each are provided below, with a detailed introduction elaborated in Appendix B, online available:
Buy-and-Hold strategy (B&H): A passive investment ap-proach, where an investor purchases stocks and holds onto them for an extended period regardless of market ﬂuctuations, is commonly used as a baseline for comparison of stock trading strategies.
DRL trading agents: As the FINMEM is practiced and exam-ined on the basis of single stock trading and discrete trading ac-tions, we choose three advanced DRL algorithms ﬁtting into the same scenarios according to the previous and shown expressive performance in the work of Liu et al. [57], [58]. The three DRL algorithms are Proximal Policy Optimization (PPO) [59], Deep Q-Network(DQN)[60]andAdvantageActor-Critic(A2C)[61]. The DRL training agents only take numeric features as inputs.
LLM trading agents: We evaluate FINMEM against two LLM agents in the context of stock trading. The ﬁrst LLM agent – General-Purpose Generative Agents (GA) [62], known for its proﬁciency in general-purpose tasks, serves as a baseline. The second agent – LLM trading agents(FINGPT) [11], a leading-edge LLM in trading, has been acclaimed for its promising performance in stock market operations.

C.  Evaluation Metrics:
We employ ﬁve widely-used metrics in ﬁnance to compare the investment rewards of FINMEM against other algorithmic trading agents. Here are their introductions:
r
Cumulative Return (CR) % [63]: Cumulative Return is a key trading performance metric because it provides a comprehensive insight into investment performance, es-pecially for strategies that emphasize long-term growth andreinvestment.Theeffectivenessofdifferentinvestment strategies is evaluated based on their Cumulative Returns, which reﬂect the total change in value over time. In this study, we compute Cumulative Returns over the speciﬁed periodbysummingdailylogarithmicreturns,asoutlinedin (7). This method is widely accepted in the ﬁnance area due

to its ability to precisely capture minor price ﬂuctuations and symmetrically address gains and losses. In essence, a higher Cumulative Return typically indicates a more effective strategy.
n	n        		
X	X
p
p
CR =		ri  =	ln	t+1	· actiont	(7) t=1               t=1                      t
where ri represents the logarithmic return forday t + 1, pt is the closing price on day t, pt+1  is the closing price on day t + 1, and actiont  denotes the trading decision made by the model for that day.
r
Sharpe Ratio (SR) [64]: Sharpe Ratio is another core met-ric for evaluating investment performance and adjusting returns for risk. It is calculated by dividing the portfolio’s averageexcessreturn(Rp)overtherisk-freerate(Rf )byits volatility (σp), as shown in (8). This metric adjusts returns for risk, with a higher ratio indicating better risk-adjusted performance. Essential in comparing different portfolios or strategies, it contextualizes performance against similar investments. Although a Sharpe Ratio above 1 is typically consideredfavorableandabove2asexcellent,thesebench-marks can vary depending on the context of comparison.

SR = Rp − Rf	(8)
σ
p
r  Annualized Volatility (AV) % and Daily Volatility (DV) % [65]: Annualized Volatility is calculated as the Daily Volatility (standard deviation of daily logarithmic returns) multiplied by the square root of the typical number of trading days in a year (252) as outlined in (9), is vital for assessing investment risk. This measure reﬂects the extentofﬂuctuationinareturnseriesoverayear,indicating
potential deviations from average returns.
√
AV = DV ×	252	(9) r  Max Drawdown (MDD) % [66]: Max Drawdown is a
metric for assessing risk. It represents the most signiﬁcant decreaseinaportfolio’svalue,fromitshighest(Ppeak)toits lowest point (Ptrough) until a new peak emerges, detailed in (10).Indicativeofinvestmentstrategyrobustness,asmaller
Max Drawdown suggests reduced risk.
MDD = maxPpeak − Ptrough 	(10) peak
P
In our experiments and ablation studies, we recorded the metric outcomes as an average from ﬁve repeated trials.

V.  EXPERIMENTS

A.  Implementation Details:
In the Trading Agents Comparison, FINMEM employs GPT-4 as its backbone algorithm. The temperature parameter of the model is set at 0.7 to maintain a balance between response content consistency and model creativity. It was trained on ﬁnancial data from August 17, 2021, to October 05, 2022, and tested with data from October 06, 2022, to April 10, 2023. The



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3451

























Fig. 3.    CR comparisons between FINMEM and other algorithmic agents for TSLA, NFLX, AMZN, and MSFT in the testing phase. FINMEM won the highest CRs for all of them by the end of the testing phase.



training period was chosen to account for the seasonal nature of corporate ﬁnancial reporting and the duration of data retention in FINMEM’s memory module. The selected training duration ensures the inclusion of at least one publication cycle of either Form 10-Q, classiﬁed as intermediate memory, or Form 10-K, regarded as deep memory, or in some instances, both. This strategy ensures that the experiences retained in FINMEM are still inﬂuential during the testing phase for a signiﬁcant period. Additionally, the training duration allowed FINMEM sufﬁcient time to establish inferential links between ﬁnancial news, mar-ket indicators, and stock market trends, thereby accumulating substantial experience. Furthermore, we set the number of top memory events retrieved from each layer of long-term memory at 5. We ran FINMEM  using each of the three available risk inclination settings. The reported performance outcomes are based on the setting that achieved the highest cumulative return during the testing phase.
To maintain consistency in the comparison, the training and testingphasesfortheothertwoLLM-basedagentswerealigned with those of FINMEM. For parameters of other LLM-based agents that are not encompassed by FINMEM’s conﬁguration, they were kept in accordance with their original settings as speciﬁed in their respective source codes.
ConsideringthatDRLalgorithmsneedextensivetrainingdata for stable and converged results, and given our daily evaluation of trading performance, we extend the DRL agents’ training period to roughly a 10-year span, from January 1, 2012, to October 05, 2022, for a fair comparison. The testing period is kept consistent with the other models. The DRL algorithms are implemented using Stable Baselines 3 [67].

FINMEM’s performance was benchmarked against that of the mosteffectivecomparativemodel,usingCumulativeReturnand Sharpe Ratio as the primary evaluation metrics. The statistical signiﬁcanceofFINMEM’ssuperiorperformancewasascertained throughthenon-parametricWilcoxonsigned-ranktest,whichis apt for the non-Gaussian distributed data.



B.  Algorithmic Trading Agents Comparison (RQ1 & RQ2)
Inthisexperiment,weassessedthestocktradingperformance of FINMEM  against other models, focusing on stocks from ﬁve companies in different trading sectors: Tesla, Inc. (TSLA), Netﬂix, Inc. (NFLX), Amazon.com, Inc. (AMZN), Microsoft Corporation (MSFT), and Coinbase Global, Inc. (COIN). The performance of all algorithmic trading agents across ﬁve key metricswasconsolidatedinTableI.GiventhepivotalroleofCu-mulativeReturninevaluatingtradingperformanceovertime,we presenteddetailedtimeseriesplotsinFigs.3and4.It’simportant to note that the trading performance of FINMEM for COIN was exclusivelycomparedwithLLMtradingagentsandthebaseline. ThiswasbecauseCoinbaseGlobal,Inc.hadcompleteditsIPOin April 2021 and, as a result, had not accumulated enough trading data to facilitate stable outcomes with DRL algorithms. These plots illustrate the changes in Cumulative Return for each of the ﬁvecompaniesthroughoutthetestingphase,offeringanin-depth comparison of performance.
In response to RQ1, the trading outcomes presented in Ta-ble I revealed that FINMEM  outperformed all other algorith-mic trading agents and the B&H baseline strategy in terms




Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3452	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025


TABLE I
TRADING PERFORMANCE COMPARISON DURING THE TESTING PERIOD BETWEEN FINMEM AND OTHER ALGORITHMIC AGENTS ACROSS FIVE STOCKS










Fig. 5.    CRs of FINMEM on trading TSLA over an extended testing period. FINMEM consistently shows high performance.




























Fig. 4.    Comparison of CRs between FINMEM and other LLM-based agents for COIN during the testing phase shows that FINMEM leads in trading tasks for recently IPO-ed stocks that can be managed by language agents. This presents a challenge to DRL-based agents.


of Cumulative Return and Sharpe Ratio. FINMEM’s superiority was statistically signiﬁcant when compared to the second-best trading strategy. Speciﬁcally, for TSLA and NFLX, FINMEM’s strategy achieved Sharpe Ratios exceeding 2.0 and Cumulative Returns surpassing 0.35 while maintaining the lowest Volatility and Max Drawdown. These indicators underscored FINMEM’s ability to generate higher returns per unit of risk. In the case of MSFT and NFLX, FINMEM also recorded a Sharpe Ratio

1Theboldnumbersinthisandsubsequenttablessignifythebestperformance for the respective metrics.

above 1.0 and a Cumulative Return over 0.2, coupled with relatively low Volatility and Max Drawdown, demonstrating its impressive trading performance. For AMZN and COIN, FIN-MEMconsistentlydeliveredpositiveCumulativeReturnsandsu-periorSharpeRatios,outperformingotherstrategiesthatyielded negative values forthesemetrics.Additionally, itsVolatilityand MaxDrawdownwereonthelowerend.Hence,theseresultscol-lectively demonstrated FINMEM’s robust trading performance across a diverse range of trading sectors. Speciﬁcally, FINMEM exhibitedsuperiorperformancecomparedtothetwootherLLM agents in our study, FINGPT and the general-purpose generative agent developed by Park et al. This underscored the effective-ness of FINMEM’s unique proﬁle and memory structure, which are particularly tailored for LLM agents dealing with ﬁnancial data, signiﬁcantly enhancing their investment decision-making capabilities.
In response to RQ2, the main challenge for DRL trading agents is that they require training data with a large volume and extended time span, which are hard to achieve when operating on stocks with limited historical data. As shown in Table I, our experiments revealed that FINMEM achieved superior trading performance with a much shorter training duration compared to DRL trading agents trained on data spanning nearly a decade. This efﬁciency makes FINMEM  particularly useful for newly publiccompanieslikeCoinbaseGlobal,Inc.,whichhavelimited trading histories. DRL agents often face convergence issues due toinadequatetrainingdatainsuchcases.Moreover,evenamong LLM-based trading agents suited for shorter training periods, FINMEM’s performance stood out, as illustrated in Fig. 4.
To further assess FINMEM’s adaptability to limited training data, we narrowed the training period down to an even shorter period, spanning from August 17, 2021, to February 10, 2022. We then extended the testing phase to cover from February 11, 2022, to April 25, 2023. This evaluation focused on the stock of Tesla, Inc., which has the largest volume of news data. The tradingperformanceofFINMEM duringthisperiodisdepictedin Fig.5.Remarkably,usinglessthansixmonthsofdailyfrequency data for training, which encompassed the publication of one Form 10-K and one Form 10-Q, FINMEM consistently ranked highingainsandattainedthehighestcumulativereturnafterthe latter half of December 2022.



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3453


TABLE II
TRADING PERFORMANCE COMPARISON OF FINMEM USING DIFFERENT LLMS AS BACKBONES









In general, the consistently strong trading performance of FINMEM can be attributed to its innovative proﬁle and mem-ory module design. This design enables FINMEM to effectively integrate,comprehend,andprioritizekeyinformationfromboth textual and numerical data. Additionally, the memory module’s core features, including varied retention times for different in-formation types and critical memory transitions, equip FINMEM to capture essential information for well-informed investment decisions.

VI.  ABLATION STUDIES
We conducted four distinct ablation studies to evaluate key component alternatives in FINMEM. The ﬁrst three studies con-centrated on the backbone algorithm, the memory module’s cognitive capacity, and the character setting in the proﬁle mod-ule, particularly examining the aspect of risk inclination. These studies were done using the stock of Tesla, Inc., with a more compact training period from March 14, 2022, to June 15, 2022, and a testing period from June 16, 2022, to December 28, 2022.Thisshorterdurationwaschosenforbudgetaryefﬁciency, yet it remained sufﬁcient to differentiate the functionality of eachcomponent.Theﬁnalstudyprovidesacomprehensivecase analysis demonstrating FINMEM’s end-to-end decision-making workﬂow on a speciﬁc trading day.

A.  FINMEM Backbone Algorithm Comparison (RQ3)
In  our  ﬁrst  study,  we  evaluated  the  trading  performance of FINMEM using various LLMs as its backbone algorithms. The LLMs under consideration included davinci-003, GPT 3.5-Turbo, GPT4, GPT4-Turbo, and Llama2-70b-chat. The param-eter settings were consistent with its optimal performance in the comparative experiment detailed in Section V, and the risk inclination was conﬁgured to be self-adaptive. All other model settingsweremaintainedasoutlinedinSectionV-A.Theresults of this evaluation are compiled in Table II.
Addressing RQ3, the ﬁndings demonstrate that FINMEM, powered by GPT-4 and GPT-4 Turbo, delivered superior trading results during the test phase. Speciﬁcally, GPT-4 recorded the highest cumulative return, while GPT-4-Turbo exhibited the mostfavorableSharpeRatio.GPT3.5-Turbo’sperformancewas alsonoteworthy,followingcloselybehind.AsdepictedinFig.6, though slightly lower than market baseline (B&H), FINMEM with GPT-4-Turbo led in cumulative returns before October 2022. This period was characterized by relative stability and a modest upward trend in TSLA stock. After October 2022, with



Fig. 6.    Comparison of CRs over time for FINMEM using different LLMs as backbones. FINMEM  with GPT-4 achieves the highest CR by the end of the testing phase.














Fig.7.    ComparisonofCRswithdifferentriskinclinationsettingsinFINMEM’s proﬁle module. The self-adaptive risk setting helps FINMEM achieve the highest and the only positive CR by the end of the testing phase.



TSLA undergoing increased volatility and a notable downward trend, the cumulative return trajectory for FINMEM with GPT-4-Turbo exhibited signiﬁcantly lower volatility and sustained stable returns not markedly lower than those of GPT-4. These results indicate that GPT-4 Turbo is the most suitable backbone algorithm for FINMEM.
FINMEM conﬁgured with davinci-003 and Llama2-70b-chat exhibited the lowest Annualized Volatility and Max Drawdown, yet their Cumulative Return and Sharpe Ratio were under-whelming. As illustrated in Fig. 6, both  models defaulted to a “Hold”strategybeyond acertainpointduringperiodsofintense ﬂuctuation in TSLA stock. The unsatisfactory performance of davinci-003 may be attributed to its limited capability, as an earlier generation language model, to capture and understand nuanced yet decisive information.
We selected Llama2-70b-chat as it was deemed to possess stronger in-context learning and instruction-following capabili-ties compared to other Llama family models with fewer param-eters, as noted in Zhao et al. [68]. Nonetheless, in the context of stock trading, it still demonstrated challenges in adequately comprehending key messages necessary for effective trading decisions. The comparatively poorer performance of Llama2-70b-chat can also be attributed to its shorter context window,



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3454	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025


TABLE IV
TRADING PERFORMANCE COMPARISON WITH    ARIOUS SETTINGS OF WORKING MEMORY CAPACITY
V











Fig. 8.    Comparison of CRs for FINMEM  with different working memory capacitysettings.ThelargestworkingmemorycapacitysettingenabledFINMEM to achieve the highest CR by the end of the testing phase.

TABLE III
TRADING PERFORMANCE COMPARISON WITH DIFFERENT RISK INCLINATION SETTINGS IN FINMEM’S PROFILE MODULE







especially when compared to the GPT models. When integrated with FINMEM, it needed to simplify prompts and shorten the length of retrieved memory insights, which could potentially result in some loss of context. The exceptional trading result demonstrated by GPT-4-Turbo across all models was a main factor in choosing it as the backbone algorithm for FINMEM in our earlier comparative analysis with other algorithmic trading agents.

B.  Inﬂuence About Varying the FINMEM Character Design (RQ4)
In our second study, we focused on evaluating the inﬂu-ence of FINMEM’s proﬁle module on its trading effectiveness. Speciﬁcally,ourassessmentcenteredontheeffectsofcustomiz-ing trader proﬁles according to speciﬁc stock trading, with a particular focus on risk inclination. As depicted in Fig. 13 in Appendix  D,  online  available,  we  equipped  FINMEM  with three  distinct  risk  proﬁles:  risk-seeking,  risk-averse,  and  a self-adaptive character. We executed a comparative analysis of FINMEM’s performance across these risk proﬁles, maintaining consistency in all other settings as outlined in Section V-A.
In response to RQ4, Table III  illustrates the varied trading performance of TSLA across different risk proﬁles. The self-adaptive proﬁle enabled FINMEM to achieve the most favorable trading performance, as it was the only one to secure a positive Cumulative Return and a Sharpe Ratio exceeding 2.0, along with the least Max Drawdown by the end of the testing phase. It further illustrates that FINMEM with self-adaptive risk set-ting can adeptly navigate substantial stock price volatility and strategically modulate its trading behavior when necessary. In contrast, the risk-seeking proﬁle exhibited increased volatility

and a decline in the face of a market downturn. The risk-averse proﬁle, on the other hand, maintained a more conservative stance, often opting to hold positions. This approach resulted in a Cumulative Return trajectory that generally lagged behind the market baseline, reﬂecting a degree of overcaution that limited trading activity and potential gains, particularly in a bullish market.
The  results  presented  in  Fig.  13  in  Appendix  D,  online available demonstrate that FINMEM’s proﬁle module offers the ﬂexibility to adjust risk inclinations, enhancing its ability to exploit rising market trends while safeguarding assets during downturns. For most stocks, FINMEM achieved optimal trading resultsunderaself-adaptiverisksetting.However,MSFTproved anexception,witharisk-seekinginclinationyieldingmarginally better outcomes. Speciﬁcally, the Cumulative Return for MSFT under risk-seeking and self-adaptive settings were 23.2613% and 19.147% respectively, while the corresponding Sharpe Ra-tioswere1.4402and1.03872.Bothconﬁgurationsoutperformed othertradingstrategies.TheseoutcomesresonatedwithMSFT’s generally bullish trend during the testing phase. In a predomi-nantly positive market environment with substantial stock price increases, the risk-seeking proﬁle enabled FINMEM  to make moreconsistentandeffective‘Buy’decisions,bettercapitalizing on market trends. This approach proved less susceptible to small historical price ﬂuctuations or noisy market signals that can otherwise trigger ‘Hold’ decisions. Nevertheless, for the rest of the stocks experiencing downward or mixed trends, the self-adaptive conﬁguration proved more effective. This setting allowed FINMEM to adopt a cautious strategy when faced with negativeshort-termcumulativereturns.Whenshort-termreturns turned positive, FINMEM  could switch to a more optimistic and assertive approach, thus avoiding excessive passivity while maintaining prudent risk management. This adaptive capability underscores FINMEM’s robustness across varied market condi-tions, demonstrating its potential to optimize trading strategies based on individual stock conditions and prevailing market trends.

C.  Impact of Adjusting the Capacity of FINMEM Working Memory (RQ5)
In our third study, we explored whether appropriately tuning the memory retrieval bandwidth of FINMEM could enhance its trading performance. This bandwidth is tied to the working memory’s capacity within its memory module. As depicted in Fig. 1, FINMEM retrieves  the top-K memory events from its



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3455






















































Fig. 9.    First section of FINMEM’s workﬂow for perceiving and processing multi-source information from market environment.





Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3456




































Fig. 10.

IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025




































Second section of FINMEM’s workﬂow for generating trading action, reasoning and reﬂection.




long-termmemoryinresponsetoatradinginquiry.Theworking memory capacity is thus set at 3 × K, mirroring the human cognitivesystem’slimitofprocessingimmediatemessagesupon speciﬁc stimuli (3 refers to the three processing layers in long-term memory). By varying the K hyperparameter, FINMEM can expand this capacity far beyond the human cognitive scope. We aimed to determine whether such ﬂexibility in adjusting memory bandwidth translates to improvements in FINMEM’s performance.
AsdemonstratedinTableIV,weadjustedthehyperparameter K to alter the number of memory events retrieved from shallow, intermediate,anddeeplong-termmemorylayersinFINMEM.We tested K values of 1, 3, 5, and 10, exploring FINMEM’s working memory capabilities at levels below, near, and above the human cognitive limit. For all these K settings, we maintained a self-adaptive risk inclination, while other settings were consistent with those described in Section V-A.
Across all K conﬁgurations, FINMEM outperformed the Buy & Hold baseline, indicating the effectiveness of its memory module in processing diverse information and capturing critical

events,whichsubsequentlyenhanceditstradingperformance,as evidenced by positive Cumulative Returns and Sharpe Ratios. Notably, higher K values, like 5 and 10, enabled FINMEM to achieve the best Cumulative Returns and Sharpe Ratios exceed-ing 2.0. With K set to 1, FINMEM still performed moderately well by capturing the most critical memory events of each layer. An in-depth analysis in Fig. 8, which shows the Cumula-tive Return over time for various K  settings, reveals that a K value of 5 is optimal for trading TSLA stock, consistently delivering robust performance with the lowest Volatility and Max-Drawdown. Before mid-October 2022, when the stock market was relatively stable and slightly upward, FINMEM’s trading actions aligned well with market trends (referring to B&H) and avoided signiﬁcant losses. During periods of high volatility and continuous downturns (post-mid-October 2022), it maintained earnings by reducing ”Buy” actions and favoring more ”Hold” and ”Sell” strategies. However, setting K to 10, while effective during market volatility, resulted in signiﬁcant losses in stable market conditions. The issue may stem from the disproportionately loose capacity constraints on incoming



Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3457



information relative to the volume of incoming data. The broad memory retrieval bandwidth might have mixed trivial messages with critical ones, hampering FINMEM’s decision precision. This challenge becomes especially evident in neutral market conditions, where the inﬂux of information includes a mix of varying market sentiments and trends.
AddressingRQ5,appropriatelytuningthenumberofmemory events (Top-K) in the FINMEM  memory module can signif-icantly enhance its trading performance. The aforementioned study illustrates that FINMEM can achieve optimal results by effectively assimilating key signals from a sufﬁcient quantity of ﬁltered memories across each layer. However, the optimal value for K may vary depending on the volume and quality of incoming information.


D.  Case Study: Forecast for TSLA on 2022-10-25 to Predict Trading Decision on 2022-10-26
In the last study, we presented the trading decision-making process for TSLA on October 25, 2022, to forecast whether to ’buy’ or ’sell’ on October 26, 2022. We outlined the workﬂow, presented the prompt template for FINMEM, and compared out-comes across three distinct risk proﬁles.
We present the complete workﬂow of FINMEM in two sec-tions.  The  ﬁrst  section,  depicted  in  Fig.  9,  illustrates  how FINMEM utilizes its layered memory module and customized prompt templates to effectively manage various operational mechanisms. These mechanisms are designed to perceive, or-ganize, and distill trading-relevant insights from multi-source market information. The second section, illustrated in Fig. 10, outlines the steps FINMEM takes after extracting investment-relatedinsightsfromthemarket,asdescribedintheﬁrstsection. It elaborates on reﬂection outcomes generated upon three risk proﬁle options of FINMEM.
AsshowninFigs.9and10,FINMEM processesthelatestdaily news about TSLA through its shallow processing layer within the long-term memory database. The Form 10-Q released on October 24, 2022, is processed through the intermediate layer, whileaccess-counterreinforcedinsightsandreﬂectivereasoning are accelerated in the deep processing layer. Given the same input information, FINMEM  executes distinct trading actions under varying risk proﬁles. With positive historical momen-tum and generally favorable market signals, FINMEM  adopts a risk-tolerant stance and executes a ’Buy’ action, consistent with its risk-seeking proﬁle. This decision proves proﬁtable, as the stock price increases by $2.22 the following trading day. However, the rationale behind this decision and the insight IDs recognized as key contributors vary between the two proﬁle options. FINMEM under a risk-seeking proﬁle displays a degree of impulsiveness by disregarding negative market information in its decision-making process. In contrast, the self-adaptive setting demonstrates a more balanced approach, considering both positive and negative aspects of available insights. This more holistic view is achieved by incorporating reminders of the ’risk-averse’ condition in the prompt for the self-adaptive proﬁle. Meanwhile, in a risk-averse proﬁle, FINMEM opts for a ’Hold’ decision, prioritizing caution over higher proﬁts.

VII.  CONCLUSION AND FUTURE WORK
In this paper, we introduce FINMEM, an innovative auto-matedtradingagentframeworkfeaturinganadjustablecognitive memory structure and dynamic character design. Our experi-ments demonstrate its exceptional proﬁciency in sequential ﬁ-nancial decision-making tasks, particularly stock trading. Com-pared  to  other  SOTA  algorithmic  trading  agents,  FINMEM achieves signiﬁcantly higher key performance metrics on real-world ﬁnancial datasets across multiple stock assets. Through ablation studies, we thoroughly assess the efﬁcacy of each critical component within FINMEM, revealing their substantial contributions to enhancing stock trading performance.
Its unique features of human-like cognitive memory modules anddynamiccharacterdesignenableittotacklethecomplexities of ﬁnancial environments and respond aptly to new situations. Compared to other LLM trading agents, FINMEM’s memory module equips it with the capability to better comprehend ﬁ-nancial data featured by various timeliness and organize them as a self-evolving long-term memory layer. The dynamic char-acterdesignendowsFINMEM withcriticalprofessionalinsights, enablingefﬁcientﬁlteringofimpactfulmessagesfromincoming ﬁnancial data for trading actions. Additionally, the integration of multiple risk proﬁles enhances FINMEM’s adaptability to a range of market conditions.
FINMEM’s exceptional performance demonstrates its ability to transform diverse ﬁnancial data into well-informed invest-ment strategies. Its proﬁciency in integrating various data types is further highlighted by a reduced training duration, which beneﬁts trading with newly established companies. We used a limited range and quality of ﬁnancial news and reports, em-ployinggeneral-purposeLLMsasthebackbonealgorithms.Yet, FINMEM is fully compatible with LLMs ﬁne-tuned for ﬁnancial applications. We anticipate its trading efﬁcacy will improve withaccesstoamorecomprehensiveandhigher-qualitydataset, along with LLMs tailored for ﬁnancial contexts.
Primarily designed for ﬁnancial decision-making, the huge potentialforFINMEM tohandlevarioustaskscouldbeexpanded infutureresearchacrossmultipledimensions.Fromtheperspec-tive of broadening the range of trading assets, FINMEM could extend beyond stock trading to include other ﬁnancial products such as cryptocurrencies and Exchange-Traded Funds (ETFs). Additionally,  from  the  standpoint  of  diversifying  decision-makingapplications,FINMEM couldbeadaptedtoaddressmore complex tasks like portfolio management and stock selection. This expansion would require further development of prompt templates and the integration of external tool-use functions in LLMs, such as API calls and tabular data reading capabilities. Moreover, FINMEM offers an innovative and ﬂexible framework that extends beyond decision-making to accommodate various ﬁnancial tasks. Its adaptable proﬁle module, built upon LLM prompts, is naturally suited for articulating nuanced risk char-acteristics in explainable texts for market participant agents. Additionally,thememorymodule,alignedwithhumancognitive architecture,canbereplicatedacrossmultipleagentstofacilitate peer-to-peer communication and self-reﬂection. These features enable FINMEM to conduct ﬁnancial market simulations in a more interpretable manner.


Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
3458	IEEE TRANSACTIONS ON BIG DATA, VOL. 11, NO. 6, NOVEMBER/DECEMBER 2025



REFERENCES

[1]  F. Black, “Noise,” J. Finance, vol. 41, no. 3, pp. 528–543, 1986.
[2]  R. D. Edwards, J. Magee, and W. C. Bassetti, Technical Analysis of Stock Trends. Boca Raton, FL, USA: CRC Press, 2018.
[3]  B. Huang, Y. Huan, L. D. Xu, L. Zheng, and Z. Zou, “Automated trading systems statistical and machine learning methods and hardware imple-mentation: A survey,” Enterprise Inf. Syst., vol. 13, no. 1, pp. 132–144, 2019.
[4]  T. G. Fischer, “Reinforcement learning in ﬁnancial markets-a survey,” FAU Discussion Papers Economics, Tech. Rep. (No. 12/2018), 2018.
[5]  A. Millea, “Deep reinforcement learning for trading—a critical survey,” Data, vol. 6, no. 11, 2021, Art. no. 119.
[6]  S. Balhara et al., “A survey on deep reinforcement learning architectures, applications and emerging trends,” IET Commun., 2022.
[7]  S. J. Gershman and B. P. Ölveczky, “The neurobiology of deep reinforce-ment learning,” Curr. Biol., vol. 30, no. 11, pp. R629–R632, 2020.
[8]  OpenAI, “Gpt-4 technical report,” 2023.
[9]  L. Wang et al., “A survey on large language model-based autonomous agents,” Front. Comput. Sci., vol. 18, no. 6, 2024, Art. no. 186345.
[10]  Y. Li et al., “Making language models better reasoners with step-aware veriﬁer,” in Proc. 61st Annu. Meeting Assoc. Comput. Linguistics, 2023, pp. 5315–5333.
[11]  H.Yang,X.-Y.Liu,andC.D.Wang,“Fingpt:Open-sourceﬁnanciallarge language models,” 2023, arXiv:2306.06031.
[12]  S. Wu et al., “Bloomberggpt: A large language model for ﬁnance,” 2023, arXiv:2303.17564.
[13]  G. A. Miller, “The magical number seven, plus or minus two: Some limits on our capacity for processing information,” Psychol. Rev., vol. 63, no. 2, 1956, Art. no. 81.
[14]  J.S.Park,J.O’Brien,C.J.Cai,M.R.Morris,P.Liang,andM.S.Bernstein, “Generative agents: Interactive simulacra of human behavior,” in Proc. 36th Annu. ACM Symp. User Interface Softw. Technol., New York, NY, USA, in UIST ’23, 2023, pp. 1–22, doi: 10.1145/3586183.3606763.
[15]  F. I. Craik and R. S. Lockhart, “Levels of processing: A framework for memory research,” J. Verbal Learn. Verbal Behav., vol. 11, no. 6, pp. 671–684, 1972.
[16]  J.Sweller,“Humancognitivearchitecture:Whysomeinstructional proce-duresworkandothersdonot,”inAPAEducationalPsychologyHandbook, Vol. 1. Theories, Constructs, and Critical Issues. K. R. Harris, S. Graham, T.Urdan,C.B.McCormick,G.M.Sinatra,andJ.Sweller,Eds.,American Psychological Association, 2012, pp. 295–325.
[17]  R. Sun, “Desiderata for cognitive architectures,” Philos. Psychol., vol. 17, no. 3, pp. 341–373, 2004.
[18]  T.  -L.  Chen,  “Forecasting  the  Taiwan  stock  market  with  a  novel momentum-based  fuzzy  time-series,”  Rev.  Econ.  Finance,  vol.  2, pp. 38–50, 2012.
[19]  R. Vaidya, “Moving average convergence-divergence (MACD) trading rule: An application in Nepalese stock market ‘NEPSE’,” Quantitative Econ. Manage. Stud., vol. 1, no. 6, pp. 366–374, 2020.
[20]  E.PätäriandM.Vilska,“Performanceofmovingaveragetradingstrategies over varying stock market conditions: The ﬁnnish evidence,” Appl. Econ., vol. 46, no. 24, pp. 2851–2872, 2014.
[21]  S. Ali, M. Naveed, A. Saleem, and M. W. Nasir, “Time-frequency co-movement between COVID-19 and Pakistan’s stock market: Empirical evidencefromwaveletcoherenceanalysis,”Ann.FinancialEcon.,vol.17, no. 04, 2022, Art. no. 2250026.
[22]  M. Naveed, S. Ali, K. Iqbal, and M. K. Sohail, “Role of ﬁnancial and non-ﬁnancial information in determining individual investor investment decision: A signaling perspective,” South Asian J. Bus. Stud., vol. 9, no. 2, pp. 261–278, 2020.
[23]  F. Abbas, S. Ali, S. Moudud-Ul-Huq, and M. Naveed, “Nexus between bank capital and risk-taking behaviour: Empirical evidence from us com-mercialbanks,”CogentBus.Manage.,vol.8,no.1,2021,Art.no.1947557.
[24]  M.F.Farah,M.Naveed,andS.Ali,“Blockchain-enabledbankingservices and customers’ perceived ﬁnancial well-being: A structural nexus,” in Proc. Nat. Brand Private Label Marketing Conf., 2023, pp. 41–49.
[25]  D. Y. Aharon, S. Ali, and M. Naved, “Too big to fail: The aftermath of Silicon Valley Bank (SVB) collapse and its impact on ﬁnancial markets,” Res. Int. Bus. Finance, vol. 66, 2023, Art. no. 102036.
[26]  M. Naveed, S. Ali, M. Gubareva, and A. Omri, “When giants fall: Tracing therippleeffectsofSiliconValleyBank(SVB)collapseonglobalﬁnancial markets,” Res. Int. Bus. Finance, vol. 67, 2024, Art. no. 102160.
[27]  S. Ali, M. Naveed, H.Hanif, and M. Gubareva,“The resilience of shariah-compliant investments: Probing the static and dynamic connectedness betweengold-backedcryptocurrenciesandGCCequitymarkets,”Int.Rev. Financial Anal., vol. 91, 2024, Art. no. 103045.


[28]  P. Zhang, X. Shi, and S. U. Khan, “QuantCloud: Enabling Big Data complex  event  processing  for  quantitative  ﬁnance  through  a  data-driven execution,” IEEE Trans. Big Data, vol. 5, no. 4, pp. 564–575, Dec. 2019.
[29]  M. Prata et al., “Lob-based deep learning models for stock price trend prediction:Abenchmarkstudy,”Artif.Intell.Rev.,vol.57,no.5,pp.1–45, 2024.
[30]  G.Sonkavde,D.S.Dharrao,A.M.Bongale,S.T.Deokate,D.Doreswamy, and S. K. Bhat, “Forecasting stock market prices using machine learning and deep learning models: A systematic review, performance analysis and discussion of implications,” Int. J. Financial Stud., vol. 11, no. 3, 2023, Art. no. 94.
[31]  Q.-V.Dang,“Reinforcementlearninginstocktrading,”inProc.Int.Conf. Comput. Sci., Appl. Math. Appl., 2019, pp. 311–322.
[32]  O. Jangmin, J. Lee, J. W. Lee, and B. -T. Zhang, “Adaptive stock trading with dynamic asset allocation using reinforcement learning,” Inf. Sci., vol. 176, no. 15, pp. 2121–2147, 2006.
[33]  Y. Shi, W. Li, L. Zhu, K. Guo, and E. Cambria, “Stock trading rule discovery with double deep q-network,” Appl. Soft Comput., vol. 107, 2021, Art. no. 107320.
[34]  H.Yang,X.-Y.Liu,S.Zhong,andA.Walid,“Deepreinforcementlearning for automated stock trading: An ensemble strategy,” in Proc. 1st ACM Int. Conf. AI Finance, 2020, pp. 1–8.
[35]  X.-Y. Liu et al., “FinRL: A deep reinforcement learning library for auto-mated stock trading in quantitative ﬁnance,” in Proc. 2nd ACM Int. Conf. AI Finance, 2021, pp. 1–9.
[36]  T. -V. Pricope, “Deep reinforcement learning in quantitative algorithmic trading: A review,” 2021, arXiv:2106.00123.
[37]  Y.-F.ChenandS.-H.Huang,“Sentiment-inﬂuencedtradingsystembased onmultimodaldeepreinforcementlearning,”Appl.SoftComput.,vol.112, 2021, Art. no. 107788.
[38]  L. Avramelou, P. Nousi, N. Passalis, and A. Tefas, “Deep reinforcement learning for ﬁnancial trading using multi-modal features,” Expert Syst. Appl., vol. 238, 2023, Art. no. 121849.
[39]  J. Devlin, M. -W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deepbidirectionaltransformersforlanguageunderstanding,”inProc.2019 Conf. North Amer. Chapter Assoc. Comput. Linguistics: Human Lang. Technol., 2019, pp. 4171–4186.
[40]  K. Ethayarajh, “How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings,” in Proc. 2019 Conf. Empir. Methods Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process., Hong Kong, China, 2019, pp. 55–65.
[41]  Q. Xie et al., “Pixiu: A comprehensive benchmark, instruction dataset and large language model for ﬁnance,” in Proc. Adv. Neural Inform. Process. Syst., 2023, vol. 36, pp. 33469–33484.
[42]  Y. Li, S. Wang, H. Ding, and H. Chen, “Large language models in ﬁnance: A survey,” in Proc. 4th ACM Int. Conf. AI Finance, 2023, pp. 374–382.
[43]  Y.Nieetal.,“Asurveyoflargelanguagemodelsforﬁnancialapplications: Progress, prospects and challenges,” 2024, arXiv:2406.11903.
[44]  H. Yang et al., “Finrobot: An open-source AI agent platform for ﬁnancial applications using large language models,” 2024, arXiv:2405.14767.
[45]  Y. Zhou et al., “Are large language models rational investors?,” 2024, arXiv:2402.12713.
[46]  J.  S.  Park,  J.  C.  O’Brien,  C.  J.  Cai,  M.  R.  Morris,  P.  Liang,  and M. S. Bernstein, “Generative agents: Interactive simulacra of human behavior,” in Proc. 36th Annu.ACM Symp. User Interface Softw. Technol., 2023, pp. 1–22.
[47]  J. Li et al., “Agent hospital: A simulacrum of hospital with evolvable medical agents,” 2024, arXiv:2405.02957.
[48]  S. Hong et al., “MetaGPT: Meta programming for multi-agent collabora-tive framework,” in Proc. 12th Int. Conf. Learn. Representations, 2023.
[49]  Z. Xi et al., “The rise and potential of large language model based agents: A survey,” Sci. China Inform. Sci., vol. 68, no. 2, 2025, Art. no. 121101.
[50]  Q. Wu et al., “Autogen: Enabling next-gen LLM applications via multi-agent conversation framework,” in Proc. 1st Conf. Lang. Model., 2024.
[51]  S. Qiao et al., “Autoact: Automatic agent learning from scratch via self-planning,” 2024, arXiv:2401.05268.
[52]  W. Zhang et al., “Finagent: A multimodal foundation agent for ﬁnancial trading: Tool-augmented, diversiﬁed, and generalist,” in Proc. 30th ACM SIGKDD Conf. Knowl. Discov. Data Mining, 2024, pp. 4314–4325.
[53]  X.Liuetal.,“Whenaimeetsﬁnance(stockagent):Largelanguagemodel-based stock trading in simulated real-world environments,” ResearchGate Preprint, 2024, doi: 10.13140/RG.2.2.29976.20489.
[54]  J. M. Murre and J. Dros, “Replication and analysis of Ebbinghaus’ forget-ting curve,” PLoS One, vol. 10, no. 7, 2015, Art. no. e0120644.
[55]  “AI guardrails,” open source library for interacting with large language models, (n.d.). [Online]. Available: https://docs.guardrailsai.com


Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
YU∗ et al.: FINMEM: A PERFORMANCE-ENHANCED LLM TRADING AGENT WITH LAYERED MEMORY AND CHARACTER DESIGN	3459



[56]  J. Johnson, M. Douze, and H. Jégou, “Billion-scale similarity search with GPUs,” IEEE Trans. Big Data, vol. 7, no. 3, pp. 535–547, Jul. 2021.
[57]  X. -Y. Liu, H. Yang, J. Gao, and C. D. Wang, “FinRL: Deep reinforcement learning framework to automate trading in quantitative ﬁnance,” in Proc. ACM Int. Conf. AI Finance, 2021, pp. 1–9.
[58]  X. -Y. Liu et al., “Finrl-meta: Market environments and benchmarks for data-driven ﬁnancial reinforcement learning,” in Proc. Adv. Neural Inf. Process. Syst., 2022, vol. 35, pp. 1835–1849.
[59]  J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov,“Proximal policy optimization algorithms,” 2017, arXiv:1707.06347.
[60]  J.Chung,“Playingatariwithdeepreinforcementlearning,”Comput.Ence, vol. 21, no. 351-362, 2013, Art. no. 2.
[61]  V. Mnih et al., “Asynchronous methods for deep reinforcement learning,” in Proc. Int. Conf. Mach. Learn., 2016, pp. 1928–1937.
[62]  J.S.Park,L.Popowski,C.Cai,M.R.Morris,P.Liang,andM.S.Bernstein, “Social simulacra: Creating populated prototypes for social computing systems,” in Proc. 35th Annu. ACM Symp. User Interface Softw. Technol., 2022, pp. 1–18.
[63]  J. Hull, Risk Management and Financial Institutions. Hoboken, NJ, USA: John Wiley & Sons, Inc., 2007.
[64]  W. F. Sharpe, “The sharpe ratio,” J. Portfolio Manage., vol. 21, no. 1, pp. 49–58, 1994.
[65]  J. H. Cochrane, “Volatility tests and efﬁcient markets: A review essay,” J. Monetary Econ., vol. 22, no. 3, pp. 463–485, 1988.
[66]  A. Ang and J. Chen, “Downside risk,” J. Portfolio Manage., vol. 29, no. 4, pp. 103–112, 2003.
[67]  A.Rafﬁn,A.Hill,A.Gleave,A.Kanervisto,M.Ernestus,andN.Dormann, “Stable-baselines3: Reliable reinforcement learning implementations,” J. Mach. Learn. Res., vol. 22, no. 268, pp. 1–8, 2021. [Online]. Available: http://jmlr.org/papers/v22/20-1364.html
[68]  W.  X.  Zhao  et  al.,  “A  survey  of  large  language  models,”  2023, arXiv:2303.18223.



Yangyang Yu received the bachelor’s degree in ap-plied mathematics from Xiamen University, and the master’s degree in applied statistics from Syracuse University. She is currently working toward the PhD degree in data science with the Stevens Institute of Technology. Her research interests include cognitive science, language agent design, bayesian statistics, and multi-modal learning, focusing on FinTech ap-plications. She is a program committee member for workshops at IJCAI and COLING.



Haohang Li received the MS degree in ﬁnancial engineering (with distinction) from the Stevens In-stitute of Technology, where he is currently working toward the PhD degree in data science. His research interests include natural language processing, large language models, and their applications in FinTech. HeisanorganizerforCOLINGandIJCAIworkshops andaprogramcommitteememberfortheINFORMS Workshop.




Zhi Chen received the BS degree in physics from Guangzhou University, China, and the MS degree in ﬁnancial engineering from the Stevens Institute of Technology, USA, where he is currently working to-ward the PhD degree in ﬁnancial engineering. He has authored or coauthored paper in the journal Research in International Business and Finance. His research interests include explainable AI, ﬁntech, ESG, and large language models.

Yuechen Jiang received the BS degree in ﬁnancial engineering from the Shenyang University of Tech-nology, China, and the two MS degrees in ﬁnancial engineering and machine learning from the Stevens Institute of Technology. She is currently working toward the PhD degree with the NJIT’s Ying Wu College of Computing, focusing on time-series with graph neural networks in ﬁnancial forecasting. Her research interests include time series with graph neu-ral networks and multi-agent LLM systems.




YangLireceivedthemaster’sdegreeinmathematical ﬁnancefromtheUniversityofSouthernCalifornia,in 2019.HeiscurrentlyworkingtowardthePhDdegree in ﬁnancial engineering with the Stevens Institute of Technology. His current research interests include theintersectionofhigh-frequencytrading,neuralnet-works, and blockchain technology in ﬁnancial mar-kets.




JordanW.Suchowiscurrentlyanassistantprofessor of information systems with the School of Business, StevensInstituteofTechnology,focusingontheinter-section of cognitive science and information systems inhisresearchandteaching.InUCBerkeley,hisafﬁli-ationsextendedtotheInstituteofCognitiveandBrain Sciences, the Social Science Matrix, the Berkeley ArtiﬁcialIntelligenceResearchlab,andtheCenterfor Technology, Society & Policy. His research interests include information systems, cognitive science, A.I., and behavioral experiments.




Denghui Zhang received the BE degree from the University  of  Science  and  Technology,  Beijing, China, in 2015, the MS degree from the Chinese Academy of Sciences, China, in 2018, and the PhD degree from Rutgers, the State University of New Jersey, in 2023. He is currently an assistant professor with the Stevens Institute of Technology. He has au-thored or coauthored proliﬁcally in refereed journals and conference proceedings, such as IEEE TRANS-ACTIONS ON KNOWLEDGE AND DATA ENGINEERING, ACM SIGKDD, AAAI, SIGIR, CIKM, and ICDM.
His research interests include data mining, business analytics, natural language processing, and representation learning.




Khaldoun Khashanah is currently the director with Financial Engineering Division, Stevens Institute of Technology. He has helped to develop one of the largestprogramsintheU.S.Heestablishedacompre-hensive ﬁnancial engineering curriculum, offering a graduatecertiﬁcate,master’sdegree,andin2009,one of the early PhD programs in ﬁnancial engineering. He is also the Co-Project Director of the Hanlon Financial Systems Laboratory.





Authorized licensed use limited to: University of Chinese Academy of SciencesCAS. Downloaded on February 09,2026 at 11:13:02 UTC from IEEE Xplore.  Restrictions apply.
