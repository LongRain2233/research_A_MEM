{
    "is_related_to_agent_memory": true,
    "title": "SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills",
    "problem_and_motivation": "现有基于轨迹（trajectory）的Web智能体方法存在两大缺陷：1. **难以显式抽象可重用知识**，导致**训练需求高**且**泛化能力有限**，无法适应新网站和任务；2. **持续更新模型**会引发**灾难性遗忘**和对网站变化的**敏感性**。\n\n本文的核心切入点是：模仿人类通过**环境探索**将经验抽象为**可复用的程序性知识（技能）** 的能力。核心假设是：智能体可以通过**自主探索网站**，将成功的交互轨迹**提炼为结构化的、可组合的API**，从而构建一个不断增长的**外部技能库**，实现**无需参数更新的自我提升**。",
    "core_method": "#### **核心流程：三阶段技能发现与提炼管道**\n1.  **技能提议（Skill Proposal）**：LLM（GPT-4o）作为自动课程生成器，基于当前网页观察（截图、可访问性树）和现有技能库，提出三类短视界、可重用的技能任务：**程序性任务**（如“根据印记和颜色识别药丸”）、**导航任务**（如“导航到产品评论页面”）、**信息检索任务**（如“提取GitHub仓库所有提交记录”）。\n2.  **技能合成（Skill Synthesis）**：\n    *   **技能实践**：基础智能体（基于Code-Act）执行提议的任务，生成动作轨迹。\n    *   **奖励模型**：另一个LLM根据**任务描述、动作轨迹、环境反馈**判断任务是否成功完成。\n    *   **API合成**：将**成功的轨迹**（状态-动作对序列）封装成可重用的Python函数。具体做法是：将轨迹转换为字符串表示，提示LLM生成包含**函数签名、文档字符串（含使用日志和前置状态描述）和Playwright代码体**的API。\n3.  **技能精炼（Skill Honing）**：通过**单元测试**和**调试**确保API的鲁棒性。对于需要参数的API，LLM会生成合适的测试用例。\n\n#### **关键技术区别**\n与基于轨迹的隐式记忆或基于自然语言的工作流不同，本文将技能**显式编码为可执行代码（API）**，存储在外部库中，实现了**非参数化、可插拔、可组合**的技能记忆。",
    "key_experiments_and_results": "#### **核心实验设置**\n*   **基准**：WebArena（5个模拟网站，812个任务）和4个真实网站（来自Online-Mind2Web，57个任务）。\n*   **基线**：基础智能体（Code-Act）、AutoEval（LLM奖励模型引导推理时探索）、SteP（使用人工编写工作流的外部记忆）。\n*   **评估指标**：任务成功率。\n\n#### **主要结果**\n1.  **性能提升**：在WebArena上，使用GPT-4o的基础智能体成功率从**22.6%提升至29.8%**，相对提升**31.8%**。在真实网站上，平均成功率从**40.2%提升至56.2%**，相对提升**39.8%**。\n2.  **技能迁移性**：将**强智能体（GPT-4o）合成的API库**直接提供给**弱智能体（GPT-4o-mini）** 使用，后者在WebArena上的成功率从**9.2%提升至14.1%**，相对提升**54.3%**。在部分网站（如CMS）上，相对提升高达**133%**（从3.3%到7.7%）。\n3.  **与人工API对比**：在API支持度**低**（如Reddit）和**中等**（如Shopping）的网站上，合成API的性能与**人工编写的官方API**相当甚至更优。在API支持度**高**（如GitLab, Maps）的网站上，合成API性能较差。\n4.  **消融分析**：实验观察到**组合式API**的涌现，即新API可以调用已合成的简单API来执行更复杂的任务。",
    "limitations_and_critique": "#### **方法本身的局限性**\n1.  **探索效率与成本**：每个网站需要**160次迭代**的探索过程，成本高昂。对于需要大量交互才能获取部分可观察信息（如动态搜索结果）的网站（如Shopping），性能提升有限（仅从19.8%提升至27.2%）。\n2.  **LLM作为执行引擎的瓶颈**：即使提供了高质量的API，LLM在**API调用**上仍不够鲁棒，尤其是在**弱LLM（如GPT-4o-mini）** 上。主要失败模式包括：**a) 无法识别合适的API**；**b) 生成错误的参数**（例如，将“不含坚果”错误生成为“巧克力豆，-坚果”导致搜索结果为空）。\n3.  **技能抽象边界**：方法依赖于LLM从单次成功轨迹中归纳通用API，对于需要**长程规划**和**回溯能力**的复杂技能（如“为多个列表请求报价”），现有智能体能力不足，限制了可合成技能的复杂度。\n\n#### **理论漏洞**\n该方法假设成功的单次轨迹足以抽象出鲁棒的通用技能，但未系统处理**轨迹噪声**和**过拟合特定交互路径**的风险，可能导致合成的API在微小环境变化下失效。",
    "ai_inspiration_and_opportunities": "#### **可迁移的组件与思想**\n1.  **非参数化技能库范式**：将**程序性知识显式编码为可执行代码**并存储在外部库中的思想，可迁移到任何需要**长期记忆**和**技能复用**的序列决策任务中，如**机器人操作**、**游戏智能体**或**软件自动化**，有效规避模型微调带来的灾难性遗忘。\n2.  **基于LLM的自动课程生成与验证循环**：**“提议-实践-评估-封装”** 的自主探索框架，为**无监督环境探索**和**课程学习**提供了通用模板。其核心——使用一个LLM作为**奖励模型**来评估另一个LLM智能体的轨迹——是一种低成本的**自我监督**机制。\n\n#### **低算力下的改进方向**\n1.  **轻量级技能抽象**：研究如何从**更少、更嘈杂的轨迹**中合成鲁棒API。一个零算力idea是：引入**基于规则的轨迹片段对齐与合并**，在LLM生成API前，先对多个相似任务的轨迹进行**对齐和共性提取**，以提升泛化性。\n2.  **分层技能选择与组合**：当前API选择模块较简单。可以设计一个**轻量级检索器**（如基于嵌入的相似性匹配），根据当前**网页状态和任务描述**动态检索最相关的API，并研究**基于图的API组合**方法，让智能体能自动将简单技能组装成复杂工作流，这只需离线计算，不增加推理开销。\n3.  **针对弱模型的知识蒸馏**：本文证明强智能体的技能库能大幅提升弱智能体。可进一步探索**技能库的压缩与精炼**技术，例如，将多个API合并为更通用的“元API”，或为每个API生成更精确的**自然语言使用说明书**，以降低弱LLM的理解和调用门槛。",
    "source_file": "SkillWeaver Web Agents can Self-Improve by Discovering and Honing Skills.md"
}