{
    "is_related_to_agent_memory": true,
    "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction",
    "problem_and_motivation": "本文旨在解决**LLM智能体系统在多轮交互中因轨迹（trajectory）持续增长导致的计算成本过高问题**。现有智能体（如Trae Agent）在解决单个GitHub问题时，平均轨迹包含48.4K个token，导致累计token使用量高达1.0M，其中99%为输入token，造成巨大浪费。现有方法（如LLMLingua-2）主要针对单轮任务的自然语言压缩，不适用于多步、结构化代码的智能体轨迹。本文的核心切入点是：**智能体轨迹中普遍存在无用（useless）、冗余（redundant）和过期（expired）信息**，这些信息可以在不影响性能的前提下被识别和移除。核心假设是：通过一个独立的、轻量级的反射模块（reflection module）对历史轨迹进行选择性压缩，可以显著降低计算成本。",
    "core_method": "本文提出**AgentDiet**，一个在推理时（inference-time）进行轨迹压缩的方法。其核心数据流如下：\n1.  **智能体主循环**：LLM智能体（$LLM_{agent}$）基于当前轨迹$T$生成工具调用（$m_{assis}$），执行后获得结果（$m_{tool}$），并将两者追加到轨迹$T$中。\n2.  **滑动窗口反射**：当智能体执行到第$s$步时，**反射模块**（$LLM_{reflect}$）被触发，其处理目标为第$s-a$步（$a$为延迟步数）。该模块仅接收一个固定大小的上下文窗口（从第$s-a-b$步到第$s$步，$b$为前向上下文步数）。\n3.  **条件化压缩**：仅当目标步骤的原始长度$l_{orig}$超过阈值$\\theta$（默认500 tokens）时，才调用$LLM_{reflect}$（使用GPT-5 mini等轻量模型）生成压缩版本$m_{reduced}$。如果压缩节省的token数（$l_{orig} - l_{reduced}$）超过$\\theta$，则用$m_{reduced}$替换轨迹中的原始步骤。\n4.  **关键超参数**：通过实验确定最优设置为$a=2$（延迟2步压缩），$b=1$（提供前1步上下文），$\\theta=500$ tokens（最小压缩收益阈值）。\n与现有方法最本质的区别在于：**将轨迹压缩作为一个独立的、延迟的、条件触发的后处理步骤**，而非修改智能体自身推理过程或一次性压缩全部输入，从而最小化对智能体工作流的干扰和KV Cache的失效开销。",
    "key_experiments_and_results": "#### **核心实验设计**\n*   **基准测试**：在SWE-bench Verified（Python）和Multi-SWE-bench Flash（多语言）两个代码修复基准上评估。\n*   **智能体模型**：使用Claude 4 Sonnet和Gemini 2.5 Pro作为主智能体（$LLM_{agent}$）。\n*   **对比基线**：原始无压缩的智能体（Original）。\n*   **反射模型**：使用GPT-5 mini作为$LLM_{reflect}$（因其在保持性能的同时成本最低）。\n\n#### **主要定量结果**\n*   **效率提升**：\n    *   在处理的步骤中，**平均保留了22.6% ~ 30.8%的token**（即移除了69.2% ~ 77.4%的“浪费”信息）。\n    *   与Original基线相比，**累计输入token（I）减少了39.9% ~ 59.7%**。\n    *   考虑反射模块开销后，**最终计算成本（$）降低了21.1% ~ 35.9%**。例如，在SWE-bench Verified上使用Claude 4 Sonnet时，单实例平均成本从$0.535降至$0.422。\n*   **性能影响**：\n    *   **任务通过率（Pass%）与基线相当**，变化范围在-1.0%到+2.0%之间。例如，在SWE-bench Verified上，Claude 4 Sonnet的通过率从64.5%略微提升至66.5%。\n    *   **所需步骤数（Step/PStep）未显著增加**，甚至在某些配置下有所减少（如Gemini 2.5 Pro在Multi-SWE-bench Flash上，平均步骤从57.20降至43.90）。\n*   **消融实验核心结论**：\n    *   **反射模型选择**：GPT-5 mini在保持与Original相同通过率（65%）的同时，是唯一能减少平均步骤数的模型，且成本效益最佳。\n    *   **超参数影响**：阈值$\\theta=500$在token节省和反射开销间达到最佳平衡；延迟$a=2$和上下文$b=1$是能在保持性能的同时实现超过22%成本节省的最小设置。",
    "limitations_and_critique": "#### **方法边界与理论漏洞**\n1.  **依赖启发式阈值与延迟**：方法的核心超参数（$\\theta$, $a$, $b$）通过实验在特定基准（SWE-bench）上确定，缺乏理论依据。在任务模式迥异（如探索性任务步骤极短或极长）或工具输出格式完全不同的领域，这些参数可能失效，需要重新调优。\n2.  **无法处理“信息价值”的动态性**：方法基于“无用/冗余/过期”的静态分类进行压缩。然而，在复杂推理任务中，早期看似“无用”的信息可能在后期被重新激活或关联。**固定延迟（$a=2$）的压缩策略可能过早地丢弃了具有潜在长期价值的信息**，导致智能体在后续步骤中需要重新获取，在极端情况下可能引发错误累积或任务失败。\n3.  **对KV Cache的次优处理**：虽然滑动窗口设计旨在最小化KV Cache失效，但**任何对历史轨迹的修改都会导致该步骤之后所有token的KV Cache失效**。在长轨迹场景下，即使只修改一个早期步骤，也可能引发大规模的重新计算，抵消部分压缩收益。论文未量化这种失效带来的实际计算开销。\n4.  **反射模块本身的成本与误差**：使用另一个LLM（GPT-5 mini）进行压缩引入了固定开销（占最终成本的5.2%~14.8%）。**该模块本身可能产生压缩错误**（如误删关键信息或生成误导性摘要），虽然实验显示对通过率影响不大，但这种误差在安全关键或高精度任务中可能是致命的。",
    "ai_inspiration_and_opportunities": "#### **可迁移的组件与思想**\n1.  **“延迟压缩”范式**：**将信息压缩决策与核心任务执行解耦**的思想具有普适性。其他序列决策AI系统（如游戏AI、机器人规划）可以借鉴此范式，设立一个独立的“记忆整理器”模块，定期对历史状态-动作序列进行压缩，只保留对当前决策有影响的摘要，从而降低状态空间的复杂度。\n2.  **基于上下文的局部压缩**：AgentDiet的**滑动窗口上下文（$b$步）** 为压缩提供了局部相关性判断。这可以迁移到**长文档问答或对话系统**中，用于动态修剪历史对话轮次：仅基于最近几轮对话的上下文，对更早的轮次进行摘要，而不是全局压缩，以保持话题连贯性。\n\n#### **低算力/零算力下的改进方向**\n1.  **基于规则的轻量级预过滤器**：在调用昂贵的$LLM_{reflect}$之前，可以插入一个**基于规则或简单分类器的预过滤层**。例如，自动识别并直接删除命令行输出中常见的“进入/离开目录”提示、构建日志的固定头尾、或JSON/XML数据中结构重复的部分。这可以大幅减少需要送入LLM进行复杂判断的文本量，降低反射模块的成本和延迟。\n2.  **信息“过期”的主动预测与标记**：当前方法被动地等待$b$步后压缩。一个零算力改进是：**让智能体在生成工具调用时，主动标记该步骤结果的“预期有效期”**。例如，在调用`grep`搜索后，智能体可以附带一个元信息（如`expiry: after_next_file_open`）。系统可根据此标记更早、更安全地压缩或丢弃该结果，无需依赖固定的延迟参数$a$，实现更自适应的记忆管理。",
    "source_file": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction.md"
}