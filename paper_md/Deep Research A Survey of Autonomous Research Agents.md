# Deep Research: A Survey of Autonomous Research Agents

WENLIN ZHANG, City University of Hong Kong, China

XIAOPENG LI, City University of Hong Kong, China

YINGYI ZHANG, Dalian University of Technology & City University of Hong Kong, China

PENGYUE JIA, City University of Hong Kong, China

YICHAO WANG, Huawei Noah’s Ark Lab, China

HUIFENG GUO, Huawei Noah’s Ark Lab, China

YONG LIU, Huawei Noah’s Ark Lab, China

XIANGYU ZHAO†, City University of Hong Kong, China

The rapid advancement of large language models (LLMs) has driven the development of agentic systems capable of autonomously performing complex tasks. Despite their impressive capabilities, LLMs remain constrained by their internal knowledge boundaries. To overcome these limitations, the paradigm of deep research has been proposed, wherein agents actively engage in planning, retrieval, and synthesis to generate comprehensive and faithful analytical reports grounded in web-based evidence. In this survey, we provide a systematic overview of the deep research pipeline, which comprises four core stages: planning, question developing, web exploration, and report generation. For each stage, we analyze the key technical challenges and categorize representative methods developed to address them. Furthermore, we summarize recent advances in optimization techniques and benchmarks tailored for deep research. Finally, we discuss open challenges and promising research directions, aiming to chart a roadmap toward building more capable and trustworthy deep research agents.

CCS Concepts: • Information systems Information retrieval; $\cdot$ Computing methodologies Natural language processing.

Additional Key Words and Phrases: Deep Research, Deep Search, Large Language Models, Information Retrieval, Autonomous Agents

# ACM Reference Format:

Wenlin Zhang, Xiaopeng Li, Yingyi Zhang, Pengyue Jia, Yichao Wang, Huifeng Guo, Yong Liu, and Xiangyu Zhao†. 2025. Deep Research: A Survey of Autonomous Research Agents. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym ’XX). ACM, New York, NY, USA, 20 pages. https://doi.org/XXXXXXX.XXXXXXX

# 1 Introduction

Large Language Models (LLMs), such as GPT-4 [43], Qwen3 [88], and DeepSeek-R1 [22], have achieved remarkable progress. These advances have enabled LLMs to serve as general-purpose language agents. However, LLMs remain

![](images/0f9c5658cca850c40012f6f1b4e601cb6700e015bb4162710045777384d7bfac.jpg)  
Fig. 1. Overview of the deep research system.

constrained by their internal knowledge boundary, thereby limiting their effectiveness in dynamic or specialized scenarios.

To overcome these limitations, Retrieval-Augmented Generation (RAG) is proposed to augment LLMs reasoning with access to external information sources. RAG systems retrieve relevant documents from a large corpus and condition the LLM’s generation on the retrieved content. Although the RAG framework enhances factuality and adaptability, it constrains the model to passively consume retrieved content without participating in deeper exploration or reasoning. As LLMs continue to gain agentic abilities such as planning, tool use, and reflective reasoning, there is a growing shift from passive retrieval to active, goal-driven interaction with external knowledge sources. This shift marks the rise of a new paradigm known as agentic search.

Within the agentic search paradigm, a prominent emerging approach is deep search, where agents dynamically engage in planning, question developing, and web exploration to iteratively retrieve information aligned with evolving task objectives. While deep search significantly enhances the agent’s ability to acquire and contextualize knowledge, it remains insufficient for completing complex research tasks that demand synthesis and reasoning. To address this gap, the deep research paradigm has been introduced, combining exploration with structured analysis and iterative generation. This paradigm enables agents to transform retrieved evidence into coherent, factual, and goal-aligned outputs, fulfilling the demands of high-level research workflows. The Deep Research process typically involves four interconnected stages:

Planning. This stage involves decomposing a high-level research question into structured sub-goals or subtasks. The agent must decide what to search for, in what order, and how intermediate information will support downstream synthesis. Unlike traditional step-by-step prompting, planning requires generating an explicit, task-aware roadmap before any retrieval or generation begins.

Question Developing. Given a sub-goal, the agent formulates one or more retrieval queries that capture specific, contextualized information needs. These queries may vary in abstraction, specificity, and granularity. In contrast to static question formulation [29], this stage requires adaptive generation of diverse and complementary queries tailored to evolving subgoals.

Web Exploration. The agent actively interacts with external sources either through web APIs or browser-based actions to collect relevant information. This process requires selecting tools, issuing queries, parsing results, and filtering

Manuscript submitted to ACM

noisy or redundant content. Unlike single-shot retrieval in traditional RAG pipelines, web exploration is iterative and agent-driven, enabling deeper coverage of sparse or scattered evidence.

Report Generation. To produce a structured report, the agent must integrate retrieved information by selecting relevant evidence, organizing content coherently, and ensuring both consistency and completeness. This process goes beyond extractive or shallow generation, requiring multi-source fusion, discourse-level planning, and task-specific formatting.

As illustrated in Figure 1, deep research systems involve multi-stage tasks and extensive information processing, which impose progressively evolving demands on agentic capabilities.

Therefore, to make the best use of Deep Research systems, we organize the major challenges and corresponding solutions according to the key capability requirements:

• Planning: Given a research question, how to construct an effective and interpretable research plan before execution? Planning serves as the first and foundational stage in deep research, translating user intent into actionable steps such as sub-question development and evidence retrieval. The key challenge lies in decomposing broad or ambiguous questions into a structured plan, especially under open-ended research goals. Unlike reactive agents that respond immediately, research agents must reason about intermediate steps and execution paths. This requires planning mechanisms that are both flexible to different question types and interpretable for human inspection. We define this process as the Planning module (see Section 2), which produces a structured, goal-driven plan to guide subsequent reasoning and retrieval.   
• Question Developing: Given a sub-goal from the planning stage, how to formulate effective, diverse queries that balance specificity and coverage. In deep research agents, question developing is crucial for guiding the retrieval process to gather relevant information. However, generating queries that are both precise and comprehensive is challenging due to the need to capture specific information needs while ensuring broad coverage of the topic. Solutions typically involve adaptive query generation strategies, where agents dynamically formulate queries based on the evolving context of the research task. Techniques such as reinforcement learning are employed to optimize query effectiveness through interaction with the search environment, while non-RL methods like supervised fine-tuning or heuristic-based approaches provide alternative means to enhance query diversity and relevance. We define this process as the Question Developing module (see Section 3), which produces a set of targeted, contextualized queries conditioned on the sub-goals from the planning stage.   
• Web Exploration: Given a search plan and query, how to efficiently retrieve the most relevant and trustworthy information while filtering noisy, redundant, or conflicting information on the web? In deep research workflows, efficiently locating and retrieving precise information online is critical for producing high-quality research reports. Recent works include autonomous web-retrieval agents that crawl hyperlinks to collect pertinent content [24, 41, 90, 101] and API-based methods that query search engines directly [1, 2, 6, 7]. We refer to this process as the Web Exploration module (see Section 4), which retrieves relevant information from diverse sources conditioned on the input research queries.   
• Report Generation: Given retrieved evidence related to the target research question, how to generate a coherent and structured report that keep factual integrity. This stage is critical for transforming fragmented knowledge into a comprehensive analysis report. However, report generation faces challenges in structure control and factual integrity. To address these challenges, recent approaches incorporate structure-aware planning and constraint-guided generation to enforce layout coherence, and adopt evidence-grounded modeling to ensure factual reliability. We

define this process as the Report Generation module (see Section 5), which produces structured, trustworthy report based on the information collected by deep research agents in web exploration stages.

In response to these challenges, we present a capability-centric survey of deep research agents, with a particular focus on how core capabilities—such as planning, retrieval, reasoning, and report generation—can be independently optimized and jointly integrated. Rather than enumerating full-system pipelines, we dissect the modular competencies underlying deep research and analyze their technical bottlenecks and coordination challenges. To this end, we first analyze key obstacles in building deep research agents; then we review representative methods across different modules, and finally highlight emerging trends, such as reasoning-driven retrieval, structured report generation, and self-evolving agents. Compared to prior surveys, our work offers a fine-grained, modular perspective. For example, Huang et al.[26] emphasize system architecture and future roadmaps, while Xu et al.[84] provide a broad, enumerative overview of tasks and tools. In contrast, our taxonomy centers on capability formation and integration, aiming to reveal deeper connections between methods and provide insights into evolving design paradigms.

# 2 Planning

Deep research systems increasingly emphasize a dedicated planning stage to orchestrate complex, long-horizon tasks. Rather than relying on reactive, step-by-step prompting alone, modern agents employ explicit planning to decide what actions or sub-tasks to execute before carrying them out. This planning process helps overcome the limitations of purely reactive prompting by improving task decomposition, guiding retrieval and reasoning more systematically, and reducing failures caused by trial-and-error strategies.

Definition 2.1 (Planning). In deep research agents, planning refers to the process by which an agent transforms a user query and prior knowledge into a structured research plan consisting of intermediate subgoals or actions. Formally, given an initial research question $q _ { 0 }$ and agent context $\mathcal { K }$ , the planning model $M ^ { \mathrm { p l a n } }$ parameterized by $\theta$ produces a plan:

$$
\mathcal {P} = \mathcal {M} ^ {\text {p l a n}} \left(q _ {0}, \mathcal {K}; \theta\right), \tag {1}
$$

where $\mathcal { P } = [ s _ { 1 } , s _ { 2 } , . . . , s _ { n } ]$ denotes a sequence of subgoals or tool-invocation steps guiding downstream execution.

This planning strategies may include world model simulation [21], modular design search [62], human-like reasoning trajectory synthesis [69], or agent self-refinement [45]. To better understand the diversity of planning strategies, we summarize representative methods in Table 1, categorized by their planning strategies. The plan is dynamically generated and may adapt to task complexity, resource availability, or user intent. It acts as the bridge between high-level goals and low-level execution in research-oriented web agents.

# 2.1 Planning with Structured World Knowledge

Research on web-based agents increasingly highlights the importance of leveraging structured world knowledge—ranging from learned latent world models to explicit graphs—to guide long-horizon decision making. Early evidence suggests that large language models (LLMs) can act as implicit world models, providing sufficient environment priors for goal-directed reasoning [20]. Building on this insight, the Simulate Before Act framework introduces an explicit simulation phase that enables agents to mentally roll out candidate action trajectories and evaluate their feasibility prior to execution, thereby improving robustness and adaptability [21].

Structured, multi-module pipelines further enrich this paradigm. WebPilot partitions web exploration across specialized sub-agents coordinated by a strategic planner, demonstrating how distributed planning built atop a shared world

Manuscript submitted to ACM

Table 1. Taxonomy of planning methods in deep research agents.   

<table><tr><td>Planning</td><td>Category</td><td>Related Works</td></tr><tr><td rowspan="3">Planning with Structured World Knowledge</td><td>Planning via simulation</td><td>WebDreamer [20], Simulate Before Act [21]</td></tr><tr><td>Planning via modularity</td><td>Webpilot Zhang et al. [96], WKM [52], Plan-and-Act Erdogan et al. [17]</td></tr><tr><td>Planning via adaptation</td><td>Thought of Search [32], MPO [83]</td></tr><tr><td rowspan="3">Planning as a Learnable Process</td><td>Planning via space exploration</td><td>Agentsquare [62], Agent-E Abuelsaad et al. [4]</td></tr><tr><td>Planning via self-training</td><td>Patel et al. [45], InSTA Trabucco et al. [71]</td></tr><tr><td>Planning via preference modeling</td><td>MindSearch Chen et al. [11], SimpleDeepSearcher [69], Search-in-the-Chain [85], WEPO [37], MPO [83]</td></tr></table>

representation boosts task efficiency [96]. Qiao et al. incorporate external knowledge graphs to ground reasoning paths, exemplifying knowledge-grounded planning that explicitly models environment structure [52]. For temporally extended tasks, Erdogan et al. couple an initial plan with iterative refinement steps to maintain coherence over long horizons [17].

Efficiency considerations also shape contemporary methods: Katz et al. quantify the computational cost of reasoning operations and propose an efficiency-aware planning strategy that balances deliberation depth against resource usage [32]. Finally, Xiong et al. present meta-plan optimization (MPO), a meta-learning approach that adaptively tunes planning strategies across diverse web environments, closing the loop between world-model fidelity and strategic flexibility [83]. Collectively, these works operationalize structured knowledge as a foundation for foresightful and computationally prudent web-agent planning.

# 2.2 Planning as a Learnable Process

A complementary research trajectory treats the planning mechanism itself as the primary object of learning, allowing agents to iteratively refine their decision procedures through search, feedback, and large-scale training. AgentSquare exemplifies this view by performing architecture search over a vast configuration space to automatically assemble task-adapted planning pipelines, dispensing with hand-crafted heuristics [62]. Agent-E distills reusable components from navigation tasks, showing how robust behavior can emerge from system-level decomposition [4].

Self-improvement via interaction feedback has also proven effective: Patel et al. demonstrate that LLM-based agents can hone their planning heuristics purely from deployment experience, without human intervention [45]. Scaling this idea, Trabucco et al. propose internet-scale training pipelines that generalize planning behaviours across a massive distribution of tasks, shifting attention from per-task optimization toward lifelong adaptability [71].

Reasoning-enhanced planning mechanisms further enrich this learnable paradigm. MindSearch simulates human-like search strategies to deepen planning trajectories [11], whereas SimpleDeepSearcher synthesizes multi-hop reasoning paths for information-seeking tasks [69]. Search-in-the-Chain closes the loop between retrieval and planning by dynamically updating action choices based on intermediate search results [85]. WEPO tailors planning to user interface preferences, optimizing action sequences at the UI-element level [37]. MPO, originally introduced for meta-learning across environments, also exemplifies how agents can refine planning strategies via higher-order gradient updates [83].

Together, these studies reconceptualize planning as a learnable, evolvable capability, foregrounding continual selfimprovement and task-driven adaptation over static execution.

# 2.3 Discussion

The planning stage has become a defining component in recent deep research agents, offering not only structure to downstream reasoning but also interpretability and control in open-ended tasks. One notable strength is the emergence of explicit, structured planning outputs—such as reasoning chains, search commands, or subgoal sketches—which help agents avoid short-sighted or repetitive behaviors. Another encouraging trend is the agent’s ability to learn and refine planning strategies over time. Through interaction feedback, meta-learning, or exposure to large-scale task distributions, agents are beginning to self-adjust their planning routines, yielding increasingly robust behaviors in dynamic web environments.

However, important limitations remain. First, plans generated by current LLMs are often brittle, lacking robustness to ambiguous research questions or underspecified goals. Even with structured formats, the internal consistency of plans is not guaranteed, and hallucinated steps can propagate errors downstream. Second, while planning modules are increasingly modular and learnable, their evaluation remains coarse, often relying on end-task accuracy rather than plan quality itself—making it hard to diagnose planning failures or compare strategies meaningfully. Third, many systems treat each research question as an isolated problem, without leveraging shared structures or transferable strategies, which limits the agent’s ability to accumulate generalizable planning knowledge across tasks.

Indeed, there is growing research aimed at addressing these challenges—from the introduction of standardized benchmarks like DeepResearch Bench [16] for evaluating report fidelity and citation accuracy, to reinforcement learning frameworks such as DeepResearcher [98] that promote iterative self-reflection and improved planning in web environments. But key limitations remain, particularly in handling ambiguity, ensuring long-range consistency, and transferring planning strategies across tasks.

# 3 Question Developing

To support deep research tasks such as multi-step reasoning and information synthesis, it is often insufficient to rely on a single static query. Instead, systems must dynamically generate more targeted, contextualized, or decomposed queries that elicit useful evidence from retrieval modules or the web. This process, known as question developing, is essential for guiding agentic retrieval strategies that operate beyond keyword matching and towards task-aware information seeking.

# 3.1 Definition

Definition 3.1 (Question Developing). Question developing denotes the process of converting each subgoal $s _ { i }$ within a structured plan $\mathcal { P } = [ s _ { 1 } , \ldots , s _ { n } ]$ into a series of search queries $Q _ { i } = \{ q _ { i , 1 } , q _ { i , 2 } , . . . \}$ . This process is guided by the overall plan $\mathcal { P }$ , the current subgoal $s _ { i }$ , and the accumulated evidence $\varepsilon$ , which comprises information retrieved from previous queries. Formally, given a query generation model $\boldsymbol { { M } } ^ { \mathrm { a s k } }$ with parameters $\theta$ , the queries for $s _ { i }$ are generated as

$$
Q _ {i} = \mathcal {M} ^ {\text {a s k}} \left(\mathcal {P}, s _ {i}, \mathcal {E}; \theta\right). \tag {2}
$$

The retrieval process that produces and updates the evidence set $\varepsilon$ is detailed in Section 4.

Question developing methods can be categorized into two types based on optimization methods: Reward-Optimized Methods, where the query formulation process is optimized through trial-and-error exploration guided by a reward signal; and Supervision-Driven Methods, which rely on supervised finetuning or manually designed strategies without Manuscript submitted to ACM

explicit reward-driven query optimization. A taxonomy of these approaches is provided in Table 2, organized by training paradigm and key modeling strategies. In the remainder of this section, we formalize the notion of question developing and examine representative strategies that instantiate this process.

Table 2. Taxonomy of question developing methods in deep research agents.   

<table><tr><td>Optimization</td><td>Category</td><td>Related Works</td></tr><tr><td rowspan="2">Reward-Optimized Methods</td><td>Rewards for format and accuracy</td><td>DEEPRESEARCHER [98], EVOLVESEARCH [93], R1-SEARCHER [66], SEARCH-R1 [31], ZEROSEARCH [68], MASKSEARCH [80], DEEP- RETRIEVAL [30]</td></tr><tr><td>Multi-dimensional reward</td><td>INFORAGE [51], OTC-PO [74], IKEA [28], AUTOREFINE [65], R-SEARCH [97], MMSEARCH-R1 [77], VRAG-RL [75]</td></tr><tr><td rowspan="2">Supervision-Driven Methods</td><td>Multi-agent systems</td><td>MANUSEARCH [25], SEARCH-O1 [35], SEARCHAGENT-X [89]</td></tr><tr><td>Supervision optimization</td><td>REASONRAG [95]</td></tr></table>

# 3.2 Reward-Optimized Methods

Reinforcement learning (RL) provides a principled framework for optimizing query generation policies through interaction with a search environment and feedback from task-specific rewards. Unlike supervised fine-tuning, which requires labeled query-response pairs, reward-optimized approaches allow agents to explore query strategies and adaptively adjust when and how to search based on end-task outcomes such as answer accuracy, retrieval coverage, or efficiency.

A common design choice in many RL-based question developing methods is to define the reward purely based on output format correctness and final answer accuracy. These methods treat retrieval and reasoning as black-box components, optimizing only for whether the output conforms to a specified format and leads to a correct answer. This simplifies the reward signal and makes it easier to scale training in noisy or simulated environments.

For instance, DeepResearcher[98] and EvolveSearch[93] use binary format rewards and token-level F1-based answer rewards, training multi-agent systems to interact with real or simulated search APIs via GRPO. R1-Searcher[66] further introduces a staged training framework where the first phase rewards only format compliance and search usage, and the second phase integrates F1-based answer quality. Similarly, Search-R1[31] enforces strict format templates (e.g., <think>, <search>, <information>, <answer>) and optimizes an exact-match-based reward via PPO or GRPO. ZeroSearch[68] follows the same principle in a simulated setting by replacing real search APIs with a learned search simulator, thus enabling reward learning with curriculum rollout without any real querying cost. DeepRetrieval[30] defines the reward over retrieval metrics such as Recall@K or NDCG@K, aligning query generation with retrieval performance. Lastly, MaskSearch[80] defines rewards purely based on masked span recovery (via retrieval-augmented answer prediction), combining format and accuracy into a unified reward function during pretraining. These reward designs help guide the agents toward generating queries that are not only syntactically valid but also more likely to retrieve answer-relevant content, especially when operating in noisy or weakly supervised environments.

In contrast to approaches that define reward solely based on output format and answer correctness, a second class of question developing methods integrates richer, multi-dimensional reward signals that better reflect the complexities of interactive reasoning and search [66]. These reward functions not only consider what answer is produced, but also how it is obtained—penalizing unnecessary search, rewarding informative intermediate steps, or adapting to the agent’s internal vs. external knowledge boundaries.

InForage[51] augments outcome-based reward with information gain (coverage of ground-truth knowledge) and an efficiency penalty that discourages redundant reasoning hops. OTC-PO[74] introduces a cost-aware reward that penalizes excessive tool usage, aiming to minimize external calls while preserving correctness. IKEA [28] further incorporates a knowledge boundary-aware reward, giving agents positive feedback for solving “easy” questions using internal knowledge alone, and penalizing unnecessary or unproductive external searches. Other works emphasize reward shaping over multi-turn trajectories. AutoRefine[65] evaluates not only final answer quality but also the completeness of intermediate refinements extracted from retrieved documents. R-Search[97] and MMSearch-R1[77] combine answer correctness with evidence quality and format compliance, supporting structured reasoning across modalities. Finally, VRAG-RL [75] adapts this idea to vision-language settings, optimizing visual search actions via fine-grained rewards for image selection, visual attention consistency, and final answer quality.

These reward-optimized methods demonstrate the viability of learning adaptive and interpretable query development policies in noisy, dynamic, and high-stakes research environments. They lay the groundwork for agents that can reason about the utility of queries in context and improve their behavior over time through reinforcement feedback.

# 3.3 Supervision-Driven Methods

In contrast to reward-optimized approaches, which rely on RL optimization, supervision-driven methods develop question generation strategies using supervised fine-tuning, rule-based decompositions, or structured multi-agent workflows. These methods avoid the complexity and instability of reinforcement learning, instead leveraging human demonstrations, task-specific heuristics, or architectural optimizations to guide the development of effective query strategies.

One line of work focuses on building structured multi-agent systems that divide the question developing process into modular roles. ManuSearch[25] implements a transparent, open-source framework with separate agents for planning subquestions, conducting external web search, and extracting structured evidence from HTML content. Each agent operates based on deterministic or supervised rules, coordinating over multiple turns to perform complex query decomposition and information synthesis. Similarly, Search-o1[35] and SearchAgent-X [89] explore system-level enhancements, such as adaptive retrieval scheduling and efficient batch processing, to improve the throughput and responsiveness of retrieval-augmented reasoning without changing the underlying language model’s behavior.

Other methods explore imitation learning or preference-based optimization to supervise the question developing process. ReasonRAG [95] replaces sparse, outcome-only supervision with fine-grained feedback over intermediate steps. Using Monte Carlo Tree Search (MCTS) to explore reasoning trajectories and Direct Preference Optimization (DPO) to rank them, ReasonRAG identifies more effective planning behaviors without relying on trial-and-error reinforcement. This allows the model to learn how to interleave search and reasoning more efficiently from fewer training samples.

These supervision-driven methods offer several practical advantages for question developing: they enable more controllable query generation, often producing well-structured and semantically faithful queries by mimicking humanwritten examples or following rule-based templates. Since they avoid interacting with noisy or expensive search environments during training, they are easier to optimize and more stable in low-resource scenarios. Furthermore, their explicit supervision makes it easier to diagnose and adjust query behavior in response to planning errors or task-specific failures. However, their effectiveness is often limited by the quality and coverage of available demonstrations; without sufficient diversity, the generated queries may fail to adapt to unseen subgoals or novel reasoning contexts. Overall, these methods complement reward-optimized approaches by offering safer and more interpretable solutions in settings where data efficiency, query consistency, and development simplicity are prioritized.

Manuscript submitted to ACM

# 3.4 Discussion

The Question Developing module is a core component in deep research agents, responsible for transforming subgoals into a series of specific retrieval queries. These queries need to accurately reflect the intent of the subgoals while being broad enough to retrieve comprehensive and relevant information from external sources. It essentially serves as the starting point for the agent’s exploration of the information space, directly impacting the quality of subsequent retrieval and answer generation. Currently, in the field of Question Developing, the main trends include reward-optimized methods and supervision-driven methods. Reward-optimized methods dynamically adjust query strategies through interaction with the search environment, using feedback signals (such as the relevance of retrieval results) to improve efficiency and accuracy. On the other hand, supervision-driven methods, such as rule-based query generation or multi-agent collaboration systems, enhance query diversity and specificity through predefined logic or teamwork. These methods are all aimed at enabling agents to "ask" more intelligently to meet complex research needs.

Despite progress, existing methods still have significant shortcomings. First, the generated queries often rely too heavily on the clarity of the subgoals; once the subgoals are vague or ambiguous, the quality of the queries drops significantly, leading to deviations in information retrieval. Second, many systems lack contextual coherence when generating queries, such as failing to effectively integrate previous query history or task background, resulting in repetitive or redundant questions. Finally, current methods perform poorly in handling open-ended problems, tending to generate overly narrow queries that fail to capture a wide range of potential information, limiting the agent’s exploratory capabilities.

# 4 Web Exploration

Deep-research workflows, ranging from scientific discovery and literature review to fact-checking and other expert investigations, critically depend on retrieving precise, context-aware, and trustworthy evidence from the vast, heterogeneous information available on the web. Yet because relevant material is sparsely distributed across countless web pages, designing agents that can accurately locate and extract the most useful content remains a critical challenge.

Definition 4.1 (Web Exploration). In the context of deep research, web exploration is the process of retrieving the most relevant information from online sources. This may involve (i) deploying an agent that recursively follows hyperlinks and filters content, or (ii) invoking search engine APIs to obtain ranked results of relevant web documents. Given the query $Q _ { i }$ , web retriever $\mathcal { R }$ , the web agent $M ^ { \mathrm { w e b } }$ parameterized by $\theta$ , and the open-web corpus $\mathcal { H }$ , the web exploration process is defined as

$$
\mathcal {D} = \mathcal {M} ^ {\mathrm {w e b}} \left(\mathcal {R}, Q _ {i}, \mathcal {H}; \theta\right), \tag {3}
$$

where $\mathcal { D }$ denotes the set of documents ultimately retrieved.

Contemporary web-retrieval methods fall into two broad categories: (1) Web agent-based systems: autonomous agent systems that conduct web retrieval including operations like browse, click, and extract information much like a human researcher. (2) API-based retrieval systems: directly using the current web search engine (e.g., Google Search, Bing Search, etc.) that let developers pull ranked documents or snippets directly into research pipelines. A summary of representative methods in both categories is presented in Table 3.

Table 3. Taxonomy of web exploration methods in deep research agents.   

<table><tr><td>Information Source</td><td>Category</td><td>Related Works</td></tr><tr><td rowspan="3">Web-based</td><td>Web scraping and crawling</td><td>Scrapy [59], BeautifulSoup [55]</td></tr><tr><td>Browser-based web agents</td><td>WebGPT [41], Selenium [60]</td></tr><tr><td>Multimodal web agents</td><td>WebVoyager [24], MM-ReAct [90], WebArena [101]</td></tr><tr><td rowspan="2">API-based</td><td>Industrial search engines</td><td>Bing [1], X posts [3], Google [2]</td></tr><tr><td>Domain-specific search engines</td><td>Reportify [54], YanXueZhiDe [12], CNKI [13], DuckSearch [67], BraveSearch [7], Bocha [6]</td></tr></table>

# 4.1 Web retrieval agents

Web retrieval agents are autonomous systems that extract information from the web, forming a critical component of deep research systems. These agents have evolved from simple extractors to sophisticated multimodal systems capable of navigating complex interactive content and synthesizing information across diverse sources.

Browser-Based Autonomous Web Agents. AI-driven browser agents marked a fundamental shift in web-based information gathering for deep research. Unlike static scrapers, these agents dynamically navigate web interfaces through real or simulated browsers, making contextual decisions about which paths to explore based on encountered content. WebGPT [41] pioneered this approach using a text-based browser that converted HTML into structured representations, enabling the language model to issue high-level commands such as Find keyword to retrieve relevant passages. More sophisticated implementations leverage real browsers via Selenium [60] to query the Document Object Model directly or execute custom JavaScript. Agent-E exemplifies the compact-representation approach, constructing accessibility trees that preserve semantic structure while removing extraneous elements. These agents perceive content through textual representations and execute actions programmatically—clicking links, completing forms, or triggering interactive elements. While effective for text-heavy research tasks, this approach fails when critical information appears in visual layouts or embedded visualizations, motivating the transition to multimodal agents.

Multimodal Web Agents. Current web retrieval agents integrate visual perception to process both textual content and visual cues—interpreting charts, recognizing interface patterns, and understanding spatial information organization. This capability proves essential for deep research requiring analysis of data visualizations or navigation of visually complex research databases. Two approaches have emerged: specialized prompting frameworks like MM-ReAct [90] and fully integrated vision-language agents. Both combine rendered screenshots with textual metadata for comprehensive page representation. WebVoyager [24], using GPT-4V, combines screenshot analysis with HTML processing to achieve $5 9 \%$ task success on real-world benchmarks, significantly outperforming text-only baselines. MM-ReAct [90] embeds screenshots directly into reasoning chains, enabling iterative action execution based on visual feedback. Sightseer [34] reconstructs HTML/CSS from screenshots, while WebArena [101] provides comprehensive evaluation frameworks for diverse interface patterns.

These multimodal agents demonstrate essential research capabilities—answering complex queries with citations and synthesizing findings from heterogeneous sources. Despite challenges in robustness and reliability, progress points toward agents that navigate web knowledge with expert-like strategies, transforming how researchers access and synthesize information. As these systems mature, they shift human effort from mechanical information gathering to higher-level analysis and insight generation.

Manuscript submitted to ACM

# 4.2 API-Based Retrieval Systems

API-based retrieval enables rapid incorporation of external knowledge into deep research pipelines. By exposing standardized endpoints, mature industrial search engines can be seamlessly integrated, yielding search results that are both reliable and trustworthy. OpenAI DeepResearch [44] leverages Microsoft Bing’s web-search infrastructure [1] to issue queries, extract passages, spawn follow-up queries, rank candidate documents, and even execute sandboxed code. Grok DeepSearch [81] operates its own crawler and supplements public web data with privileged access to X posts [3]. Gemini DeepResearch [18] invokes Google’s proprietary search stack [2], whereas Perplexity DeepResearch [46] employs a hybrid solution that fuses a Bing-style web index with Perplexity’s Sonar API, which provides BM25, keyword, and dense-vector reranking. Several domain-specific systems further narrow the scope to specialized corpora, such as Reportify [54] integrates licensed market-research reports and authoritative financial news, while YanXueZhiDe [12] offers academic retrieval over the CNKI [13] repository. Open-source frameworks, in contrast, typically depend on third-party engines such as DuckSearch [67], BraveSearch [7], or Bocha [6].

# 4.3 Discussion

Web exploration is vital for deep research, providing precise and relevant evidence from the expansive web through two main approaches: Web retrieval agents excel at dynamically navigating web interfaces, mimicking human browsing to access interactive or unindexed content, making them highly adaptable for complex inquiries. However, their resourceintensive nature and lack of real-time trustworthiness assessment pose significant drawbacks. Conversely, API-based systems offer rapid, efficient access to pre-indexed data from established search engines, ensuring reliability but often overlooking niche or dynamic content. Both approaches, while effective in isolation, are limited by their disconnect from upstream research stages like question formulation and evidence planning, hindering their ability to fully adapt to specific research needs.

The future of web exploration lies in hybrid architectures that systematically address these core challenges through integrated solutions combining the strengths of both approaches. Advanced systems must incorporate specialized modules for evidence extraction, correctness verification, and content quality assessment, enabling rapid initial retrieval followed by deep interactive analysis. Critical developments in multimodal processing and real-time verification frameworks will prove essential for maintaining pace with the evolving web landscape. By establishing robust categorization frameworks for different system modules and systematically addressing these technical challenges, web exploration can mature into a comprehensive foundation for deep research, delivering more precise and trustworthy evidence retrieval capabilities.

# 5 Report Generation

In deep research, generation extends beyond traditional QA tasks, aiming to produce a comprehensive and analytical report. This process, known as report generation, seeks to synthesize fragmented information retrieved from the web into a report that is coherent in structure, logically organized, and faithful to the underlying evidence.

Definition 5.1 (Report Generation). In the context of deep research agents, report generation refers to the process by which an agent synthesizes a comprehensive report integrated by web information. Given the initial research question $q _ { 0 }$ , the research plan $\mathcal { P }$ , the retrieved documents $\mathcal { D }$ , and the web agent $M _ { \theta }$ , the report generation process can be defined as

$$
\mathcal {Y} = \mathcal {M} _ {\theta} (q _ {0}, \mathcal {P}, Q, \mathcal {D}) \tag {4}
$$

Manuscript submitted to ACM

where $_ y$ denotes the generated report.

The methods for report generation can be broadly categorized into two types: structure control and factual integrity. (1) Structure control focuses on organizing multi-step reasoning and retrieved content into coherent formats, often relying on planning-aware generation or constraint-guided generation. (2) Factual integrity aims to ensure that the generated report is faithful to the retrieved evidence, typically through grounding mechanisms or post-generation verification. Representative methods under each category are summarized in Table 4.

Table 4. Taxonomy of artifact generation methods in deep research agents.   

<table><tr><td>Field</td><td>Category</td><td>Representative Works</td></tr><tr><td rowspan="3">Structure Control</td><td>Planning-based Generation</td><td>Agent Laboratory [58], AI Scientist v2 [87], LongEval [78], Long-Writer [5], LongDPO [49]</td></tr><tr><td>Constraint-guided Generation</td><td>WebThinker [36], Suri [47], Wan et al. [73]</td></tr><tr><td>Structural-aware Evaluation</td><td>Long2RAG [50], ExPerT [56], Long et al. [38], Kim et al. [33], Huang et al. [27]</td></tr><tr><td rowspan="3">Factual Integrity</td><td>Faithful Modeling</td><td>RAGSynth [63], BRIDGE [15], Zhou et al. [102], Shi et al. [64]</td></tr><tr><td>Conflict Reasoning</td><td>FaithfulRAG [94], DRAGged [8], Yuan et al. [92], Ying et al. [91]</td></tr><tr><td>Factuality Evaluation</td><td>Face4RAG [86], SFR-RAG [42], Wallat et al. [72], FaithJudge [70], RAG-QA Arena [23], MT-RAIG [61]</td></tr></table>

# 5.1 Structure Control

Structure control refers to generating long-form outputs that are both structurally coherent and globally consistent. These long-form outputs often span multiple sections and require effective planning, topical alignment, and layout adherence. Recent research has tackled this problem from three perspectives: planning-based generation, constraintguided generation, and structure-aware alignment.

Planning-based generation focuses on organizing content before or during generation by leveraging document outlines or hierarchical decomposition. Agent Laboratory [58] applies paragraph-level planning strategies with structureaware prompting, while AI Scientist v2 [87] performs recursive tree-structured planning to maintain global layout consistency. In parallel, LongEval [78] and LongWriter [5] exemplify this approach by decomposing the generation process into high-level outline planning and section-level synthesis. This hierarchical strategy helps maintain coherence across thousands of tokens. LongDPO [49] further incorporates critique-augmented supervision at each generation step, ensuring local completions are aligned with the global document structure.

Constraint-guided generation aims to enforce specific format, style, or content requirements during decoding. WebThinker [36] introduces section-aware decomposition that bridges the planning and generation stages, enabling the model to map structured subtasks to content sections. Suri [47] investigates instruction tuning under multiple layout and stylistic constraints, including tone, structure, and topic coverage. Wan et al. [73] propose a human-like revision-based framework that reflects realistic writing dynamics under constraint.

Structure-aware evaluation provides critical signals for both training and post-hoc alignment. Long2RAG [50] develops key-point recall metrics to assess whether retrieved evidence is adequately incorporated into long-form responses. ExPerT [56] proposes evaluation strategies that consider layout-specific completeness and explainability. Similarly, Long et al. [38] explore attribute-guided alignment during training to align generation structure with task-specific expectations. Kim et al. [33] assess verbatim fidelity in long-context models to detect structural drift or Manuscript submitted to ACM

misalignment, while Huang et al. [27] propose calibration methods to correct structural inconsistencies in output generation.

# 5.2 Factual Integrity

Factual consistency is a cornerstone of report generation in Deep Research, where generated outputs must faithfully reflect retrieved evidence. Despite advances in RAG, challenges remain in maintaining fact fidelity, resolving conflicting evidence, and verifying content reliability. To address these issues, recent work explores three key directions: faithful modeling, conflict resolution, and factual evaluation.

Faithful modeling focuses on ensuring that generation aligns with verified and contextually relevant evidence. RAGSynth [63] generates synthetic supervision signals under known factual variations, improving robustness and trustworthiness. BRIDGE [15] proposes a verification layer between retrieval and generation to assess factual adequacy. Shi et al. [64], Zhou et al. [102] propose context-aware decoding methods that prioritize high-confidence evidence spans and reduce hallucinated completions, enhancing the factual accuracy of responses.

Conflict resolution is essential when the retrieved sources contain contradicting claims. FaithfulRAG [94] introduces fact-level conflict modeling to promote alignment with consistent retrieved facts. DRAGged [8] identifies and mitigates inter-source conflicts using detection and intervention models. Entropy-based decoding strategies [92] adaptively adjust to evidence uncertainty, promoting more reliable generation under ambiguous input. Ying et al. [91] explore LLM behavior under conflicting prompts and improve conflict awareness by behavioral tuning.

Factuality evaluation provides metrics and benchmarks for guiding training and assessing generation reliability. Face4RAG [86] evaluates attribution consistency in Chinese RAG systems using fine-grained metrics. SFR-RAG [42] develops contextual attribution indicators to measure alignment at the passage level. Wallat et al. [72] distinguish surface correctness from true source faithfulness. FaithJudge[70] and RAG-QA Arena [23] construct evolving leaderboards and domain-robust benchmarks for long-form factual evaluation. MT-RAIG [61] extend factuality assessment to structured data contexts, such as multi-table insight generation.

# 5.3 Discussion

Report generation serves as the final step of deep research, transforming fragmented evidence from subproblems into a structured and trustworthy report. To ensure report quality, recent efforts have centered on structure control and factual integrity. Structure control techniques incorporate planning-based generation and constraint-guided decoding, often coupled with structure-aware evaluation and alignment. In parallel, factual integrity is promoted through faithful modeling, which grounds generation in relevant evidence, and conflict reasoning, which enforces consistency across retrieved sources. Together, these approaches aim to produce well-organized and reliable outputs to meet the demands of research-oriented tasks.

Despite this progress, current approaches to report generation in deep research remain in an early stage. Most existing methods target isolated subskills of report generation while lacking joint optimization with upstream components. However, effective report synthesis is inherently coupled with prior planning, question developing, and web exploration stages, making this disconnect a major limitation. Furthermore, current structure control methods often rely on fixed outlines or static planning strategies, lacking the flexibility to adapt to task-specific complexity, evidence distribution, or reasoning flow. On the factuality front, while some methods enhance local grounding, few can model consistency across multi-document and multi-hop contexts, a critical need for scientific research scenarios involving long input contexts and conflicting sources.

# 6 Optimization

# 6.1 Workflow

Deep Research workflows are generally categorized into single-agent and multi-agent systems. Single-agent workflows consolidate all research stages within a unified model, enabling integrated reasoning and end-to-end learning. In contrast, multi-agent workflows decompose the process into specialized modules, promoting parallel execution, modular optimization, and greater flexibility.

Single-Agent Systems. A single LLM agent is responsible for the entire process. Systems such as DeepResearcher [98], WebThinker [36], and Search-R1 [31] typically adopt a monolithic workflow, where the agent sequentially performs task decomposition, query generation, retrieval (via tools or APIs), and final artifact synthesis. All reasoning and decisions are internally managed by the same model, often trained end-to-end using reinforcement learning.

Multi-Agent Systems. In contrast, multi-agent systems assign different agents to specific stages of the pipeline. Planner agents handle task decomposition and subgoal scheduling; query agents focus on generating diversified and contextual queries; retriever agents interact with external tools for evidence gathering; and writer agents perform structured synthesis. This modular division, seen in systems like AgentRxiv [57], AI Scientist [39], and OpenResearcher [99], allows each component to be specialized and optimized independently, facilitating parallelism and flexibility in complex research tasks.

# 6.2 Parameter Optimization

Effective Deep Research relies not only on system architecture but also on optimizing agent behavior through training paradigms tailored to complex tasks. Current research focuses on three main optimization approaches: contrastive learning, reinforcement learning, and curriculum training, each addressing different challenges in agent coordination and decision making. Contrastive Learning. This approach teaches agents to distinguish between effective and ineffective behaviors by contrasting successful and failed trajectories, especially in tool usage such as retrieval, search, and summarization. For example, Avatar [79] trains agents to recognize when and how to invoke external resources, improving precision in multi-step reasoning. Reinforcement Learning. RL methods fine-tune agents for long-term planning and decision making, using reward signals based on retrieval accuracy and final artifact quality. Systems like Search-O1 [35] and Learning-to-Search [9] leverage RL to promote effective question formulation and document acquisition in complex research workflows. Curriculum Training. To handle progressively complex tasks, curriculum training employs staged learning pipelines where agents first master fundamental skills before advancing to full workflow orchestration. Notable examples include AI Scientist- $\nu 2$ [87] and SimpleDeepSearcher [69], which improve agent robustness in open-ended scenarios.

Discussion. Optimization methods often complement architectural choices: single-agent systems tend to use RL for end-to-end reward alignment, while multi-agent systems benefit more from modular training and agent-level feedback. Some recent frameworks, such as AgentLab [58], begin to explore hybrid optimization combining symbolic planning, search augmentation, and human-in-the-loop feedback.

# 7 Benchmark and Evaluation

To understand the progress and limitations of Deep Research systems, recent benchmarks have been designed to evaluate the four core technical modules: Planning, Question Developing, Web Exploration, and Report generation. Table 5 summarizes major benchmarks and the specific modules they cover within the Deep Research workflow. These Manuscript submitted to ACM

Table 5. Coverage of core Deep Research modules across major benchmarks. P: Planning, QD: Question Developing, WE: Web Exploration, RG: report generation.   

<table><tr><td>Benchmark</td><td>P</td><td>QD</td><td>WE</td><td>RG</td><td>Task</td><td>Evaluation Metrics</td></tr><tr><td>MIND2WEB 2 [19]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Web search</td><td>Success rate, Partial Completion</td></tr><tr><td>BROWSECOMP [76]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Web search</td><td>Accuracy, Calibration Error</td></tr><tr><td>WEBARENA [101]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Web search</td><td>Success Rate</td></tr><tr><td>GAIA [40]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Multi-step assistant tasks</td><td>EM</td></tr><tr><td>HUMANITY&#x27;S LAST EXAM [48]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Multidomain reasoning</td><td>Accuracy, Calibration Error</td></tr><tr><td>BROWSECOMP-ZH [100]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Web search in Chinese</td><td>Accuracy, Calibration Error</td></tr><tr><td>MEDBROWSECOMP [10]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Medical web search</td><td>Accuracy</td></tr><tr><td>GPQA [53]</td><td>✓</td><td>✓</td><td>✓</td><td>✗</td><td>Graduate-level QA</td><td>Accuracy</td></tr><tr><td>INFODEEPSEEK [82]</td><td>✗</td><td>✓</td><td>✓</td><td>✗</td><td>Open-domain QA</td><td>Accuracy, Information Accu-racy</td></tr><tr><td>DEEPRESEARCH BENCH [16]</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Research report generation</td><td>Pairwise Agreement Rate, Over-all Pearson Correlation</td></tr><tr><td>DEEPRESEARCHGYM [14]</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Research task sandbox</td><td>KPR, KPC, Precision, Recall, Clarity, Insight</td></tr></table>

benchmarks can be broadly categorized into two types based on their task scope: search-oriented and researchoriented.

Search-oriented benchmarks primarily focus on information-seeking tasks involving Web Exploration and Question Developing, often in interactive or language-specific browsing scenarios. For example, Mind2Web 2 [19] assesses the ability to reformulate queries and navigate websites dynamically, while BrowseComp [76] and BrowseComp-Zh [100] evaluate multilingual browsing performance through accuracy and calibration error. WebArena [101] extends this with structured tasks that require simple Planning components, such as multi-step goal tracking. However, these benchmarks rarely involve final content generation, leaving report generation under-evaluated.

Research-oriented benchmarks go beyond search to evaluate long-range reasoning, synthesis, and structured output production. These tasks require full-pipeline coordination across all four modules. DeepResearch Bench [16] and DeepResearchGym [14] are representative, covering everything from Planning subtask decomposition, Question Developing for subqueries, real-time Web Exploration, to final report generation in the form of research-style reports. They provide detailed evaluation metrics like clarity, knowledge precision/recall (KPR/KPC), and agreement scores. MedBrowseComp [10] specializes this pipeline in the biomedical domain, while GAIA [40] evaluates agent workflows on general assistant tasks with full modular coverage. Additionally, benchmarks like Humanity’s Last Exam [48] and GPQA [53] emphasize Planning and Question Developing under minimal retrievability, simulating open-book but non-searchable QA scenarios.

# 8 Limitations and Future Directions

Despite impressive progress, Deep Research systems remain in their infancy and face multiple limitations across architecture, reliability, modality, and scalability. We outline several key challenges and future directions below.

Multi-Tool Integration. Most current systems rely solely on traditional search engines as their primary external tool, which severely restricts their access to diverse and task-specific knowledge sources. However, real-world research often involves querying APIs, parsing structured databases, navigating code repositories, and retrieving information

from documents, tables, or charts. Future Deep Research agents must support dynamic orchestration over multiple heterogeneous tools and flexibly decide which to invoke at each reasoning step.

Factuality. Ensuring factual consistency is a core challenge in Deep Research systems, especially when content is synthesized from multiple sources or spans multi-step reasoning. Agents may inadvertently introduce factual inaccuracies, outdated claims, or unsupported assertions, especially when aggregating content from inconsistent sources. To address this, future systems should incorporate explicit grounding mechanisms, such as source attribution, factuality-aware reward functions, and post-hoc verification modules.

Multimodal Reasoning Capabilities. Current pipelines are almost exclusively textual, making them unsuitable for domains that require visual or multimodal understanding. Research tasks in science, medicine, and engineering often involve diverse modalities such as images, textual descriptions, and scanned documents. Extending Deep Research frameworks to process and reason over multimodal inputs—including images, PDFs, and structured data—remains a largely unexplored but essential direction.

Workflow Design and Model Optimization. Effective Deep Research requires agents to coordinate complex workflows involving task decomposition, tool usage, and synthesis. While earlier systems often relied on static prompts and hard-coded workflow, recent works have begun to adopt more adaptive and agentic paradigms, enabling dynamic planning and tool use. However, current methods still lack mechanisms for learning workflows that generalize across tasks and evolve with new objectives. On the optimization side, aligning large language models with such dynamic workflows remains challenging due to high computational costs and sparse reward signals. Future directions may include scalable training strategies such as parameter-efficient tuning, staged or curriculum-based pretraining, and reinforcement learning guided by human or automated feedback.

Personalization Personalization aims to align agents with users’ goals and preferences for better performance. Existing methods lack persistent user modeling and dynamic adaptation, often treating personalization as secondary. Future work should develop scalable, privacy-aware user models with continual learning, addressing overfitting and fairness challenges.

In summary, addressing these limitations—through richer tool integration, stronger factual grounding, multimodal expansion, and efficient agent training—will be critical to realizing the full potential of autonomous Deep Research systems.

# 9 Conclusion

Deep research is revolutionizing the search paradigm and has emerged as one of the most promising directions in agent research. In this survey, we provide a systematic overview of the deep research pipeline, which comprises four core stages: planning, question developing, web exploration, and report generation. For each stage, we analyze the key technical challenges and categorize representative methods developed to address them. Furthermore, we summarize recent advances in optimization techniques and benchmarks tailored for deep research. Finally, we discuss open challenges and promising research directions, aiming to chart a roadmap toward building more capable and trustworthy deep research agents.

# References

[1] [n. d.]. Bing. https://www.bing.com/   
[2] [n. d.]. Google. https://www.google.com/   
[3] [n. d.]. X posts. https://x.com/

Manuscript submitted to ACM

[4] Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku. 2024. Agent-e: From autonomous web navigation to foundational design principles in agentic systems. arXiv preprint arXiv:2407.13032 (2024).   
[5] Y Bai, J Zhang, X Lv, L Zheng, S Zhu, L Hou, Y Dong, J Tang, and J LongWriter Li. 2024. Unleashing $^ { 1 0 , 0 0 0 + }$ word generation from long context llms. arXiv preprint arXiv:2408.07055 (2024).   
[6] Bocha AI. n.d.. Bocha AI Open Platform — Real-Time Search API for AI Applications. https://open.bochaai.com/ Site copyright notice shows ${ } ^ { \circ \circ } ( \varphi )$ 2023–2024”; earliest independent write-up located April 10 2025..   
[7] Brave Software. 2022. Brave Search — Private Search Engine. Public launch June 2022; privacy-focused search engine continually updated.   
[8] Arie Cattan, Alon Jacovi, Ori Ram, Jonathan Herzig, Roee Aharoni, Sasha Goldshtein, Eran Ofek, Idan Szpektor, and Avi Caciularu. 2025. DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs. arXiv preprint arXiv:2506.08500 (2025).   
[9] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z Pan, Wen Zhang, Huajun Chen, Fan Yang, et al. 2025. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470 (2025).   
[10] Shan Chen, Pedro Moreira, Yuxin Xiao, Sam Schmidgall, Jeremy Warner, Hugo Aerts, Thomas Hartvigsen, Jack Gallifant, and Danielle S Bitterman. 2025. MedBrowseComp: Benchmarking Medical Deep Research and Computer Use. arXiv preprint arXiv:2505.14963 (2025).   
[11] Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. 2024. Mindsearch: Mimicking human minds elicits deep ai searcher. arXiv preprint arXiv:2407.20183 (2024).   
[12] China National Knowledge Infrastructure. 2024. CNKI AI Academic Research Assistant (XAI). First public write-up on 6 Feb 2024; version 4.0 announced 21 Jun 2024..   
[13] CNKI. n.d.. CNKI Overseas Homepage. Copyright notice “1998–” indicates an evolving portal with multiple revisions.   
[14] João Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, Abhijay Paladugu, Pranav Setlur, Jiahe Jin, Jamie Callan, João Magalhães, Bruno Martins, et al. 2025. Deepresearchgym: A free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253 (2025).   
[15] Xinbang Dai, Huikang Hu, Yuncheng Hua, Jiaqi Li, Yongrui Chen, Rihui Jin, Nan Hu, and Guilin Qi. 2025. After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG. arXiv preprint arXiv:2505.17118 (2025).   
[16] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. 2025. DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents. arXiv preprint arXiv:2506.11763 (2025).   
[17] Lutfi Eren Erdogan, Nicholas Lee, Sehoon Kim, Suhong Moon, Hiroki Furuta, Gopala Anumanchipalli, Kurt Keutzer, and Amir Gholami. 2025. Plan-and-act: Improving planning of agents for long-horizon tasks. arXiv preprint arXiv:2503.09572 (2025).   
[18] Google. 2024. Gemini Deep Research — Your Personal Research Assistant. Feature announced December 11 2024, see TechCrunch coverage.   
[19] Boyu Gou, Zanming Huang, Yuting Ning, Yu Gu, Michael Lin, Weijian Qi, Andrei Kopanev, Botao Yu, Bernal Jiménez Gutiérrez, Yiheng Shu, et al. 2025. Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge. arXiv preprint arXiv:2506.21506 (2025).   
[20] Yu Gu, Kai Zhang, Yuting Ning, Boyuan Zheng, Boyu Gou, Tianci Xue, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, et al. 2024. Is your llm secretly a world model of the internet? model-based planning for web agents. arXiv preprint arXiv:2411.06559 (2024).   
[21] Yu Gu, Boyuan Zheng, Boyu Gou, Kai Zhang, Cheng Chang, Sanjari Srivastava, Yanan Xie, Peng Qi, Huan Sun, and Yu Su. 2025. Simulate Before Act: Model-Based Planning for Web Agents. openreview (2025).   
[22] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948 (2025).   
[23] Rujun Han, Yuhao Zhang, Peng Qi, Yumo Xu, Jenyuan Wang, Lan Liu, William Yang Wang, Bonan Min, and Vittorio Castelli. 2024. RAG-QA Arena: Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 4354–4374.   
[24] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. WebVoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919 (2024).   
[25] Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin Zhao. 2025. ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework. arXiv preprint arXiv:2505.18105 (2025).   
[26] Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, et al. 2025. Deep Research Agents: A Systematic Examination And Roadmap. arXiv preprint arXiv:2506.18096 (2025).   
[27] Yukun Huang, Yixin Liu, Raghuveer Thirukovalluru, Arman Cohan, and Bhuwan Dhingra. 2024. Calibrating long-form generations from large language models. arXiv preprint arXiv:2402.06544 (2024).   
[28] Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, and Kang Liu. 2025. Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent. arXiv preprint arXiv:2505.07596 (2025).   
[29] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language models. Journal of Machine Learning Research 24, 251 (2023), 1–43.   
[30] Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. arXiv preprint arXiv:2503.00223 (2025).   
[31] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516 (2025).

[32] Michael Katz, Harsha Kokel, Kavitha Srinivas, and Shirin Sohrabi Araghi. 2024. Thought of search: Planning with language models through the lens of efficiency. Advances in Neural Information Processing Systems 37 (2024), 138491–138568.   
[33] Kyu Won Kim, Suhwan Choi, and Myeongho Jeon. [n. d.]. Say as It Is: Verbatim Fidelity Evaluation of Long-Context Language Model. In ICML 2025 Workshop on Long-Context Foundation Models.   
[34] Hugo Laurençon, Léo Tronchon, and Victor Sanh. 2024. Unlocking the conversion of web screenshots into html code with the websight dataset. arXiv preprint arXiv:2403.09029 (2024).   
[35] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366 (2025).   
[36] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776 (2025).   
[37] Jiarun Liu, Jia Hao, Chunhong Zhang, and Zheng Hu. 2025. Wepo: Web element preference optimization for llm-based web navigation. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 26614–26622.   
[38] Do Xuan Long, Duong Ngoc Yen, Do Xuan Trong, Luu Anh Tuan, Kenji Kawaguchi, Shafiq Joty, Min-Yen Kan, and Nancy F Chen. 2025. Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines. arXiv preprint arXiv:2506.01265 (2025).   
[39] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292 (2024).   
[40] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2023. Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations.   
[41] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 (2021).   
[42] Xuan-Phi Nguyen, Shrey Pandit, Senthil Purushwalkam, Austin Xu, Hailin Chen, Yifei Ming, Zixuan Ke, Silvio Savarese, Caiming Xong, and Shafiq Joty. 2024. Sfr-rag: Towards contextually faithful llms. arXiv preprint arXiv:2409.09916 (2024).   
[43] OpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023).   
[44] OpenAI. 2025. Introducing Deep Research.   
[45] Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris Callison-Burch, and Sepp Hochreiter. 2024. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309 (2024).   
[46] Perplexity Team. 2025. Introducing Perplexity Deep Research.   
[47] Chau Pham, Simeng Sun, and Mohit Iyyer. 2024. Suri: Multi-constraint Instruction Following in Long-form Text Generation. In Findings of the Association for Computational Linguistics: EMNLP 2024. 1722–1753.   
[48] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. 2025. Humanity’s last exam. arXiv preprint arXiv:2501.14249 (2025).   
[49] Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, and Shanghang Zhang. 2025. LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information. arXiv preprint arXiv:2502.02095 (2025).   
[50] Zehan Qi, Rongwu Xu, Zhijiang Guo, Cunxiang Wang, Hao Zhang, and Wei Xu. 2024. LONG2RAG: Evaluating Long-Context & Long-Form Retrieval-Augmented Generation with Key Point Recall. In Findings of the Association for Computational Linguistics: EMNLP 2024. 4852–4872.   
[51] Hongjin Qian and Zheng Liu. 2025. Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging. arXiv preprint arXiv:2505.09316 (2025).   
[52] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. 2024. Agent planning with world knowledge model. Advances in Neural Information Processing Systems 37 (2024), 114843–114871.   
[53] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. 2024. Gpqa: A graduate-level google-proof q&a benchmark. In First Conference on Language Modeling.   
[54] Reportify. n.d.. Reportify — AI Platform for Investment Research. Continuously updated product site; no formal release date displayed.   
[55] Leonard Richardson. 2025. Beautiful Soup – HTML/XML parsing and screen-scraping library. https://pypi.org/project/beautifulsoup4/ MIT License. Accessed 2025-06-26.   
[56] Alireza Salemi, Julian Killingback, and Hamed Zamani. 2025. ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation. arXiv preprint arXiv:2501.14956 (2025).   
[57] Samuel Schmidgall and Michael Moor. 2025. Agentrxiv: Towards collaborative autonomous research. arXiv preprint arXiv:2503.18102 (2025).   
[58] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. 2025. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227 (2025).   
[59] Scrapy developers. 2025. Scrapy – A fast, high-level web-crawling & scraping framework. https://github.com/scrapy/scrapy BSD 3-Clause License. Accessed 2025-06-26.   
[60] Selenium contributors. 2025. Selenium WebDriver – Browser-automation framework. https://pypi.org/project/selenium/ Apache 2.0 License. Accessed 2025-06-26.   
[61] Kwangwook Seo, Donguk Kwon, and Dongha Lee. 2025. Mt-raig: Novel benchmark and evaluation framework for retrieval-augmented insight generation over multiple tables. arXiv preprint arXiv:2502.11735 (2025).

Manuscript submitted to ACM

[62] Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, and Yong Li. 2024. Agentsquare: Automatic llm agent search in modular design space. arXiv preprint arXiv:2410.06153 (2024).   
[63] Haiyang Shen, Hang Yan, Zhongshi Xing, Mugeng Liu, Yue Li, Zhiyang Chen, Yuxiang Wang, Jiuzheng Wang, and Yun Ma. 2025. RAGSynth: Synthetic Data for Robust and Faithful RAG Component Optimization. arXiv preprint arXiv:2505.10989 (2025).   
[64] Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. 2024. Trusting your evidence: Hallucinate less with context-aware decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers). 783–791.   
[65] Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. 2025. Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs. arXiv preprint arXiv:2505.11277 (2025).   
[66] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592 (2025).   
[67] Raphael Sourty. 2024. DuckSearch, efficient search with DuckDB. https://github.com/lightonai/ducksearch   
[68] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. 2025. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588 (2025).   
[69] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, et al. 2025. Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834 (2025).   
[70] Manveer Singh Tamber, Forrest Sheng Bao, Chenyu Xu, Ge Luo, Suleman Kazi, Minseok Bae, Miaoran Li, Ofer Mendelevitch, Renyi Qu, and Jimmy Lin. 2025. Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards. arXiv preprint arXiv:2505.04847 (2025).   
[71] Brandon Trabucco, Gunnar Sigurdsson, Robinson Piramuthu, and Ruslan Salakhutdinov. 2025. Towards Internet-Scale Training For Agents. arXiv preprint arXiv:2502.06776 (2025).   
[72] Jonas Wallat, Maria Heuss, Maarten de Rijke, and Avishek Anand. 2024. Correctness is not Faithfulness in RAG Attributions. arXiv preprint arXiv:2412.18004 (2024).   
[73] Kaiyang Wan, Honglin Mu, Rui Hao, Haoran Luo, Tianle Gu, and Xiuying Chen. 2025. A cognitive writing perspective for constrained long-form text generation. arXiv preprint arXiv:2502.12568 (2025).   
[74] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. 2025. Acting Less is Reasoning More! Teaching Model to Act Efficiently. arXiv preprint arXiv:2504.14870 (2025).   
[75] Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao. 2025. VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning. arXiv preprint arXiv:2505.22019 (2025).   
[76] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: A simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516 (2025).   
[77] Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. 2025. MMSearch-R1: Incentivizing LMMs to Search. arXiv:2506.20670 [cs.CV] https://arxiv.org/abs/2506.20670   
[78] Siwei Wu, Yizhi Li, Xingwei Qu, Rishi Ravikumar, Yucheng Li, Tyler Loakman, Shanghaoran Quan, Xiaoyong Wei, Riza Batista-Navarro, and Chenghua Lin. 2025. Longeval: A comprehensive analysis of long-text generation through a plan-based paradigm. arXiv preprint arXiv:2502.19103 (2025).   
[79] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis Ioannidis, Karthik Subbian, Jure Leskovec, and James Y Zou. 2024. Avatar: Optimizing llm agents for tool usage via contrastive reasoning. Advances in Neural Information Processing Systems 37 (2024), 25981–26010.   
[80] Weiqi Wu, Xin Guan, Shen Huang, Yong Jiang, Pengjun Xie, Fei Huang, Jiuxin Cao, Hai Zhao, and Jingren Zhou. 2025. MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability. arXiv preprint arXiv:2505.20285 (2025).   
[81] xAI. 2025. Grok 3 Beta — The Age of Reasoning Agents.   
[82] Yunjia Xi, Jianghao Lin, Menghui Zhu, Yongzhao Xiao, Zhuoying Ou, Jiaqi Liu, Tong Wan, Bo Chen, Weiwen Liu, Yasheng Wang, et al. 2025. InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation. arXiv preprint arXiv:2505.15872 (2025).   
[83] Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, and Sujian Li. 2025. Mpo: Boosting llm agents with meta plan optimization. arXiv preprint arXiv:2503.02682 (2025).   
[84] Renjun Xu and Jingwen Peng. 2025. A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications. arXiv preprint arXiv:2506.12594 (2025).   
[85] Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Seng Chua. 2024. Search-in-the-chain: Interactively enhancing large language models with search for knowledge-intensive tasks. In Proceedings of the ACM Web Conference 2024. 1362–1373.   
[86] Yunqi Xu, Tianchi Cai, Jiyan Jiang, and Xierui Song. 2024. Face4RAG: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 6083–6094.   
[87] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066 (2025).   
[88] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388 (2025).

Manuscript submitted to ACM

[89] Tiannuo Yang, Zebin Yao, Bowen Jin, Lixiao Cui, Yusen Li, Gang Wang, and Xiaoguang Liu. 2025. Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents. arXiv preprint arXiv:2505.12065 (2025).   
[90] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381 (2023).   
[91] Jiahao Ying, Yixin Cao, Kai Xiong, Yidong He, Long Cui, and Yongbin Liu. 2023. Intuitive or Dependent? Investigating LLMs’ Behavior Style to Conflicting Prompts. arXiv preprint arXiv:2309.17415 (2023).   
[92] Xiaowei Yuan, Zhao Yang, Yequan Wang, Shengping Liu, Jun Zhao, and Kang Liu. 2024. Discerning and resolving knowledge conflicts through adaptive decoding with contextual information-entropy constraint. arXiv preprint arXiv:2402.11893 (2024).   
[93] Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, et al. 2025. EvolveSearch: An Iterative Self-Evolving Search Agent. arXiv preprint arXiv:2505.22501 (2025).   
[94] Qinggang Zhang, Zhishang Xiang, Yilin Xiao, Le Wang, Junhui Li, Xinrun Wang, and Jinsong Su. 2025. FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation. arXiv preprint arXiv:2506.08938 (2025).   
[95] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, et al. 2025. Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning. arXiv preprint arXiv:2505.14069 (2025).   
[96] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. 2025. Webpilot: A versatile and autonomous multi-agent system for web task execution with strategic exploration. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 23378–23386.   
[97] Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, and Limin Liu. 2025. R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning. arXiv preprint arXiv:2506.04185 (2025).   
[98] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160 (2025).   
[99] Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, et al. 2024. OpenResearcher: Unleashing AI for Accelerated Scientific Research. arXiv preprint arXiv:2408.06941 (2024).   
[100] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. 2025. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314 (2025).   
[101] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. 2023. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854 (2023).   
[102] Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful Prompting for Large Language Models. Findings of the Association for Computational Linguistics: EMNLP 2023 (2023).