# R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning

Huatong Song1∗, Jinhao Jiang1∗, Yingqian ${ { \bf { M } } { \bf { i n } } ^ { 1 } }$ , Jie Chen1, Zhipeng Chen1, Wayne Xin Zhao1†, Lei Fang2, Ji-Rong Wen1

1Gaoling School of Artificial Intelligence, Renmin University of China.

2DataCanvas Alaya NeW

{songhuatong123, jiangjinhao}@ruc.edu.cn

batmanfly@gmail.com

# Abstract

Existing Large Reasoning Models (LRMs) have shown the potential of reinforcement learning (RL) to enhance the complex reasoning capabilities of Large Language Models (LLMs). While they achieve remarkable performance on challenging tasks such as mathematics and coding, they often rely on their internal knowledge to solve problems, which can be inadequate for time-sensitive or knowledgeintensive questions, leading to inaccuracies and hallucinations. To address this, we propose R1-Searcher, a novel two-stage outcome-based RL approach designed to enhance the search capabilities of LLMs. This method allows LLMs to autonomously invoke external search systems to access additional knowledge during the reasoning process. Our framework relies exclusively on RL, without requiring process rewards or distillation for a cold start. Our experiments demonstrate that our method significantly outperforms previous strong RAG methods, even when compared to the closed-source GPT-4o-mini. The code is available at https://github.com/RUCAIBox/R1-Searcher.

# 1 Introduction

Large Reasoning Models (LRMs), such as OpenAI-o1 [1], Deepseek-R1 [2] and Kimi-k1.5 [3], have demonstrated the significant impact of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs) [4]. However, since they primarily rely on their internal knowledge, these models may struggle with open-ended tasks, particularly those involving knowledge-intensive questions [5, 6], private information in local databases [7, 8], and time-sensitive issues [9, 10]. This reliance may easily lead to inaccuracies and hallucinations. Therefore, it is crucial to enable LLMs to access external information during the reasoning process to achieve more deliberative reasoning [11].

To address this issue, extensive research has focused on augmenting LLMs with external information sources (a.k.a., retrieval-augmented generation (RAG) [12, 13]). Early approaches emphasize specific prompting strategies to guide LLMs in iterative question decomposition, query generation, and sub-question answering [14, 15, 16]. While effective, these complex prompt designs may rely on closed-source LLMs for achieving optimal performance. Subsequent studies investigate to distill this capability into smaller LLMs through supervised fine-tuning (SFT) [17]. However, recent findings suggest that SFT-based distillation can cause models to memorize solution paths, limiting their generalization to novel scenarios [18]. Recent proposals include a test-time scaling method [11, 19], notably employing the Monte Carlo Tree Search (MCTS) framework to enhance solution-finding by expanding the search space during inference. Despite its promise, this approach incurs significant

![](images/3283a8b25b153af2f40f6ed9aa2b52656b2a315fd3b1cd46822a3e1fb98c138b.jpg)  
Figure 1: Performance comparisons between R1-Searcher and other methods on four multi-hop QA benchmarks. R1-Searcher achieves significant performance improvement on each dataset. The metric is LLM-as-Judge Score.

inference overhead, reducing its practicality for widespread use. Therefore, we propose integrating an external retrieval environment during training, enabling models to explore and learn to effectively utilize retrieval for problem-solving. This approach aims to incentivize the search capability in LLMs, thereby enhancing LLMs’ generalization and improving inference efficiency.

In this paper, we introduce R1-Searcher, a novel framework to enhance the RAG capabilities of LLMs with RL. Our core motivation is to incentivizing the search capability in LLMs via exploring with an external retrieval environment. To implement it, we design a two-stage, outcome-based RL approach, enabling the model to freely explore how to invoke an external retrieval system to acquire relevant knowledge during the reasoning process through a tailored reward design. Specifically, in the first stage, we employ the retrieve-reward to incentivize the model to conduct retrieval operations without considering the final answer accuracy. In this way, the LLMs can quickly learn the correctly retrieval invocation format. In the second stage, we further introduce the answer reward to encourage the model to learn to effectively utilize the external retrieval system to solve question correctly. Our method relies solely on outcome-based RL, allowing the model to learn autonomously through exploration and learning without requiring any distillation or cold start with SFT. To support the exploration between LLMs and the external retrieval environment during the training process, we further propose a modified RL training method based on Reinforce++ [20] with RAG-based rollout and retrieval mask-based loss calculation.

We conduct extensive experiments to verify the effectiveness of our method using various LLM backbones on four representative benchmarks, based on Llama-3.1-8B-Instruct and Qwen-2.5-7B-Base. Notably, our method surpasses the strong baseline with GPT-4o-mini (i.e., ReARTeR) by up to $4 8 . 2 2 \%$ on HotpotQA and $2 1 . 7 2 \%$ on 2Wiki when using Qwen-2.5-7B-Base. To access generalization capability, we evaluate our method on the Bamboogle dataset using an online search, which is not seen during training. Our model achieved an $1 1 . 4 \%$ performance improvement on Bamboogle compared to the Search-o1 [14] with 32B parameters.

Our principal contributions are as follows:

• We introduce R1-Searcher, which utilizes a two-stage RL framework to enable autonomous retrieval during the reasoning process.   
• Extensive experiments on four multi-hop datasets show that R1-Searcher consistently and significantly surpasses existing RAG methods, even close-sourced GPT-4o-mini.

• Our approach exclusively employs RL for training without any distillation or cold start, while showing great generalization to out-of-domain datasets and online search scenarios. It is effective for both base and instruction-tuned models.

# 2 Method

# 2.1 Data Selection

In this study, we aim to enhance the search capabilities of LLMs for problem-solving within a retrieval environment using an outcome-based RL method. However, the independence of the retrieval environment may lead to issues that exceed its query scope during the RL training process, posing challenges to successful problem resolution and affecting the training efficiency. To address this, we conduct data selection and incorporate questions with varying difficulty levels.

Specifically, we select training data from the training sets of two representative multi-hop question answering datasets, namely HotpotQA [5] and 2WikiMultiHopQA [6]. We utilize the Qwen-2.5-7B-Instruct [21] model, equipped with a local retrieval system, and prompt the model (Prompt 2.1) in solving questions from these datasets using the external retrieval system. The prompt is provided below. Based on the number of rollouts required to correctly answer a question, we categorize the data into three levels of difficulty: easy (fewer than 10 rollouts), medium (10 to 20 rollouts), and difficult (more than 20 rollouts). These difficulty levels are then combined as delineated in Table 1 to construct our training dataset.

Table 1: The information of the data used during RL training.   

<table><tr><td>Stage</td><td>Dataset</td><td>Easy</td><td>Medium</td><td>Difficult</td></tr><tr><td rowspan="2">Stage-1</td><td>HotpotQA</td><td>-</td><td>200</td><td>-</td></tr><tr><td>2WikiMultiHopQA</td><td>-</td><td>150</td><td>-</td></tr><tr><td rowspan="2">Stage-2</td><td>HotpotQA</td><td>-</td><td>2561</td><td>2000</td></tr><tr><td>2WikiMultiHopQA</td><td>-</td><td>1087</td><td>2500</td></tr></table>

# System Prompt for Data Selection

You are a helpful assistant. Given a question, you should answer it by first thinking about the reasoning process in the mind and then providing the final answer. The output format of reasoning process and final answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., "<think> reasoning process here </think><answer> final answer here </answer>". You should perform thinking with decomposing, reflecting, brainstorming, verifying, refining, and revising. Besides, you can perform searching for uncertain knowledge if necessary with the format of "<|begin_of_query|> search query (only keywords) here <|end_of_query|>".""" Then, the search system will provide you with the retrieval information with the format of "<|begin_of_documents|> ...search results... <|end_of_documents|>".

# 2.2 Two-Stage Outcome-based Reinforcement Learning

To progressively improve the search capabilities of LLMs, we propose a two-stage outcome-based RL training method. In Stage-1, the model is trained to effectively utilize an external retrieval system. In Stage-2, the model is trained to incorporate search during the reasoning process to accurately solve questions.

# 2.2.1 Reward Design

Due to the absence of intermediate annotations in the training data, the RL process is primarily influenced by outcome rewards. By assigning distinct rewards across two stages, the model progressively learns to invoke the external retrieval system and effectively integrate retrieved documents into the reasoning process to answer questions.

In Stage-1, the reward function comprises a retrieval reward and a format reward. The primary goal here is to enable the model to recognize its ability to invoke the external retrieval system and learn its utilization, without considering the correctness of the model’s answers. The model is explicitly

encouraged to initiate search queries, and thus, no answer reward is assigned at this stage. Specifically, the retrieval reward is defined as follows:

$$
R _ {\text {r e t r i e v a l}} = \left\{ \begin{array}{l l} 0. 5, & n \geq 1 \\ 0, & n = 0 \end{array} \right. \tag {1}
$$

where $n$ represents the number of retrieval invocations. For the format reward, we first define the correct format as follows:

1. The model’s thinking process and final answer should be enclosed within the <think>...</think> and <answer>...</answer> tags, respectively. Additionally, only the final short answer is permitted within the <answer>...</answer> tag.   
2. The generated output must be free of any garbled or unreadable content.   
3. When invoking retrieval, the model should propose a query and encapsulate the query within the <begin_of_query>...</end_of_query> tags. Furthermore, the model is unable to generate documents directly without invoking retrieval.

Based on the above format requirements, the format reward is defined as follows:

$$
R _ {f o r m a t} = \left\{ \begin{array}{l l} 0. 5, & \text {i f t h e f o r m a t i s c o r r e c t} \\ 0, & \text {i f t h e f o r m a t i s i n c o r r e c t} \end{array} \right. \tag {2}
$$

Therefore, the final reward of Stage-1 is the sum of the retrieval reward and format reward.

In Stage-2, we eliminate the retrieval reward and incorporate the answer reward. We apply the same format judgment criteria as in Stage-1, but with different penalties:

$$
R _ {f o r m a t} ^ {\prime} = \left\{ \begin{array}{l l} 0, & \text {i f t h e f o r m a t i s c o r r e c t} \\ - 2, & \text {i f t h e f o r m a t i s i n c o r r e c t} \end{array} \right. \tag {3}
$$

For the answer reward, we utilize the F1 score of the ground-truth answer and predicted answer, which is calculated as follows:

$$
R _ {\text {a n s w e r}} = \frac {2 * I N}{P N + R N} \tag {4}
$$

where PN represents the word count of the predicted answer, RN denotes the word count of the reference answer, and IN indicates the word count of the intersection between the two answers.

Therefore, the final reward of Stage-2 is the sum of the answer reward and the format reward.

# 2.2.2 Training Algorithm

Our training algorithm is based on the Reinforce $^ { + + }$ algorithm, which we have modified to suit our retrieval-augmented generation scenario. During the reasoning process, the model engages an external retrieval system to solve problems, receiving a reward for correct solutions. We enhance the model’s ability to utilize retrieval during the reasoning process by maximizing this reward. Our goal is to enable the model to autonomously access external knowledge when faced with uncertainty, effectively integrating reasoning and retrieval. To incorporate retrieved documents seamlessly and ensure rational model optimization, we implement two modifications to the original algorithm: RAG-based Rollout and Retrieval Mask-based Loss Calculation.

RAG-based Rollout. As demonstrated in Prompt 2.2.2, we guide the model to utilize the external retrieval system during the generation process by employing the tags <begin_of_query>...<end_of_query> to indicate the invocation of the search tool. Upon generating <end_of_query>, the process pauses, allowing the extraction and use of the query for retrieval. The retrieved documents are encapsulated within <begin_of_documents>...<end_of_documents> tags and integrated into the model’s reasoning. This method ensures that retrieval is seamlessly incorporated into the reasoning process, allowing the model to continue its reasoning based on the retrieved documents without disruption.

# System Prompt for Base Model

The User asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the User with the final answer. The output format of reasoning process and final answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., "<think> reasoning process here </think><answer> final answer here </answer>". During the thinking process, **the Assistant can perform searching** for uncertain knowledge if necessary with the format of "<|begin_of_query|> search query (only list keywords, such as "keyword_1 keyword_2 ...")<|end_of_query|>". $* * _ { \mathrm { { A } } }$ query must involve only a single triple**. Then, the search system will provide the Assistant with the retrieval information with the format of "<|begin_of_documents|> ...search results... <|end_of_documents|>".

Retrieve Mask-based Loss Calculation. During the training process, the aforementioned solutions are employed to compute the RL loss, involving the reward, KL divergence, and advantages. When the model performs retrieval, the retrieved documents are integrated into the reasoning process, serving as environment observations. The model is not intended to generate these documents. To mitigate the environmental effect, we designate <begin_of_documents>...<end_of_documents> as special tokens and mask them during training. This prevents these external tokens from influencing the loss calculation, ensuring that the retrieved documents do not interfere with the model’s intrinsic reasoning and generation processes.

# 3 Experiment

# 3.1 Datasets and Evaluation Metrics

In training the R1-Searcher, we perform data selection from the training sets of HotpotQA and 2WikiMultiHopQA (see 1). We evaluate using four multi-hop datasets: HotpotQA [5], 2WikiMulti-HopQA [6], Musique [22], and Bamboogle [9]. HotpotQA and 2WikiMultiHopQA are in-domain benchmarks since parts of their training sets are used for reinforcement learning. In contrast, Musique and Bamboogle serve as out-of-domain benchmarks to assess our model’s generalization capabilities.

For evaluation metrics, following existing work [23], we utilize Cover Exact Match (ACC_R) and LLM-as-Judge (ACC_L), given the nature of open-ended multi-hop questions. Cover Exact Match assesses whether the ground truth answer is included in the predicted answer, while LLM-as-Judge uses GPT-4o-mini to evaluate the correctness of the predictions. The evaluation prompt for ACC_L is as follows:

# Judge Prompt

Given a Question and its Golden Answer, verify whether the Predicted Answer is correct. The prediction is correct if it fully aligns with the meaning and key information of the Golden Answer. Respond with True if the prediction is correct and False otherwise.

Question:

Golden Answer:

Predicted Answer:

# 3.2 Baselines

We utilize Qwen-2.5-7B-Base and Llama-3.1-8B-Instruct as the backbone models for our training. We compare R1-Searcher against the following baselines, based on GPT-4o-mini and Llama-3.1-8B-Instruct:

• Naive Generation: Direct generation of answers without retrieval.   
• Standard RAG: Traditional retrieval-augmented generation systems.   
• Branching Methods (Branching): SuRe [24] and REPLUG [25], which execute multiple reasoning paths in parallel for a single query.   
• Summarization-based Methods (Summary): LongLLMLingua [26], RECOMP [27], and Selective-Context [28], which employ compressors to summarize retrieved documents.

• Adaptive Retrieval Methods (AR): SKR [29], which adaptively retrieves based on the generator’s knowledge.   
• RAG-CoT Methods (RAG-CoT): Self-Ask [30], Iter-RetGen [31], and IRCoT [32], integrating retrieval-augmented generation with chain-of-thought reasoning.   
• Test-time Scaling Methods (Test-Time): CR-Planner [19], ReARTeR [23], which scale retrievalaugmented generation at test time using Monte Carlo Tree Search (MCTS).   
• Reasoning Models (Reasoning): Marco-o1-Qwen-7B [33] and Skywork-o1-Llama-3.1-8B [34], employing standard retrieval-augmented generation.

# 3.3 Implementation Details

All baseline models adhere to the ReARTeR framework and are evaluated using FlashRAG [35]. The retrieval corpus comprises the English Wikipedia as provided by KILT [36] in 2019, segmented into 100-word passages with appended titles, totaling 29 million passages. We employ BGE-large-en-v1.5 as the text retriever. Given the timeliness of knowledge in Bamboogle, we utilize the Google Web Search API for online webpage search tests to further evaluate our model’s generalization capabilities to online search (Section 4.4).

For our R1-Searcher, the backbone model incorporates Llama-3.1-8B-Instruct or Qwen-2.5-7B-Base. The training data of the Stage-1 includes 200 medium samples from the HotpotQA training set and 150 medium samples from the 2WikiMultiHopQA training set. And the training data of Stage-2 consists of 4561 samples from HotpotQA, with 2561 medium and 2000 hard samples (Table 1), and 3581 samples from 2WikiMultiHopQA, also with 1087 medium and 2500 hard samples. Each data sample undergoes 16 rollouts during training, with a train batch size of 256 and a rollout batch size of 64. The learning rate is 2e-6. We utilize DeepSpeed’s Zero-2 [37], with a sampling temperature of 1.0 and a maximum retrieval count of 8. The training epoch is set to 1, with KL divergence set to 0 for Qwen-2.5-7B-Base and 1e-4 for Llama-3.1-8B-Instruct. The discount factor $\gamma$ is set to 1 in the cumulative discounted reward calculation.

# 3.4 Main Results

Table2 shows the results of R1-Searcher and the baselines on four mutil-step benchmarks. We can obtain the following observations:

• Achieving Significant Performance Improvement on Multi-Hop QA. ReARTeR demonstrates superior performance among existing baselines, highlighting the advantages of the test-time scaling method. However, it relies on MCTS for solution exploration, which incurs significant overhead due to increased retrieval invocations. In contrast, our proposed R1-Searcher, utilizing the same LLaMA-3.1-8B-Instruct backbone model, achieves notable performance enhancements over ReARTeR and other baselines. Specifically, our method yields improvements of $4 8 . 2 \%$ on HotpotQA, $2 1 . 7 \%$ on 2WikiMultiHopQA, and $4 . 0 \%$ on Bamboogle according to the LLM-as-Judge metric. This indicates that our method can efficiently facilitates the model to conduct accurate retrieval invocations during the reasoning process.   
• Supporting RL Learning from Base LLM without Cold Start. Furthermore, we also conduct RL learning from scratch using a powerful base model, such as Qwen-2.5-7B-Base. Surprisingly, we can achieve better results and obtain the best performance on most in-domain and out-of-domain datasets, even surpassing the closed-source LLM such as GPT-4o-mini. These results demonstrate the effectiveness of our two-stage RL method in guiding the LLMs’ learning process.   
• Maintaining Generalization Ability. We employ only 8148 samples from the training sets of HotpotQA and 2WikiMultiHopQA for RL training. The model not only excels on these in-domain datasets but also demonstrates strong generalization by performing well on the out-of-domain datasets, such as Musique and Bamboogle. This suggests that the model effectively learns retrieval and integrates it with reasoning through exploration during RL training, enabling robust performance on new test datasets requiring retrieval. Furthermore, it can also seamlessly generalizes to online search, as detailed in Section 4.4.

<table><tr><td rowspan="2">Models</td><td rowspan="2">Types</td><td rowspan="2">Methods</td><td colspan="2">HotpotQA</td><td colspan="2">2Wiki</td><td colspan="2">Bamboogle</td><td colspan="2">Musique</td></tr><tr><td>\(ACC_R\)</td><td>\(ACC_L\)</td><td>\(ACC_R\)</td><td>\(ACC_L\)</td><td>\(ACC_R\)</td><td>\(ACC_L\)</td><td>\(ACC_R\)</td><td>\(ACC_L\)</td></tr><tr><td rowspan="13">GPT</td><td rowspan="2">Zero-Shot</td><td>Naive Generation</td><td>0.324</td><td>0.404</td><td>0.348</td><td>0.346</td><td>0.240</td><td>0.280</td><td>0.134</td><td>0.170</td></tr><tr><td>Standard RAG</td><td>0.342</td><td>0.450</td><td>0.344</td><td>0.292</td><td>0.272</td><td>0.328</td><td>0.172</td><td>0.188</td></tr><tr><td rowspan="2">Branching</td><td>SuRe</td><td>0.270</td><td>0.380</td><td>0.244</td><td>0.264</td><td>0.168</td><td>0.208</td><td>0.128</td><td>0.146</td></tr><tr><td>REPLUG</td><td>0.350</td><td>0.428</td><td>0.296</td><td>0.254</td><td>0.224</td><td>0.256</td><td>0.132</td><td>0.138</td></tr><tr><td rowspan="3">Summary</td><td>LongLLMlingua</td><td>0.358</td><td>0.450</td><td>0.324</td><td>0.316</td><td>0.248</td><td>0.288</td><td>0.150</td><td>0.172</td></tr><tr><td>RECOMP</td><td>0.332</td><td>0.398</td><td>0.298</td><td>0.306</td><td>0.136</td><td>0.176</td><td>0.118</td><td>0.134</td></tr><tr><td>Selective-Context</td><td>0.366</td><td>0.442</td><td>0.350</td><td>0.290</td><td>0.240</td><td>0.288</td><td>0.152</td><td>0.172</td></tr><tr><td>Adaptive</td><td>SKR</td><td>0.360</td><td>0.454</td><td>0.364</td><td>0.314</td><td>0.248</td><td>0.288</td><td>0.162</td><td>0.174</td></tr><tr><td rowspan="3">RAG-CoT</td><td>Self-Ask</td><td>0.392</td><td>0.462</td><td>0.336</td><td>0.478</td><td>0.336</td><td>0.416</td><td>0.260</td><td>0.270</td></tr><tr><td>Iter-RetGen</td><td>0.374</td><td>0.456</td><td>0.326</td><td>0.270</td><td>0.232</td><td>0.256</td><td>0.178</td><td>0.188</td></tr><tr><td>IRCoT</td><td>0.434</td><td>0.308</td><td>0.492</td><td>0.114</td><td>0.272</td><td>0.184</td><td>0.192</td><td>0.214</td></tr><tr><td rowspan="2">Test-Time</td><td>CR-Planner</td><td>0.404</td><td>0.416</td><td>0.520</td><td>0.478</td><td>0.488</td><td>0.524</td><td>0.272</td><td>0.262</td></tr><tr><td>ReARTeR</td><td>0.468</td><td>0.506</td><td>0.554</td><td>0.534</td><td>0.496</td><td>0.544</td><td>0.296</td><td>0.302</td></tr><tr><td rowspan="15">Llama</td><td rowspan="2">Zero-Shot</td><td>Naive Generation</td><td>0.208</td><td>0.268</td><td>0.326</td><td>0.254</td><td>0.144</td><td>0.168</td><td>0.068</td><td>0.096</td></tr><tr><td>Standard RAG</td><td>0.334</td><td>0.398</td><td>0.336</td><td>0.212</td><td>0.168</td><td>0.216</td><td>0.104</td><td>0.098</td></tr><tr><td rowspan="2">Branching</td><td>SuRe</td><td>0.266</td><td>0.346</td><td>0.122</td><td>0.262</td><td>0.160</td><td>0.192</td><td>0.106</td><td>0.144</td></tr><tr><td>REPLUG</td><td>0.290</td><td>0.348</td><td>0.334</td><td>0.204</td><td>0.168</td><td>0.232</td><td>0.078</td><td>0.090</td></tr><tr><td rowspan="3">Summary</td><td>LongLLMlingua</td><td>0.314</td><td>0.382</td><td>0.304</td><td>0.294</td><td>0.168</td><td>0.216</td><td>0.088</td><td>0.100</td></tr><tr><td>RECOMP</td><td>0.318</td><td>0.380</td><td>0.324</td><td>0.322</td><td>0.104</td><td>0.160</td><td>0.112</td><td>0.126</td></tr><tr><td>Selective-Context</td><td>0.296</td><td>0.358</td><td>0.266</td><td>0.204</td><td>0.144</td><td>0.200</td><td>0.092</td><td>0.104</td></tr><tr><td>Adaptive</td><td>SKR</td><td>0.300</td><td>0.372</td><td>0.336</td><td>0.212</td><td>0.176</td><td>0.208</td><td>0.100</td><td>0.112</td></tr><tr><td rowspan="3">RAG-CoT</td><td>Self-Ask</td><td>0.316</td><td>0.408</td><td>0.306</td><td>0.322</td><td>0.360</td><td>0.432</td><td>0.222</td><td>0.226</td></tr><tr><td>Iter-RetGen</td><td>0.302</td><td>0.362</td><td>0.310</td><td>0.224</td><td>0.144</td><td>0.176</td><td>0.084</td><td>0.084</td></tr><tr><td>IRCoT</td><td>0.210</td><td>0.146</td><td>0.338</td><td>0.312</td><td>0.120</td><td>0.104</td><td>0.060</td><td>0.042</td></tr><tr><td rowspan="2">Test-Time</td><td>CR-Planer</td><td>0.332</td><td>0.350</td><td>0.420</td><td>0.350</td><td>0.304</td><td>0.336</td><td>0.144</td><td>0.098</td></tr><tr><td>ReARTeR</td><td>0.424</td><td>0.434</td><td>0.470</td><td>0.364</td><td>0.438</td><td>0.484</td><td>0.244</td><td>0.252</td></tr><tr><td rowspan="2">Reasoning</td><td>Marco-o1</td><td>0.352</td><td>0.348</td><td>0.442</td><td>0.184</td><td>0.224</td><td>0.200</td><td>0.134</td><td>0.104</td></tr><tr><td>Skywork-o1</td><td>0.306</td><td>0.256</td><td>0.344</td><td>0.190</td><td>0.176</td><td>0.160</td><td>0.092</td><td>0.060</td></tr><tr><td rowspan="2">Llama Qwen</td><td rowspan="2">RL RL-Zero</td><td rowspan="2">R1-Searcher</td><td>0.648</td><td>0.746</td><td>0.594</td><td>0.628</td><td>0.504</td><td>0.544</td><td>0.254</td><td>0.282</td></tr><tr><td>0.654</td><td>0.750</td><td>0.636</td><td>0.650</td><td>0.528</td><td>0.544</td><td>0.282</td><td>0.314</td></tr></table>

Table 2: Performance comparisons between R1-Searcher and the baselines on four multi-hop QA benchmarks. The boldface indicates the best performance. GPT, Qwen, and Llama are the abbreviations of GPT-4o-mini, Qwen-2.5-7B-Base, and Llama-3.1-8B-Instruct, respectively.

# 4 Further Analysis

In this section, we present a detailed discussion of several key aspects that should be considered during the training process.

# 4.1 Basic Training Methods

GRPO or Reinforce++. As two representative RL algorithms that do not require a critic model, we compare the differences between GRPO [38] and Reinforce $^ { + + }$ on our RAG tasks. We perform twostage training on Llama-3.1-8B-Instruct, setting the KL divergence to $1 e - 4$ and utilizing HotpotQA and 2Wiki as the training datasets. As shown in Figure 2, although there are no significant differences in rewards between the two algorithms during training, GRPO demonstrates a clear advantage in both the length of generated text and the frequency of retrievals. The generation of longer text may widen the reasoning scope, and the increased frequency of retrievals could potentially improve the accuracy in responding to queries where the model itself has uncertainty. Moreover, it also demonstrates better performance on the out-of-domain dataset (i.e.,Bamboogle), suggesting that GRPO may possess superior generalization capabilities. However, Reinforce++ exhibits superior performance on the in-domain test set (i.e., HotpotQA and 2Wiki), which seemingly indicates a higher learning efficiency towards in-domain data.

RL or SFT. In this part, we aim to understand the enhancement effects of SFT and RL through comparison. We conduct RL training according to the same settings in Section 3.3. For the SFT

![](images/ddc4ee9023e0ed2c9061c26fd24dbf53644e1c98676d9aab92ee6fc66d8a6d53.jpg)

![](images/4751b7ec45e7f07077f1f56727513fdc11c117848b035ac43be7bff6110ef782.jpg)

![](images/f0fc1fc0fe848ac6f874c1485cd1db637b52b4d9c10d21821301c8dd429c676a.jpg)  
Figure 2: The log of reward, response length, and retrieval numbers for Llama-3.1-8B-Instruct comparison between using GRPO and Reinforce++.

Table 3: Performance comparison of Llama-3.1-8B-Instruct trained using GRPO and Reinforce++ on three multi-hop QA benchmarks.   

<table><tr><td rowspan="2">Method</td><td colspan="3">HotpotQA</td><td colspan="3">2Wiki</td><td colspan="3">Bamboogle</td><td rowspan="2">Avg (CEM)</td></tr><tr><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td></tr><tr><td>GRPO</td><td>53.0</td><td>60.5</td><td>68.6</td><td>58.0</td><td>60.5</td><td>63.0</td><td>48.0</td><td>56.0</td><td>60.5</td><td>59.0</td></tr><tr><td>Reinforce++</td><td>58.4</td><td>64.8</td><td>70.6</td><td>57.5</td><td>61.5</td><td>62.9</td><td>44.0</td><td>50.4</td><td>57.1</td><td>58.9</td></tr></table>

data, we select Qwen-2.5-7B-instruct and conduct rollouts from the training sets of HotpotQA and 2Wiki, obtaining 4768 pieces of data with good reasoning paths. Among them, 4268 pieces of data undergo retrieval, and the training epoch is set to 3. The results are shown in Table 4. We can see that RL outperforms SFT in both in-domain and out-of-domain test sets, indicating superior retrieval capability and generalization across varying datasets. After inspecting the outputs of models trained with both methods (see Section 5.1), we find that although SFT assists the model in generating retrieval queries, the timing and relevance of these queries are inferior to those produced by RL training. Specifically, SFT tends to rely on the model’s internal knowledge, which can often be erroneous or misleading. This indicates that RL may be more effective in enhancing the model’s retrieval skills.

Table 4: Performance comparison of Qwen-2.5-7B-Base and Llama-3.1-8B-Instruct trained using RL and SFT on three multi-hop QA benchmarks. Qwen-Base and Llama-Instruct are the abbreviations of Qwen-2.5-7B-Base and Llama-3.1-8B-Instruct, respectively.   

<table><tr><td rowspan="2">Method</td><td colspan="3">HotpotQA</td><td colspan="3">2Wiki</td><td colspan="3">Bamboogle</td><td rowspan="2">Avg (CEM)</td></tr><tr><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td></tr><tr><td>Qwen-Base-RL</td><td>58.0</td><td>65.4</td><td>71.9</td><td>55.4</td><td>63.6</td><td>63.7</td><td>45.6</td><td>52.8</td><td>57.7</td><td>60.6</td></tr><tr><td>Qwen-Base-SFT</td><td>37.0</td><td>49.5</td><td>51.3</td><td>42.5</td><td>54.5</td><td>51.3</td><td>40.8</td><td>46.4</td><td>51.0</td><td>50.1</td></tr><tr><td>Llama-Instruct-RL</td><td>58.4</td><td>64.8</td><td>70.6</td><td>55.0</td><td>59.4</td><td>61.2</td><td>44.0</td><td>50.4</td><td>57.1</td><td>58.2</td></tr><tr><td>Llama-Instruct-SFT</td><td>36.0</td><td>47.0</td><td>50.4</td><td>38.0</td><td>51.0</td><td>48.3</td><td>39.4</td><td>46.6</td><td>48.2</td><td>48.2</td></tr></table>

# 4.2 Reward Design

Answer Reward. Here, we investigate the impact of various answer rewards on RL training. We specifically compare the performance of using Exact Match (EM), Cover Exact Match (CEM), and F1 score as answer rewards. The F1 score is used directly as its own reward, while the rewards for EM and CEM are defined as follows:

$$
R _ {\text {a n s w e r}} = \left\{ \begin{array}{l l} 1, & \text {i f E M / C E M i s T r u e} \\ - 1, & \text {i f E M / C E M i s F a l s e} \end{array} \right. \tag {5}
$$

The training log and final results are presented in Figure 3 and Table 5. Firstly, the F1-based answer reward yields longer response lengths and superior final results compared to CEM and EM-based

rewards. Notably, it achieves up to a $5 2 . 6 \%$ average performance improvement over the EM-based reward. Secondly, the EM-based reward results in shorter response lengths during training and poorer performance during testing compared to CEM or F1-based reward. This may be due to EM’s strictness, making it unsuitable for open-ended question generation scenarios. Overall, F1 provides a more balanced measure of answer accuracy, serving as a more effective outcome-based reward in this scenario.

![](images/d3ec4248fe747175aa5d59c58ef2561288404695adfbdab04ebe45dcad383ce2.jpg)

![](images/e97321d55e78cc448f8ce84cbdce564bf8a14697027c0f5f6c1d4d8c4f43eebd.jpg)

![](images/e2d9e104c6b128b9da0e5647918e93b5969e497c05f8d32db5d727e79a18608a.jpg)  
Figure 3: The log of reward, response length, and retrieval numbers for the Qwen-2.5-7B-Base model utilizing different metrics for outcome-supervised reward calculation.

Table 5: Performance comparison of the Qwen-2.5-7B-Base model utilizing different metrics for outcome-supervised reward calculation on three mutil-hop QA benchmarks.   

<table><tr><td rowspan="2">Method</td><td colspan="3">HotpotQA</td><td colspan="3">2Wiki</td><td colspan="3">Bamboogle</td><td rowspan="2">Avg (CEM)</td></tr><tr><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td></tr><tr><td>EM</td><td>55.0</td><td>62.0</td><td>69.3</td><td>29.0</td><td>29.0</td><td>30.0</td><td>24.8</td><td>28.0</td><td>33.2</td><td>39.7</td></tr><tr><td>CEM</td><td>53.4</td><td>65.0</td><td>68.8</td><td>51.8</td><td>59.2</td><td>61.7</td><td>46.4</td><td>54.4</td><td>59.0</td><td>59.5</td></tr><tr><td>F1</td><td>58.0</td><td>65.4</td><td>71.9</td><td>55.4</td><td>63.6</td><td>63.7</td><td>45.6</td><td>52.8</td><td>57.7</td><td>60.6</td></tr></table>

Format Reward. During training, we impose strict constraints on the format reward (see Section 2.2.1). These constraints are iteratively refined to address instances of reward hacking and the generation of unreasonable solutions. The primary issues observed include:

1. The model produces <begin_of_documents>...<end_of_documents> without generating <begin_of_query>...<end_of_query>, effectively creating “external documents” independently.   
2. When training with the Base model and setting KL to 0, the model occasionally generates nonsensical output in later training phases, failing to adhere to specified formats.   
3. With the Llama model, omitting the Stage-1 training causes the model to bypass retrieval entirely, directly answering questions without engaging in the retrieval process.   
4. Using CEM as the supervisory signal, the model often produces lengthy responses containing extraneous information, though the correct answer is included.

Through our designed format rewards, we can train the model more stably in the RL training process, avoiding abnormal outputs and reward hacking.

# 4.3 Training Data

Difficulty Distribution. In this study, we examine the effect of data difficulty on training by constructing two distinct datasets. The first dataset, used for primary training, is labeled w. Difficult (Table 1). The second dataset, w/o Difficult, substitutes questions requiring more than 20 rollouts with those requiring 10 to 20 rollouts. Both datasets are trained under identical configurations. As shown

in Figure 4, training with the w/o Difficult dataset results in shorter generation lengths and fewer retrievals compared to the w. Difficult dataset. This suggests that more challenging problems prompt the model to perform additional retrievals to answer questions. Furthermore, Table 6 indicates that models trained on the w. Difficult dataset achieves superior performance on the evaluation dataset compared to those trained on the w/o Difficult dataset (achieving $3 . 4 \%$ average CEM performance improvements on three datasets). This underscores the importance of data difficulty distribution for model performance in RL, as more challenging questions enhance the model’s reasoning capabilities.

![](images/dbc7c438da8cda0f3feb49afce9151119c126d3986299bf0fc83bb27fdee52a6.jpg)

![](images/985d94eb6ea936d5b68e75307404f3eaa41ac8ac4c47f1bcd143baf7d9555d8a.jpg)

![](images/8d298f2ddabae492dfeca3043301feac81cdf3e9719e68f02f104d6498b315f8.jpg)  
Figure 4: The log of reward, response length, and retrieval numbers for the Qwen-2.5-7B-Base model, trained on datasets of varying difficulty levels.

Table 6: Performance comparison of the Qwen-2.5-7B-Base model trained on datasets of different difficulty levels on three mutil-hop QA benchmarks.   

<table><tr><td rowspan="2">Method</td><td colspan="3">HotpotQA</td><td colspan="3">2Wiki</td><td colspan="3">Bamboogle</td><td rowspan="2">Avg (CEM)</td></tr><tr><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td></tr><tr><td>w/o Difficult</td><td>54.8</td><td>61.8</td><td>69.3</td><td>55.4</td><td>63.6</td><td>63.7</td><td>44.8</td><td>51.2</td><td>56.9</td><td>58.8</td></tr><tr><td>w. Difficult</td><td>58.0</td><td>65.4</td><td>71.9</td><td>54.8</td><td>64.2</td><td>63.8</td><td>45.6</td><td>52.8</td><td>57.7</td><td>60.8</td></tr></table>

Data Diversity. We investigate the effect of data diversity during the RL training process. Specifically, we compare the performance of using a combination of the HotpotQA and 2Wiki datasets, as well as each dataset individually. The training log and final results are presented in Figure 5 and Table 7, respectively. We can find that models trained on the mixed dataset show an increase in the number of retrievals and the length of generated responses compared to those trained on either dataset alone, achieving higher scores on the test set, with improvements of up to $1 0 . 9 \%$ in average CEM performance. Additionally, models trained solely on the 2Wiki dataset demonstrate superior training rewards but inferior average performance across three datasets compared to those trained on the HotpotQA dataset. This may be attributed to the relatively low diversity within the 2Wiki dataset, potentially leading to overfitting during RL training. These findings demonstrate that the diversity of training datasets significantly affects both training efficacy and generalizability, underscoring the importance of data diversity.

Table 7: Performance comparison of the Qwen-2.5-7B-Base model trained on different datasets.   

<table><tr><td rowspan="2">Method</td><td colspan="3">HotpotQA</td><td colspan="3">2Wiki</td><td colspan="3">Bamboogle</td><td rowspan="2">Avg (CEM)</td></tr><tr><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td><td>EM</td><td>CEM</td><td>F1</td></tr><tr><td>HotpotQA</td><td>53.8</td><td>59.2</td><td>67.2</td><td>46.7</td><td>54.3</td><td>54.7</td><td>44.0</td><td>50.4</td><td>55.1</td><td>54.6</td></tr><tr><td>2Wiki</td><td>46.0</td><td>50.5</td><td>58.7</td><td>45.0</td><td>47.5</td><td>48.2</td><td>31.2</td><td>32.8</td><td>39.4</td><td>43.6</td></tr><tr><td>Mixture</td><td>58.0</td><td>65.4</td><td>71.9</td><td>55.4</td><td>63.6</td><td>63.7</td><td>45.6</td><td>52.8</td><td>57.7</td><td>60.6</td></tr></table>

![](images/a4ae589dfe10a2fac6295ee932302ad18bc3bff3a45938ede0254a5845ed0498.jpg)

![](images/751556d75504ce9aeb05f9b4cd6ec4655c265dbe929befdba0574d284dc3b8b5.jpg)

![](images/9861eeb770bc9ec095973455693c1e856b2354b9812ff9f593a3e11e14b4b0cf.jpg)  
Figure 5: The log of reward, response length, and retrieval numbers for the Qwen-2.5-7B-Base model trained on different datasets.

![](images/f2410f0766a06ecd0e08447cc5ae584a952af007686b17a621c31f228b7af020.jpg)  
Figure 6: Preference comparison of our models that utilize local search and online search and the baselines on the Bamboogle dataset. Search-o1 utilizes online search, and all other baselines employ local search. The metric is CEM (Cover Exact Match).

# 4.4 Generalization to Online Search

In consideration of training speed and cost, we implement a local dense embedding-based retrieval system using Wikipedia as the external retrieval environment. To evaluate the model’s generalization capabilities beyond this knowledge scope, we test its performance on the newly proposed Bamboogle task using an online search scenario, which is not encountered during the RL training. Specifically, when retrieval is required, we employ the Google API to perform online searches, obtaining relevant web pages. Due to the extensive length of these pages, we first summarize them using GPT-4o-mini before integrating the summaries into the reasoning model. As illustrated in Figure 6, our model demonstrates an $1 8 . 2 \%$ improvement over the local retrieval system with the same Qwen-2.5-7B-Base backbone. Additionally, it achieves an $1 1 . 4 \%$ performance increase compared to Search-o1, which utilizes a larger 32B backbone model with the same online search. This indicates our model’s adaptability to online search scenarios and shows that R1-Searcher enables the model to retrieve information during reasoning rather than merely memorizing response formats.

# 5 Case Study

# 5.1 RL v.s. SFT

Compared to SFT, RL is better at invoking external retrieval for querying when it doesn’t know how to answer by itself. However, SFT is prone to being influenced by internal knowledge and directly uses the incorrect internal knowledge to answer, ultimately resulting in wrong answers to the questions. We have presented a detailed comparison as shown in Table 8.

# 5.2 Qwen v.s. Llama

As shown in Table 9, after RL, the Qwen-2.5-7B-Base model more effectively decomposes complex questions than the Llama-3.1-8B-Instruct model. It refines reasoning by breaking down multi-step problems into subproblems and sequentially retrieving relevant information, thereby increasing the likelihood of retrieving pertinent documents and enhancing accuracy.

Question:

When did the last king from Britain’s House of Hanover die?

Answer:

20 June 1837

Response (RL) (True):

To solve this question, I need to find out when the last king from Britain’s House of Hanover died.

Step 1: Identify the name of the last king from the House of Hanover. Step 2: Determine the date of the last king’s death.

**Step 1: Identify the name of the last king from the House of Hanover.**

I’ll use a search query to find out:

<|begin_of_query|> last king from Britain’s House of Hanover<|end_of_query|> <|begin_of_documents|>

<|end_of_documents|>

From the search results, I can see that the last British monarch who reigned in Hanover was William IV.

**Step 2: Determine the date of the last king’s death.**

I’ll use a search query to find out when William IV died:

<|begin_of_query|> William IV of Hanover death date<|end_of_query|>

<|begin_of_documents|>

<|end_of_documents|>

From the search results, I can see that William IV died on 20 June 1837.

Therefore, the last king from Britain’s House of Hanover died on 20 June 1837.</think>

<answer> 20 June 1837 </answer>

Response (SFT) (False):

To answer this question, I need to recall the British monarchs who belonged to the House of Hanover and identify the last one. The House of Hanover includes kings like George I, George II, and George III. I need to find out when George III, the last Hanoverian king, passed away.

George III ruled from 1760 to 1820. After his death, the throne passed to his son, George IV, who was from the House of Hanover’s successor, the House of Saxe-Coburg and Gotha.

I will verify the date of George III’s death to provide the correct answer. <|begin_of_query|> George III death date<|end_of_query|>

<|begin_of_documents|>

<|end_of_documents|>

George III died on 29 January 1820. This confirms that he was the last king from Britain’s House of Hanover, and there was no Hanoverian king after him.

The answer is 29 January 1820. </think> <answer> 29 January 1820 </answer>

Table 8: Case study from the Bamboogle dataset comparing Qwen-2.5-7B-Instruct models trained with RL and SFT.

```txt
Question: Who was president of the United States in the year that Citibank was founded? Answer: james madison 
```

```txt
Response (Qwen-2.5-7B-Base) (True):   
To solve this question, I need to find out who the president of the United States was in the year that Citibank was founded and then identify that president.   
Step 1: Determine the year Citibank was founded. I'll use a search query to find out: <begin_of_queryI> Citibank founding year<lend_of_queryI>   
</begin_of_documents|>   
...   
</end_ofdocuments|>   
From the search results, I can see that Citibank was founded in 1812.   
Step 2: Identify the president of the United States in 1812. I'll use a search query to find out: <begin_of_queryI> president of the United States in 1812<lend_of_queryI>   
</begin_of_documents|>   
...   
</end_ofdocuments|>   
The search results mention that James Madison was the President of the United States in 1812.   
Therefore, the president of the United States in the year that Citibank was founded was James Madison.<think>   
<answer>James Madison </answer> 
```

```txt
Response (Llama-3.1-8B-Instruct) (True):  
Given the information, I don't have specific knowledge about the president of the United States in the year Citibank was founded. I will perform a search to find the required information. The query to search for is "<begin_of_query>| president of united states citibank founded<end_of_query>|"  
<begin_of_documents|>  
...  
<end_of_documents|>  
Based on the search results, I found that Citibank was founded in 1812, and I couldn't find specific information about the president of the United States in that year. I will perform another search to find the required information. The query to search for is "<begin_of_query>| president united states 1812<end_of_query|"  
<begin_of_documents|>  
...  
<end_of_documents|>  
Based on the search results, I found that the president of the United States in 1812 was James Madison.<think>  
<answer>James Madison</answer> 
```

Table 9: Case study from the Bamboogle dataset comparing Qwen-2.5-7B-Instruct and Llama-3.1- 8B-Instruct models trained with RL.

# 6 Conclusion

In this paper, we propose R1-Searcher, a novel framework integrating RAG with RL. This framework employs a two-stage outcome-supervised RL approach, which enables the model to learn to invoke an external search engine to acquire relevant knowledge during the reasoning process through a meticulously designed reward mechanism. The proposed method relies solely on RL, allowing the model to learn autonomously through exploration without requiring any instruction fine-tuning for cold start. It demonstrates the ability to generalize from in-domain training datasets to out-of-domain test datasets while seamlessly switching to online search to obtain up-to-date information. Moreover, R1-Searcher is applicable to both base models and instruction-tuned models. Extensive experiments conducted on multiple datasets show that R1-Searcher outperforms traditional RAG methods and other reasoning approaches. Additionally, we analyze the training process from various aspects, including training methods, data, and reward designing.

# 7 Future Work

In future work, we aim to refine our training methodology in two key areas. First, we will explore more sophisticated data curricula, as we have observed that the distribution and difficulty of training

data significantly influence the learning process. So far, we have only employed simple data mixing, and a more structured approach may further enhance performance. Second, we plan to scale up our model beyond the current 7B configuration, investigating larger models (e.g., 32B) to better assess the effectiveness of our approach.

# References

[1] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian O’Connell, Ian Osband, Ignasi Clavera Gilaberte, and Ilge Akkaya. Openai o1 system card. CoRR, abs/2412.16720, 2024.   
[2] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025.   
[3] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, and Zonghan Yang. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025.

[4] Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models, 2025.   
[5] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369–2380, 2018.   
[6] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625, 2020.   
[7] Shuting Wang, Jiejun Tan, Zhicheng Dou, and Ji-Rong Wen. Omnieval: An omnidirectional and automatic rag evaluation benchmark in financial domain, 2025.   
[8] Joohyun Lee and Minji Roh. Multi-reranker: Maximizing performance of retrieval-augmented generation in the financerag challenge, 2024.   
[9] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687–5711, 2023.   
[10] Jie He, Nan Hu, Wanqiu Long, Jiaoyan Chen, and Jeff Z. Pan. Mintqa: A multi-hop question answering benchmark for evaluating llms on new and tail knowledge, 2025.   
[11] Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and Tao Zhang. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification and refinement. CoRR, abs/2412.12881, 2024.   
[12] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.   
[13] Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A survey on rag meeting llms: Towards retrieval-augmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD ’24, page 6491–6501, New York, NY, USA, 2024. Association for Computing Machinery.   
[14] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models, 2025.   
[15] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. Open-Review.net, 2024.   
[16] Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, and Yuyu Luo. Atom of thoughts for markov llm test-time scaling, 2025.   
[17] Liang Wang, Haonan Chen, Nan Yang, Xiaolong Huang, Zhicheng Dou, and Furu Wei. Chainof-retrieval augmented generation. CoRR, abs/2501.14342, 2025.   
[18] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: A comparative study of foundation model post-training, 2025.   
[19] Xingxuan Li, Weiwen Xu, Ruochen Zhao, Fangkai Jiao, Shafiq Joty, and Lidong Bing. Can we further elicit reasoning in llms? critic-guided planning with retrieval-augmentation for solving challenging tasks. arXiv preprint arXiv:2410.01428, 2024.   
[20] Jian Hu. Reinforce $^ { + + }$ : A simple and efficient approach for aligning large language models, 2025.   
[21] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang

Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025.   
[22] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022.   
[23] Zhongxiang Sun, Qipeng Wang, Weijie Yu, Xiaoxue Zang, Kai Zheng, Jun Xu, Xiao Zhang, Song Yang, and Han Li. Rearter: Retrieval-augmented reasoning with trustworthy process rewarding, 2025.   
[24] Jaehyung Kim, Jaehyun Nam, Sangwoo Mo, Jongjin Park, Sang-Woo Lee, Minjoon Seo, Jung-Woo Ha, and Jinwoo Shin. Sure: Summarizing retrievals using answer candidates for opendomain QA of LLMs. In The Twelfth International Conference on Learning Representations, 2024.   
[25] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.   
[26] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839, 2023.   
[27] Fangyuan Xu, Weijia Shi, and Eunsol Choi. RECOMP: Improving retrieval-augmented LMs with context compression and selective augmentation. In The Twelfth International Conference on Learning Representations, 2024.   
[28] Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. Compressing context to enhance inference efficiency of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6342–6353, Singapore, December 2023. Association for Computational Linguistics.   
[29] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. Self-knowledge guided retrieval augmentation for large language models. arXiv preprint arXiv:2310.05002, 2023.   
[30] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.   
[31] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. arXiv preprint arXiv:2305.15294, 2023.   
[32] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10014–10037, 2023.   
[33] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. arXiv preprint arXiv:2411.14405, 2024.   
[34] Skywork o1 Team. Skywork-o1 open series. https://huggingface.co/Skywork, November 2024.   
[35] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular toolkit for efficient retrieval-augmented generation research. arXiv preprint arXiv:2405.13576, 2024.   
[36] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick S. H. Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 2523–2544. Association for Computational Linguistics, 2021.

[37] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models, 2020.   
[38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.