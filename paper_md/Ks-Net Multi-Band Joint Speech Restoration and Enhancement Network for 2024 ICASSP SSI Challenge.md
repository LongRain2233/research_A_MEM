# KS-NET: MULTI-BAND JOINT SPEECH RESTORATION AND ENHANCEMENT NETWORK FOR 2024 ICASSP SSI CHALLENGE

Guochen Yu⋆†, Runqiang Han⋆†, Chenglin Xu⋆, Haoran Zhao⋆, Nan $L i ^ { \star }$ , Chen Zhang⋆ Xiguang Zheng⋆, Chao Zhou⋆, Qi Huang⋆, Bing $Y u ^ { \star }$

⋆ Kuaishou Technology, Beijing, China

# ABSTRACT

This paper presents the speech restoration and enhancement system created by the 1024K team for the ICASSP 2024 Speech Signal Improvement (SSI) Challenge. Our system consists of a generative adversarial network (GAN) in complex-domain for speech restoration and a fine-grained multi-band fusion module for speech enhancement. In the blind test set of SSI, the proposed system achieves an overall mean opinion score (MOS) of 3.49 based on ITU-T P.804 and a Word Accuracy Rate (WAcc) of 0.78 for the real-time track, as well as an overall P.804 MOS of 3.43 and a WAcc of 0.78 for the non-real-time track, ranking $1 ^ { s t }$ in both tracks.

Index Terms— speech signal improvement, speech restoration GAN, multi-band fusion, speech enhancement

# 1. INTRODUCTION

During audio communication, speech signals recorded by microphones may be severely degraded by various distortions, including environmental noise, reverberation, coloration, discontinuity and loudness. Although existing methods have shown effectiveness in the noise suppression, restoring highquality speech with the presence of multiple simultaneous distortions still remains challenging. The ICASSP SSI Challenge aims to restore the high-qulity speech signal from the captured signal distorted by the aforementioned complex acoustic conditions in a real-time communication system [1]. In this paper, we propose a multi-band joint restorationand-enhancement framework called KS-Net to address the aforementioned distortions. Firstly, the framework adopts a complex-domain based generative adversarial network (GAN) to restore the coarse-grained high-quality speech, aiming to remove various types of distortions. Subsequently, the remaining transient noise and artifacts after the restoration GAN are eliminated by a multi-band fusion network for speech enhancement. The proposed KS-Net approach achieves the $1 ^ { s t }$ places in both real-time and non-real-time tracks in the 2024 SSI challenge.

# 2. SYSTEM ARCHITECTURE

As illustrated in Fig 1, the training procedure of our system consists of two stages. The first stage involves the

primary restoration GAN, which aims to address various distortions, such as band-width extension (BWE), speech distortion restoration, loudness adjustment, packet loss concealment (PLC), noise suppression (NS) and dereverberation (Dereverb). In the second stage, the multi-band fusion SE (MF-Net) focuses specifically on removing residual transient noise and artifacts generated by the restoration GAN.

# 2.1. Complex-domain based restoration GAN

Inspired by GAN-related methods presented in ICASSP 2023 SSI-Challenge[2, 3], we proposed a restoration GAN to restore the coarse-grained high-quality speech in the complex domain. To facilitate information exchange between low frequency bands and high frequency bands, we divide the full-band complex spectrum into three subbands and concatenate them along the channel dimension as inputs to the generator. The restoration GAN’s generator follows a convolutional encoder-decoder architecture. It includes five groups of squeezed temporal convolution modules (S-TCMs) and two residual time-frequency LSTM (TF-LSTM) for sequence modeling in the bottleneck. The encoder consists of five convolutional layers, with a dilated dense-block inserted after the first convolutional layer, which contains five dilated convolutional layers. On the other hand, the decoder comprises five corresponding transposed convolutional layers and a corresponding dilated dense-block. To improve temporal modeling, each group of S-TCMs is composed of four S-TCN blocks with exponentially increasing dilation rates. After the last S-TCMs, two TF-LSTM are stacked. The time-LSTM focuses on capturing long-term dependencies along the time axis, while the bi-directional frequency-LSTM targets the long-term dependencies along the frequency axis.

Regarding the discriminators, multi-resolution STFT discriminators are used to capture various spectral patterns in speech signals[4]. The training loss is a combination of a reconstruction loss, an adversarial loss, and a feature match loss. The reconstruction loss includes both a multi-resolution STFT loss and a waveform loss.

# 2.2. Multi-band fusion SE

The restoration GAN’s output lacks certain spectral details in the low-frequency parts and still has some leftover transient noise. To address this, we introduce the MF-Net, a multi-band

![](images/50f000b3fa51188e7a3fda09f856f88fef4dc0546d381018212a58b6b8d3830f.jpg)  
Fig. 1. The flowchart of the proposed KS-Net.

fusion enhancement module. This module aims to restore the spectral details and eliminate the remaining noise.

Based on the insights from our prior study [5], we have developed MF-Net - a two-stage solution that simultaneously restores the overall full-band spectral pattern and the intricate sub-band spectral details. In the first stage, we apply coarse denoising to the compressed ERB-scaled spectrum. This step effectively reduces noise and enhances the general features. In the second stage, due to the varying spectral characteristics among different frequency bands, we employ two distinct sub-networks to refine the low-frequency and high-frequency bands separately.

# 3. EXPERIMENTS

# 3.1. Data Augmentation

The training set for this project is constructed using multiple sources: the DNS Challenge dataset[6], the VCTK corpus[7] and our own private datasets. Specifically, we create a large dataset of 800 hours of paired data. In this dataset, we add different distortions, such as various types of noise, reverberation, clipping, packet loss, low-pass filtering, traditional noise reduction algorithms, and low bit-rate codecs, to the original clean full-band speech. We add these distortions with different probabilities to simulate real-life scenarios. During the training stage for MF-Net, transient noises are further added to the outputs of the restoration GAN with signal-to-noise ratios (SNRs) ranging from -5dB to 10dB.

# 3.2. Experimental Setup

In our experiments, we utilized a Hanning window with a 20 ms window length and a 10 ms frame shift. It is worth noting that the difference between the submitted system for SSI realtime track (KS-Net-1) and non-real-time track (KS-Net-2) is that the generator and MF-Net in KS-Net-2 is designed with a large amount of network parameters and computational complexity. KS-Net-1 has a total of 15.64 million(M) trainable parameters and its real time factor (RTF) is 0.42 on an Intel Core i5 Quadcore CPU clocked at $2 . 4 \ : \mathrm { G H z }$ , while KS-Net-2 has a total parameter number of 19.15 M and an RTF of 0.62.

# 3.3. Results and analysis

Table 1 presents the results of the P.804 subjective test conducted on the official blind set. It is evident that both KS-Net-1 and KS-Net-2 exhibit considerable improvements across

Table 1. ITU-T P.804 MOS and WAcc results on the SSI 2024 blind test set.   

<table><tr><td rowspan="2">System</td><td colspan="7">MOS</td><td rowspan="2">WAcc (%)</td></tr><tr><td>COL</td><td>DISC</td><td>LOUD</td><td>NOISE</td><td>REVERB</td><td>SIG</td><td>ORVL</td></tr><tr><td>Noisy</td><td>3.34</td><td>3.70</td><td>3.78</td><td>3.21</td><td>3.40</td><td>3.05</td><td>2.58</td><td>82.68</td></tr><tr><td>KS-Net-1</td><td>4.08</td><td>3.89</td><td>4.34</td><td>4.30</td><td>4.27</td><td>3.83</td><td>3.49</td><td>78.23</td></tr><tr><td>KS-Net-2</td><td>4.01</td><td>3.89</td><td>4.34</td><td>4.28</td><td>4.24</td><td>3.74</td><td>3.43</td><td>77.30</td></tr></table>

all subjective metrics when compared to the noisy signals. This suggests that our proposed system effectively enhances speech quality by alleviating distortions introduced by various factors such as noise, reverberation, coloration, discontinuity, and loudness. Moving forward, our future research endeavors will focus on further exploring the underlying factors contributing to the superior performance of real-time KS-Net-1 in contrast to non-real-time KS-Net-2.

# 4. REFERENCES

[1] R. Cutler, A. Saabas, B. Naderi, N.-C. Ristea, S. Braun, and S. Branets, “Icassp 2023 speech signal improvement challenge,” arXiv preprint arXiv:2303.06566, 2023.   
[2] J. Chen, Y. Shi, W. Liu, et al., “Gesper: A unified framework for general speech restoration,” in Proc. ICASSP. IEEE, 2023.   
[3] W. Zhu, Z. Wang, J. Lin, et al., “SSI-Net: A multi-stage speech signal improvement system for icassp 2023 ssi challenge,” in Proc. ICASSP. IEEE, 2023.   
[4] J. Kong, J. Kim, and J. Bae, “Hifi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,” Advances in Neural Information Processing Systems, vol. 33, pp. 17022–17033, 2020.   
[5] G. Yu, H. Wang, A. Li, et al., “FSI-Net: A dual-stage fulland sub-band integration network for full-band speech enhancement,” Applied Acoustics, vol. 211, pp. 109539, 2023.   
[6] H. Dubey, V. Gopal, R. Cutler, et al., “Icassp 2022 deep noise suppression challenge,” in Proc. ICASSP. IEEE, 2022, pp. 9271–9275.   
[7] C. Veaux, J. Yamagishi, K. MacDonald, et al., “Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit,” University of Edinburgh. CSTR, vol. 6, pp. 15, 2017.