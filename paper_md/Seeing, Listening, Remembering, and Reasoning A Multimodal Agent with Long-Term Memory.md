# Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory

Lin Long1,2,∗,, Yichen $\mathsf { H e } ^ { 1 , * }$ , Wentao $\pmb { \gamma } _ { \pmb { \theta } ^ { 1 , 2 } }$ , Yiyuan Pan1,3, Yuan Lin1,†, Hang Li1, Junbo Zhao2, Wei Li1

1ByteDance Seed, 2Zhejiang University, 3Shanghai Jiao Tong University

∗Equal contribution, †Corresponding author

# Abstract

We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update episodic and semantic memories, gradually accumulating world knowledge. Its memory is organized in an entity-centric, multimodal manner, enabling deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn reasoning and retrieves relevant memories to complete tasks. To evaluate memory effectiveness and memorybased reasoning in multimodal agents, we develop M3-Bench, a long-video question answering benchmark comprising 100 newly recorded robot-perspective videos (M3-Bench-robot) and 920 diverse web-sourced videos (M3-Bench-web). We annotate QA pairs designed to test capabilities essential for agent applications, such as person understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving $6 . 7 \%$ , $7 . 7 \%$ , and $5 . 3 \%$ higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances multimodal agents toward more human-like long-term memory and provides insights for their practical design. Model, code and data are available at https://github.com/bytedance-seed/m3-agent.

Date: October 10, 2025

Correspondence: linyuan.0@bytedance.com

Project Page: https://m3-agent.github.io

![](images/8db12f1416edb59db3c56afc2bc55ad073e73ece8723f4362d2e7b19c94250fa.jpg)

![](images/971d9fb546721b3334c5c943effe8883832c6a5dd915beedefd6234b0fd93ea5.jpg)

![](images/804005ccfc6d66a133004962b0212b3e0a90169acd5dc14e0460004300c5abd0.jpg)

![](images/7d27162de8874ef03ee03302bbbcb2a810b0ae0ac22b75e9afb46456d7150716.jpg)

![](images/c6f83b08aeec12144aca40ac080fb30ec8804820aa2a93e508ed4256593a4ea9.jpg)

Multimodal agents continuously perceive their environment, build entity-centric, multimodal long-term memories, and reason over them.

# 1 Introduction

Imagine that in the future a household robot can autonomously carry out household tasks without your explicit instructions; it must have learned the operational rules of your home through daily experiences. In the morning, it hands you a cup of coffee without asking “coffee or tea?", because it has gradually formed a memory of you, tracking your preferences and routines through long-term interactions. For a multimodal agent, achieving this level of intelligence fundamentally relies on three capabilities: (1) continuously perceiving the world via multimodal sensors; (2) storing and organizing its experiences into a long-term memory, and gradually building knowledge of its environment; (3) reasoning over this accumulated memory to guide its actions.

To achieve the goals, we propose M3-Agent, a novel multimodal agent framework equipped with long-term memory. As shown in Figure 1, it operates through two parallel processes: memorization, which continuously perceives real-time multimodal inputs to construct and update long-term memory; and control, which interprets external instructions, reasons over the stored memory, and executes the corresponding tasks.

During memorization, M3-Agent processes the incoming video stream, capturing both fine-grained details and high-level abstractions by generating two types of memory, analogous to human cognitive systems [44, 45]:

• Episodic memory: Concrete events observed within the video. For example, "Alice takes the coffee and says, ‘I can’t go without this in the morning,’" and "Alice throws an empty bottle into the green garbage bin."   
• Semantic memory: General knowledge from the clip. For example, "Alice prefers to drink coffee in the morning" $\eta$ and " $\boldsymbol { \eta }$ The green garbage bin is used for recycling."

The generated contents are then integrated into the agent’s long-term memory, which supports multimodal information such as faces, voices, and textual knowledge. The memory is organized in an entity-centric structure. For example, information related to the same person (e.g., their face, voice, and associated knowledge) is connected within a graph, as shown in Figure 1. These connections are incrementally established as the agent extracts and integrates episodic and semantic memory.

![](images/055afb97ebcceb984b26e11855e4ffa28781fc87bca6d661d8aa7e5ed17ce717.jpg)  
Figure 1 Architecture of M3-Agent, comprising a multimodal large language model (MLLM) and a multimodal long-term memory. The system consists of two parallel processes: memorization and control. During memorization, M3-Agent processes video and audio streams online to generate episodic and semantic memory. During control, it executes instructions by iteratively reasoning and retrieving from long-term memory. The long-term memory is structured as a multimodal graph.

During control, M3-Agent leverages its long-term memory to reason and complete tasks. It autonomously retrieves relevant information from its long-term memory across different dimensions, such as events or characters. Instead of using single-turn retrieval-augmented generation (RAG) to load memory into context [22], M3-Agent employs reinforcement learning to enable multi-turn reasoning and iterative memory retrieval, resulting in higher task success rates.

The memorization task relates to long video description [13, 18, 57] but goes beyond it, introducing two key challenges: (1) Infinite information processing. Memorization requires handling infinitely long input streams. Existing methods optimize architectural efficiency to process longer, but still finite, offline videos [14, 40– 42, 58]. In contrast, M3-Agent continuously processes arbitrarily long multimodal streams online, more closely mimicking how human long-term memory forms, through ongoing perception and incremental experience integration. (2) World knowledge construction. Traditional video description [1, 24, 26, 27, 55] often focuses on low-level visual details while overlooking high-level world knowledge [12, 19, 36] such as character identity and entity attributes, which may lead to ambiguity and inconsistency in long-term contexts. M3-Agent addresses this by incrementally building world knowledge through an entity-centric memory structure. It forms rich, multimodal representations of key entities, enabling coherent and consistent long-term memory.

We evaluate M3-Agent on long video question answering (LVQA), where the videos simulate the multimodal input streams (visual and auditory) received by an agent. Most existing LVQA benchmarks [2, 11, 50, 62] mainly focus on visual understanding, such as action recognition and spatial/temporal perception, leaving a gap in evaluating higher-level cognitive abilities that rely on long-term memory and are crucial for real-world agents, such as understanding persons, extracting general knowledge, and performing cross-modal reasoning. To bridge this gap, we introduce M3-Bench, a new LVQA benchmark designed to evaluate a multimodal agent’s ability to reason with long-term memory. M3-Bench consists videos from two sources: (1) M3-Bench-robot, consisting of 100 real-world videos recorded from a robot’s perspective, and (2) M3-Bench-web, comprising 920 YouTube videos spanning a broader range of content and scenarios. We define five question types, as shown in Table 1, targeting different aspects of memory-based reasoning. In total, we annotate 1,276 QA pairs for M3-Bench-robot and 3,214 QA pairs for M3-Bench-web.

We conduct experiments on the M3-Bench-robot, M3-Bench-web, and VideoMME-long [11]. Results show that M3-Agent trained via reinforcement learning outperforms all baselines on all three benchmarks. Compared to the strongest baseline, Gemini-GPT4o-Hybrid, which implements M3-Agent framework by prompting Gemini-1.5-Pro [43] for memorization and GPT-4o [17] for control, M3-Agent improves accuracy by 6.7%, 7.7%, and 5.3% on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Our ablation study demonstrates the importance of semantic memory: removing it reduces accuracy by 17.1%, 19.2% and $1 3 . 1 \%$ on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Furthermore, we examine the impact of RL training, inter-turn instructions, and reasoning mode on control performance. Specifically, RL training improves accuracy by $1 0 . 0 \%$ , $8 . 0 \%$ , and $9 . 3 \%$ on the respective benchmarks. Removing inter-turn instruction results in a $1 0 . 5 \%$ , 5.8% and $5 . 9 \%$ decrease in accuracy, while disabling reasoning mode leads to accuracy declines of 11.7%, 8.8% and $9 . 5 \%$ on the three benchmarks.

The main contributions of this paper are summarized as follows:

• We introduce M3-Agent, a novel framework for multimodal agents with long-term memory. M3-Agent continuously processes real-time multimodal inputs (seeing and listening), incrementally builds world knowledge by generating both episodic and semantic memories (remembering), and performs reasoning over these memories to complete complex instructions (reasoning).   
• We develop M3-Bench, a new LVQA benchmark designed to evaluate the effectiveness of memory and memory-based reasoning for multimodal agents.   
• Our experiments demonstrate that M3-Agent, trained by reinforcement learning, consistently outperforms agents based on prompted commercial models across multiple benchmarks.

# 2 Related Work

# 2.1 Long-Term Memory of AI Agents

Long-term memory is essential for AI agents [10], enabling them to retain distant contextual information and support more advanced reasoning. A common approach is to append entire agent trajectories, such as dialogues [29, 33, 46, 61] or execution trajectories [16, 29, 31, 37, 38, 48], directly to memory. Beyond raw data, some methods incorporate summaries [16, 23, 46, 61], latent embeddings [6, 30, 42, 58], or structured knowledge representations [35, 52]. Recent systems further construct sophisticated memory architectures, giving agents finer control on memory management [5, 20, 46].

However, most existing approaches focus on LLM agents. In contrast, multimodal agents process a broader range of inputs and store richer, multimodal content and concepts in memory [7, 8]. This also introduces new challenges, particularly in maintaining consistency of long-term memory. Moreover, just as humans acquire world knowledge through experience, multimodal agents should form internal world knowledge in memory, rather than merely storing description of experience.

# 2.2 Online Video Understanding

For multimodal agent, memory formation is closely related to online video understanding, a challenging task requires real-time processing of video streams and decision-making based on past observations. Traditional approaches to long video understanding, such as extending the context window in multimodal models [4, 60] or compressing visual tokens to increase temporal coverage [21, 49, 49], do not scale effectively for infinitely long video streams. In practical settings, such as interactive agent scenarios, reprocessing the entire video history for each new instruction is computationally prohibitive.

To improve scalability, memory-based methods [14, 42, 58, 59] introduce memory modules that store encoded visual features for future retrieval. These architectures are suited for online video processing. However, they face a fundamental limitation: maintaining long-term consistency. Because they store only visual features, these methods struggle to maintain coherent tracking of entities such as human identities or evolving events over time.

With the rapid advancement of large multimodal and language models [1, 17, 43, 53, 55], the Socratic Models framework [28, 56, 57] has emerged as a promising approach for online video understanding. By leveraging multimodal models to generate video descriptions as language-based memory, this method improves scalability. Nevertheless, it still encounters challenges in maintaining long-term consistency across complex, evolving video content.

# 3 Datasets

In this section, we introduce M3-Bench, an LVQA dataset designed to evaluate the capability of multimodal agents to perform reasoning over long-term memory. Each instance in M3-Bench comprises a long video simulating the perceptual input of an agent, along with a series of open-ended question-answer pairs. The dataset is organized into two subsets: (1) M3-Bench-robot, which contains 100 real-world videos recorded from a robot’s first-person perspective, and (2) M3-Bench-web, which includes 920 web-sourced videos covering a wider variety of content and scenarios. To comprehensively assess an agent’s ability to recall past observations and perform memory-based reasoning, we curate five distinct types of questions, as summarized in Table 1. Overall, M3-Bench is featured by (1) long-duration, real-world videos that encompass diverse real-life scenarios relevant to the deployment of multimodal agents, and (2) challenging questions that extend beyond shallow perceptual understanding and require complex reasoning over long-term contexts.

Figure 2 presents examples from M3-Bench. The overall statistics of M3-Bench is shown in Figure 3. Table 2 provides a comparative analysis with existing LVQA benchmarks. The remainder of this section elaborates on the data collection and annotation procedures for M3-Bench-robot and M3-Bench-web, respectively.

![](images/d07b24ebd6a8ecbb7b5b5a15978f9bb8b7cbd7ded2373a855bb51ec740fa1b85.jpg)  
Figure 2 Examples from M3-Bench. M3-Bench-robot features long videos from realistic robotic work scenarios, while M3-Bench-web expands the video diversity to support broader evaluation. The question-answering tasks are designed to assess a multimodal agent’s ability to construct consistent and reliable long-term memory, as well as to reason effectively over that memory.

# 3.1 M3-Bench-robot

Robots are representative examples of multimodal agents. A general-purpose robot should be able to maintain long-term memory and reason with it to guide its actions. For example, as it processes observations, the robot may remember a person’s name, where they left their coat, or their coffee preference. Reasoning over long-term memory enables higher-level cognitive functions, such as inferring a person’s personality, understanding relationships among individuals, or identifying the functions of surrounding objects. To systematically evaluate these capabilities, we record a new collection of videos from robot’s perspective and manually annotate corresponding question-answer pairs.

Scripts Design We begin by designing video scripts for M3-Bench-robot across seven everyday scenarios where robots are expected to operate: living room, kitchen, bedroom, study, office, meeting room, and gym. Each script involves one robot interacting with two to four humans. Annotators are instructed to design human–robot interactions that reflect the desirable capabilities of general-purpose service robots.

To ensure diversity in the script content, we introduce multiple thematic variations for each scenario. For example, the living room scenario may include themes such as meeting friends, engaging in family conversations, or hosting a Thanksgiving party. Annotators write one script for each theme, thereby ensuring broad coverage and high variability across scripts. Specifically, each script is structured as a sequence of discrete events and questions. Some events are designed as reference events, containing information relevant to a future question. Questions may appear after any event or at the end of the script. When appearing within the event sequence, questions are typically closely tied to the current plot; moving them can alter their answers or affect difficulty. An example script is provided in Table 8 (§ A.5).

Table 1 Explanations of different question types and their corresponding examples in M3-Bench.   

<table><tr><td>Question Type</td><td>Explanation and Example</td></tr><tr><td>Multi-evidence Reasoning</td><td>This requires aggregating multiple pieces of information distributed across the video. Example: Which collection has the highest starting price among the five items shown in the video? The agent must identify and recall the starting price from five distinct segments, then compare these recalled prices to determine the highest.</td></tr><tr><td>Multi-hop Reasoning</td><td>This involves step-by-step reasoning across different segments to reach a conclusion. Example: Which bubble tea shop did they visit after going to Ding Cha? The agent must first locate the visit to Ding Cha, then follow subsequent segments to identify the next bubble tea shop.</td></tr><tr><td>Cross-modal Reasoning</td><td>This requires reasoning across multiple modalities, such as visual and audio content. Example: (Bob shows Robot a red folder and says, &quot;The confidential documents should go in this folder,&quot; then shows a white folder and says, &quot;The normal documents should go in this one.) Which folder should confidential documents be placed in? The agent must combine visual cues (folder color) with dialogues to infer the correct answer.</td></tr><tr><td>Person Understanding</td><td>This involves reasoning about person-related attributes such as identity, emotions, personality, or relationships. Example: Is Lucas skilled at cooking? The video does not directly reveal the answer, but the agent must aggregate Lucas&#x27;s behavior across multiple cooking scenes to infer his skill level.</td></tr><tr><td>General Knowledge Extraction</td><td>This evaluates whether the agent can extract general knowledge from specific events. Example: (A person is shown classifying different groceries into various shelves of a refrigerator) Which shelf is suitable for storing vegetables? The agent must recognize typical storage rules from its observation to answer correctly.</td></tr></table>

To ensure the complexity of video content and the quality of downstream video filming and annotation, annotators must meet the following criteria:

• Annotate at least 15 questions, each labeled with the reference events required to answer them.   
• Ensure each question is assigned to at least one type listed in Table 1.   
• Each script must contain at least 70 events to ensure a minimum video duration of 30 minutes.

Video Filming Recording videos with actual robots poses significant challenges due to high operational costs, hardware limitations, and deployment complexities. To address these constraints, we adopt a practical alternative: employing human actors to simulate robot behavior. This approach simplifies data collection while preserving both the first-person robot perspective and the multimodal quality required for our benchmark.

Each script involves multiple actors, with one designated to simulate the robot. This actor wears head-mounted camera equipment to capture the robot’s egocentric visual and auditory perspective. The resulting footage constitute the final videos in M3-Bench-robot. To ensure diversity and minimize location bias, we recruit 67 actors and film across 51 distinct locations, with no more than three videos recorded at each location.

We collect two types of audio tracks for each video. The first is directly recorded by the head-mounted device, reflecting the raw auditory input a robot would naturally receive, including ambient sounds and spatial acoustic variations. The second is captured using individual lapel microphones worn by each actor, providing high-fidelity voice recordings to complement the primary audio stream.

Annotations After recording the videos, annotators curate QA pairs for each video. Although some questions are pre-scripted, the final video content may deviate from the original script due to realistic filming conditions. Consequently, not all scripted questions remain applicable. Annotators carefully review each scripted question to determine whether it should be retained, revised, or discarded, and provide corresponding answers when

![](images/7e3561b806d7ca38af3f8f6fa086cc2508f609371c81b2d2538bb915868d13e5.jpg)  
(a) Distribution of filming location in M3-Bench-robot.

![](images/93a911df339f48979a3605476b82fb178b4cd6a54da23872b87fc83563f4b593.jpg)  
(b) Distribution of video category in M3-Bench-web.

![](images/9675788642e8d0f8a5564537982ea37e76b2970db286462b70103d1c5afdda5b.jpg)  
(c) Distribution of question type in M3-Bench.

Figure 3 Statistical overview of M3-Bench benchmark. Each question may correspond to multiple question types.   

<table><tr><td>Benchmark</td><td>#Videos</td><td>Len.(s)</td><td>#QAs</td><td>Anno.</td><td>Form.</td><td>Agent Present</td><td>Cross-Modal QA</td><td>Person QA</td><td>Knowledge QA</td></tr><tr><td>EgoSchema [32]</td><td>5,063</td><td>180.0</td><td>5,063</td><td>M/A</td><td>C</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>LongVideoBench [50]</td><td>3,763</td><td>473.0</td><td>6,678</td><td>M</td><td>C</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>HourVideo [2]</td><td>500</td><td>2,742.0</td><td>12,976</td><td>M/A</td><td>C</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>MVBench [25]</td><td>3,641</td><td>16.0</td><td>4,000</td><td>A</td><td>C</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>Video-MME [11]</td><td>900</td><td>1,017.9</td><td>2,700</td><td>M</td><td>C</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>MLVU [62]</td><td>1,730</td><td>930.0</td><td>3,102</td><td>M/A</td><td>O/C</td><td>X</td><td>X</td><td>X</td><td>X</td></tr><tr><td>M3-Bench-robot</td><td>100</td><td>2,039.9</td><td>1,276</td><td>M</td><td>O</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td></tr><tr><td>M3-Bench-web</td><td>920</td><td>1,630.7</td><td>3,214</td><td>M</td><td>O</td><td>X</td><td>✓</td><td>✓</td><td>✓</td></tr></table>

Table 2 Comparison of M3-Bench with existing long-video question answering benchmarks across key dimensions: number of videos (#Videos), average video length in seconds (Len.), number of QA pairs (#QAs), annotation method (Anno., M/A denote manually/automatic), question format (Form., O/C indicate open-ended/close-ended), presence of an agent in the video (Agent Present), inclusion of cross-modal reasoning questions (Cross-Modal QA), person understanding questions (Person QA), and questions about general knowledge (Knowledge QA).

necessary. For all retained or revised questions, annotators are required to specify the precise timestamp at which the question should be asked. Importantly, the timestamp must precede the robot’s corresponding response or action to avoid inadvertently revealing the answer.

In addition to the script-based questions, annotators are also required to create new questions to ensure that each video contains at least 12 QA pairs. All newly added questions should also align with one or more of the question types listed in Table 1.

Besides QA pair creation, annotators also generate subtitles to enhance the usability of the dataset. Specifically, they manually annotate the start and end timestamps for each dialogue segment, together with the speaker’s identity and the transcribed dialogue content.

Full annotation guidelines, annotators information and quality control details for M3-Bench-robot annotation are presented in Appendix A.

# 3.2 M3-Bench-web

To further increase video diversity, we collect extra videos from YouTube following existing practice [9, 11, 34].

Video Collection The video collection adopts a question-driven approach: annotators select videos that could support the design of at least five questions belong to the types listed in Table 1. This strategy naturally leads to the selection of videos with rich narratives and complex inter-entity relationships, making them well-suited for assessing agent’s capability of reasoning with long-term memory.

To promote video diversity and avoid overrepresentation of easily annotated content, we provide annotators

with a reference list of video categories emphasizing high information density and relevance to real-world multimodal agent applications. Annotators are required to submit up to 20 videos from each category and are allowed to suggest new categories, which are included if deemed sufficiently distinct from the existing category list by the authors. The final dataset comprises 46 distinct video types, as summarized in Figure 3.

QA Annotations The same annotator who collects the video also generates at least five corresponding questionanswer pairs. Each question must correspond to at least one type defined in Table 1. In M3-Bench-web, all question timestamps are set to the end of the video. All questions are required to be specific, objective, and have a single unambiguous answer that can be reasonably derived from clues in the video, ensuring both the effectiveness and fairness of subsequent evaluation. For example, questions answerable from multiple perspectives or with ambiguous references, such as "the man" or "in the middle part of the video," are not considered valid. Appendix B provides the full annotation guidelines, annotators’ information, and quality control details for M3-Bench-web.

# 3.3 Automatic Evaluation

We use GPT-4o as an automatic evaluator for M3-Bench by prompting it to assess the correctness of a generated answer by comparing it to the corresponding reference answer for the same question. The prompt template is shown in Table 18 (§ H.1).

To validate GPT-4o as a reliable judge, we construct a test set of 100 randomly sampled triples, each consisting of a question, its reference answer, and a generated answer from our method or various baselines (§ 5.1). Three authors independently evaluate the correctness of each generated answer, and GPT-4o’s judgments are compared with the majority vote of human annotations. GPT-4o achieves 96% agreement with human judges, confirming its effectiveness as an automatic evaluator.

# 4 Approach

As shown in Figure 1, M3-Agent consists of a multimodal LLM and a long-term memory module. It operates through two parallel processes: memorization, which enables continuous processing of arbitrarily long video streams and builds a lifelong memory; and control, which reasons over long-term memory to execute instructions. In the following subsections, we detail long-term memory storage, memorization, and control, respectively.

Table 3 Attributes and their descriptions for a memory node.   

<table><tr><td>Attribute</td><td>Description</td></tr><tr><td>id</td><td>A unique identifier for the node.</td></tr><tr><td>type</td><td>The modality type of the node (e.g., text, image, audio). For example, natural language memory is stored as a text node, a face as an image node, and spoken dialogue as an audio node.</td></tr><tr><td>content</td><td>The raw content of the node, such as plain text, base64 image, or base64 audio.</td></tr><tr><td>embedding</td><td>The vector representation of the node content, used for similarity-based retrieval.</td></tr><tr><td>weight</td><td>A numeric value indicating the confidence of the node.</td></tr><tr><td>extra_data</td><td>A JSON object containing additional metadata, such as timestamps.</td></tr></table>

# 4.1 Long-Term Memory

Long-term memory is implemented as an external database that stores information in a structured, multimodal format (text, images, audio). Specifically, Memories are organized as an entity-centric multimodal graph, where each node represents a distinct memory item. Each node includes a unique ID, modality type, raw content, weight, embeddings, and other metadata such as timestamps. See Table 3 for details. Nodes are

Table 4 Search functions supported by long-term memory.   

<table><tr><td>Function</td><td>Description</td></tr><tr><td>search_node</td><td>Accepts a query and returns the top-k most relevant nodes. Supports multimodal queries (text, image, or audio) and modality-specific retrieval.</td></tr><tr><td>search Clip</td><td>Return top-k most relevant memory clips for the given query. A memory clip refers to the agent&#x27;s episodic and semantic memory of a segment (typically 30 seconds) generated during clip-by-clip streaming video processing.</td></tr></table>

connected by undirected edges that represent logical relationships between memory items. For example, items sharing the same entity ID are linked to form an entity-centric memory graph. This design supports not only sequential retrieval of memories based on timestamps but also associative retrieval based on entities.

The agent constructs its memory by incrementally adding new text, image, or audio nodes. When a memory generated by the memorization process already exists in long-term memory, the corresponding node or edge is reactivated and its weight increased; if it is new, a corresponding node or edge is added to the graph. Conflicting information may be introduced during construction. To resolve this, M3-Agent applies a weight-based voting mechanism during inference: frequently activated entries accumulate higher weights and override conflicting entries with lower weights. This mechanism ensures the robustness and consistency of the memory graph over time.

Search Tool To facilitate memory retrieval, we provide a suite of search tools that enable the agent to retrieve relevant memories based on specific requirements. In particular, we implement two types of search mechanisms operating at different levels of granularity, as summarized in Table 4. Detailed implementation of these retrieval mechanisms is provided in Appendix C.

# 4.2 Memorization

As shown in Figure 1, during memorization, M3-Agent processes the incoming video stream in clip-by-clip manner, generating two types of memory: episodic memory, which captures visual and auditory content from the raw video; and semantic memory, which extracts general knowledge such as character identities, attributes, relationships, and other world knowledge. Semantic memory not only enriches the memory content, but also provides additional retrieval cues, enhancing retrieval effectiveness for control process.

Consistent Entity Representation A key challenge in constructing high-quality long-term memory is maintaining consistent representations of core concepts—such as main characters and objects—across arbitrarily long time spans. Existing works typically generates language-based descriptions, such as "a man with a beard" or "a woman in a red dress". However, such textual descriptions are inherently ambiguous and prone to inconsistencies when accumulated over time. To address this issue, M3-Agent preserves the original multimodal features and constructs persistent identity representations within its long-term memory. This approach provides a more stable and robust foundation ensuring consistency over time.

Specifically, we equip M3-Agent with a suite of external tools, including facial recognition and speaker identification. These tools extract the faces and voices of characters appearing in the clip and return their corresponding identities from the long-term memory. Each extracted face or voice is associated with an existing node by using search_node function or assigned to a newly created node. The resulting identifiers (face_id or voice_id) serve as persistent references to the corresponding characters. By leveraging the globally maintained memory graph as a unifying structure, M3-Agent ensures consistent character identity mapping across local memories from different clips, thereby forming a coherent long-term memory.

This approach can be generalized to encode more concepts, such as key locations or objects, into long-term memory, thereby further improving the consistency of memory generation. Detailed implementations of both tools are provided in Appendix C.

Memory Generation Having the face and voice identities, M3-Agent continues to generate both episodic and semantic memory. Each character must be referenced by their face_id or voice_id. For example: "<face_1> wears a red hat and blue top," or "<voice_2> speaks to <face_3>, ‘How are you doing today?’" This mechanism ensures that each character is unambiguously grounded with physical features stored in long-term memory. Specially, in semantic memory, M3-Agent can perform cross-modal reasoning to infer relationships between different entity IDs (e.g., linking a face and a voice belonging to the same person). These inferred equivalences can then be used to update the connections between face and voice nodes in the memory graph. Once linked, the pair is treated as a single character. During retrieval, connected nodes are unified under a shared <character_id>, enabling the model to reason about characters more consistently across modalities.

With respect to the output format, M3-Agent generates both episodic and semantic memory as a list of text entries. Each entry is stored in the memory graph as a text node, except for entity ID relationships represented as edges. As described in the memory storage, conflicting information is resolved through a voting mechanism. For example, <voice_3> corresponds to <face_0>, but in some challenging clips, the system might temporarily link it to a different face. Over time, as correct associations accumulate, the weight of the correct mapping (<voice_3>, <face_0>) increases and dominates. This allows the system to robustly learn and maintain accurate knowledge, even in the presence of occasional local errors.

# 4.3 Control

When an instruction is received, the control process is triggered. As illustrated in Figure 1, during control, M3-Agent autonomously performs multi-turn reasoning and invokes search functions to retrieve relevant memories. Unlike traditional single-turn RAG, this iterative approach enables more complex planning, making the system more flexible and more capable. Specifically, the control process follows Algorithm 1, with prompts in Table 22 (§ H.3). Here $\pi _ { \theta }$ is the control policy, $q$ is user question, and $\mathcal { D }$ is the long-term memory. At each round, $\pi \theta$ generates a response consisting of reasoning, an action, and associated argument. If the action is [Search], the system queries $\mathcal { D }$ with the argument and appends retrieved results to the context for the next round. Depending on the context, it can call different search functions to retrieve memories from multiple perspective (e.g., search_node for people or search_clip for events). If the action is [Answer], the system returns the content and the process terminates. This loop continues for up to $H$ rounds.

# 4.4 Training

We apply reinforcement learning to optimize the M3-Agent. Although the memorization and control are conceptually handled by a single model, we trained two separate policy models to achieve optimal performance. Memorization relies strong multimodal understanding, while control requires strong reasoning capabilities. Accordingly, we initialized each policy model with different foundation models: Qwen2.5-Omni [51], an advanced open-source multimodal model supporting both visual and audio inputs, for memorization; and Qwen3 [53], an open-source large language model with powerful reasoning abilities, for control.

The training data are sourced from our in-house video dataset, which we have permissions for model training. We collect videos along with corresponding question-answer pairs, adhering to the same annotation standards used in the M3-Bench-web dataset. In total, the training dataset comprises 500 long videos, corresponding to 26,943 30-second clips, and 2,736 question-answer pairs.

Memorization To improve the model’s ability to generate desired memory, we perform imitation learning on Qwen2.5-Omni-7b to create memory-7b-sft. The process begins with constructing a high-quality synthetic demonstration dataset. We segment each video in the dataset into 30-second clips, and corresponding memory annotations are generated through a three-stage process: (1) Episodic memory synthesis: We perform a hybrid annotation strategy by jointly prompting Gemini-1.5-Pro and GPT-4o. Accordingly, GPT-4o supplies frame-level cues, which serve as priors for Gemini-1.5-Pro; the two outputs are merged to form richer narrative summaries than either alone. (2) Identity equivalence detection: We propose an algorithm that automatically mines high-confidence meta-clips, short monologue clips containing exactly one face and one voice, from a long video to construct a global face-voice correspondence. These meta-clips offer clear identity cues, enabling accurate face-voice pairing. Once the global mapping is established, it can be used to automatically annotate

Algorithm 1 Control Process   
Require: Input question $q$ , policy model $\pi_{\theta}$ long-term memory $\mathcal{M}$ , maximum number of rounds $H$ Ensure: A complete trajectory $\tau$ generated by the agent.   
1: $\tau \gets [\{\mathrm{role}:\mathrm{"system"}$ , content: Format(system_prompt, $q)\} ,$ 2: {role:"user", content: instruction_prompt}] ▷ Initialize the trajectory   
3: $i\gets 0$ 4: while $i <   H$ do ▷ Execute up to $H$ rounds   
5: $\tau_{i}\gets \pi_{\theta}(\cdot |\tau)$ 6: Append {role:"assistant", content: $\tau_{i}\} \mathrm{to}\tau$ 7: reasoning, action, argument $\leftarrow$ PARSE( $\tau_{i}$ ) ▷ Extract action and argument from $\tau_{i}$ 8: if action $=$ ["Search"] then   
9: memory $\leftarrow$ SEARCH(M, argument) ▷ Search memory using the argument as query   
10: else   
11: Break ▷ The trajectory ends when action is ["Answer"]   
12: end if   
13: $i\gets i + 1$ 14: Append {role:"user", content: memory $^+$ instruction_prompt} to $\tau$ ▷ Append search results and prompt for next round   
15: if $i = H - 1$ then   
16: Append {role:"user", content: memory $^+$ last_round_prompt} to $\tau$ 17: end if   
18: end while   
19: return $\tau$

face-voice associations in any 30-second subclip. (3) Other semantic memory synthesis: We design prompt templates to extract semantic memories from various perspectives, guiding semantic memories to include information listed in Table 10 (§ D). Details of the data synthesis process are provided in Appendix D. In total, we synthesize 10,952 samples: 10,752 for training and 200 for validation.

Fine-tuning is conducted for 3 epochs with a learning rate of $1 e - 5$ and batch size of 16, using 16 GPUs with 80GB memory.

Control We first set up the environment for RL training. For each video in the dataset, we generate the corresponding long-term memory using memory-7b-sft. For any given question, the agent is restricted to searching within the memory generated from the video associated with that question.

We then train the policy model $\pi _ { \theta }$ using DAPO [54], which initialized from control-32b-prompt. For each question-answer pair $( q , a )$ sampled from training dataset $\mathcal { D }$ , the policy $\pi _ { \theta }$ rollouts a group of $G$ trajectories $\tau _ { i = 1 } ^ { G }$ , using the algorithm shown in Algorithm 1. For each trajectory $\tau _ { i }$ , the final submitted answer $y _ { i }$ is extracted and evaluated using the GPT-4o evaluator introduced in Section 3.3. The reward of the $i$ -th trajectory is given by:

$$
R _ {i} = \left\{ \begin{array}{l l} 1, & \operatorname {g p t 4 o \_ e v a l u a t o r} (q, a, y _ {i}) = \text {T r u e} \\ 0, & \text {o t h e r w i s e} \end{array} \right. \tag {1}
$$

Then, the advantage of the $i$ -th response is calculated by normalizing the group-level rewards $\{ R _ { i } \} _ { i = 1 } ^ { G }$

$$
\hat {A} _ {i, t} = \frac {R _ {i} - \operatorname {m e a n} \left(\left\{R _ {i} \right\} _ {i = 1} ^ {G}\right)}{\operatorname {s t d} \left(\left\{R _ {i} \right\} _ {i = 1} ^ {G}\right)}. \tag {2}
$$

Note that during training, we compute loss only on LLM-generated tokens. The optimization objective is:

$$
\begin{array}{l} \mathcal {J} _ {\mathrm {D A P O}} (\theta) = \mathbb {E} _ {(q, a) \sim \mathcal {D}, \{\tau_ {i} \} _ {i = 1} ^ {G} \sim \pi_ {\theta} ^ {\mathrm {o l d}} (\cdot | q)} \left[ \frac {1}{\sum_ {i = 1} ^ {G} \sum_ {t = 1} ^ {| \tau_ {i} |} \mathbb {I} (\tau_ {i , t})} \sum_ {i = 1} ^ {G} \sum_ {t = 1} ^ {| \tau_ {i} |} \mathbb {I} (\tau_ {i, t}) \cdot \min \left(\frac {\pi_ {\theta} (\tau_ {i , t} | \tau_ {i , <   t})}{\pi_ {\theta} ^ {\mathrm {o l d}} (\tau_ {i , t} | \tau_ {\tau , <   t})} \hat {A} _ {i, t}, \right. \right. \\ \left. \operatorname {c l i p} \left(\frac {\pi_ {\theta} \left(\tau_ {i , t} \mid \tau_ {i , <   t}\right)}{\pi_ {\theta} ^ {\text {o l d}} \left(\tau_ {i , t} \mid \tau_ {i , <   t}\right)}, 1 - \epsilon_ {\text {l o w}}, 1 + \epsilon_ {\text {h i g h}}\right) \hat {A} _ {i, t}\right)\left. \right], \quad \text {s . t .} 0 <   \sum_ {i = 1} ^ {G} R _ {i} <   G, \tag {3} \\ \end{array}
$$

where the indicator $\mathbb { I } ( \tau _ { i , t } ) = 1$ if $\tau _ { i , t }$ is an LLM-generated token; and 0 otherwise. Table 14 (§ F) lists the hyperparameters used during the DAPO training process.

# 5 Experiments

# 5.1 Baselines

We evaluate M3-Agent against three types of baselines:

Socratic Models This baseline adapts the Socratic Models framework [56], which uses a multimodal model to describe 30-second video clips. These descriptions are stored as long-term memory. To answer a question, an LLM performs retrieval augmented generation (RAG) [22]: It first invokes a search_clip function to retrieve memory relevant to the question, and then generates a response based on the retrieved content.

We implement both closed-source and open-source multimodal models for memory generation:

• Gemini-1.5-Pro [43]: Takes the full 30-second video clip as input.   
• GPT-4o [17]: Since it does not process audio, we provide video frames sampled at 0.5 fps and ASR transcripts.   
• Qwen2.5-Omni-7b [51]: An advanced open-source multimodal model that supports both visual and audio inputs. It receives the full video as input.   
• Qwen2.5-VL-7b [1]: An open-source vision-language models with SOTA results in visual-language tasks. Like GPT-4o, it receives both video frames (sampled at 0.5 fps) and ASR transcripts.

For all variants, GPT-4o serves as the LLM for RAG-based question answering. We apply extensive prompt engineering to optimize performance for each setup. All prompts are provided in Appendix H.2.

Online Video Understanding Methods We further compare our approach with three online video understanding frameworks: MovieChat [42], MA-LMM [14], and Flash-VStream [58]. Unless otherwise specified, we adopt their official pretrained weights and default configurations.

• MovieChat [42]: It uses a sliding-window to extract frame-level features and stores them in a hybrid memory; the LLM performs QA conditioned on this memory.   
• MA-LMM [14]: It processes frames in an online manner, consisting of feature extraction (1 fps), temporal modeling (100-frame input), and LLM decoding.   
• Flash-VStream [58]: It adopts a two-stage asynchronous pipeline: stream video frame compression (1 fps), and LLM-based QA over the compressed features.

Agent Methods We also compare M3-Agent with agents implemented via prompting closed-source commercial models. Specifically, we consider the following two baselines:

• Gemini-Agent: Gemini-1.5-Pro is prompted separately for memorization and control process. During memorization, it receives the full video with audio, facial recognition results and speaker identification results to generate episodic and semantic memories, denoted as memory-gemini-prompt. In the control, it performs memory searches and generates responses, referred to as control-gemini-prompt.   
• Gemini-GPT4o-Hybrid: We also evaluate a setup where GPT-4o is prompted to perform memory search and generate responses (control-gpt4o-prompt). The memorization remains handled by memory-gemini-prompt.

The prompts are provided in Appendix H.3.

We set the maximum number of execution rounds $H$ to 5 for M3-Agent and all agent-based baselines. In the implementation of search_clip, the top 2 most relevant memory clips (i.e., $k = 2$ ) are returned if any relevant clips are found. If none of such clips can be found, the method returns an empty result.

# 5.2 Dataset and Evaluation

We evaluate M3-Agent and all baselines on both M3-Bench-robot and M3-Bench-web. To demonstrate the generality of our approach, we also test M3-Agent on a long-video understanding benchmark, VideoMMElong [11], following its official evaluation protocol1.

# 5.3 Main Results

Table 5 Results on M3-Bench-robot, M3-Bench-web, and VideoMME-long. We also present a comparison of all methods across different question types in M3-Bench: multi-evidence reasoning (ME), multi-hop reasoning (MH), cross-modal reasoning (CM), person understanding (PU), and general knowledge extraction (GK).   

<table><tr><td rowspan="2">Method</td><td colspan="6">M3-Bench-robot</td><td colspan="6">M3-Bench-web</td><td rowspan="2">Video-MME-Long</td></tr><tr><td>ME</td><td>MH</td><td>CM</td><td>PU</td><td>GK</td><td>All</td><td>ME</td><td>MH</td><td>CM</td><td>PU</td><td>GK</td><td>All</td></tr><tr><td colspan="14">Socratic Model</td></tr><tr><td>Qwen2.5-Omni-7b</td><td>2.1</td><td>1.4</td><td>1.5</td><td>1.5</td><td>2.1</td><td>2.0</td><td>8.9</td><td>8.8</td><td>13.7</td><td>10.8</td><td>14.1</td><td>11.3</td><td>42.2</td></tr><tr><td>Qwen2.5-VL-7b</td><td>2.9</td><td>3.8</td><td>3.6</td><td>4.6</td><td>3.4</td><td>3.4</td><td>11.9</td><td>10.5</td><td>13.4</td><td>14.0</td><td>20.9</td><td>14.9</td><td>46.9</td></tr><tr><td>Gemini-1.5-Pro</td><td>6.5</td><td>7.5</td><td>8.0</td><td>9.7</td><td>7.6</td><td>8.0</td><td>18.0</td><td>17.9</td><td>23.8</td><td>23.1</td><td>28.7</td><td>23.2</td><td>38.0</td></tr><tr><td>GPT-4o</td><td>9.3</td><td>9.0</td><td>8.4</td><td>10.2</td><td>7.3</td><td>8.5</td><td>21.3</td><td>21.9</td><td>30.9</td><td>27.1</td><td>39.6</td><td>28.7</td><td>38.8</td></tr><tr><td colspan="14">Online Video Understanding Methods</td></tr><tr><td>MovieChat</td><td>13.3</td><td>9.8</td><td>12.2</td><td>15.7</td><td>7.0</td><td>11.2</td><td>12.2</td><td>6.6</td><td>12.5</td><td>17.4</td><td>11.1</td><td>12.6</td><td>19.4</td></tr><tr><td>MA-LMM</td><td>25.6</td><td>23.4</td><td>22.7</td><td>39.1</td><td>14.4</td><td>24.4</td><td>26.8</td><td>10.5</td><td>22.4</td><td>39.3</td><td>15.8</td><td>24.3</td><td>17.3</td></tr><tr><td>Flash-VStream</td><td>21.6</td><td>19.4</td><td>19.3</td><td>24.3</td><td>14.1</td><td>19.4</td><td>24.5</td><td>10.3</td><td>24.6</td><td>32.5</td><td>20.2</td><td>23.6</td><td>25.0</td></tr><tr><td colspan="14">Agent Method</td></tr><tr><td>Gemini-Agent</td><td>15.8</td><td>17.1</td><td>15.3</td><td>20.0</td><td>15.5</td><td>16.9</td><td>29.3</td><td>20.9</td><td>33.8</td><td>34.6</td><td>45.0</td><td>34.1</td><td>55.1</td></tr><tr><td>Gemini-GPT4o-Hybrid</td><td>21.3</td><td>25.5</td><td>22.7</td><td>28.8</td><td>23.1</td><td>24.0</td><td>35.9</td><td>26.2</td><td>37.6</td><td>43.8</td><td>52.2</td><td>41.2</td><td>56.5</td></tr><tr><td>M3-Agent</td><td>32.8</td><td>29.4</td><td>31.2</td><td>43.3</td><td>19.1</td><td>30.7</td><td>45.9</td><td>28.4</td><td>44.3</td><td>59.3</td><td>53.9</td><td>48.9</td><td>61.8</td></tr></table>

As shown in Table 5, M3-Agent outperforms all baselines on M3-Bench-robot, M3-Bench-web, and VideoMMElong. Specifically, on M3-Bench-robot, M3-Agent achieves a 6.3% accuracy improvement over the strongest baseline, MA-LLM. On M3-Bench-web and VideoMME-long, it surpasses the strongest baseline, Gemini-GPT4o-Hybrid, by 7.7% and 5.3%, respectively.

We further evaluate M3-Agent against all baselines across different question types in M3-Bench. M3-Agent shows strong performance in human understanding and cross-modal reasoning. Specifically, compared to the best-performing baseline on M3-Bench-robot, MA-LMM, M3-Agent achieves improvements of 4.2% in human understanding and 8.5% in cross-modal reasoning. On M3-Bench-web, M3-Agent outperforms the top baseline, Gemini-GPT4o-Hybrid, with gains of $1 5 . 5 \%$ and 6.7% in the respective categories. These results demonstrate M3-Agent ’s superior ability to maintain character consistency, deepen human understanding, and effectively integrate multimodal information.

We also assess the memorization model via precision and comprehension, as reported in Appendix E.

# 5.4 Ablation Study

To evaluate the impact of memorization on overall performance, we fixed the control model to control-7b-rl and compared different memorization methods, as shown in Table 6. First, we replaced the memory with

Table 6 Impact of different memorization models on final performance. The control model is fixed as control-32b-rl.   

<table><tr><td>Memorization Model</td><td>M3-Bench-robot</td><td>M3-Bench-web</td><td>Video-MME-Long</td></tr><tr><td>memory-gemini-prompt</td><td>28.7</td><td>46.3</td><td>52.7</td></tr><tr><td>memory-7b-prompt</td><td>25.3</td><td>39.9</td><td>50.8</td></tr><tr><td>memory-7b-sft (M3-Agent)</td><td>30.7</td><td>48.9</td><td>61.8</td></tr><tr><td>memory-7b-sft w/o equivalence</td><td>19.5</td><td>39.7</td><td>52.1</td></tr><tr><td>memory-7b-sft w/o semantic memory</td><td>13.6</td><td>29.7</td><td>48.7</td></tr></table>

that generated by memory-gemini-prompt, resulting in accuracy drops of $2 . 0 \%$ , $2 . 6 \%$ , and $9 . 1 \%$ on M3- Bench-robot, M3-Bench-web, and VideoMME-long, respectively. This suggests that memory-7b-sft produces higher-quality memory than memory-gemini-prompt. Next, we evaluated memory-7b-prompt, which led to accuracy reductions of $5 . 4 \%$ , $9 . 0 \%$ , and $1 1 . 0 \%$ on the same benchmarks, highlighting the importance of imitation learning in generating effective memory. Finally, we ablated key components in the memory generation process. The results show that removing character identity equivalence or semantic memory significantly degrades QA performance.

Table 7 Impact of control methods on final performance, including: (1) a comparison between GRPO and DAPO training algorithms; (2) performance gains from DAPO scale with model size; (3) the effect of removing inter-turn instruction and reasoning. The memorization model is fixed as memory-7b-sft.   

<table><tr><td>Control Model</td><td>M3-Bench-robot</td><td>M3-Bench-web</td><td>Video-MME-Long</td></tr><tr><td>control-32b-grpo</td><td>30.0</td><td>47.7</td><td>58.7</td></tr><tr><td>control-8b-prompt</td><td>16.4</td><td>35.7</td><td>45.3</td></tr><tr><td>control-8b-r1</td><td>24.6</td><td>40.5</td><td>50.8</td></tr><tr><td>control-14b-prompt</td><td>18.3</td><td>36.9</td><td>49.1</td></tr><tr><td>control-14b-r1</td><td>28.2</td><td>46.9</td><td>56.0</td></tr><tr><td>control-32b-prompt</td><td>20.7</td><td>40.9</td><td>52.5</td></tr><tr><td>control-32b-r1 (M3-Agent)</td><td>30.7</td><td>48.9</td><td>61.8</td></tr><tr><td>control-32b-prompt w/o inter-turn instruction</td><td>12.8</td><td>32.3</td><td>48.3</td></tr><tr><td>control-32b-r1 w/o inter-turn instruction</td><td>20.2</td><td>43.1</td><td>55.9</td></tr><tr><td>control-32b-r1 w/o reasoning</td><td>19.0</td><td>40.1</td><td>52.3</td></tr></table>

Next, we investigate the impact of control on final performance. We fix memorization model as memory-7b-sft and evaluate various control models, as shown in Table 7. First, we compare two RL algorithms: GRPO and DAPO. Training details for GRPO are provided in Appendix F. Our results show that control-32b-rl trained with DAPO consistently outperform control-32b-grpo across all test sets. Second, we analyze how DAPO’s performance scales with model size. The results indicate substantial improvements across all sizes. Specifically, after DAPO training, control-32b-rl achieves improvements of $1 0 . 0 \%$ , $8 . 0 \%$ , and $9 . 3 \%$ in accuracy over control-32b-prompt on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Finally, we ablate two designs: inter-instruction and reasoning. Both are shown to be critical. Removing inter-instruction results in accuracy drops of $1 0 . 5 \%$ , $5 . 8 \%$ , and $5 . 9 \%$ on M3-Bench-robot, M3-Bench-web, and VideoMME-long, respectively. Removing reasoning leads to decreases of $1 1 . 7 \%$ , $8 . 8 \%$ , and $9 . 5 \%$ on the same benchmarks.

# 5.5 Case Study

Memorization Table 15, 16 (§ G) present two examples illustrating the episodic and semantic memories generated during memory access. Compared to memory-gemini-prompt, memory-7b-sft demonstrates (1) more detailed episodic memory generation, including richer scene descriptions, character actions and expressions, and dialogue; (2) improved recognition of identity equivalence, enabling consistent long-term tacking of human identities; and (3) richer semantic memory extraction, proactively generating knowledge

about characters and environments.

Control To illustrate the control process in detail, Table 17 (§ G) presents a complete generation trajectory of control-32b-rl. The input question is: "Is Tomasz a person with rich imagination or someone who lacks imagination?"

In the first round, the agent searches its memory for Tomasz’s character ID. In the second round, having identified Tomasz as <character_4>, it attempts a direct query: "What is <character_4>’s personality regarding imagination?" Finding no relevant memory in the third round, the agent reasons based on <character_4>’s role as CTO of a company and generates a more targeted query: "What are <character_4>’s creative problemsolving methods?" This yields a relevant memory: "<character_4> is innovative and forward-thinking, as evidenced by his interest in scaling drone technology for personal flight."—a piece of semantic memory. By the fourth round, the agent has collected enough information in its context to generate the final answer.

Hard Case in M3-Bench The accuracy of various methods demonstrates that M3-Bench, particularly M3- Bench-robot, presents a significant challenge. We perform a detailed error analysis of M3-Agent on M3-Bench, identifying two representative hard cases and their associated challenges that demand further investigation.

The first category involves reasoning about fine-grained details. For instance, questions like "Who wants to eat the ham sausage?" or "Which coat rack should Emma’s hat be laced, taller one or shorter one?" require the agent to extract precise information from its observations. However, retaining all such details in memory is impractical and may cause cognitive overload. To address this, the agent must use attention mechanisms that enables selective memorization. During execution, it can develop task-specific world knowledge, allowing it to focus on relevant details while ignoring the irrelevant, thereby improving task performance.

Another category of hard cases is related to spatial reasoning. In the M3-Bench-robot, a number of questions challenge the agent’s capability on spatial cognition, such as understanding spatial layout and tracking spatial changes. Examples include: "Where can the robot get the snacks?" and "Is Leo’s water cup currently on the second or third shelf from the top of the rack?" Since verbal memory is generally less effective than visual memory for retaining spatial information, the long-term memory should be designed to incorporate richer visual content, e.g., snapshots, to better support spatial reasoning.

# 6 Conclusion and Future Work

In this paper, we introduce M3-Agent, a multimodal agent framework equipped with long-term memory. M3-Agent perceives real-time video and audio streams to build both episodic and semantic memories, enabling it to accumulate world knowledge and maintain consistent, context-rich memory over time. When responding to instruction, M3-Agent can autonomously reason and retrieve relevant information from memory to complete tasks more effectively. To evaluate memory effectiveness and reasoning, we develop M3-Bench, a LVQA benchmark featuring real-world, robot-perspective videos in practical environments, and challenging questions revolving human understanding, knowledge extraction, and cross-modal reasoning, also closely reflecting real-world demands. We evaluate our method against various baselines, including Socratic models, online video understanding methods, and M3-Agent implemented by prompting closed-source models. Experimental results on M3-Bench-robot, M3-Bench-web and VideoMME-long show that M3-Agent consistently outperforms all baselines, demonstrating its superior memorization and reasoning capabilities. Furthermore, by conducting detailed case studies, we identify key limitations that point to promising future directions. These including enhancing attention mechanisms for semantic memory formation and developing richer yet more efficient visual memory.

# 7 Acknowledgment

We would like to thank Xiran Suo, Wanjun Wang, Liu Ding, and Jianghui Xie of ByteDance for their help with data annotation, and Peng Lin for creating the illustration.

# References

[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.   
[2] Keshigeyan Chandrasegaran, Agrim Gupta, Lea M Hadzic, Taran Kota, Jimming He, Cristóbal Eyzaguirre, Zane Durante, Manling Li, Jiajun Wu, and Fei-Fei Li. Hourvideo: 1-hour video-language understanding. Advances in Neural Information Processing Systems, 37:53168–53197, 2024.   
[3] Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Qian Chen, Shiliang Zhang, and Junjie Li. Eres2netv2: Boosting short-duration speaker verification performance with computational efficiency. arXiv preprint arXiv:2406.02167, 2024.   
[4] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024.   
[5] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building productionready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025.   
[6] Anxhelo Diko, Tinghuai Wang, Wassim Swaileh, Shiyan Sun, and Ioannis Patras. Rewind: Understanding long videos with instructed learnable memory. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13734–13743, 2025.   
[7] Yue Fan, Xiaojian Ma, Rongpeng Su, Jun Guo, Rujie Wu, Xi Chen, and Qing Li. Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding. arXiv preprint arXiv:2501.00358, 2024.   
[8] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: A memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pages 75–92. Springer, 2024.   
[9] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: A long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:89098–89124, 2024.   
[10] Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen Zhang, and Hang Li. Agile: A novel reinforcement learning framework of llm agents. Advances in Neural Information Processing Systems, 37: 5244–5284, 2024.   
[11] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 24108–24118, 2025.   
[12] Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hervé Jégou, Alessandro Lazaric, Arjun Majumdar, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025.   
[13] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie, and Andrew Zisserman. Autoad: Movie description in context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18930–18940, 2023.   
[14] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.   
[15] Yichen He, Yuan Lin, Jianchao Wu, Hanchong Zhang, Yuchen Zhang, and Ruicheng Le. Storyteller: Improving long video description through global audio-visual character identification. arXiv preprint arXiv:2411.07076, 2024.   
[16] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model. arXiv preprint arXiv:2408.09559, 2024.

[17] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.   
[18] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18198–18208, 2024.   
[19] Anna A Ivanova, Aalok Sathe, Benjamin Lipkin, Unnathi Kumar, Setayesh Radkani, Thomas H Clark, Carina Kauf, Jennifer Hu, RT Pramod, Gabriel Grand, et al. Elements of world knowledge (ewok): A cognition-inspired framework for evaluating basic world knowledge in language models. arXiv preprint arXiv:2405.09605, 2024.   
[20] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent. arXiv preprint arXiv:2506.06326, 2025.   
[21] Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Vidcompress: Memory-enhanced temporal compression for video understanding in large language models. arXiv preprint arXiv:2410.11417, 2024.   
[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459–9474, 2020.   
[23] Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again! llm-powered personalized agent for long-term dialogue. arXiv preprint arXiv:2406.05925, 2024.   
[24] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.   
[25] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195–22206, 2024.   
[26] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023.   
[27] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26689–26699, 2024.   
[28] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al. Mm-vid: Advancing video understanding with gpt-4v (ision). arXiv preprint arXiv:2310.19773, 2023.   
[29] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. From llm to conversational agent: A memory enhanced architecture with fine-tuning of large language models, 2024. URL https://arxiv. org/abs/2401.02777.   
[30] Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, and Min Zhang. Memlong: Memory-augmented retrieval for long text modeling. arXiv preprint arXiv:2408.16967, 2024.   
[31] Zhiwei Liu, Weiran Yao, Jianguo Zhang, Liangwei Yang, Zuxin Liu, Juntao Tan, Prafulla K Choubey, Tian Lan, Jason Wu, Huan Wang, et al. Agentlite: A lightweight library for building and advancing task-oriented llm agent system. arXiv preprint arXiv:2402.15538, 2024.   
[32] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: A diagnostic benchmark for very long-form video language understanding. Advances in Neural Information Processing Systems, 36:46212–46244, 2023.   
[33] Kai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system. arXiv preprint arXiv:2403.16971, 2024.   
[34] Junbo Niu, Yifei Li, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, Xiaoyi Dong, Haodong Duan, Shuangrui Ding, Rui Qian, et al. Ovo-bench: How far is your video-llms from real-world online video understanding? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 18902–18913, 2025.   
[35] Felix Ocker, Jörg Deigmöller, Pavel Smirnov, and Julian Eggert. A grounded memory system for smart personal assistants. arXiv preprint arXiv:2505.06328, 2025.

[36] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843–114871, 2024.   
[37] Gabriel Sarch, Yue Wu, Michael J Tarr, and Katerina Fragkiadaki. Open-ended instructable embodied agents with memory-augmented large language models. arXiv preprint arXiv:2310.15127, 2023.   
[38] Yu Shang, Yu Li, Keyu Zhao, Likai Ma, Jiahe Liu, Fengli Xu, and Yong Li. Agentsquare: Automatic llm agent search in modular design space. arXiv preprint arXiv:2410.06153, 2024.   
[39] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.   
[40] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. Longvu: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024.   
[41] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26160–26169, 2025.   
[42] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18221–18232, 2024.   
[43] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.   
[44] Endel Tulving. “episodic and semantic memory,” in organization of memory. (No Title), page 381, 1972.   
[45] Endel Tulving. How many memory systems are there? American psychologist, 40(4):385, 1985.   
[46] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Enhancing large language model with self-controlled memory framework. arXiv preprint arXiv:2304.13343, 2023.   
[47] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024.   
[48] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   
[49] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13587–13597, 2022.   
[50] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:28828–28857, 2024.   
[51] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025.   
[52] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025.   
[53] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger

Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.   
[54] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025.   
[55] Liping Yuan, Jiawei Wang, Haomiao Sun, Yuchen Zhang, and Yuan Lin. Tarsier2: Advancing large vision-language models from detailed video description to comprehensive video understanding. arXiv preprint arXiv:2501.07888, 2025.   
[56] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.   
[57] Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Mm-narrator: Narrating long-form videos with multimodal in-context learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13647–13657, 2024.   
[58] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. arXiv preprint arXiv:2406.08085, 2024.   
[59] Pan Zhang, Xiaoyi Dong, Yuhang Cao, Yuhang Zang, Rui Qian, Xilin Wei, Lin Chen, Yifei Li, Junbo Niu, Shuangrui Ding, et al. Internlm-xcomposer2. 5-omnilive: A comprehensive multimodal system for long-term streaming video and audio interactions. arXiv preprint arXiv:2412.09596, 2024.   
[60] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024.   
[61] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724–19731, 2024.   
[62] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 13691–13701, 2025.

# Appendix

# A M3-Bench-robot 21

A.1 Script Annotation Guidelines 21   
A.2 QA Annotation Guidelines 21   
A.3 Quality Control 2 2   
A.4 Annotator Information 22   
A.5 Data Examples 22

# B M3-Bench-web 23

B.1 Annotation Guidelines 2 3   
B.2 Quality Control 2 4   
B.3 Annotator Information 2 4

# C Implementation Details of Tools 24

# D Demonstration Data Synthesis for Memorization 25

D.1 Episodic Memory Synthesis 26   
D.2 Entity ID Relationship Detection 28   
D.3 Semantic Memory Synthesis 28   
D.4 Quality of the Synthetic Data 31

# E Evaluation of Memorization 31

# F RL Training Details . 31

F.1 Details of DAPO Training 3 1   
F.2 GRPO Training . 32

# G Case Study 32

# H Prompt Templates . 40

H.1 Prompt for Automatic Evaluator of M3-Bench 40   
H.2 Prompts for Socratic Models 41   
H.3 Prompts for M3-Agent 43

# A M3-Bench-robot

# A.1 Script Annotation Guidelines

# Actor Setup

Four to five actors participate, including one playing the role of robot. The robot actor wears a head-mounted camera, either an iPhone 16 Pro, Xiaomi 14 Ultra, or GoPro HERO13, to capture a single point-of-view video from the robot’s perspective.

# Definitions

1. Script: Consists of events and questions and provides actors with dialogue and stage instructions.   
2. Robot: Played by a human actor. It is an ideal highly intelligent robot with reasoning and memory abilities similar to humans.   
3. Scenario: living room, kitchen, bedroom, study, office, meeting room, and gym.   
4. Event: A complete, short plot within the script. A reference event includes information relevant to future questions, such as robots interacting with humans while observing and learning human preferences or the placement of objects in real-world scenes.   
5. Question: Designed to evaluate the robot’s memory. Each question must align with at least one type listed in Table 1.

# Requirements

• Annotate at least 15 questions, each labeled with the corresponding reference events.   
• Each script must contain at least 70 events to ensure a minimum video duration of 30 minutes.   
• Avoid asking questions that rely solely on common sense or that can be answered without watching the video.   
• Do not ask questions that remain unanswerable even after watching the video.   
• Avoid questions that can be answered based solely on the dialogue.   
• Do no include questions that are weakly related to the reference events.   
• The question should have a clear and unambiguous answer that can be objectively verified by comparing it to the reference answer.

# A.2 QA Annotation Guidelines

# Background

• In the future, robots will help humans complete many tasks in indoor environments such as homes. Based on this imagination, we filmed a video from the perspective of a robot.   
• In order to evaluate the model’s ability, we set questions at different timestamps, typically related to the robot’s upcoming tasks. Correct answers are essential for the successful completion of these tasks.   
• Some questions require manual review or additional annotations to ensure each video includes at least 10 questions.

# Task

Provide a 30–45 minute video along with a corresponding script that includes a series of questions. Note: Minor script modifications may occur during filming to accommodate practical constraints. As a result, the script may not perfectly align with the final video.

1. Review existing questions.

For each question in the script:

• Annotate the corresponding timestamp in the video based on the related script event.   
• Determine whether the question can be answered using the video content up to that point. If so, annotate the answer.   
• If the question is unanswerable, consider whether modifying it could make it answerable. If applicable, revise the question and provide the answer.   
• For each question-answer pair, annotate the reasoning process used to derive the answer and specify the question types according to Table 1.

# 2. Annotate additional questions:

If fewer than 10 questions remain after reviewing the script, generate new questions that must belong to at least one type listed in Table 1.

# A.3 Quality Control

The annotation process consists of two rounds. In the first round, the goal is to ensure that annotators fully understand the annotation guidelines. Each annotator is required to perform QA annotations on three videos. The authors then review the annotations, provide feedback, and the annotators may revise their annotation accordingly. Based on the quality of these initial annotations, the authors determine whether the annotator is qualified to proceed to the formal annotation phase. In the second round, each annotator annotates five videos at a time. The authors randomly select one video from each batch for quality inspection. If more than one invalid question-answer is found in the selected video, the entire batch must be re-annotated. Otherwise, the batch is considered accepted. Two authors are involved in the quality control process throughout the annotation workflow.

In addition, to ensure the quality of the questions in M3-Bench-robot, we recruited five annotators to answer each question. Annotators were allowed to first read the question and then watch the video as many times as needed. The final human accuracy on M3-Bench-robot is 90.7%. Our error analysis shows that the most common mistakes are counting-related problems.

# A.4 Annotator Information

All annotators are employed by a commercial data annotation company. We sign a contract with the company and pay the company for the annotation work at a market price. The annotators are all college graduates with strong English proficiency. For script annotation, eleven annotators are involved. Video filming engage 67 actors. For QA annotation, five annotators participate.

# A.5 Data Examples

Table 8 provides an example of script annotation.   

<table><tr><td>Event ID</td><td>Event</td></tr><tr><td>1</td><td>Rose is in the room talking to Amy on the phone. She thanks Amy for the tulips and takes a photo of the blooming flowers to share with her. (reference)</td></tr><tr><td>2</td><td>Rose tells the robot that the delicate teddy bear is a gift for Rachel. (reference)</td></tr><tr><td>3</td><td>After hanging up with Amy, Rose calls Rachel and Leo to remind them not to forget to come over today.</td></tr><tr><td>4</td><td>Rose looks at a pile of packages in the corner of the bedroom. They are recently purchased clothes. She asks the robot to unpack them and place the clothes on the first shelf of the wardrobe.</td></tr></table>

(Continued on next page)

Table 8 An example of the M3-Bench-robot script.   

<table><tr><td>5</td><td>She points to the bottom of the wardrobe, where a pile of delicate little toys is stored, and tells the robot, &quot;Put the teddy bear there.&quot; (reference)</td></tr><tr><td>6</td><td>At that moment, the doorbell rings and Rose excitedly runs to open the door.</td></tr><tr><td>...</td><td>...</td></tr><tr><td>10</td><td>Rachel sees the dolls on the bed and exclaims, &quot;Wow, these dolls are so cute, let me pamper them!&quot;</td></tr><tr><td>11</td><td>Rose says, &quot;Don&#x27;t rush, there&#x27;s another surprise,&quot; and then calls the robot. 
Question: Is Rachel&#x27;s gift on the top shelf or the bottom shelf of the wardrobe? 
Reference: event-2 and event-5</td></tr><tr><td>12</td><td>The robot takes a teddy bear from the wardrobe, hands it to Rachel, and says, &quot;This is a gift prepared for you.&quot;</td></tr><tr><td>...</td><td>...</td></tr><tr><td>58</td><td>Rachel teases that Rose just doesn&#x27;t want to admit it, but the robot surely knows. 
She then turns to the robot and asks who gave Rose the flowers. 
Question: Who gave Rose the flowers? 
Reference: event-1</td></tr><tr><td>...</td><td>...</td></tr></table>

# B M3-Bench-web

# B.1 Annotation Guidelines

To better help the annotators understand the requirements and better ensure the overall quality, safety, and validity of the datasets, we provide the following detailed guidelines, which clearly specify the acceptable and unacceptable annotation practices.

• Questions must allow for verifiable and objective evaluation of correctness. This entails avoiding overly open-ended questions, compound questions that mix multiple sub-questions, or questions with multiple equally valid answers.   
• Each video must include at least two questions targeting character attribute modeling and two questions involving commonsense reasoning.   
• All visual information required to answer a question must remain clearly recognizable at lower resolutions $( \leq 7 2 0 0 )$ , ensuring that all questions are answerable.   
• For videos between 20 and 40 minutes in length, 5 questions should be generated; for videos exceeding 40 minutes, 10 questions should be provided. Compensation considers both the number and duration of the videos.   
• For commonsense reasoning questions, annotators must also specify the commonsense knowledge being tested, in addition to the question and its answer.   
• It is not permissible for all questions to be answerable using only audio. A reasonable proportion of questions must be vision-centric, requiring understanding of visual content in the video.   
• Redundant questions within the same video are not allowed. For instance, asking "Describe David’s appearance" and "Describe Alice’s appearance" would be considered repetitive.   
• Questions that can be answered solely based on a brief moment or a short clip should be avoided. Specifically, the context required to answer a valid question should span more than 10 seconds of video content.

• Videos must not contain sensitive, offensive, or NSFW content.   
• Avoid asking questions that rely solely on commonsense knowledge and do not require viewing the video. Such questions do not meaningfully test video understanding.   
• Avoid questions that are too easy to guess based on social priors or language bias alone. For example, a question like "Did the teacher appear impatient when students repeatedly interrupted the class?" may be too easily answered with "No" due to cultural expectations of teacher behavior, regardless of the actual video content. This undermines the goal of evaluating visual understanding.   
• Do not directly convert characters’ spoken lines into questions. These are typically answerable via simple string matching or keyword retrieval, which again does not effectively test video comprehension.   
• Balance the number of questions with answer Yes and No.

# B.2 Quality Control

The annotation process includes the following quality control stages:

• Stage 1: Candidate annotators complete a trial task, collecting one video and labeling corresponding QA pairs. The authors review the submission and provide feedback. Once the annotator demonstrates a clear understanding of the annotation guidelines, they proceed to formal annotation.   
• Stage 2: The annotator submits a batch of 10 videos with corresponding QA pairs. The authors randomly review 2 of them and provide feedback. The annotator revise the entire batch accordingly. If the qualified rate of the submitted questions is below 90%, the authors re-sample the revised batch for further inspection. Otherwise, the batch is accepted. Annotators who pass this stage on the first attempt can proceed to Stage 3.   
• Stage 3: The annotator submits a batch of 30 videos with QA pairs. The authors randomly inspect 5 of them and provide feedback. The annotator revises the full batch as needed. If the QA qualified rate is below $9 0 \%$ , a follow-up review of the revised batch is conducted. Otherwise, the batch is accepted.

Two authors are involved in the quality control process.

# B.3 Annotator Information

All annotators are from a commercial data annotation company. We have a contract with this company and compensate them at market rates for the annotation work. All annotators are college graduates with strong English proficiency. A total of ten annotators participated in the annotation of M3-Bench-web.

# C Implementation Details of Tools

Here, we provide the implementation details of the tools for representation extraction introduced in Section 4.2.

Facial Recognition To perform facial recognition, we uniformly sample video frames at a rate of 5 frames per second. For each sampled frame, we employ the buffalo_l predefined model suite from the Insight-Face2 library to extract facial attributes, including bounding box coordinates, identity embeddings, and detection/quality scores. Low-quality detections—such as those with abnormal aspect ratios or extremely low confidence scores—are discarded. We then apply HDBSCAN clustering on the embeddings of the remaining high-quality faces to group them by character identity. This yields a set of reliable facial representations, clustered by character.

Voice Identification For speaker identification, we use Gemini-1.5-Pro to extract audio segments corresponding to distinct speaker voices, while simultaneously performing automatic speech recognition (ASR) on each segment. Segments shorter than 2 seconds are filtered out to ensure reliability. We then apply voice embedding model ERes2NetV2[3] to encode each segment into a speaker-specific representation. Based on the resulting voice embeddings, we cluster and merge segments that correspond to the same speaker—i.e., those with similar

vocal characteristics. This process produces a set of high-quality speaker representations, also grouped by character. The prompt used for voice processing is shown in Table 9.

# The Prompt for Voice Processing

You are given a video. Your task is to perform Automatic Speech Recognition (ASR) and audio diarization on the provided video. Extract all speech segments with accurate timestamps and segment them by speaker turns (i.e., different speakers should have separate segments), but without assigning speaker identifiers.

Return a JSON list where each entry represents a speech segment with the following fields:

• start_time: Start timestamp in MM:SS format.   
• end_time: End timestamp in MM:SS format.   
• asr: The transcribed text for that segment.

Example Output:

```json
[ "start_time": "00:05", "end_time": "00:08", "asr": "Hello, everyone.", "start_time": "00:09", "end_time": "00:12", "asr": "Welcome to the meeting." ] 
```

Strict Requirements:

• Ensure precise speech segmentation with accurate timestamps.   
• Segment based on speaker turns (i.e., different speakers’ utterances should be separated).   
• Preserve punctuation and capitalization in the ASR output.   
• Skip the speeches that can hardly be clearly recognized.   
• Return only the valid JSON list (which starts with "[" and ends with "]") without additional explanations.   
• If the video contains no speech, return an empty list ("[]").

Now generate the JSON list based on the given video:

Table 9 Prompt used for voice processing.

Search All memory-based retrieval is implemented via Maximum Inner Product Search (MIPS), with modalityspecific adaptations.

Each face and voice node maintains a set of representative feature snapshots. When new face or voice features are extracted from a video clip, we compute the average cosine similarity between each extracted feature and all stored snapshots per node. The node with the highest similarity exceeding a pre-defined threshold (0.3 for image, 0.6 for voice) is considered a match; otherwise, a new node is created. Matched nodes are updated with the new features to refine their representations over time.

For textual memory, we apply MIPS between the input query and all existing text nodes, using OpenAI’s text-embedding-3-large3 as the embedding model. To support multi-entry retrieval, we apply a top- $k$ retrieval with a similarity threshold $t$ . Specifically, we return the $k$ most relevant nodes whose similarities exceed $t$ . To ensure retrieval coherence, we also perform clip-level retrieval: each clip is scored by the highest similarity among its memory entries, and we return the top-ranked clips accordingly. For all experiments, we adopt a relatively strict hyperparameter setting ( $k = 2$ , $t = 0 . 5$ ) to reduce retrieval randomness and enable consistent evaluation across models.

# D Demonstration Data Synthesis for Memorization

During memorization, the multimodal model takes inputs including: video, audio, facial identifications (via facial recognition), and voice identities (via voice identification). It generates two outputs, episodic memory

and semantic memory. To construct training data, we segment training videos into 30-second clips. For each clip, we then synthesize the corresponding episodic memory, entity identity relationships in semantic memory, and other semantic memory, as detailed below. In total, we synthesize 10,752 training samples for 200 validation samples.

Table 10 Explanations of different memory types.   

<table><tr><td>Memory Type</td><td>Explanation</td></tr><tr><td>Episodic Memory</td><td>Specific events or experience, capturing not just what happened, but also when, where, and in what context. The episodic memory should captures details such as the people involved, their appearance, actions and spoken words, and the broader environment.</td></tr><tr><td>Semantic Memory</td><td>Character-Identity Equivalence: Captures equivalence relationships across different character modality identityCharacter-Level Attributes: Extracts attributes for each character, such as name, personality traits (e.g., confident, nervous), role or profession (e.g., host, newcomer), interests, and background information.Interpersonal Relationships: Describes the relationships and interactions among characters, such as social roles (e.g., host-guest, leader-subordinate), emotional tone (e.g., respect, tension), power dynamics (e.g., who leads), and evidence of cooperation, exclusion, or conflictContextual and General Knowledge: Encompasses general knowledge inferred from the video, such as likely setting or genre (e.g., corporate meeting, game show), cultural or procedural norms, real-world facts (e.g., &quot;Alice Market is pet-friendly&quot;), common sense, and the functional roles or attributes of objects within the scene.</td></tr></table>

# D.1 Episodic Memory Synthesis

We employ a hybrid synthetic strategy that integrates the complementary strengths of Gemini-1.5-Pro and GPT-4o. Gemini-1.5-Pro supports audio inputs and excels at generating high-level, event-based descriptions, whereas GPT-4o provides more fine-grained visual details. To leverage both models effectively, we first prompt GPT-4o to generate a detailed visual description of the video using frames sampled at 0.5 fps. This output serves as contextual input for Gemini-1.5-Pro, which is then prompted to generate the final episodic memory. The prompt explicitly instructs Gemini-1.5-Pro to incorporate information from GPT-4o’s description when it deems it accurate. We find that using GPT-4o’s detailed visual output as context significantly enhances the richness of the final memory produced by Gemini-1.5-Pro. The full prompt template is shown in Table 11.

# Prompt of Episodic Memory Synthesis (GPT-4o)

[Video] includes 16 frames of a video.

Using this information, generate a detailed description of the video. Following the requirements below:

1. Carefully describe the visual elements in each frame, noting colors, objects, movements, environment, people (including actions, clothing, expressions), and any noticeable details or changes between frames.   
2. If audio elements or sounds are visible through textual or visual cues within the frames (such as subtitles, audio indicators, or written sound effects), accurately describe these details.   
3. Do not speculate or infer information beyond what is explicitly visible in these 16 frames. Avoid using external knowledge or assumptions.   
4. Generate only the detailed description based solely on the given frames. Do not produce any additional commentary or explanations.

# Prompt of Episodic Memory Synthesis (Gemini-1.5-Pro)

You are provided with the following data:

(Continued on next page)

[Video]: A video clip in mp4 format.

[Faces]: A list of facial features detected in the video, each linked to a unique face ID (e.g., <face_1>). [Dialogues]: A list of speech segments in the video, including start_time, end_time, speaker ID (e.g., <voice_2>), and the corresponding transcribed text.

[Reference Description]: A description of the video that may contain both accurate and inaccurate details.

# Your Tasks:

Based on the video content and reference descriptions, generate a detailed and cohesive description of the video clip. The description should focus on the entire event, incorporating all relevant aspects of the characters, their actions, spoken dialogue, and interactions in a narrative format. The description should include (but is not limited to) the following categories:

$\bullet$ Characters’ Appearance: Describe clothing, physical features, notable accessories, etc.   
• Characters’ Actions & Movements: Describe gestures, movement across the scene, or interactions.   
• Characters’ Spoken Dialogue: Quote—or, if necessary, summarize—spoken content from the dialogue track.   
$\bullet$ Characters’ Contextual Behavior: Describe emotional states, relationships, roles, and reactions.   
$\bullet$ Environmental or Temporal Cues: Describe the physical setting and time-of-day if visible.

# Strict Requirements:

$\bullet$ Incorporate correct elements from the [Reference Description], and correct any mistakes you identify.   
$\bullet$ Add any additional details visible or inferable from the [Video], [Faces], and [Dialogues] that are missing from the reference.   
• Since the given dialogues may be incomplete, reconstruct the entire conversation from the raw audio as precisely as possible.   
$\bullet$ If a character has an associated feature ID in the input context (either face or voice), refer to them only using that feature ID (e.g., <face_1>, <voice_2>)

• Use face ID (e.g., <face_1>) when the detail is grounded in visual data.   
• Use speaker ID (e.g., <voice_1>) when the detail is grounded in speech.

• Do not use non-existent <face_ID> or <voice_ID>.

• We reiterate the above-mentioned list of available IDs here: $\{ \mathrm { I D \_ l i s t } \}$

$\bullet$ For characters without associated feature IDs, refer to them using a concise visual or contextual descriptor (e.g., "a man in a blue shirt", "a young woman by the window").   
$\bullet$ Do not use pronouns (e.g., "he", "she", "they") or inferred character names.

Your output should be a Python list of well-formed, concise English sentences (one detail per sentence).

# Example Output:

[

"In the bright conference room, <face_1> enters confidently, adjusting his black suit with a white shirt and tie. He has short black hair and wears glasses, giving a professional appearance as he approaches <face_2> to shake hands.",   
"<face_2>, dressed in a striking red dress with long brown hair, smiles warmly and greets <face_1>. She then sits down at the table beside him, glancing at her phone briefly while occasionally looking up.",   
"<voice_1> speaks to the group, ‘Good afternoon, everyone. Let’s begin the meeting.’ His voice commands attention as the room quiets, and all eyes turn to him.",   
"<face_2> listens attentively to <voice_1>’s words, nodding in agreement while still occasionally checking her phone. The atmosphere is professional, with the participants settling into their roles for the meeting.",   
"<face_1> adjusts his tie and begins discussing the agenda, engaging the participants in a productive conversation."

]

Please only return the valid string list (which starts with "[" and ends with "]"), without any additional explanation or formatting.

Table 11 Prompt templates used for generating synthetic episodic memory.

# D.2 Entity ID Relationship Detection

There is a special type of semantic memory, extracting cross-modal identity equivalences from video. This remains a challenging task, even for advanced models like Gemini-1.5-Pro, particularly in scenes with multiple faces and voices [15]. To address this, we propose a progressive annotation algorithm. The key idea is to identify meta-clips, segments containing exactly one face identity and one voice identity, from the raw long video. These meta-clips are used to build a meta-dictionary that maps voice IDs to face IDs across the entire video. This dictionary enables automatic annotation of any 30-second clip extracted from the original video.

Meta-Clip Extraction First, for a long video, we can use facial recognition tools and voice identity tools introduced in Appendix C to construct a corresponding global ID for each face and voice that appears in the video. Next, we segment the video into a series of short clips, each no longer than 5 seconds in duration, using keyframe-based division. This method ensures that each clip is visually stable, with minimal changes in characters or scenes. Then, we apply facial recognition and voice identity tools to each short clip individually to extract the faces and voices present, along with their global IDs. If a clip contains only one face ID and one voice ID, we refer to it as a meta-clip. In this case, it is highly likely that the face and voice in the clip belong to the same person. Therefore, we can use the meta-clip as a high-confidence sample for establishing the association between faces and voices.

Meta-Dictionary Construction Based on all meta-clips extracted from the long video, we construct a set of mappings between face IDs and voice IDs. However inconsistencies may arise due to a small number of clips where the speaker is not visible. To address this issue, we employ a voting mechanism to generate the final meta-dictionary. The detailed algorithm is described in Algorithm 2.

New-Clip Annotation After obtaining the meta-dictionary, we can use it to annotate arbitrary clips from the full-length video. Specifically, for each 30-second clip, if both a face ID and a voice ID appearing in the clip and also found in the meta-dictionary, we generate a semantic memory in the form: "Equivalence: <face_id>, <voice_id>". Since not all IDs can be found using the meta-dictionary, we reject any clip containing a voice ID that is not present in the meta-dictionary from the final training dataset for memorization. In total, we collected 10,952 30-second clips with valid identity equivalence annotations. We manually review 48 randomly sampled mappings, and found the accuracy to be $9 5 . 8 3 \%$ .

# D.3 Semantic Memory Synthesis

To construct semantic memory, we adopt a hybrid strategy similar to that used for episodic memory. We define several key dimensions that semantic memory should address, as outlined in Table 10. Specifically, we first prompt GPT-4o to generate preliminary semantic memory based on video frames and episodic memory. Next, we provide the video, episodic memory, and GPT-4o-generated semantic memory to Gemini-1.5-Pro, prompting it to produce the final semantic memory. Detailed prompts are provided in Table 12.

Algorithm 2 Meta-Dictionary Construction   
Require: A long video $V$ , threshold $p$ Ensure: A mapping dictionary $\mathcal{M}:\mathcal{V}\to \mathcal{F}$ from voice IDs to face IDs 1: Extract global face ID set $\mathcal{F} = \{f_1,\dots ,f_N\}$ and voice ID set $\mathcal{V} = \{v_{1},\ldots ,v_{N}\}$ from video $V$ 2: Divide $V$ into a sequence of short clips $\mathcal{C} = \{c_1,c_2,\dots ,c_T\}$ using keyframes-based segmentation 3: Initialize meta-clip set $\mathcal{C}_{\mathrm{meta}}\gets \emptyset$ 4: for $c_{t}\in \mathcal{C}$ do 5: Detect face set $\mathcal{F}_t\subseteq \mathcal{F}$ and voice set $\nu_{t}\subseteq \nu$ in $c_{t}$ 6: if $|\mathcal{F}_t| = 1$ and $|\mathcal{V}_t| = 1$ then 7: Add pair $(c_t,f,v)$ where $f\in \mathcal{F}_t$ $v\in \mathcal{V}_t$ to $\mathcal{C}_{\mathrm{meta}}$ 8: end if 9: end for 10: Construct bipartite graph $G = (\mathcal{F},\mathcal{V},E)$ where edge $(f,v)$ has weight: $w(f,v) = |\{(c_t,f,v)\in \mathcal{C}_{\mathrm{meta}}\} |$ 11: Remove all edges from $G$ with weight equal to 1.   
12: for $f\in \mathcal{F}$ do   
13: Let $\mathcal{N}_f = \{v_i\mid (f,v_i)\in E\}$ 14: Let $v^{*} = \arg \max_{v_{i}\in \mathcal{N}_{f}}w(f,v_{i})$ 15: if $\frac{w(f,v^{*})}{\sum v_{i}\in\mathcal{N}_{f}} w(f,v_{i})\geq p$ then   
16: Keep only edge $(f,v^{*})$ and remove others   
17: else   
18: Remove all edges incident to $f$ 19: end if   
20: end for   
21: for $v\in \mathcal{V}$ do   
22: Let $\mathcal{N}_v = \{f_j\mid (f_j,v)\in E\}$ 23: Let $f^{*} = \arg \max_{f_{j}\in \mathcal{N}_{v}}w(f_{j},v)$ 24: Keep only edge $(f^{*},v)$ and remove others   
25: end for   
26: Initialize mapping dictionary $\mathcal{M}\gets \emptyset$ 27: for $(f,v)\in E$ do   
28: Add mapping $\mathcal{M}[v]\gets f$ 29: end for   
30: return M

Prompt of Semantic Memory Synthesis (GPT-4o, Gemini-1.5-Pro)   
You are provided with the following data:  
[Video]: 16 frames of a video. (Gemini-1.5-Pro Variant: A video clip in mp4 format.)  
[Faces]: A list of facial features detected in the video, each linked to a unique face ID (e.g., <face_1>).  
[Dialogues]: A list of speech segments in the video, including start_time, end_time, speaker ID (e.g., <voice_2>), and the corresponding transcribed text.  
[Video Descriptions]: A description of the video.  
(Gemini-1.5-Pro Variant: [Refence conclusions]: A list of high-level conclusions that may contain inadequate or incorrect information.)

Your Task:

Based on the given character features, video content, and reference conclusions, generate a list of high-level, reasoning-based conclusions within the scope of the following category:

1. Character-Level Attributes

Infer abstract attributes for each character, such as:

• Name (if explicitly stated),   
• Personality (e.g., confident, nervous),   
• Role/profession (e.g., host, newcomer),   
• Interests or background (when inferable),   
• Distinctive behaviors or traits (e.g., speaks formally, fidgets).

Avoid restating visual facts—focus on identity construction.

# 2. Interpersonal Relationships & Dynamics

Describe the relationships and interactions between multiple characters:

• Roles (e.g., host-guest, leader-subordinate),   
• Emotions or tone (e.g., respect, tension),   
• Power dynamics (e.g., who leads),   
• Evidence of cooperation, exclusion, conflict, etc.   
• For individual character or cases where character relationships cannot be determined, do not generate conclusion relevant to the corresponding character.

# 3. Video-Level Plot Understanding

Summarize the scene-level narrative, such as:

• Main event or theme,   
• Narrative arc or sequence (e.g., intro discussion reaction),   
• Overall tone (e.g., formal, tense),   
• Cause-effect or group dynamics.   
• Do not involve specific characters.

# 4. Contextual & General Knowledge

Include general knowledge that can be learned from the video, such as:

• Likely setting or genre (e.g., corporate meeting, game show),   
• Cultural/procedural norms,   
• Real-world knowledge (e.g., "Alice market is pet-friendly"),   
• Common-sense or format conventions.   
• Attributes and functional roles of objects in the video (e.g., the trash bin is used for disposing of kitchen waste).

# Output Format:

$\bullet$ A Python list of concise English sentences, each expressing one high-level conclusion.   
$\bullet$ Do not include reasoning steps or restate input observations. Only output the final conclusions.

# Strict Requirements:

• Only include conclusions under the given category. Do not go beyond it.   
$\bullet$ Your conclusions must be informed by the video and reference content.   
• Each conclusion should reflect deeper reasoning and insight, not surface-level observations already evident from the plot description.   
$\bullet$ If a character has an associated feature ID in the input context (either face or voice), refer to them only using that feature ID (e.g., <face_1>, <voice_2>).

• Use face ID (e.g., <face_1>) when the detail is grounded in visual data.   
• Use speaker ID (e.g., <voice_1>) when the detail is grounded in speech.

• Do not use non-existent <face_ID> or <voice_ID>.

• We reiterate the above-mentioned list of available IDs here: $\{ \mathrm { I D \_ l i s t } \}$

• For characters without associated feature IDs, refer to them using a concise visual or contextual descriptor (e.g., "a man in a blue shirt", "a young woman by the window").   
• Do not use pronouns (e.g., "he", "she", "they") or inferred character names.

(Continued on next page)

• Maintain strict accuracy in referring to characters and their correct IDs or descriptions.   
• Do not restate the input observations or reasoning steps—only output the final, distilled conclusions.   
$\bullet$ Your output should be a Python list of well-formed, concise English sentences (one per item).

Table 12 The prompt used in generating synthetic semantic memory.   
Table 14 lists the hyperparameters used during the training process. Figure 4 depicts the RL training curves, which show a steady increase in score with the training steps.   
```txt
Example Output (Note: example only represent the format, not fully corresponding to the provided category):   
[ "<face_1>'s name is David.", "<face_1>' holds a position of authority, likely as the meeting's organizer or a senior executive.", "<voice_2>' shows social awareness and diplomacy, possibly indicating experience in public or client-facing roles.", "<face_1>' demonstrates control and composure, suggesting a high level of professionalism and confidence under pressure.", "The interaction between <face_1> and <voice_2> suggests a working relationship built on mutual respect.", "The overall tone of the meeting is structured and goal-oriented, indicating it is part of a larger organizational workflow." ] Please only return the valid string list (which starts with ["and ends with"] without any additional explanation or formatting. 
```

# D.4 Quality of the Synthetic Data

Although the demonstration data is synthetic, it is of high quality. Our synthetic memory averages 245.7 words for episodic memory and 276.2 words for semantic memory, compared to 151.3 and 81.4 words respectively for Gemini-1.5-pro, indicating our memory captures more detail. For content accuracy, we randomly sampled 10 clips from different videos, totaling 353 memory items. Manual review showed an accuracy of 95.5%. Most errors stemmed from the speaker recognition tool: background noise and overlapping speech occasionally caused minor omissions or misidentifications in extracting speaker dialogue for episodic memory.

# E Evaluation of Memorization

we evaluate the memorization model during training using a held-out validation set of 200 samples and select the best checkpoint. Two evaluation metrics are used. First, AutoDQ [47] assesses memory description quality by comparing generated outputs to reference descriptions, measuring episodic and semantic memory excluding identity equivalence. Second, for identity equivalence, we compute precision, recall and F1 score against ground-truth in the validation set. Based on the results in Table 13, we select the checkpoint obtained after training for 3 epochs. For additional comparison, we also report results from two baseline models, memory-gemini-prompt and memory-7b-prompt, on the same validation set. Our model, memory-7b-sft, significantly outperforms both baselines.

# F RL Training Details

# F.1 Details of DAPO Training

<table><tr><td>Model</td><td>AutoDQ-P</td><td>AutoDQ-R</td><td>AutoDQ-F1</td><td>Eq.-P</td><td>Eq.-R</td><td>Eq.-F1</td></tr><tr><td>memory-gemini-prompt</td><td>0.692</td><td>0.539</td><td>0.606</td><td>0.472</td><td>0.805</td><td>0.595</td></tr><tr><td>memory-7b-prompt</td><td>0.495</td><td>0.355</td><td>0.414</td><td>0.117</td><td>0.192</td><td>0.145</td></tr><tr><td>memory-7b-sft (1 epoch)</td><td>0.634</td><td>0.596</td><td>0.616</td><td>0.742</td><td>0.817</td><td>0.778</td></tr><tr><td>memory-7b-sft (2 epochs)</td><td>0.628</td><td>0.610</td><td>0.619</td><td>0.845</td><td>0.810</td><td>0.827</td></tr><tr><td>memory-7b-sft (3 epochs)</td><td>0.635</td><td>0.620</td><td>0.627</td><td>0.836</td><td>0.856</td><td>0.846</td></tr><tr><td>memory-7b-sft (4 epochs)</td><td>0.616</td><td>0.618</td><td>0.617</td><td>0.825</td><td>0.839</td><td>0.832</td></tr><tr><td>memory-7b-sft (5 epochs)</td><td>0.609</td><td>0.621</td><td>0.615</td><td>0.813</td><td>0.840</td><td>0.827</td></tr></table>

Table 13 Evaluation of memorization models using AutoDQ and Equivalence (Eq.) metrics. Here, P, R, and F1 denote precision, recall, and the F1 score, respectively.   
Table 14 The hyperparameters used in DAPO training.   

<table><tr><td rowspan="2">Parameter Name</td><td colspan="3">Model Size</td></tr><tr><td>8B</td><td>14B</td><td>32B</td></tr><tr><td>Batch Size</td><td>32</td><td>32</td><td>32</td></tr><tr><td>GPU with 80GB memory</td><td>16</td><td>16</td><td>32</td></tr><tr><td>Rollout Model Parallel Size</td><td>1</td><td>1</td><td>2</td></tr><tr><td>Learning Rate</td><td>1e-6</td><td>1e-6</td><td>1e-6</td></tr><tr><td>Maximum Number of Rounds H</td><td>5</td><td>5</td><td>5</td></tr><tr><td>Number of Samples in a Group G</td><td>4</td><td>4</td><td>4</td></tr><tr><td>Total Steps</td><td>180</td><td>180</td><td>180</td></tr><tr><td>εlow</td><td>0.2</td><td>0.2</td><td>0.2</td></tr><tr><td>εhigh</td><td>0.28</td><td>0.28</td><td>0.28</td></tr></table>

# F.2 GRPO Training

We also use Group Relative Policy Optimization (GRPO)[39] to optimize the policy model in the ablation study. GRPO optimizes the policy model $\pi _ { \theta }$ by maximizing the following objective:

$$
\begin{array}{l} \mathcal {J} _ {\mathrm {G R P O}} (\theta) = \mathbb {E} _ {(q, a) \sim \mathcal {D}, \{\tau_ {i} \} _ {i = 1} ^ {G} \sim \pi_ {\theta} ^ {\mathrm {o l d}} (\cdot | q)} \left[ \frac {1}{G} \sum_ {i = 1} ^ {G} \frac {1}{\sum_ {t = 1} ^ {| \tau_ {i} |} \mathbb {I} (\tau_ {i , t})} \sum_ {t = 1} ^ {| \tau_ {i} |} \mathbb {I} (\tau_ {i, t}) \cdot \min \left(\frac {\pi_ {\theta} (\tau_ {i , t} | \tau_ {i , <   t})}{\pi_ {\theta} ^ {\mathrm {o l d}} (\tau_ {i , t} | \tau_ {i , <   t})} \hat {A} _ {i, t}, \right. \right. \\ \left. \operatorname {c l i p} \left(\frac {\pi_ {\theta} \left(\tau_ {i , t} \mid \tau_ {i , <   t}\right)}{\pi_ {\theta} ^ {\mathrm {o l d}} \left(\tau_ {i , t} \mid \tau_ {i , <   t}\right)}, 1 - \epsilon , 1 + \epsilon\right) \hat {A} _ {i, t}\right) - \beta \mathbb {D} _ {K L} \left[ \pi_ {\theta} | | \pi_ {\mathrm {r e f}} \right]\left. \right] \tag {4} \\ \end{array}
$$

$$
\mathbb {D} _ {K L} \left[ \pi_ {\theta} \right| | \pi_ {\mathrm {r e f}} ] = \frac {1}{\sum_ {t = 1} ^ {| \tau |} \mathbb {I} (\tau_ {t})} \sum_ {t = 1} ^ {| \tau |} \mathbb {I} (\tau_ {t}) \cdot \left(\frac {\pi_ {\mathrm {r e f}} (\tau_ {t} | \tau_ {<   t})}{\pi_ {\theta} (\tau_ {t} | \tau_ {<   t})} - \log \frac {\pi_ {\mathrm {r e f}} (\tau_ {t} | \tau_ {<   t})}{\pi_ {\theta} (\tau_ {t} | \tau_ {<   t})} - 1\right) \tag {5}
$$

where $\epsilon$ and $\beta$ are set to 0.2 and 0.01 respectively, and the other hyperparameters are the same as those in DAPO training.

# G Case Study

Table 15 and Table 16 present two examples illustrating the episodic and semantic memories generated during memorization.

Table 17 presents a complete generation trajectory in the control.

Video (Illustrated as 12 frames)

(Continued on next page)

![](images/30d56fa50228ec5df0d1f47404ea02354d486f109ca3cf32bda9a8edea776112.jpg)

![](images/9250d8946f462cd9979c17c5e85e186f720933e040e7903ed80efc3239138721.jpg)

![](images/a8ccd598277867c4a4ebac6f804f1f0484bd85123c05bcaea9f438a196b37f85.jpg)

![](images/55456f9dde995ebddc236159d4f0cde28ba8615d77ef4a965f85e52e7b68e757.jpg)

![](images/52b1a179ab2e5254d86b2315b22e7527119c8fc9d63fdaa82615a4077ad5f6d3.jpg)

![](images/b35bc6acbaf2668991007a03b648df38a73081d5d6b87d0fd6a67e19aef6fedd.jpg)

![](images/fac096fee89e48b91e2e37327c3c57404a47ade7cf56826dc81d97f42a5bf67e.jpg)

![](images/4ca4f6729f01417baddbaa8b3b2aad0e08f3577a81ef45bff4353c47d7281d7e.jpg)

![](images/5bfbd54c6742c32a073fdde09ec48aedf8d40b05811e2da87c717345b7894efe.jpg)

![](images/285e5556a0bdd3708129fcd4f9443166b65519107009555c2ccf72095171409c.jpg)

![](images/6f2747dc7df92675c873a70f29277b9997598752b4c201a297f7795cd3be2b50.jpg)

![](images/cb49c372efa22d849071bf3b498a29dcb47ad00ca084ffd279e9cca1d30a8e2e.jpg)

# Faces

![](images/e04c8d92b058e85fef7bd1cdb1644c53da2a250a8ad3ed0058df68df8c6afe09.jpg)  
<face_1413>

![](images/9519317280803e53a78feed1b053dbb41ab6f9c2fe086e8f0918ff3b70b3e644.jpg)  
<face_1412>

# Dialogues

• <voice_1>: (00:03-00:12) Every, but I feel like every sibling dynamic is different. I was the youngest and I just took it. I just took it. I didn’t have the aggression to fight back like   
• <voice_2>: (00:00-00:02) these experiences through my children, (00:11-00:16) close to, uh, it’s neck and neck. Yeah, they they go after each other. (00:24-00:29) we’re gonna have the fish coming to the fish feed and then we’re gonna have ducks coming too. It’s gonna be a   
• <voice_812>: (00:17-00:23) Wow. So we’re gonna we’re gonna go fishing at the pond. We’re gonna I’m gonna get a fish feeder and we’re gonna have

# memory-gemini-prompt

# Episodic memory

• <face_1412> is wearing a light blue t-shirt with a graphic design, a black baseball cap, and mirrored sunglasses.   
• He is standing in a backyard with a brick house, a wooden deck, a fence, and some trees.   
• <voice_2> says he’s reliving experiences through his children.   
• A woman, presumably his wife, walks by in the background carrying some items.   
• <voice_1> says every sibling dynamic is different and that she was the youngest and just took it.   
• <voice_2> says the children are neck and neck and go after each other.   
• <face_1412> and the woman are holding small yellow ducklings.   
• <voice_812> says they’re going fishing at the pond and will get a fish feeder.   
• Two children are playing in the backyard with squirt guns.   
• <voice_812> says they’ll have fish coming to the fish feed and ducks coming too.

# Semantic memory

• <face_1412> appears to be a father who enjoys spending time with his family and doing outdoor activities.   
• <face_1412> and <face_1413> have a casual and comfortable relationship.   
• The video depicts a family preparing for a fishing outing at their pond and discussing their children’s sibling dynamics.   
• The setting is a rural or suburban home with a large backyard and a pond.

# memory-7b-sft (Ours)

# Episodic memory

• <face_1412> wears a white t-shirt with a green and yellow graphic design, a black baseball cap, and green mirrored sunglasses.

(Continued on next page)

• <face_1412> is outdoors on a sunny day, in a grassy area with a brick building and a wooden fence in the background.   
• <face_1412> speaks to the camera, saying, "these experiences through my children, which is"   
<face_1413> joins <face_1412>, wearing a white tank top and denim shorts.   
• <voice_1> says, "Every, but I feel like every sibling dynamic is different. I was the youngest and I just took it. I just took it. I didn’t have the aggression to fight back like"   
• <face_1412> points a finger upwards.   
• <voice_2> says, "close to, uh, it’s neck and neck. Yeah, they they go after each other."   
• <face_1413> and <face_1412> hold two small, fluffy yellow ducklings.   
The ducklings are placed on the grass near a black pet carrier.   
<face_1413> kneels by the pet carrier, holding a yellow duckling.   
• <voice_812> says, "Wow. So we’re gonna we’re gonna go fishing at the pond. We’re gonna I’m gonna get a fish feeder and we’re gonna have"   
• <face_1412> gestures with his hand.   
• <voice_2> says, "we’re gonna have the fish coming to the fish feed and then we’re gonna have ducks coming too. It’s gonna be a"   
• A child in a blue outfit is visible in the background.

# Semantic memory

Equivalence: <face_1412>, <voice_812>   
Equivalence: <face_1412>, <voice_2>   
Equivalence: <face_1413>, <voice_1>   
• <face_1412> exhibits a relaxed and informal demeanor.   
• <face_1412> displays an interest in outdoor activities and family-oriented experiences.   
• <face_1413> demonstrates a nurturing and caring personality.   
<voice_1> appears to be reflective and analytical when discussing personal experiences.   
<voice_2> expresses enthusiasm and anticipation for future activities.   
<voice_812> seems knowledgeable about pet care and enjoys engaging in outdoor recreational activities.   
• <face_1412> and <face_1413> appear to have a comfortable and supportive relationship, likely romantic partners or close family members.   
• <voice_1> and <face_1412> share a close bond, possibly as parent and child, marked by open communication and shared experiences.   
• The video depicts a slice-of-life scene centered around family interaction and leisure activities.   
• The narrative progresses from a reflective discussion about sibling relationships to a forward-looking anticipation of shared activities.   
• The overall tone is lighthearted and positive, emphasizing the joys of family connection and shared experiences.   
• The video showcases a blend of indoor and outdoor settings, creating a dynamic visual narrative.   
The video likely belongs to the family vlog genre, documenting everyday life and experiences.   
The video showcases a typical suburban or rural setting with a house, fence, and outdoor space.   
• Keeping ducklings requires specific pet care knowledge, including appropriate housing and feeding.   
• The video implies the cultural norm of family members engaging in shared recreational activities.   
• Fishing is a popular outdoor leisure activity often enjoyed in natural settings like ponds.   
• The video suggests the practice of using a fish feeder to attract and manage fish populations in a pond.

Table 15 Comparison of memory generations between memory-7b-sft and memory-gemini-prompt for case KHslnSzK2SU, clip 23:00-23:30, M3-Bench-web. Yellow highlights indicate additional important details provided by our model.

![](images/21158a2d775f0c06e0765db4133c8403b7c7200d93686d8a3f2370939c43174a.jpg)

![](images/0ae03e2c543a8ae436aeeb54291f97017258e56bf40974ae07e4bbdb6c97590b.jpg)  
Figure 4 Average scores (on training set) and accuracy (on dev set) curves during the DAPO training process. The smoothing method of the curve in the left figure is the exponential moving average(EMA) formula that aligns with the one used in WandB, and the smoothing weight is set to 0.9

# Video (Illustrated as 12 frames)

![](images/32b8d851812595b594ebcc3150116e4f996f7f682fd2af9e0fcd7cd9c16a4207.jpg)

![](images/1b481a853feea7213561dba5240856f4d1ea094c4e8593356e9f7243b714d8ac.jpg)

![](images/cba19cb97fda7c141af725d00d09d6efd073a3a433e16f3c8befcce893241a2a.jpg)

![](images/fb09d1ffc5d624014734cbc15ef2556d0a7ec3b2d0c47b54bbe6575ae1f9c06f.jpg)

![](images/dfbf39142ccb8bf09e03b8cbb33dd2e3f81d77e97b0be87568e72c66ac92ebdf.jpg)

![](images/d0b934122e0ca760ed750db417d6832b28cc8053d73a3f5c6525e3c07d8f86e9.jpg)

![](images/d2577dd03f5c5654876641375bd77bbeb8353dbd0eee6cd6caea33131a08d405.jpg)

![](images/63b5b1ec8b0630aac3200b0b0565e641b5a97d41569501b8583ab633f307c284.jpg)

![](images/b6c7b68232b456c6bb82dccf9ec38151cbf5bcbd65c388ab9408b177d232bf70.jpg)

![](images/fc9007294d5e0ad900401ec6e01b767aee29cc24bcf4da29e613743dc4541cba.jpg)

![](images/4ea0a4588c85c2f6d9780bf95034f7454c8c87369dc8d8723dd224412a25f271.jpg)

![](images/949620ea3e78053b300f411f4394df0dceb4df0bdd4e9faa04e35fea8d4386c5.jpg)

# Faces

![](images/e7fb9d769cb6e4fc5c1ae79393d4e09ede83bf0e9b0ee828287167c85c771d34.jpg)  
<face_5>

![](images/e087627d65ed7754a0c64caffa1134910e139ca9e0fa915bdd75d04263e9e79b.jpg)  
<face_4>

# Dialogues

• <voice_53>: (00:00-00:03) Robot, can you at least bring some fun games or puzzles we can play? 00:13-00:17) What am I supposed to do? Sometimes they come with no pieces. What am I supposed to o? (00:20-00:22) Well, what am I supposed to? I’m bored. I want to play a game.   
• <voice_204>: (00:04-00:08) Really? Puzzle is wasting time for you. You don’t like puzzles. 00:08-00:13) But every time you play puzzle, you miss many pieces of it, which I love. (00:23-00:27) I on’t know. Go out and play some games, some games outside.   
• <voice_316>: (00:18-00:20) So just leave it. Just don’t take it.

# memory-gemini-prompt

# Episodic memory

• <face_4> sits on a light-colored couch, facing <face_5>, who sits at a small table.   
• <face_ $_ { 5 > }$ wears a light pink sweatshirt, while <face_4> wears a dark shirt and dark pants.   
• <voice_53> asks <face_5> to bring some fun games or puzzles.   
• <face_ $. 5 >$ tells <voice_53> that puzzles are a waste of time.   
• <face_ $_ { 5 > }$ explains that <voice_ $. 5 3 >$ always loses puzzle pieces.   
• <voice_53> expresses frustration about missing puzzle pieces and asks what to do.   
• <face_ $. 5 >$ suggests <voice_53> just leave the puzzle incomplete.   
• <voice_53> complains of boredom and wanting to play a game.   
• <face_ $. 5 >$ suggests <voice_53> go outside and play.   
• <voice_53> asks if <face_5> dislikes Legos too, because Lego pieces can also get lost.

(Continued on next page)

# Semantic memory

• <face_4>/<voice_53> appears bored and frustrated.   
• <face_5>/<voice_204> appears slightly annoyed or dismissive.   
• <face_4>/<voice_53> and <face_5>/<voice_204> seem to have a casual, possibly familial relationship.   
• The interaction revolves around <face_4>/<voice_53>’s boredom and <face_5>/<voice_204>’s suggestions for alternative activities.   
• The setting appears to be a living room or family room.

# memory-7b-sft (Ours)

# Episodic memory

• <face_4> sits on a beige couch, wearing a black t-shirt and black pants.   
• <face_5> sits next to <face_4> on the couch, wearing a pink sweatshirt.   
A small round table with a red and white checkered tablecloth is placed between <face_4> and <face_5>.

On the table are a bottle of red liquid, a white cup, a notebook, and a pen.   
A vase with dried flowers and a decorative metal piece are on a small table behind the couch.   
• A projector and a small green figurine are visible in the background.   
• <voice_53> asks the robot to bring games or puzzles.   
• <voice_204> tells <voice_53> that puzzles are a waste of time and that <voice_53> doesn’t like puzzles.   
• <voice_204> says that <voice_53> always misses pieces when doing puzzles, which <voice_204> enjoys finding.

• <voice_53> expresses frustration, saying that sometimes puzzles come with no pieces.   
• <voice_204> suggests that <voice_53> just leave the puzzle if they don’t want to do it.   
• <voice_53> says they are bored and want to play a game.   
• <voice_204> suggests that <voice_53> go outside and play games.   
• <voice_53> asks if <voice_204> doesn’t like Legos either, because sometimes Lego pieces are missing.   
• <voice_204> responds with "I don’t know".

<face_ $_ { 5 > }$ gestures with their hands while speaking.   
<face_4> gestures with their hands while speaking.   
<face_5> writes in the notebook while speaking.

# Semantic memory

• Equivalence: <face_4>, <voice_53>   
Equivalence: <face_4>, <voice_316>   
• Equivalence: <face_5>, <voice_204>   
• <voice_53> appears to be easily bored and seeks immediate entertainment.   
<voice_204> displays a pragmatic and solution-oriented approach to problem-solving.   
<voice_204> seems to enjoy challenges and finding solutions, even in seemingly mundane tasks like puzzles.

<voice_53> expresses a preference for active recreation over quiet, puzzle-based activities.   
• <voice_204> and <voice_53> appear to have a comfortable, familiar dynamic, likely indicative of a close relationship.   
• The interaction between <voice_204> and <voice_53> suggests a playful banter, with <voice_204> teasing <voice_53> about their attitude.   
• <voice_204> takes on a mentoring or guiding role, offering suggestions and solutions to <voice_53>’s requests.   
• The scene depicts a common household scenario of differing preferences and mild conflict regarding leisure activities.   
• The narrative unfolds as a request for entertainment, followed by disagreement on suitable options and a suggestion for a more intellectually engaging activity.

(Continued on next page)

• The overall tone is casual and familiar, reflecting the dynamics of a comfortable, possibly familial or romantic, relationship.   
• The scene showcases a power dynamic where one individual attempts to influence the other’s choices, leading to a mild power negotiation.   
The video likely takes place in a private residence, possibly an apartment or a recently built home, given the modern interior design.   
• The presence of a projector suggests a focus on entertainment and media consumption within the household.   
The availability of puzzles and Lego sets implies a focus on recreational activities and potentially family-oriented entertainment.   
• The red liquid in the bottle is likely a type of soda or juice, commonly consumed as a refreshing beverage during leisure time.   
• The checkered tablecloth on the table signifies a casual dining or activity setup, possibly for a snack or a casual game session.   
• The dried flowers and decorative metal piece function as home decor, contributing to the overall aesthetic and ambiance of the space.   
• The small green figurine may be a collectible item or a decorative ornament, adding a personal touch to the room’s decor.

Table 16 Comparison of memory generations between memory-7b-sft and memory-gemini-prompt for living_room_22, clip 4:00-4:30, M3-Bench-robot. Yellow highlights indicate additional important details provided by our model.   

<table><tr><td>Question: Is Tomasz a person with rich imagination or someone who lacks imagination? 
Answer: A person with rich imagination.</td></tr><tr><td>system (system_prompt) 
You are given a question and some relevant knowledge. Your task is to reason about whether the provided 
knowledge is sufficient to answer the question. If it is sufficient, output [Answer] followed by the answer. 
If it is not sufficient, output [Search] and generate a query that will be encoded into embeddings for a 
vector similarity search. The query will help retrieve additional information from a memory bank. 
Question: What is Theresa Yeap&#x27;s attitude like in her explanation of emergency services?</td></tr><tr><td>user (instruction_prompt) 
Searched knowledge: {} 
Output the answer in the format: 
Action: [Answer] or [Search] 
Content: {content} 
If the answer cannot be derived yet, the {content} should be a single search query that would help retrieve 
the missing information. The search {content} needs to be different from the previous. 
You can get the mapping relationship between character ID and name by using search query such as: 
&quot;What is the name of &lt;character_i&gt;&quot; or &quot;What is the character id of {name}&quot;. 
After obtaining the mapping, it is best to use character ID instead of name for searching. 
If the answer can be derived from the provided knowledge, the {content} is the specific answer to the 
question. Only name can appear in the answer, not character ID like &lt;character_i&gt;.</td></tr><tr><td>assistant
&lt;think&gt;Okay, let&#x27;s see. The user is asking whether Tomasz is a person with rich imagination or someone 
who lacks imagination. The problem is that there&#x27;s no provided knowledge about Tomasz in the given 
context. The Searched knowledge is empty, so I don&#x27;t have any information to base an answer on.</td></tr></table>

(Continued on next page)

First, I need to check if the existing knowledge can answer the question. Since the knowledge is empty, there’s nothing to reference. The user mentioned that if the answer can’t be derived, I should output a search query. The instructions also say to use character IDs if possible, but since we don’t know Tomasz’s ID yet, I need to find that first.

So the first step is to find the character ID for Tomasz. The user suggested using a query like "What is the character id of Tomasz" to get that mapping. Once I have the ID, I can then search for information related to his imagination. But without the ID, any search might not be effective. Therefore, the initial search should be to find the character ID. Then, using that ID, look into his traits. Since the current knowledge is empty, the correct action is to search for the character ID first.</think>

Action: [Search]

Content: What is the character id of Tomasz

# user

Searched knowledge: {"CLIP_4": ["<character_4>’s name is Tomasz Patan.", "The man in the white shirt, now identified as Tomasz Patan, Founder & Chief Technology Officer of Jetson, speaks to the camera in a workshop setting."], "CLIP_5": ["<character_4>’s name is Patán.", "<character_ $2 >$ identifies <character_4> as Patán, the founder and CTO of Jetson, a company developing an eVTOL vehicle."], "CLIP_8": [" $<$ <character_2>’s name is Angus Whitley."], "CLIP_15": ["<character_0>’s name is Akshat Rathi.", "<character_0> wears glasses and has a beard."], "CLIP_17": [" $<$ character_5>’s name is Matthew Clark."], "CLIP_20": [" $<$ <character_3>’s name is Tom Muniz.", "<character_3> identifies himself as Tom Muniz, Chief Technology Officer of Archer Aviation."], "CLIP_21": ["<character_- 7> is likely a pilot or flight controller, given their interaction with air traffic control."], "CLIP_26": ["<character_2>’s name is Robin Riedel.", "<character_2>, wearing glasses and a light blue shirt, is identified as Robin Riedel, a partner at McKinsey & Company and co-leader of the McKinsey Center for Future Mobility.", "<character_2> is a certified commercial pilot."], "CLIP_31": ["The graphic then shows a user profile icon appearing, representing ’pilot requirements’."], "CLIP_36": [" $<$ character_2> is shown again in the library."], "CLIP_37": ["<character_1> displays a cautious and rule-abiding personality."], "CLIP_40": ["<character_12>’s name is Isabel Metz.", "<character_12> identifies the Hamburg airport on the map."], "CLIP_42": ["A name tag identifies $<$ character_8> as ’Bianca Schuchardt, Researcher, German Aerospace Center’."]}

instruction_prompt

# assistant

<think $\vartriangleright$ Okay, let’s see. The user wants to know if Tomasz has a rich imagination or lacks it. From the previous search, we found that Tomasz Patan is $<$ character_4>. Now, I need to check the provided knowledge to see if there’s any info about his imagination.

Looking through the CLIP entries for $<$ <character_4 $>$ (Tomasz Patan), the info given is about his role as Founder & CTO of Jetson, speaking in a workshop. There’s no mention of his imagination. Other clips mention other characters but not their traits related to imagination. Since there’s no data here about Tomasz’s imagination, I can’t derive an answer. The next step is to search for information on his imagination using his character ID. The query should be about his imagination, so maybe "Does <character_4> have rich imagination?" or similar. Need to make sure the query is different from previous ones. Let’s go with "What is $<$ <character_4>’s personality regarding imagination?" to retrieve relevant info.</think>

Action: [Search]

Content: What is <character_4>’s personality regarding imagination?

# user

Searched knowledge: {}

(The search result is empty. Please try searching from another perspective.)

instruction_prompt

(Continued on next page)

# assistant

<think $\ b >$ Okay, let’s see. The user is asking if Tomasz is a person with rich imagination or someone who lacks imagination. So far, we’ve tried searching and couldn’t find any info about his imagination. The first step was to get his character ID, which is $<$ <character_4>. Then, we tried searching for his personality regarding imagination, but that came back empty.

Since there’s no direct information about Tomasz’s imagination in the provided knowledge, I can’t derive an answer from the existing data. The user’s instructions say that if the answer can’t be derived yet, I should generate a new search query. The previous query was about his personality regarding imagination, which didn’t yield results. So I need to approach it from another angle.

Maybe instead of asking about his personality, I can look for any mentions of his traits in general. Perhaps there’s a clip where his characteristics are discussed, but it’s not related to imagination. Alternatively, maybe his role as a CTO or founder of a company could imply something about his creativity or innovation, which are related to imagination. However, the knowledge provided doesn’t mention anything about his traits beyond his professional role.

Given that the knowledge is still empty, I should create a new query that’s different from the previous one. Perhaps asking about his creative abilities or how others perceive his problem-solving skills. Alternatively, checking for any quotes or statements he made that might indicate his imaginative nature.

The key is to generate a query that’s specific enough to retrieve information if it exists. So maybe "What are $<$ <character_4>’s creative problem-solving methods?" or "How does $<$ <character_4> approach innovation in his work?" These queries might uncover information about his imagination through his work style or achievements.

Since the user wants the search query to be different from the previous ones, I’ll go with one of these alternatives. Let’s pick "What are $<$ <character_4 $>$ ’s creative problem-solving methods?" to see if that yields any relevant information about his imagination.</think>

Action: [Search]

Content: What are <character_ $4 >$ ’s creative problem-solving methods?

# user

Searched knowledge: {"CLIP_6": ["<character_4>, wearing a white polo shirt with the ’Jetson’ logo, stands in a room with a partially assembled aircraft frame behind him.", " $<$ <character_4 $>$ holds a clear plastic cup and gestures with his hands as he speaks.", "<character_4> says, ’test and we have lots of data from it that we had redundant sensors on the vehicle that day.’ ", "A person wearing a yellow vest labeled ’TEST TEAM’ adjusts a red propeller on the aircraft outdoors.", "Another person wearing a black jacket and white and orange sneakers steps onto a scale.", "The person in the black jacket, now wearing a Sparco racing suit and helmet, walks towards an open field.", " $<$ <character_4> says, ’So for 11 years, I’ve been doing drones with camera gimbals.’ ", "<character_4> continues, ’These drones were getting bigger and bigger.’ ", " $<$ <character_4> says, ’And at some point, I asked myself the question, what if we scaled it?’ ", "<character_4> puts on a Sparco helmet and fastens the chin strap.", "<character_4> adjusts the visor on his helmet.", " $<$ <character_4> buckles the seatbelt inside the aircraft.", " $<$ <character_4> says, ’So I could sit in there and pilot myself.’ ", "<character_4> sits in the aircraft as it takes off, raising one hand in a gesture.", "The aircraft is white and black with four propellers.", " $<$ character_4> is an engineer or designer involved in the development of the Jetson aircraft.", " $<$ <character_4> has a background in working with drones and camera gimbals, demonstrating expertise in aerial technology.", "<character_4> is innovative and forward-thinking, as evidenced by his interest in scaling drone technology for personal flight.","<character_4> possesses a hands-on, experimental approach to his work, as shown by his involvement in testing and data analysis.", " $<$ character_4> displays a sense of pride and enthusiasm for his work on the Jetson project.", "The video follows a narrative arc of innovation and realization, starting

(Continued on next page)

with the concept of scaling drone technology and culminating in a successful test flight.", "The video showcases a blend of technical detail and personal narrative, offering insights into both the engineering process and the driving force behind the project.", "The overall tone of the video is optimistic and forward-looking, emphasizing the potential of personal aerial vehicles.", "The video implicitly highlights the importance of data analysis and redundant sensor systems in ensuring flight safety and reliability.", "The video likely belongs to the genre of technology demonstration or promotional content, showcasing the Jetson aircraft.", "The setting includes both indoor and outdoor locations, suggesting a combination of design, testing, and operational phases of the project.", "The use of safety gear like helmets and vests indicates adherence to safety protocols in aviation and engineering environments.", "The presence of a test team suggests a structured approach to quality control and validation of the aircraft’s performance.", "The scale model aircraft implies a phased approach to development, starting with smaller prototypes before moving to full-sized vehicles.", "The video showcases procedural norms in aviation, such as preflight checks, helmet usage, and controlled testing environments.", "The Jetson aircraft represents a novel application of drone technology, aiming to bridge the gap between unmanned aerial vehicles and personal flight.", "The video implies a growing trend of personal aerial mobility, driven by technological advancements and innovative design.", "The video’s narrative structure follows a common format in technology demonstrations, starting with the problem statement (drones’ size limitations) and presenting a solution (the Jetson aircraft).", "The video’s focus on data analysis suggests a data-driven approach to design and optimization in the development of the Jetson aircraft."]} instruction_prompt

# assistant

<think $\triangleright$ Okay, let’s tackle this question. The user wants to know if Tomasz is a person with rich imagination or someone who lacks imagination. From the provided knowledge, we have a bunch of clips about Tomasz Patan (character_4). Let me go through the info again.

In CLIP_6, there’s a lot of detail about his work with drones and the Jetson aircraft. He mentions scaling drone technology for personal flight, which shows he’s thinking beyond the current applications. The notes say he’s innovative and forward-thinking, with a hands-on approach. He’s involved in testing and data analysis, and he’s enthusiastic about his work. The video’s tone is optimistic, highlighting his problem-solving methods and the development process.

The key points here are his background in drones, his idea to scale them into personal aircraft, and his active role in testing. The description explicitly states he’s innovative and forward-thinking. These traits are closely related to having a rich imagination. Since he’s taking existing technology (drones) and envisioning a new application (personal flight), that requires creativity and imagination. Also, the narrative of the video presents him as someone who asks "what if" questions, which is a hallmark of imaginative thinking.

I don’t see any information suggesting he lacks imagination. All the provided details support the idea that he’s imaginative. Therefore, the answer should be that Tomasz is a person with rich imagination.</think> Action: [Answer]

Content: Tomasz is a person with rich imagination.

Table 17 A complete generation trajectory in the control process.

# H Prompt Templates

# H.1 Prompt for Automatic Evaluator of M3-Bench

Table 18 presents the prompt used by GPT-4o to assess M3-Bench.

# The prompt for GPT-4o evaluation

You are provided with a question, a ground truth answer, and an answer from an agent model. Your task is to determine whether the ground truth answer can be logically inferred from the agent’s answer, in the context of the question.

Do not directly compare the surface forms of the agent answer and the ground truth answer. Instead, assess whether the meaning expressed by the agent answer supports or implies the ground truth answer. If the ground truth can be reasonably derived from the agent answer, return "Yes". If it cannot, return "No".

Important notes:

$\bullet$ Do not require exact wording or matching structure.   
$\bullet$ Semantic inference is sufficient, as long as the agent answer entails or implies the meaning of the ground truth answer, given the question.   
$\bullet$ Only return "Yes" or "No", with no additional explanation or formatting.

Input fields:

• question: the question asked   
• ground_truth_answer: the correct answer   
• agent_answer: the model’s answer to be evaluated

Now evaluate the following input:

Input:

• question: {question}   
• ground_truth_answer: {ground_truth_answer}   
• agent_answer: {agent_answer}

Output (‘Yes’ or ‘No’):

Table 18 Prompt used by GPT-4o to evaluate M3-Bench.

# H.2 Prompts for Socratic Models

Table 19 presents the prompt used in Socratic Models baselines. Through prompt engineering, we find that placing the question after the long context (e.g., video detailed descriptions) enhances the model’s ability to retain the question and focus on relevant information, leading to improved answer accuracy. Accordingly, in our Socratic Models experiments, we adopt this approach by appending the question to the end of the retrieved clip descriptions during the RAG-based QA stage.

# Caption Generation Prompt (Gemini-1.5-Pro, Qwen-2.5-Omni)

You are an advanced video description generator tasked with providing a detailed, cohesive description of a video clip.

Follow these high-level principles to ensure your output is accurate and meaningful:

1. Focus on Observable Content.   
2. Provide Context for the Environment and Timing.   
3. Incorporate Audio Dialogue Information.

You are provided with a current video clip. (GPT-4o, Qwen2.5-VL-7b Variant: You are provided with 15 key frames from a current video clip and audio text information <a list where each item represents a speech segment dict with the following fields: start time, end time, asr. The time information is the time in the current clip and not the global time $>$ .)

(Continued on next page)

# Your Task:

Based on the video clip, generate a detailed and cohesive description of the video clip. The description should focus on the entire event, incorporating all relevant aspects of the characters, their actions, spoken dialogue, and interactions in a narrative format. The description should include (but is not limited to) the following categories:

1. Characters’ Appearance: Describe the characters’ appearance, including their clothing, facial features, body language, or any distinguishing characteristics that are noticeable in the frames.   
2. Characters’ Actions & Movements: Describe specific gestures, movements, or interactions performed by the characters. Include both major and minor actions that contribute to the overall scene, emphasizing any transitions between different actions.   
3. Characters’ Spoken Dialogue: Use the provided audio dialogue information to accurately transcribe or summarize the dialogue spoken by the characters. Include emotional tone, volume, or context if relevant (e.g., shouting, whispering, laughing).   
4. Characters’ Contextual Behavior and Attributes: Describe the characters’ roles in the scene, their emotional states, motivations, or relationships with other characters. Highlight any conflict, bonding, or change in dynamics.   
5. Environmental Context: Include relevant details about the environment where the scene takes place. Describe the physical location, setting, lighting, or any other environmental factors that affect the atmosphere or context of the video clip.   
6. Temporal Context: Provide information about the timing of events within the scene. Describe the natural progression of time (e.g., morning, afternoon, evening) or any time-sensitive elements that contribute to the unfolding of the events.

# Strict Requirements:

• Do not use generic descriptions, inferred names, or pronouns to refer to characters (e.g., "he," "they," "the man").   
$\bullet$ The generated descriptions of the video clip should include every detail observable in the frames and mentioned in the audio dialogues. (GPT-4o, Qwen2.5-VL-7b Variant: • The generated descriptions of the video clip should include every detail observable in the frames and mentioned in the audio dialogues.)   
$\bullet$ Pay close attention to any introduction of characters’ names, titles, or other identifiers provided in the frames or audio.   
$\bullet$ Whenever possible, include natural time expressions and physical location cues in the descriptions to improve contextual understanding. These should be based on inferred situational context (e.g., "in the evening at the dinner table," "early morning outside the building").   
$\bullet$ Include relevant background, common knowledge and environmental factors when needed (e.g., location, weather, setting) to provide a fuller understanding of the context.   
$\bullet$ Maintain a natural, narrative flow in the description, ensuring that it reads like a coherent summary of the events in the video.   
• Remember you are looking at key frames and audio dialogue information, not the full video, so focus on what can be observed from these specific materials. (GPT-4o, Qwen2.5-VL-7b Variant: • Remember you are looking at key frames and audio dialogue information, not the full video, so focus on what can be observed from these specific materials.)

Example Output:

"As Margaret returns with the teapot, Tom stands up to help her pour the tea, gesturing politely as she hands him a cup. Margaret sits back down. Margaret leans forward slightly, her hands resting on the table, and after a moment of silence, she speaks again, her voice steady but filled with a hint of urgency. Tom listens closely, his brow furrowing slightly as he takes in her words. He responds quietly, nodding slowly as he processes the information."

# RAG Answer Prompt (GPT-4o)

Based on the following video description, answer the question as concisely as possible. Provide only the direct answer without explanations or reasoning.

Question: {question}

Relevant Video Clip Captions: {retrived_clips}

Answer:

Table 19 The prompts for the experiments of the Socratic Models. For models that take either raw video (gemini-1.5-pro, Qwen2.5-Omni-7b) input or video frames with ASR transcripts (GPT4o, Qwen2.5-VL-7b), the description generation prompt has minor differences, which are indicated in italicized parentheses.

# H.3 Prompts for M3-Agent

Table 20 shows the prompt used by Gemini-Agent and Gemini-GPT4o-Hybrid during memorization. Table 21 shows the prompt used by Gemini-Agent and Gemini-GPT4o-Hybrid during control.

Table 22 shows the prompt used by M3-Agent during the control process. The system prompt at the beginning of each session specifies the overall task objectives. The instruction prompt appended at the start of each round provides the question and detailed guidance. The last-round prompt, used only in the final round, signals the agent that it is the final opportunity to respond.

# Memorization Prompt ( memory-gemini-prompt, memory-7b-prompt )

You are given a video along with a set of character features. Each feature is either:

• Face: a single video frame with a bounding box, or   
• Voice: one or more speech segments, each containing start_time (MM:SS), end_time (MM:SS) and asr (transcript).

Every feature has a unique ID enclosed in angle brackets (e.g. <face_1>, <voice_2>).

Your Tasks (produce both in the same response) :

1. Episodic Memory (the ordered list of atomic captions)

$\bullet$ Using the provided feature IDs, generate a detailed and cohesive description of the current video clip. The description should capture the complete set of observable and inferable events in the clip. Your output should incorporate the following categories (but is not limited to them):

(a) Characters’ Appearance: Describe the characters’ appearance, such as their clothing, facial features, or any distinguishing characteristics.   
(b) Characters’ Actions & Movements: Describe specific gesture, movement, or interaction performed by the characters.   
(c) Characters’ Spoken Dialogue: Quote—or, if necessary, summarize—what are spoken by the characters.   
(d) Characters’ Contextual Behavior: Describe the characters’ roles in the scene or their interaction with other characters, focusing on their behavior, emotional state, or relationships.

2. Semantic Memory (the ordered list of high-level thinking conclusions)

• Produce concise, high-level reasoning-based conclusions across five categories: (a) Equivalence Identification – Identify which face and voice features refer to the same character. Use the exact format: Equivalence: $<$ <face_x $>$ , $<$ <voice_y $>$ . Include as many confident matches as possible.   
(b) Character-level Attributes – Infer abstract attributes for each character, such as: Name (if explicitly stated), Personality (e.g., confident, nervous), Role/profession (e.g., host, newcomer), Interests or background (when inferable), istinctive behaviors or traits (e.g., speaks formally, fidgets). Avoid restating visual facts—focus on identity construction.   
(c) Interpersonal Relationships & Dynamics – Describe the relationships and interactions between characters: Roles (e.g., host-guest, leader-subordinate), Emotions or tone (e.g., respect, tension), Power dynamics (e.g., who leads), Evidence of cooperation, exclusion, conflict, etc.   
(d) Video-level Plot Understanding – Summarize the scene-level narrative, such as: Main event or theme, Narrative arc or sequence (e.g., intro $\longrightarrow$ discussion reaction), Overall tone (e.g., formal, tense), Cause-effect or group dynamics.   
(e) Contextual & General Knowledge – Include general knowledge that can be learned from the video, such as: Likely setting or genre (e.g., corporate meeting, game show), Cultural/procedural norms, Real-world knowledge (e.g., "Alice market is pet-friendly"), Common-sense or format conventions.

Strict Requirements (apply to both sections unless noted)

1. If a character has a provided feature ID, refer to that character only with the ID (e.g. <face_1>, <voice_2 $>$ ).   
2. If no ID exists, use a short descriptive phrase (e.g. "a man in a blue shirt").   
3. Do not use "he," "she," "they," pronouns, or invented Names.   
4. Keep face/voice IDs consistent throughout.   
5. Describe only what is grounded in the video or obviously inferable.   
6. Include natural Time & Location cues and setting hints when inferable.   
7. Each Episodic Memory line must express one event/detail; split sentences if needed.   
8. Output English only.   
9. Output a Python list of sentences for each memory type.

Additional Rules for Episodic Memory

1. Do not mix unrelated aspects in one memory sentence.   
2. Focus on appearance, actions/movements, spoken dialogue (quote or summary), contextual behavior.

Additional Rules for Semantic Memory

1. For Equivalence lines, use the exact format: Equivalence: <face_x>, $<$ <voice_y $>$   
2. Do not repeat simple surface observations already in the captions.   
3. Provide only final conclusions, not reasoning steps.

Expected Output Format

Return the result as a single Python dict containing exactly two keys:

{

"episodic_memory": [

"In the bright conference room, <face_1> enters confidently, giving a professional appearance as he approaches $<$ <face_2> to shake hands.",

"<face_1> wears a black suit with a white shirt and tie. He has short black hair and wears glasses.", "<face_2>, dressed in a striking red dress with long brown hair.",

(Continued on next page)

"<face_2> smiles warmly and greets <face_1>. She then sits down at the table beside him, glancing at her phone briefly while occasionally looking up.",

"<voice_1> speaks to the group, ’Good afternoon, everyone. Let’s begin the meeting.’ His voice commands attention as the room quiets, and all eyes turn to him.",

"<face_2> listens attentively to $<$ <voice_1>’s words, nodding in agreement while still occasionally checking her phone. The atmosphere is professional, with the participants settling into their roles for the meeting.",

"<face_1> adjusts his tie and begins discussing the agenda, engaging the participants in a productive conversation." ],

"semantic_memory": [

"Equivalence: <face_1>, $<$ <voice_1>",

"<face_1 $>$ ’s name is David.",

"<face_1> holds a position of authority, likely as the meeting’s organizer or a senior executive.",

"<face_2> shows social awareness and diplomacy, possibly indicating experience in public or client-facing roles.",

" $<$ <face_1> demonstrates control and composure, suggesting a high level of professionalism and confidence under pressure.",

"The interaction between <face_1> and $<$ <face_2> suggests a working relationship built on mutual respect.",

"The overall tone of the meeting is structured and goal-oriented, indicating it is part of a larger organizational workflow." ] }

Please only return the valid python dict (which starts with $" \{ "$ and ends with $" \} "$ ) containing two string lists in "episodic_memory" and "semantic_memory", without any additional explanation or formatting.

Table 20 Memorization prompt for memory-gemini-prompt and memory-7b-prompt.

# Control Prompt

You are given a question and some relevant knowledge about a specific video. You are also provided with a retrieval plan, which outlines the types of information that should be retrieved from a memory bank in order to answer the question. Your task is to reason about whether the provided knowledge is sufficient to answer the question. If it is sufficient, output [ANSWER] followed by the answer. If it is not sufficient, output [SEARCH] and generate a query that will be encoded into embeddings for a vector similarity search. The query will help retrieve additional information from a memory bank that contains detailed descriptions and high-level abstractions of the video, considering the question, the provided knowledge, and the retrieval plan.

Your response should contain two parts:

1. Reasoning

• Analyze the question, the knowledge, and the retrieval plan.   
• If the current information is sufficient, explain why and what conclusions you can draw.   
• If not, clearly identify what is missing and why it is important.

2. Answer or Search

• [ANSWER]: If the answer can be derived from the provided knowledge, output [ANSWER] followed by a short, clear, and direct answer.   
• When referring to a character, always use their specific name if available.   
• Do not use ID tags like <character_{1}> or $< \mathrm { f a c e } _ { - } \{ 1 \} >$

(Continued on next page)

• [SEARCH]: If the answer cannot be derived yet, output [SEARCH] followed by a single search query that would help retrieve the missing information.

Instructions for [SEARCH] queries:

$\bullet$ Use the retrieval plan to inform what type of content should be searched for next. These contents should cover aspects that provide useful context or background to the question, such as character names, behaviors, relationships, personality traits, actions, and key events.   
$\bullet$ Use keyword-based queries, not command sentences. Queries should be written as compact keyword phrases, not as full sentences or instructions. Avoid using directive language like "Retrieve", "Describe", or question forms such as "What", "When", "How".   
$\bullet$ Keep each query short and focused on one point. Each query should target one specific type of information, without combining multiple ideas or aspects.   
$\bullet$ Avoid over-complexity and unnecessary detail. Do not include too many qualifiers or conditions. Strip down to the most essential keywords needed to retrieve valuable content.   
• The query should target information outside of the existing knowledge that might help answer the question.   
$\bullet$ For time-sensitive or chronological information (e.g., events occurring in sequence, changes over time, or specific moments in a timeline), you can generate clip-based queries that reference specific clips or moments in time. These queries should include a reference to the clip number, indicating the index of the clip in the video (a number from 1 to N, where a smaller number indicates an earlier clip). Format these queries as "CLIP_x", where x should be an integer that indicates the clip index. Note only generate clip-based queries if the question is about a specific moment in time or a sequence of events.   
$\bullet$ You can also generate queries that focus on specific characters or characters’ attributes using the id shown in the knowledge.   
$\bullet$ Make sure your generated query focus on some aspects that are not retrieved or asked yet. Do not repeatedly generate queries that have high semantic similarity with those generated before.

# Example 1:

Input:

Question: How did the argument between Alice and Bob influence their relationship in the story?

Knowledge:

```json
{
    {"query": "What happened during the argument between Alice and Bob?", "related memories": {"CLIP_2": ["face_1> and <face_2> are seen arguing in the living room." <face_1> raises her voice, and <face_2> looks upset.">
    ,"<face_1> accuses <face_2> of not listening to her."
} 
```

Output:

It seems that <face_1> and <face_2> are arguing about their relationship. I need to figure out the names of <face_1> and ${ < } \mathrm { f a c e } _ { - } 2 { > }$ .

[SEARCH] What are the names of <face_1> and $<$ <face_2>?

(Continued on next page)

# Example 2:

Input:

Question: How did the argument between Alice and Bob influence their relationship in the story?

```txt
Knowledge:   
{ "query": "What happened during the argument between Alice and Bob?", "related memories": {{ "CLIP_2": [ "<face_1> and <face_2> are seen arguing in the living room." "<face_1> raises her voice, and <face_2> looks upset." "<face_1> accuses <face_2> of not listening to her." ], }} } }, { {" query": "What are the names of <face_1> and <face_2>?" , "related memories": {{ "CLIP_1": [ "<face_1> says to <face_2>: 'I am done with you Bob'""", "<face_2> says to <face_1>: 'What about now, Alice'"""] }, }} } } ] 
```

Output:

It seems that content in CLIP_2 shows exactly the argument between Alice and Bob. To figure out how did the argument between Alice and Bob influence their relationship, I need to see what happened next in CLIP_3.

[SEARCH] What happened in CLIP_3?

Now, generate your response for the following input:

Question: {question}

Knowledge: {search_results}

Output:

# Control Prompt (last round)

You are given a question about a specific video and a dictionary of some related information about the video. Each key in the dictionary is a clip ID (an integer), representing the index of a video clip. The corresponding value is a list of video descriptions from that clip.

Your task is to analyze the provided information, reason over it, and produce the most reasonable and well-supported answer to the question.

# Output Requirements:

• Your response must begin with a brief reasoning process that explains how you arrive at the answer.   
• Then, output [ANSWER] followed by your final answer.   
• The format must be: Here is the reasoning... [ANSWER] Your final answer here.   
• Your final answer must be definite and specific — even if the information is partial or ambiguous, you must infer and provide the most reasonable answer based on the given evidence.   
$\bullet$ Do not refuse to answer or say that the answer is unknowable. Use reasoning to reach the best possible conclusion.

# Additional Guidelines:

• When referring to a character, always use their specific name if it appears in the video information.   
• Do not use placeholder tags like $<$ <character_1 $>$ or <face_1 $>$ .   
• Avoid summarizing or repeating the video information. Focus on reasoning and answering.   
• The final answer should be short, clear, and directly address the question.

# Input:

• Question: {question}   
• Video Information: {search_results}

# Output:

Table 21 Control prompt for Gemini-Agent and Gemini-GPT4o-Hybrid.   
Table 22 The prompts used by M3-Agent during the control process.   

<table><tr><td>system_prompt</td></tr><tr><td>You are given a question and some relevant knowledge. Your task is to reason about whether the provided knowledge is sufficient to answer the question. If it is sufficient, output [Answer] followed by the answer. If it is not sufficient, output [Search] and generate a query that will be encoded into embeddings for a vector similarity search. The query will help retrieve additional information from a memory bank.</td></tr><tr><td>Question:</td></tr><tr><td>instruction_prompt</td></tr><tr><td>Output the answer in the format: 
Action: [Answer] or [Search] 
Content: {content}</td></tr><tr><td>If the answer cannot be derived yet, the {content} should be a single search query that would help retrieve the missing information. The search {content} needs to be different from the previous. 
You can get the mapping relationship between character ID and name by using search query such as: 
&quot;What is the name of &lt;character_i&gt;&quot; or &quot;What is the character id of {name}&quot;. 
After obtaining the mapping, it is best to use character ID instead of name for searching. 
If the answer can be derived from the provided knowledge, the {content} is the specific answer to the question. Only name can appear in the answer, not character ID like &lt;character_i&gt;.</td></tr><tr><td>last_round_prompt</td></tr><tr><td>The Action of this round must be [Answer]. If there is insufficient information, you can make reasonable guesses.</td></tr></table>