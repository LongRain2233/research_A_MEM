# Large Language Diffusion Models

Shen $\mathbf { N i e ^ { 1 , 2 , 3 * \dagger } }$ Fengqi $\mathbf { Z } \mathbf { h } \mathbf { u } ^ { 1 , 2 , 3 * \dagger }$ Zebin You1,2,3† Xiaolu Zhang4‡ Jingyang Ou1,2,3

Jun $\mathbf { H } \mathbf { u } ^ { 4 \ddagger }$ Jun Zhou4 Yankai Lin1,2,3‡ Ji-Rong Wen1,2,3 Chongxuan Li1,2,3‡ §

1 Gaoling School of Artificial Intelligence, Renmin University of China

2 Beijing Key Laboratory of Research on Large Models and Intelligent Governance

3 Engineering Research Center of Next-Generation Intelligent Search and Recommendation, MOE

4 Ant Group

{nieshen,fengqizhu,chongxuanli}@ruc.edu.cn

# Abstract

The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised finetuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong scalability and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multiturn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.

# 1 Introduction

Large language models (LLMs) [1] fall entirely within the framework of generative modeling. Specifically, LLMs aim to capture the true but unknown language distribution $p _ { \mathrm { d a t a } } ( \cdot )$ by optimizing a model distribution $p _ { \theta } ( \cdot )$ through maximum likelihood estimation, or equivalently KL divergence minimization between the two distributions:

$$
\underbrace {\max  _ {\theta} \mathbb {E} _ {p _ {\text {d a t a}} (x)} \log p _ {\theta} (x) \Leftrightarrow \min  _ {\theta} \mathrm {K L} \left(p _ {\text {d a t a}} (x) \mid \mid p _ {\theta} (x)\right)} _ {\theta}. \tag {1}
$$

{zGenerative modeling principles

The predominant approach relies on the autoregressive modeling (ARM)—commonly referred to as the “next-token prediction” paradigm—to define the model distribution:

$$
p _ {\theta} (x) = p _ {\theta} \left(x ^ {1}\right) \prod_ {i = 2} ^ {L} p _ {\theta} \left(x ^ {i} \mid x ^ {1}, \dots , x ^ {i - 1}\right), \tag {2}
$$

{zAutoregressive formulation

![](images/ac473e0f763dc84a670e6b9c6c7fab9b2b293845cf7ebd53198c920549361029.jpg)

![](images/f503a422656336f2a4323a383bffcc628b73e46173882cf180a0c8cfb0154d27.jpg)  
Figure 1: Zero/Few-Shot Benchmarks. We scale LLaDA to 8B parameters from scratch and observe competitive zero/few-shot performance compared with strong autoregressive LLMs [6].

where $x$ is a sequence of length $L$ , and $x ^ { i }$ is the $i$ -th token. This paradigm has proven remarkably effective [2–5] and has become the foundation of current LLMs. Despite its widespread adoption, a fundamental question remains unanswered: Is the autoregressive paradigm the only path to achieving the core capabilities of LLMs, such as scalability, in-context learning, and instruction-following?

We argue that the answer is not a simple “yes”. The key insight overlooked previously is: It is the generative modeling principles (i.e., Eq. (1)), rather than the autoregressive formulation (i.e., Eq. (2)) itself, that fundamentally underpin the essential properties of LLMs.

In particular, we argue that scalability is primarily a consequence of the interplay between Transformers [7], model size, data size, and Fisher consistency5 [8] induced by the generative principles in Eq. (1), rather than a unique result of the ARMs in Eq. (2). The success of diffusion transformers [9, 10] on visual data [11] supports this claim. Furthermore, the instruction-following and in-context learning [4] capabilities appear to be intrinsic properties of all conditional generative models on structurally consistent linguistic tasks, rather than exclusive advantages of ARMs. In addition, while ARMs can be interpreted as a lossless data compressor [12, 13], any sufficiently expressive probabilistic model can achieve similar capabilities [14].

However, certain inherent limitations of LLMs can be directly attributed to their autoregressive nature. For instance, the left-to-right generation process restricts their ability to handle reversal reasoning tasks [15], highlighting a representative failure in the generalization capabilities of current models.

Motivated by these insights, we introduce LLaDA (Large Language Diffusion with mAsking) to investigate whether the capabilities exhibited by LLMs can emerge from generative modeling principles beyond ARMs, thereby addressing the fundamental question posed earlier. In contrast to traditional ARMs, LLaDA leverages a masked diffusion model (MDM) [16–20], which incorporates a forward data masking process and trains a mask predictor to approximate its reverse process. This design enables LLaDA to construct a model distribution with bidirectional dependencies and optimize a variational lower bound of its log-likelihood, offering a principled and previously unexplored perspective on the core capabilities of LLMs discussed above.

We adopt the standard pipeline of data preparation, pre-training, supervised fine-tuning (SFT), and evaluation, scaling LLaDA to an unprecedented language diffusion of size 8B. In particular, LLaDA 8B was pre-trained from scratch on 2.3 trillion tokens using 0.13 million H800 GPU hours, followed by SFT on 4.5 million pairs. Across diverse tasks, including language understanding, math, code, and Chinese, LLaDA demonstrates the following contributions:

• LLaDA scales effectively to a compute budget of $1 0 ^ { 2 3 }$ FLOPs, achieving comparable results to ARM baselines trained on the same data across six tasks, e.g., MMLU and GSM8K.

![](images/31c10cc57c733d3395c5e5e6060dc74994d9e00f38d3be20acf4e7e1d9ef23ae.jpg)

![](images/c34c702c5d8b7c720100357da88400ce2bd06e1ba0e6778148c8335044a8b10b.jpg)

![](images/3ec625220dcffd9d595069a45d3bad4b6c478c30dc5d0068c30373200a263e5e.jpg)  
Figure 2: Overview of LLaDA. (a) Pre-training. LLaDA is trained on text with random masks applied independently to all tokens at the same ratio $t \sim U [ 0 , 1 ]$ . (b) SFT. Only response tokens are possibly masked. (c) Sampling. LLaDA simulates a diffusion process from $t = 1$ (fully masked) to $t = 0$ (unmasked), predicting all masks simultaneously at each step with flexible remask strategies.

• The pre-trained LLaDA 8B Base surpasses LLaMA2 7B Base [21] on nearly all 15 standard zero/few-shot learning tasks while performing on par with LLaMA3 8B Base [6], showcasing effective in-context learning capability.   
• LLaDA significantly enhances the ability to follow instructions after SFT, as demonstrated in case studies such as multi-turn dialogue.   
• LLaDA effectively breaks the reversal curse [15] with consistent performance across forward and reversal tasks. Notably, it outperforms GPT-4o in a reversal poem completion task.

# 2 Approach

In this section, we introduce the probabilistic formulation6, along with the pre-training, supervised fine-tuning, and inference procedures for LLaDA, as illustrated in Fig. 2.

# 2.1 Probabilistic Formulation

Unlike ARMs in Eq. (2), LLaDA defines a model distribution $p _ { \theta } ( x _ { 0 } )$ through a forward process and a reverse process [16–20]. The forward process gradually masks tokens independently in $x _ { 0 }$ until the sequence is fully masked at $t = 1$ . For $t \in ( 0 , 1 )$ , the sequence $x _ { t }$ is partially masked, with each being masked with probability $t$ or remaining unmasked with probability $1 - t$ . The reverse process recovers the data distribution by iteratively predicting masked tokens as $t$ moves from 1 to 0.

The core of LLaDA is a mask predictor, a parametric model $p _ { \theta } ( \cdot | x _ { t } )$ that takes $x _ { t }$ as input and predicts all masked tokens (denoted as M) simultaneously. It is trained using a cross-entropy loss computed only on the masked tokens [18–20]:

$$
\mathcal {L} (\theta) \triangleq - \mathbb {E} _ {t, x _ {0}, x _ {t}} \left[ \frac {1}{t} \sum_ {i = 1} ^ {L} \mathbf {1} \left[ x _ {t} ^ {i} = \mathbf {M} \right] \log p _ {\theta} \left(x _ {0} ^ {i} \mid x _ {t}\right) \right], \tag {3}
$$

where $x _ { 0 }$ is a training sample, $t$ is a continuous random variable drawn uniformly from $[ 0 , 1 ]$ , $x _ { t }$ is sampled from the forward process and $L$ is the sequence length. The indicator function $1 [ \cdot ]$ ensures that the loss is computed only for masked tokens.

Once trained, we can simulate a reverse process (see Sec. 2.4 for details) parameterized by the mask predictor and define the model distribution $p _ { \theta } ( x _ { 0 } )$ as the marginal distribution induced at $t = 0$ . The loss function in Eq. (3) has been proven to be an upper bound on the negative log-likelihood of the model distribution, making it a principled objective for generative modeling:

$$
- \mathbb {E} _ {p _ {\text {d a t a}} (x _ {0})} \left[ \log p _ {\theta} (x _ {0}) \right] \leq \mathcal {L} (\theta). \tag {4}
$$

Notably, LLaDA employs a masking ratio that varies randomly between 0 and 1 while BERT [22] uses a fixed ratio. The subtle differences have significant implications, especially at scale: as shown in

Eq. (4), LLaDA is a principled generative model with the potential to perform in-context learning and instruction-following naturally, akin to LLMs. Moreover, its generative perspective implies strong scalability with large data and models as discussed in Sec. 1. In addition, MaskGIT [23] adopts a heuristic training objective, which misses the $\textstyle { \frac { 1 } { t } }$ term compared to Eq. (3), and lacks a theoretical link to maximum likelihood. We emphasize that it is precisely the theoretical foundation of maximum likelihood estimation that motivated us to scale discrete diffusion models for language modeling.

# 2.2 Pre-training

LLaDA employs a Transformer [7] as the mask predictor, similar to existing LLMs. However, LLaDA does not use a causal mask, as its formulation allows it to see the entire input for predictions.

We trained two variants of LLaDA with different sizes: 1B and 8B. We summarize the model architecture of LLaDA 8B and LLaMA3 8B [6] here, and details are provided in Appendix B.2. We have ensured consistency in most hyperparameters while making several necessary modifications. We use vanilla multi-head attention instead of grouped query attention [24] for simplicity, as LLaDA is incompatible with KV caching, resulting in a different number of key and value heads. Consequently, the attention layer has more parameters, and we reduce the FFN dimension to maintain a comparable model size. Additionally, the vocabulary size differs due to a tokenizer [4] adapted on our data.

The LLaDA model is pre-trained on a dataset comprising 2.3 trillion (T) tokens, adhering to a data protocol that aligns closely with existing LLMs [25, 26], without the incorporation of any special techniques. The data are derived from online corpora, with low-quality content filtered through manually designed rules and LLM-based approaches. Beyond general text, the dataset encompasses high-quality code, math, and multilingual data. Please refer to Appendix B.1 for more details about datasets. The mixing of data sources and domains is guided by scaled-down ARMs. The pre-training process utilizes a fixed sequence length of 4096 tokens, incurring a total computational cost of 0.13 million H800 GPU hours, similar to ARMs of the same scale and dataset size.

For a training sequence $x _ { 0 }$ , we randomly sample $t \in [ 0 , 1 ]$ , mask each token independently with the same probability $t$ to obtain $x _ { t }$ (see Fig. 2 (a)) and estimate Eq. (3) via the Monte Carlo method for stochastic gradient descent training. In addition, following Nie et al. [27], to enhance the ability of LLaDA to handle variable-length data, we set $1 \%$ of the pre-training data to a random length that is uniformly sampled from the range [1, 4096].

We adopted the Warmup-Stable-Decay [28] learning rate scheduler to monitor the training progress without interrupting continuous training. Specifically, we linearly increased the learning rate from 0 to $4 \times 1 0 ^ { - 4 }$ over the first 2000 iterations and maintained it at $4 \times 1 0 ^ { - 4 }$ . After processing 1.2T tokens, we decayed the learning rate to $1 \times 1 0 ^ { - 4 }$ and held it constant for the next $0 . 8 \mathrm { T }$ tokens to ensure stable training. Finally, we linearly reduced the learning rate from $1 \times 1 0 ^ { - 4 }$ to $1 \times 1 0 ^ { - 5 }$ for the last $0 . 3 \mathrm { T }$ tokens. Furthermore, we utilized the AdamW optimizer [29] with a weight decay of 0.1, a batch size of 1280, and a local batch size of 4 per GPU. The 8B experiment was executed once, without any hyperparameter tuning.

# 2.3 Supervised Fine-Tuning

We enhance the capability of LLaDA to follow instructions by supervised fine-tuning (SFT) with paired data $( p _ { 0 } , r _ { 0 } )$ , where $p _ { 0 }$ is the prompt and $r _ { 0 }$ denotes the response. This is the simplest and most basic post-training method for LLMs. Technically, this requires to model the conditional distribution $p _ { \theta } ( r _ { 0 } \bar { | } p _ { 0 } )$ instead of $p _ { \theta } ( x _ { 0 } )$ in pre-training.

The implementation is similar to pre-training. As shown in Fig. 2 (b), we leave the prompt unchanged and mask the tokens in the response independently, as done for $x _ { 0 }$ . Then, we feed both the prompt and the masked response $r _ { t }$ to the pre-trained mask predictor to compute the loss for SFT:

$$
- \mathbb {E} _ {t, p _ {0}, r _ {0}, r _ {t}} \left[ \frac {1}{t} \sum_ {i = 1} ^ {L ^ {\prime}} \mathbf {1} \left[ r _ {t} ^ {i} = \mathbf {M} \right] \log p _ {\theta} \left(r _ {0} ^ {i} \mid p _ {0}, r _ {t}\right) \right], \tag {5}
$$

where $L ^ { \prime }$ denotes a dynamic length specified later, and all other notations remain the same as before.

Note that this approach is fully compatible with pre-training. Essentially, the concatenation of $p _ { 0 }$ and $r _ { 0 }$ can be treated as clean pre-training data $x _ { 0 }$ , while the concatenation of $p _ { 0 }$ and $r _ { t }$ serves as the

masked version $x _ { t }$ . The process is identical to pre-training, with the only difference being that all masked tokens happen to appear in the $r _ { 0 }$ portion.

The LLaDA 8B model undergoes SFT on a dataset comprising 4.5 million pairs. Consistent with the pre-training process, both data preparation and training follow the SFT protocols utilized in existing LLMs [25, 26], without introducing any additional techniques to optimize LLaDA’s performance. The dataset spans multiple domains, including code, mathematics, and instruction-following. We append $| \mathrm { E O S } |$ tokens to the end of short pairs in each mini-batch to ensure equal lengths across all data. We treat $| \mathrm { E O S } |$ as a normal token during training and remove it during sampling, enabling LLaDA to control the response length automatically. Please refer to Appendix B.1 for more details.

We train for 3 epochs on the SFT data using a similar schedule to the pre-training phase. The learning rate is linearly increased from 0 to $2 . 5 \times 1 0 ^ { - 5 }$ over the first 50 iterations and then kept constant. During the final $1 0 \%$ of iterations, it is linearly reduced to $2 . 5 \times 1 0 ^ { - 6 }$ . Additionally, we set the weight decay to 0.1, the global batch size to 256, and the local batch size to 2 per GPU. The SFT experiment was executed once, without any hyperparameter tuning.

# 2.4 Inference

As a generative model, LLaDA can sample new text and evaluate the likelihood of candidate text in a diffusion manner instead of the left-to-right autoregressive fashion.

We begin with the reverse generation process. As illustrated in Fig. 2 (c), given a prompt $p _ { 0 }$ , we discretize the reverse process to sample from the model distribution $p _ { \theta } ( r _ { 0 } | p _ { 0 } )$ , starting from a fully masked response. The total number of sampling steps is a hyperparameter, which naturally provides LLaDA with a trade-off between efficiency and sample quality, as analyzed in Sec. 3.3. We employ uniformly distributed timesteps by default. In addition, the generation length is also treated as a hyperparameter, specifying the length of the fully masked sentence at the beginning of the sampling process. After generation, tokens appearing after the $| \mathrm { E O S } |$ token are discarded. As detailed in Appendix B.5, since both pre-training and SFT are conducted using datasets with variable lengths, the final results are insensitive to this length hyperparameter.

At an intermediate step from time $t \in ( 0 , 1 ]$ to $s \in [ 0 , t )$ , we feed both $p _ { 0 }$ and $r _ { t }$ into the mask predictor and predict all masked tokens simultaneously. Subsequently, we remask $\frac { s } { t }$ of the predicted tokens in expectation to obtain $r _ { s }$ , ensuring that the transition of the reverse process aligns with the forward process for accurate sampling [18–20]. In principle, the remasking strategy should be purely random. However, inspired by the annealing tricks of sampling in LLMs [4, 30], we adopt a low-confidence remasking strategy, where $\frac { s } { t }$ of predicted tokens with the lowest confidence are remarked based on the predictions, same as the approach of Chang et al. [23].

We mention that LLaDA enables flexible sampling. In particular, it supports autoregressive and block diffusion [31] sampling directly after the pre-training or SFT processes described above, without requiring any further modifications or training. We provide a detailed analysis in Appendix B.4. Nevertheless, the diffusion sampling (i.e., the reverse generation process) yields the best performance and is adopted as the default throughout this paper, especially for all experiments presented in Sec. 3.

For conditional likelihood evaluation, we can naturally utilize the upper bound in Eq. (5). However, we find that the following equivalent form [20] exhibits lower variance and is more stable:

$$
- \mathbb {E} _ {l, r _ {0}, r _ {l}} \left[ \frac {L}{l} \sum_ {i = 1} ^ {L} \mathbf {1} \left[ r _ {l} ^ {i} = \mathbf {M} \right] \log p _ {\theta} \left(r _ {0} ^ {i} \mid p _ {0}, r _ {l}\right) \right], \tag {6}
$$

where $L$ is the sequence length of $r _ { 0 }$ , $l$ is uniformly sampled from $\{ 1 , 2 , \ldots , L \}$ , and $r _ { l }$ is obtained by uniformly sampling $l$ tokens from $r _ { 0 }$ without replacement for masking.

We present the training and inference algorithms, along with theoretical details, in Appendix A.

# 3 Experiments

We evaluate the scalability, instruction-following, and in-context learning capabilities of LLaDA on standard benchmarks, followed by analyses and case studies to provide a comprehensive assessment.

![](images/d0433b0631444f4584742443d4f434d7b2b984ab582d245f4bd6ca744826bc18.jpg)

![](images/6329385a7152e8854cd8a4b9278f77b67286f95ea4e3d0717a99b6bfc524d108.jpg)

![](images/4e11fff4ed0ec9216875d28bf60611d9b3884155f9721617382792a2171a53cb.jpg)

![](images/1b1a0bc58ad9a7627d41f35fa33215f59cbe342dca7ed52dc597f13176e5ddb9.jpg)

![](images/e5ea8283aee659abdb9c3fc02f9f8da8a3d201fe1c1f7c0d0605ec623837506f.jpg)

![](images/f67eb04b6bc83de8df883179835a3468f790d18ecff16850122212a0c38c0f71.jpg)  
Figure 3: Scalability of LLaDA. We evaluate the performance of LLaDA and our ARM baselines trained on the same data across increasing pre-training computational FLOPs. LLaDA exhibits strong scalability, matching the overall performance of ARMs on six tasks.

# 3.1 Scalability of LLaDA on Language Tasks

We first investigate the scalability of LLaDA on downstream tasks in comparison with the ARM baselines we constructed. Specifically, at the 1B scale, we ensured that LLaDA and ARM shared the same architecture, data, and all other configurations. At larger scales, we also report results for LLaDA and ARM models of slightly different sizes trained on the same data due to resource limitations. Please refer to Appendix B.2 for more details. We use the pre-training computational cost as a unified scaling metric. For evaluation, we focused on six standard and diverse tasks.

Fig. 3 shows that LLaDA demonstrates impressive scalability, with its overall trend highly competitive with ARMs. Notably, on tasks such as MMLU and GSM8K, LLaDA exhibits even stronger scalability. Even on relatively weaker tasks like PIQA, the performance gap with ARMs narrows as scale increases. To account for the influence of outliers, we opted not to fit quantitative curves, avoiding potential misinterpretation. Nevertheless, the results clearly demonstrate the scalability of LLaDA.

Considering LLaDA’s advantages on certain benchmarks, we hypothesize that this performance gain stems from a key architectural difference: while autoregressive models optimize only left-to-right conditional probabilities, LLaDA is trained to consider multiple conditioning directions, as detailed in Appendix A.2, which may offer greater flexibility and lead to better generalization. This hypothesis is motivated by LLaDA’s strong performance on reversal reasoning in Sec. 3.3 and the ablation studies on sampling strategies in Appendix B.4.

Nie et al. [27] suggests that MDM requires 16 times more computation than ARM to achieve the same likelihood. However, key differences make our findings more broadly applicable. In particular, likelihood is a relatively indirect metric for downstream task performance, and diffusion optimizes a bound of the likelihood, making it not directly comparable to ARM. Additionally, we extended the scaling range from $1 0 ^ { 1 8 } \sim 1 0 ^ { 2 0 }$ FLOPs in Nie et al. [27] to $1 0 ^ { 2 0 } \sim 1 0 ^ { 2 3 }$ FLOPs in this work.

# 3.2 Benchmark Results

To comprehensively evaluate the in-context learning and instruction-following capabilities of LLaDA 8B, we conducted detailed comparisons with existing LLMs [6, 21, 25, 26, 32, 33] of similar scale. Task selection and evaluation protocols followed existing studies, covering popular benchmarks in general tasks, mathematics, code, and Chinese. Further details are provided in Appendix B.6. For a more direct comparison, we re-evaluated representative LLMs [6, 21] in our implementation.

As shown in Tab. 1, after pretraining on 2.3T tokens, LLaDA 8B Base demonstrates remarkable performance, surpassing LLaMA2 7B Base on nearly all tasks, and is overall competitive with LLaMA3 8B Base. LLaDA shows advantages in math and Chinese tasks. We conjecture that the

Table 1: Benchmark Results of Pre-trained LLMs. ∗ indicates that models are evaluated under the same protocol, detailed in Appendix B.6. Results indicated by † and ¶ are sourced from Yang et al. [25, 26] and Bi et al. [32] respectively. The numbers in parentheses represent the number of shots used for in-context learning. “-” indicates unknown data.   

<table><tr><td></td><td>LLaDA 8B*</td><td>LLaMA3 8B*</td><td>LLaMA2 7B*</td><td>Qwen2 7B†</td><td>Qwen2.5 7B†</td><td>Mistral 7B†</td><td>Deepseek 7B¶</td></tr><tr><td>Model</td><td>Diffusion</td><td>AR</td><td>AR</td><td>AR</td><td>AR</td><td>AR</td><td>AR</td></tr><tr><td>Training tokens</td><td>2.3T</td><td>15T</td><td>2T</td><td>7T</td><td>18T</td><td>-</td><td>2T</td></tr><tr><td colspan="8">General Tasks</td></tr><tr><td>MMLU</td><td>65.9 (5)</td><td>65.4 (5)</td><td>45.9 (5)</td><td>70.3 (5)</td><td>74.2 (5)</td><td>64.2 (5)</td><td>48.2 (5)</td></tr><tr><td>BBH</td><td>49.7 (3)</td><td>62.1 (3)</td><td>39.4 (3)</td><td>62.3 (3)</td><td>70.4 (3)</td><td>56.1 (3)</td><td>39.5 (3)</td></tr><tr><td>ARC-C</td><td>45.9 (0)</td><td>53.1 (0)</td><td>46.3 (0)</td><td>60.6 (25)</td><td>63.7 (25)</td><td>60.0 (25)</td><td>48.1 (0)</td></tr><tr><td>Hellaswag</td><td>70.5 (0)</td><td>79.1 (0)</td><td>76.0 (0)</td><td>80.7 (10)</td><td>80.2 (10)</td><td>83.3 (10)</td><td>75.4 (0)</td></tr><tr><td>TruthfulQA</td><td>46.1 (0)</td><td>44.0 (0)</td><td>39.0 (0)</td><td>54.2 (0)</td><td>56.4 (0)</td><td>42.2 (0)</td><td>-</td></tr><tr><td>WinoGrande</td><td>74.8 (5)</td><td>77.3 (5)</td><td>72.5 (5)</td><td>77.0 (5)</td><td>75.9 (5)</td><td>78.4 (5)</td><td>70.5 (0)</td></tr><tr><td>PIQA</td><td>73.6 (0)</td><td>80.6 (0)</td><td>79.1 (0)</td><td>-</td><td>-</td><td>-</td><td>79.2 (0)</td></tr><tr><td colspan="8">Mathematics &amp; Science</td></tr><tr><td>GSM8K</td><td>70.3 (4)</td><td>48.7 (4)</td><td>13.1 (4)</td><td>80.2 (4)</td><td>85.4 (4)</td><td>36.2 (4)</td><td>17.4 (8)</td></tr><tr><td>Math</td><td>31.4 (4)</td><td>16.0 (4)</td><td>4.3 (4)</td><td>43.5 (4)</td><td>49.8 (4)</td><td>10.2 (4)</td><td>6.0 (4)</td></tr><tr><td>GPQA</td><td>25.2 (5)</td><td>25.9 (5)</td><td>25.7 (5)</td><td>30.8 (5)</td><td>36.4 (5)</td><td>24.7 (5)</td><td>-</td></tr><tr><td colspan="8">Code</td></tr><tr><td>HumanEval</td><td>35.4 (0)</td><td>34.8 (0)</td><td>12.8 (0)</td><td>51.2 (0)</td><td>57.9 (0)</td><td>29.3 (0)</td><td>26.2 (0)</td></tr><tr><td>HumanEval-FIM</td><td>73.8 (2)</td><td>73.3 (2)</td><td>26.9 (2)</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MBPP</td><td>40.0 (4)</td><td>48.8 (4)</td><td>23.2 (4)</td><td>64.2 (0)</td><td>74.9 (0)</td><td>51.1 (0)</td><td>39.0 (3)</td></tr><tr><td colspan="8">Chinese</td></tr><tr><td>CMMLU</td><td>69.9 (5)</td><td>50.7 (5)</td><td>32.5 (5)</td><td>83.9 (5)</td><td>-</td><td>-</td><td>47.2 (5)</td></tr><tr><td>C-Eval</td><td>70.5 (5)</td><td>51.7 (5)</td><td>34.0 (5)</td><td>83.2 (5)</td><td>-</td><td>-</td><td>45.0 (5)</td></tr></table>

Table 2: Benchmark Results of Post-trained LLMs. LLaDA only employs an SFT procedure, while other models have extra reinforcement learning (RL) alignment. ∗ indicates models are evaluated under the same protocol, detailed in Appendix B.6. Results indicated by † and ¶ are sourced from Yang et al. [26] and Bi et al. [32] respectively. The numbers in parentheses represent the number of shots used for in-context learning. “-” indicates unknown data.   

<table><tr><td></td><td>LLaDA 8B*</td><td>LLaMA3 8B*</td><td>LLaMA2 7B*</td><td>Qwen2 7B†</td><td>Qwen2.5 7B†</td><td>Gemma2 9B†</td><td>Deepseek 7B¶</td></tr><tr><td>Model</td><td>Diffusion</td><td>AR</td><td>AR</td><td>AR</td><td>AR</td><td>AR</td><td>AR</td></tr><tr><td>Training tokens</td><td>2.3T</td><td>15T</td><td>2T</td><td>7T</td><td>18T</td><td>8T</td><td>2T</td></tr><tr><td>Post-training</td><td>SFT</td><td>SFT+RL</td><td>SFT+RL</td><td>SFT+RL</td><td>SFT+RL</td><td>SFT+RL</td><td>SFT+RL</td></tr><tr><td>Alignment pairs</td><td>4.5M</td><td>-</td><td>-</td><td>0.5M + -</td><td>1M + 0.15M</td><td>-</td><td>1.5M + -</td></tr><tr><td colspan="8">General Tasks</td></tr><tr><td>MMLU</td><td>65.5 (5)</td><td>68.4 (5)</td><td>44.1 (5)</td><td>-</td><td>-</td><td>-</td><td>49.4 (0)</td></tr><tr><td>MMLU-pro</td><td>37.0 (0)</td><td>41.9 (0)</td><td>4.6 (0)</td><td>44.1 (5)</td><td>56.3 (5)</td><td>52.1 (5)</td><td>-</td></tr><tr><td>Hellaswag</td><td>74.6 (0)</td><td>75.5 (0)</td><td>51.5 (0)</td><td>-</td><td>-</td><td>-</td><td>68.5 (-)</td></tr><tr><td>ARC-C</td><td>88.5 (0)</td><td>82.4 (0)</td><td>57.3 (0)</td><td>-</td><td>-</td><td>-</td><td>49.4 (-)</td></tr><tr><td colspan="8">Mathematics &amp; Science</td></tr><tr><td>GSM8K</td><td>69.4 (4)</td><td>78.3 (4)</td><td>29.0 (4)</td><td>85.7 (0)</td><td>91.6 (0)</td><td>76.7 (0)</td><td>63.0 (0)</td></tr><tr><td>Math</td><td>31.9 (0)</td><td>29.6 (0)</td><td>3.8 (0)</td><td>52.9 (0)</td><td>75.5 (0)</td><td>44.3 (0)</td><td>15.8 (0)</td></tr><tr><td>GPQA</td><td>33.3 (5)</td><td>31.9 (5)</td><td>28.4 (5)</td><td>34.3 (0)</td><td>36.4 (0)</td><td>32.8 (0)</td><td>-</td></tr><tr><td colspan="8">Code</td></tr><tr><td>HumanEval</td><td>49.4 (0)</td><td>59.8 (0)</td><td>16.5 (0)</td><td>79.9 (0)</td><td>84.8 (0)</td><td>68.9 (0)</td><td>48.2 (-)</td></tr><tr><td>MBPP</td><td>41.0 (4)</td><td>57.6 (4)</td><td>20.6 (4)</td><td>67.2 (0)</td><td>79.2 (0)</td><td>74.9 (0)</td><td>35.2 (-)</td></tr></table>

strengths stem from the same factors as its relatively weaker performance in some tasks—differences in data quality and distribution, largely due to the closed-source situation of LLM datasets.

Notably, we have carefully ruled out the possibility of data leakage by taking GSM8K as an example. First, as shown in Fig. 3, LLaDA outperformed ARM baselines regarding GSM8K. Moreover, the conclusion remains on a fully unseen GSM8K-like task [34] in Appendix B.8.

Further, Tab. 2 compares the performance of LLaDA 8B Instruct with existing LLMs. SFT improved LLaDA’s performance on most downstream tasks. A few metrics, such as MMLU, showed declines, possibly due to the suboptimal quality of the SFT data. Overall, since we did not perform alignment

Table 3: Visualization of the Sampling Process and a Generated Multi-round Dialogue. In the response of LLaDA, darker colors indicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.   

<table><tr><td colspan="14">Sampling Process</td></tr><tr><td>User</td><td colspan="13">Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?</td></tr><tr><td rowspan="3">LLaDA</td><td rowspan="2">In hours</td><td>4 hours</td><td>,</td><td>Lily</td><td>runs</td><td>12</td><td>*</td><td>4 =</td><td>48</td><td>kilometers</td><td>.</td><td>After 4</td><td></td></tr><tr><td>she</td><td>runs</td><td>6</td><td>*</td><td>4 =</td><td>24</td><td>kilometers</td><td>.</td><td>In total</td><td>she</td><td>runs</td><td>48</td></tr><tr><td>+</td><td>24</td><td>=</td><td>72</td><td>kilometers</td><td>in</td><td>8</td><td>hours</td><td>.</td><td>Result</td><td>72</td><td></td><td></td></tr><tr><td colspan="14">Multi-round Dialogue Case</td></tr><tr><td>User</td><td colspan="13">Could you please share with me what the initial two lines of the renowned poem ‘The Road Not Taken’?</td></tr><tr><td>LLaDA</td><td colspan="13">Certainly! The initial two lines of the renowned poem “The Road Not Taken” by Robert Frost are: “Two roads diverged in a yellow wood, And sorry I could not travel both.” These lines set the stage for the poem, introducing the idea of a speaker facing a choice between two paths.</td></tr><tr><td>User</td><td colspan="13">Please help me translate into Chinese.</td></tr><tr><td>LLaDA</td><td colspan="13">Sure! The first two lines of “The Road Not Taken” by Robert Frost can be translated into Chinese as: “两条路分岔在黄色的树林中，遗憾我不能同时走”</td></tr><tr><td>User</td><td colspan="13">Please translate into German.</td></tr><tr><td>LLaDA</td><td colspan="13">Sure! The first two lines of “The Road Not Taken” by Robert Frost can be translated into German as: “Zwei Wege trennten sich im gelben Wald, und leider konnte ich nicht bereits tun.”</td></tr><tr><td>User</td><td colspan="13">Please also write a poem about life choices. I require 5 sentences, and the first word of each sentence starts with C as the first letter.</td></tr><tr><td>LLaDA</td><td colspan="13">Certainly! Here&#x27;s a poem about life choices, with each sentence starting with the letter “C”: Choices cascade through life&#x27;s journey, Creating connections with each decision, Careful consideration leads to better paths, Courageous choices shape our destiny, Choosing wisely guides us through the years.</td></tr></table>

with reinforcement learning (RL), our results are slightly behind LLaMA3 8B Instruct, though the gaps in many metrics remain small. Notably, even with only SFT, LLaDA demonstrates impressive instruction-following abilities, as detailed in Sec. 3.4. We leave RL-based alignment for future work.

All results in Sec. 3 are based on pure diffusion methods, as they achieve better overall performance than approaches incorporating autoregressive components. Specifically, we use Eq. (6) for conditional likelihood estimation and apply low-confidence remasking for sampling. For LLaDA 8B Instruct, block diffusion style sampling performs better on GSM8K and Math, with scores of 78.6 and 42.2, compared to 69.4 and 31.9 in Tab. 2. This gain is due to extensive |EOS| token padding in the SFT data, causing early termination in low-confidence remasking. Please refer to Appendix B.4 for details.

Overall, despite the lack of data transparency, we have made every effort to adopt standardized procedures and introduce diverse tasks, we believe they sufficiently demonstrate the extraordinary capabilities of LLaDA, which is the only competitive non-autoregressive model to our knowledge.

# 3.3 Reversal Reasoning and Analyses

To quantify the reversal reasoning [15] ability of models, we follow the protocol established in Allen-Zhu and Li [35]. Specifically, we construct a dataset of 496 famous Chinese poem sentence pairs. Given a sentence from a poem, models are tasked with generating the subsequent line (forward) or the preceding line (reversal) without additional fine-tuning. Examples can be found in Section B.9. This setting provides a straightforward and more realistic evaluation compared to previous studies [27, 36].

As shown in Tab. 4, LLaDA effectively addresses the reversal curse [15], demonstrating consistent zero-shot performance across both forward and reversal tasks. In contrast, both Qwen 2.5 and GPT-4o exhibit a significant gap between the two. The results on forward generation confirm that both ARMs are strong, benefiting from significantly larger datasets and greater computational resources than LLaDA. However, LLaDA outperforms both by a large margin in the reversal task.

We did not design anything special for reversal tasks. Intuitively, LLaDA treats tokens uniformly without inductive bias, leading to balanced performance. See Appendix A.2 for details.

We also analyze the effect of different sampling strategies for LLaDA, including autoregressive sampling, block diffusion [31] sampling, and pure diffusion sampling, showing that pure diffusion sampling achieves the best overall performance, as detailed in Appendix B.4.

Table 4: Comparison on the Poem Completion task.   

<table><tr><td></td><td>Forward</td><td>Reversal</td></tr><tr><td>GPT-4o (2024-08-06)</td><td>82.7</td><td>34.3</td></tr><tr><td>Qwen2.5-7B Instruct</td><td>75.9</td><td>38.0</td></tr><tr><td>LLaDA-8B Instruct</td><td>51.8</td><td>45.6</td></tr></table>

In addition, we examine LLaDA’s sampling speed and memory consumption, showing that it enables a flexible trade-off between generation quality and speed. See Appendix B.7 for more details.

Classifier-free guidance (CFG) [37, 27] is a widely used technique in diffusion models to improve generation quality. To ensure a fair comparison with ARMs, we do not apply CFG to LLaDA in the main text. However, we show that LLaDA is compatible with CFG and consistently benefits from its application. See Appendix B.3 for more details.

# 3.4 Case Studies

We present samples generated by LLaDA 8B Instruct in Tab. 3, showcasing its instruction-following capabilities. First, the table illustrates LLaDA’s ability to generate coherent, fluent, and extended text in a non-autoregressive manner. Second, it highlights the model’s multi-turn dialogue capability, effectively retaining conversation history and producing contextually appropriate responses across multiple languages. Such chat capabilities of LLaDA are impressive, as it departs from conventional ARMs for the first time, to the best of our knowledge. See more case studies in Appendix B.10.

# 4 Related Work

Diffusion models [38–40] have achieved remarkable success in visual domains but remain unverified for large-scale (e.g., models trained with over $1 0 ^ { 2 3 }$ FLOPs) language modeling, despite growing interest and extensive research efforts.

A simple approach is to continuousize text data and apply continuous diffusion models directly [41– 51]. Alternatively, some methods model continuous parameters of discrete distributions instead [52– 56]. However, scalability remains a significant challenge for these approaches. For instance, a 1B model may require 64 times the compute of an ARM to achieve comparable performance [57].

Another approach replaces continuous diffusion with discrete processes featuring new forward and reverse dynamics, leading to numerous variants [58–71]. The original diffusion model paper [38] introduced both continuous-state and discrete-state transition kernels under a unified diffusion framework. Austin et al. [16] was among the pioneering works that introduced discrete diffusion models into language modeling, demonstrating the feasibility of this approach. Lou et al. [17] showed that masked diffusion, as a special case of discrete diffusion, achieves perplexity comparable to or surpassing ARMs at GPT-2 scale. Shi et al. [18], Sahoo et al. [19], Ou et al. [20] established fundamental theoretical results, which motivated our model design, training, and inference (see Appendix A for details). Nie et al. [27] introduced the scaling laws for MDMs in language modeling and explored how MDMs can be leveraged for language tasks such as question answering at the GPT-2 scale. Gong et al. [72] demonstrated the potential of fine-tuning an ARM within the MDM framework. However, the improvements observed by Gong et al. [72] are limited to specific metrics, and their approach does not address the performance achievable through pure diffusion-based training. Concurrent work [73] demonstrates the potential of diffusion language models in code generation and highlights their advantages in inference efficiency. Nonetheless, as it is a closed-source product, specific details such as training procedures and sampling methods remain unknown.

In comparison, this study scales MDM to an unprecedented size of 8B parameters from scratch, achieving performance comparable to leading LLMs such as LLaMA 3.

Additionally, a parallel line of work on image generation [23, 74, 75] aligns well with the application of MDMs to text data. Moreover, MDMs have also shown promise in other domains such as protein

generation [76, 77], where they have achieved promising results. Notably, a series of studies [31, 78– 87] have explored techniques such as architectural optimization, distillation, and sampling algorithm design to accelerate MDMs sampling.

# 5 Conclusion and Discussion

We introduce LLaDA, a diffusion language model trained from scratch with an unprecedented scale of 8B parameters. LLaDA demonstrates strong capabilities in scalability, in-context learning, and instruction-following, achieving performance comparable to strong LLMs such as LLaMA3. In addition, LLaDA offers unique advantages, such as bidirectional modeling and enhanced robustness, effectively addressing the relevant limitations of existing LLMs. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that these essential capabilities are inherently tied to ARMs. These results represent a new paradigm for language modeling and uncover novel insights, demonstrating a high degree of scientific innovation.

Limitations. While promising, the full potential of diffusion models remains to be fully explored. Several limitations of this work present significant opportunities for future research. The generation length is a user-specified hyperparameter. Although LLaDA is insensitive to this hyperparameter as detailed in Appendix B.5, we believe that adopting an adaptive generation length would offer a more efficient solution. Due to computational constraints, direct comparisons between LLaDA and ARMs—such as training on identical datasets—were restricted to a computational budget of less than $1 0 ^ { 2 3 }$ FLOPs. To allocate resources for training the largest possible LLaDA model and showcasing its potential, we were unable to scale the ARM baseline to the same extent. Moreover, no specialized attention mechanisms or position embeddings were designed for LLaDA, nor were any system-level architectural optimizations such as KV cache applied. On the inference side, more efficient and controllable [37, 88, 89] sampling algorithms remain preliminary. Furthermore, LLaDA has yet to undergo alignment with reinforcement learning [90, 91], which is crucial for improving its performance and alignment with human intent.

Looking ahead, both the model scale and the amount of training data for LLaDA remain smaller than those of leading ARM counterparts [6, 26, 92–95], highlighting the need for further scaling to fully evaluate its capabilities. In addition, LLaDA’s ability to process multi-modal data remains unexplored. Its impact on prompt tuning techniques [96] and integration into agent-based systems [97, 98] is still not fully understood. Finally, a systematic investigation into post-training for LLaDA (e.g., O1-like systems [99, 100]) is needed to further unlock the potential of diffusion language models.

# Acknowledgements

This work was supported by the National Natural Science Foundation of China (No. 92470118); Beijing Natural Science Foundation (No. L247030); Beijing Nova Program (No. 20220484044); and Ant Group Research Fund.

# References

[1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.   
[2] Alec Radford. Improving language understanding by generative pre-training, 2018.   
[3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.   
[4] Tom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.   
[5] OpenAI. ChatGPT: Optimizing Language Models for Dialogue. OpenAI blog, November 2022. URL https://openai.com/blog/chatgpt/.

[6] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.   
[7] Ashish Vaswani. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.   
[8] Ronald A Fisher. On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character, 222(594-604):309–368, 1922.   
[9] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22669–22679, 2023.   
[10] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195–4205, 2023.   
[11] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators.   
[12] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. In The Twelfth International Conference on Learning Representations.   
[13] Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. Compression represents intelligence linearly. arXiv preprint arXiv:2404.09937, 2024.   
[14] Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423, 1948.   
[15] Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on" a is b" fail to learn" b is a". arXiv preprint arXiv:2309.12288, 2023.   
[16] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981–17993, 2021.   
[17] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023.   
[18] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K Titsias. Simplified and generalized masked diffusion for discrete data. arXiv preprint arXiv:2406.04329, 2024.   
[19] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024.   
[20] Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024.   
[21] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.   
[22] Jacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.   
[23] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11315–11325, 2022.

[24] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multihead checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4895–4901, 2023.   
[25] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671.   
[26] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024.   
[27] Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024.   
[28] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024.   
[29] I Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017.   
[30] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.   
[31] Marianne Arriola, Aaron Gokaslan, Justin T Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025.   
[32] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024.   
[33] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.   
[34] Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan Allen-Zhu. Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process. ArXiv e-prints, abs/2407.20311, July 2024. Full version available at http://arxiv.org/abs/2407.20311.   
[35] Zeyuan Allen-Zhu and Yuanzhi Li. Physics of Language Models: Part 3.2, Knowledge Manipulation. ArXiv e-prints, abs/2309.14402, September 2023. Full version available at http://arxiv.org/abs/2309.14402.   
[36] Ouail Kitouni, Niklas Nolte, Diane Bouchacourt, Adina Williams, Mike Rabbat, and Mark Ibrahim. The factorization curse: Which tokens you predict underlie the reversal curse and more. arXiv preprint arXiv:2406.05183, 2024.   
[37] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.

[38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256–2265. PMLR, 2015.   
[39] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020.   
[40] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.   
[41] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S Liang, and Tatsunori B Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35:4328–4343, 2022.   
[42] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022.   
[43] Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplexbased diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022.   
[44] Robin Strudel, Corentin Tallec, Florent Altché, Yilun Du, Yaroslav Ganin, Arthur Mensch, Will Grathwohl, Nikolay Savinov, Sander Dieleman, Laurent Sifre, et al. Self-conditioned embedding diffusion for text generation. arXiv preprint arXiv:2211.04236, 2022.   
[45] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022.   
[46] Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022.   
[47] Pierre H. Richemond, Sander Dieleman, and Arnaud Doucet. Categorical sdes with simplex diffusion, 2022.   
[48] Tong Wu, Zhihao Fan, Xiao Liu, Yeyun Gong, Yelong Shen, Jian Jiao, Hai-Tao Zheng, Juntao Li, Zhongyu Wei, Jian Guo, Nan Duan, and Weizhu Chen. Ar-diffusion: Auto-regressive diffusion model for text generation, 2023.   
[49] Rabeeh Karimi Mahabadi, Hamish Ivison, Jaesung Tae, James Henderson, Iz Beltagy, Matthew E. Peters, and Arman Cohan. Tess: Text-to-text self-conditioned simplex diffusion, 2024.   
[50] Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Mingxuan Wang. Dinoiser: Diffused conditional sequence learning by manipulating noises. arXiv preprint arXiv:2302.10025, 2023.   
[51] Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Joshua Susskind, and Navdeep Jaitly. Planner: Generating diversified paragraph via latent language diffusion model. Advances in Neural Information Processing Systems, 36:80178–80190, 2023.   
[52] Aaron Lou and Stefano Ermon. Reflected diffusion models, 2023.   
[53] Alex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow networks. arXiv preprint arXiv:2308.07037, 2023.   
[54] Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Nan Duan, and Weizhu Chen. Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise. In International Conference on Machine Learning, pages 21051–21064. PMLR, 2023.   
[55] Kaiwen Xue, Yuhao Zhou, Shen Nie, Xu Min, Xiaolu Zhang, Jun Zhou, and Chongxuan Li. Unifying bayesian flow networks and diffusion models through stochastic differential equations. arXiv preprint arXiv:2404.15766, 2024.

[56] Ruixiang Zhang, Shuangfei Zhai, Yizhe Zhang, James Thornton, Zijing Ou, Joshua Susskind, and Navdeep Jaitly. Target concrete score matching: A holistic framework for discrete diffusion. arXiv preprint arXiv:2504.16431, 2025.   
[57] Ishaan Gulrajani and Tatsunori B Hashimoto. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36, 2024.   
[58] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forré, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in Neural Information Processing Systems, 34:12454–12465, 2021.   
[59] Emiel Hoogeboom, Alexey A Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. arXiv preprint arXiv:2110.02037, 2021.   
[60] Zhengfu He, Tianxiang Sun, Kuanning Wang, Xuanjing Huang, and Xipeng Qiu. Diffusionbert: Improving generative masked language models with diffusion models. arXiv preprint arXiv:2211.15029, 2022.   
[61] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:28266–28279, 2022.   
[62] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:34532–34545, 2022.   
[63] Machel Reid, Vincent J. Hellendoorn, and Graham Neubig. Diffuser: Discrete diffusion via edit-based reconstruction, 2022.   
[64] Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuoustime discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022.   
[65] Ouail Kitouni, Niklas Nolte, James Hensman, and Bhaskar Mitra. Disk: A diffusion model for structured knowledge. arXiv preprint arXiv:2312.05253, 2023.   
[66] Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. A reparameterized discrete diffusion model for text generation. ArXiv, abs/2302.05737, 2023.   
[67] Zixiang Chen, Huizhuo Yuan, Yongqian Li, Yiwen Kou, Junkai Zhang, and Quanquan Gu. Fast sampling via de-randomization for discrete diffusion models. arXiv preprint arXiv:2312.09193, 2023.   
[68] Jiasheng Ye, Zaixiang Zheng, Yu Bao, Lihua Qian, and Quanquan Gu. Diffusion language models can perform many tasks with scaling and instruction-finetuning. arXiv preprint arXiv:2308.12219, 2023.   
[69] Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and Yaron Lipman. Discrete flow matching. arXiv preprint arXiv:2407.15595, 2024.   
[70] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling, 2024. URL https://arxiv.org/abs/2409.02908.   
[71] Shreyas Kapur, Erik Jenner, and Stuart Russell. Diffusion on syntax trees for program synthesis. arXiv preprint arXiv:2405.20519, 2024.   
[72] Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024.   
[73] Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025.

[74] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.   
[75] Zebin You, Jingyang Ou, Xiaolu Zhang, Jun Hu, Jun Zhou, and Chongxuan Li. Effective and efficient masked image generation models. arXiv preprint arXiv:2503.07197, 2025.   
[76] Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Diffusion language models are versatile protein learners. arXiv preprint arXiv:2402.18567, 2024.   
[77] Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Dplm-2: A multimodal diffusion protein language model. arXiv preprint arXiv:2410.13782, 2024.   
[78] Siqi Kou, Lanxiang Hu, Zhezhi He, Zhijie Deng, and Hao Zhang. Cllms: Consistency large language models. arXiv preprint arXiv:2403.00835, 2024.   
[79] Chenkai Xu, Xu Wang, Zhenyi Liao, Yishun Li, Tianqi Hou, and Zhijie Deng. Show-o turbo: Towards accelerated unified multimodal understanding and generation. arXiv preprint arXiv:2502.05415, 2025.   
[80] Sulin Liu, Juno Nam, Andrew Campbell, Hannes Stärk, Yilun Xu, Tommi Jaakkola, and Rafael Gómez-Bombarelli. Think while you generate: Discrete diffusion with planned denoising. arXiv preprint arXiv:2410.06264, 2024.   
[81] Yuanzhi Zhu, Xi Wang, Stéphane Lathuilière, and Vicky Kalogeiton. Dimo: Distilling masked diffusion models into one-step generator. arXiv preprint arXiv:2503.15457, 2025.   
[82] Yinuo Ren, Haoxuan Chen, Yuchen Zhu, Wei Guo, Yongxin Chen, Grant M Rotskoff, Molei Tao, and Lexing Ying. Fast solvers for discrete diffusion models: Theory and applications of high-order algorithms. arXiv preprint arXiv:2502.00234, 2025.   
[83] Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, and Yuki Mitsufuji. Distillation of discrete diffusion through dimensional correlations. arXiv preprint arXiv:2410.08709, 2024.   
[84] Yixiu Zhao, Jiaxin Shi, Feng Chen, Shaul Druckmann, Lester Mackey, and Scott Linderman. Informed correctors for discrete diffusion models. arXiv preprint arXiv:2407.21243, 2024.   
[85] Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024.   
[86] Yong-Hyun Park, Chieh-Hsin Lai, Satoshi Hayakawa, Yuhta Takida, and Yuki Mitsufuji. Jump your steps: Optimizing sampling schedule of discrete diffusion models. In The Thirteenth International Conference on Learning Representations, 2024.   
[87] Justin Deschenaux and Caglar Gulcehre. Beyond autoregression: Fast llms via self-distillation through time. arXiv preprint arXiv:2410.21035, 2024.   
[88] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780–8794, 2021.   
[89] Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dallatorre, Bernardo P de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024.   
[90] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730–27744, 2022.

[91] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.   
[92] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
[93] Google. Our next-generation model: Gemini 1.5, 2024. URL https://blog.google/ technology/ai/google-gemini-next-generation-model-february-2024.   
[94] Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet.   
[95] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024.   
[96] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824–24837, 2022.   
[97] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 1–22, 2023.   
[98] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024.   
[99] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/.   
[100] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.   
[101] Benigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In Proceedings of the 31th International Conference on Machine Learning, 2014.   
[102] Andy Shih, Dorsa Sadigh, and Stefano Ermon. Training and inference on any-order autoregressive models the right way. In Proceedings of the 31th International Conference on Machine Learning, 2022.   
[103] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024.   
[104] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct. arXiv preprint arXiv:2312.02120, 2023.   
[105] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019.   
[106] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.   
[107] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   
[108] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

[109] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.   
[110] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.   
[111] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging bigbench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.   
[112] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.   
[113] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.   
[114] Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021.   
[115] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106, 2021.   
[116] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, 2020.   
[117] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.   
[118] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.   
[119] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023.   
[120] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.   
[121] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen. Efficient training of language models to fill in the middle. arXiv preprint arXiv:2207.14255, 2022.   
[122] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.   
[123] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023.   
[124] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36, 2024.

[125] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/ records/12608602.

Algorithm 1 Pre-training of LLaDA   
Require: mask predictor $p_{\theta}$ data distribution $p_{\mathrm{data}}$ 1: repeat   
2: $x_0\sim p_{\mathrm{data}}$ # with a probability of $1 \%$ , the sequence length of $x_0$ follows U[1, 4096]   
3: $t\sim \mathrm{U}(0,1]$ 4: $x_{t}\sim q_{t|0}(x_{t}|x_{0})$ # $q_{t|0}$ is defined in Eq. (7)   
5: Calculate $\mathcal{L} = -\frac{1}{t*L}\sum_{i = 1}^{L}\mathbf{1}[x_t^i = \mathbf{M}]\log p_\theta (x_0^i |x_t)$ # $L$ is the sequence length of $x_0$ 6: Calculate $\nabla_{\theta}\mathcal{L}$ and run optimizer.   
7: until Converged   
8: Return $p_{\theta}$

Algorithm 2 Supervised Fine-Tuning of LLaDA   
Require: mask predictor $p_{\theta}$ , pair data distribution $p_{\mathrm{data}}$ 1: repeat   
2: $p_0,r_0\sim p_{\mathrm{data}}$ # please refer to Appendix B.1 for details about the SFT dat   
3: $t\sim \mathrm{U}(0,1]$ 4: $r_t\sim q_{t|0}(r_t|r_0)$ # $q_{t|0}$ is defined in Eq. (7)   
5: Calculate $\mathcal{L} = -\frac{1}{t*L^{\prime}}\sum_{i = 1}^{L^{\prime}}\mathbf{1}[r_t^i = \mathbf{M}]\log p_\theta (r_0^i |p_0,r_t)$ # $L^{\prime}$ is the sequence length of $r_0$ 6: Calculate $\nabla_{\theta}\mathcal{L}$ and run optimizer.   
7: until Converged   
8: Return $p_{\theta}$

Algorithm 3 Conditional Log-likelihood Evaluation of LLaDA   
Require: mask predictor $p_{\theta}$ prompt $p_0$ response $r_0$ , the number of Monte Carlo estimations $n_{mc}$ 1: log_likelihood $\equiv 0$ 2: for $i\gets 1$ to $n_{mc}$ do   
3: $l\sim \{1,2,\dots ,L\}$ # $L$ is the sequence length of $r_0$ 4: Obtain $r_l$ by uniformly sampling $l$ tokens from $r_0$ without replacement for masking   
5: log_likelihood $\equiv$ log_likelihood $+ \frac{L}{l}\sum_{i = 1}^{L}\mathbf{1}[r_l^i = \mathbf{M}]\log p_\theta (r_0^i |p_0,r_l)$ 6: end for   
7: log_likelihood $\equiv$ log_likelihood/nmc   
8: Return log_likelihood

# A Formulation of Masked Diffusion Models

# A.1 Training

MDMs [16–20] define the model distribution $p _ { \theta } ( x _ { 0 } )$ in a manner distinct from autoregressive models.

These models introduce a forward process $\{ x _ { t } \}$ indexed by a time $t \in [ 0 , 1 ]$ . This process gradually and independently masks all tokens in the sequence $x _ { 0 }$ . At time $t = 0$ , the data point $x _ { 0 }$ is fully observed with no masks, while for $t \in ( 0 , 1 ]$ , $x _ { t }$ represents latent variables with varying mask ratios in expectation.

Formally, the conditional distribution of $x _ { t }$ given $x _ { 0 }$ is defined by a fully factorized form:

$$
q _ {t | 0} \left(x _ {t} \mid x _ {0}\right) = \prod_ {i = 1} ^ {L} q _ {t | 0} \left(x _ {t} ^ {i} \mid x _ {0} ^ {i}\right), \tag {7}
$$

where the conditional distribution for each token is given by:

$$
q _ {t \mid 0} \left(x _ {t} ^ {i} \mid x _ {0} ^ {i}\right) = \left\{ \begin{array}{l l} 1 - t, & x _ {t} ^ {i} = x _ {0} ^ {i}, \\ t, & x _ {t} ^ {i} = \mathbf {M}. \end{array} \right. \tag {8}
$$

Here, M denotes the mask token. Intuitively, each token either remains unchanged or is masked, with the probability of being masked increasing linearly as $t$ progresses from 0 to 1. At $t = 1$ , all

Algorithm 4 Random Remasking Strategy of LLaDA   
Require: mask predictor $p_{\theta}$ , prompt $p_0$ , answer length $L$ , sampling steps $N$ 1: Set $r_1$ is a fully masked sequence of length $L$ 2: for $t\gets 1$ down to $\frac{1}{N}$ step $\frac{1}{N}$ do   
3: $s = t - \frac{1}{N}$ 4: $r_0 = \arg \max_{r_0}p_\theta (r_0|p_0,r_t)\#$ we employ greedy sampling when predicting masked tokens   
5: for $i\gets 1$ to $L$ do   
6: if $r_t^i\neq M$ then   
7: $r_0^i = r_t^i$ 8: else   
9: with probability $\frac{s}{t},r_0^i$ is set to M   
10: end if   
11: end for   
12: $r_s = r_0$ 13: end for   
14: Return $r_0$

tokens are guaranteed to be masked, meaning that $x _ { 1 }$ follows a Dirac distribution concentrated on a sequence of fully masked tokens. Notably, the linear masking probability is analogous to but distinct from, the noise schedule in continuous diffusion models [38–40]. This linearity is motivated by the assumption that the information in the text is proportional to the number of tokens on average, making it reasonable to lose information linearly during the forward process.

The forward process is not only reversible but also corresponds to a reverse process that is fully factorized across all tokens. The reverse process, from time $t = 1$ to 0, generates new data from sequences of fully masked tokens. The conditional distribution for the reverse process, for $0 \leq s <$ $t \leq 1$ , is factorized as:

$$
q _ {s \mid t} \left(x _ {s} \mid x _ {t}\right) = \prod_ {i = 1} ^ {L} q _ {s \mid t} \left(x _ {s} ^ {i} \mid x _ {t}\right), \tag {9}
$$

where the conditional distribution for each token is:

$$
q _ {s \mid t} \left(x _ {s} ^ {i} \mid x _ {t}\right) = \left\{ \begin{array}{l l} 1, & x _ {t} ^ {i} \neq \mathbf {M}, x _ {s} ^ {i} = x _ {t} ^ {i}, \\ \frac {s}{t}, & x _ {t} ^ {i} = \mathbf {M}, x _ {s} ^ {i} = \mathbf {M}, \\ \frac {t - s}{t} q _ {0 \mid t} \left(x _ {s} ^ {i} \mid x _ {t}\right), & x _ {t} ^ {i} = \mathbf {M}, x _ {s} ^ {i} \neq \mathbf {M}, \\ 0, & \text {o t h e r w i s e .} \end{array} \right. \tag {10}
$$

Thus, the key function to estimate is the conditional distribution $q _ { 0 | t } \big ( x _ { s } ^ { i } | x _ { t } \big )$ , which predicts the original token if it is masked in the input $x _ { t }$ . This is analogous to the data prediction form in continuous diffusion models.

As proven in [20], an equivalent yet time-free parameterization can be derived as:

$$
q _ {0 | t} \left(x _ {s} ^ {i} \mid x _ {t}\right) = p _ {\text {d a t a}} \left(x _ {0} ^ {i} \mid x _ {t} ^ {\mathrm {U M}}\right), \quad \forall i \text {s u c h t h a t} x _ {t} ^ {i} = \mathbf {M}, \tag {11}
$$

where $x _ { t } ^ { \mathrm { U M } }$ denotes the collection of unmasked tokens in $x _ { t }$ , which is identical to the corresponding tokens in the original data $x _ { 0 }$ since unmasked tokens are solely determined by $x _ { 0 }$ and are independent of time $t$ . Intuitively, this implies that estimating the data prediction function is equivalent to estimating the conditional distributions on clean data, which is time-invariant. Consequently, the time $t$ need not be provided as input to the parametric model.

Although the development of masked diffusion is nontrivial, the implementation is straightforward. We first introduce the mask predictor, a parametric model $p _ { \theta } ( \cdot | x _ { t } )$ (e.g., a Transformer without causal mask), which takes $x _ { t }$ for any $t$ as input and predict all masked tokens simultaneously. Then, we define the model distribution $p _ { \theta } ( x _ { 0 } )$ as follows: starting with $x _ { 1 }$ as a sequence of fully masked tokens, we simulate an approximate reverse process parameterized by $p _ { \theta } ( \cdot | x _ { t } )$ from $t = 1$ to 0. The marginal distribution induced at $t = 0$ then represents the model distribution $p _ { \theta } ( x _ { 0 } )$ .

Algorithm 5 Low-confidence Remasking Strategy of LLaDA   
Require: mask predictor $p_{\theta}$ , prompt $p_0$ , answer length $L$ , sampling steps $N$ 1: Set $r_1$ is a fully masked sequence of length $L$ 2: for $t\gets 1$ down to $\frac{1}{N}$ step $\frac{1}{N}$ do   
3: $s = t - \frac{1}{N}$ 4: for $i\gets 1$ to $L$ do   
5: if $r_t^i\neq M$ then   
6: $r_0^i = r_t^i$ $c^i = 1$ 7: else   
8: $r_0^i = \arg \max_{r_0^i}p_\theta (r_0^i |p_0,r_t)$ 9: $c^i = p_\theta (r_0^i |p_0,r_t)_{r_0^i}$ 10: end if   
11: end for   
12: $n_{un} = \lfloor L(1 - s)\rfloor$ # the number of unmasked tokens is $n_{un}$ in timestep s   
13: for $i\gets 1$ to $L$ do   
14: if $c^i\in$ Lowest- $n_{un}$ $\{\{c^i\} _1^L\}$ then   
15: $r_0^i = M$ # the $n_{un}$ positions with the least confidence are selected for remasking.   
16: end if   
17: end for   
18: $r_s = r_0$ 19: end for   
20: Return $r_0$

Formally, the mask predictor is trained using a cross-entropy loss with masking:

$$
\mathcal {L} (\theta) \triangleq - \mathbb {E} _ {t, x _ {0}, x _ {t}} \left[ \frac {1}{t} \sum_ {i = 1} ^ {L} \mathbf {1} \left[ x _ {t} ^ {i} = \mathrm {M} \right] \log p _ {\theta} \left(x _ {0} ^ {i} \mid x _ {t}\right) \right], \tag {12}
$$

where $x _ { 0 }$ is sampled from the training data, $t$ is sampled uniformly from [0, 1], and $x _ { t }$ is sampled from $q _ { t \mid 0 } \big ( x _ { t } | x _ { 0 } \big )$ . The indicator function $1 [ \cdot ]$ ensures that the cross-entropy loss is computed only for masked tokens. In Shi et al. [18], Sahoo et al. [19], Ou et al. [20], it has been proven that the loss function $\mathcal { L } ( \boldsymbol { \theta } )$ is an upper bound on the negative log-likelihood of the model distribution:

$$
- \mathbb {E} _ {x _ {0} \sim p _ {\text {d a t a} (x _ {0})}} \left[ \log p _ {\theta} \left(x _ {0}\right) \right] \leq \mathcal {L} (\theta). \tag {13}
$$

In summary, this principled approach trains a generative model by progressively masking tokens during a forward process and learning to recover the data distribution during a reverse process, all under the (approximate) maximum likelihood estimation framework.

# A.2 Inference

The cross-entropy loss in Eq. (12) has several equivalent forms [20]. The first one is given by

$$
- \mathbb {E} _ {l \sim \{1, 2, \dots , L \}, x _ {0}, x _ {l}} \left[ \frac {L}{l} \sum_ {i = 1} ^ {L} \mathbf {1} \left[ x _ {l} ^ {i} = \mathbf {M} \right] \log p _ {\theta} \left(x _ {0} ^ {i} \mid x _ {l}\right) \right], \tag {14}
$$

where $l$ is uniformly sampled from $\{ 1 , 2 , \ldots , L \}$ , and $x _ { l }$ is obtained by uniformly sampling $l$ tokens from $x _ { 0 }$ without replacement for masking. Despite masking exactly $l$ tokens is different from masking each token independently with probability $t$ , these two masking methods lead to equivalent results in expectation [20].

While Eq. (12) and Eq. (14) share the same expectation, their variances differ. Intuitively, in Eq. (12), we expect $x _ { t }$ to have a fraction of $t$ tokens masked. However, the randomness of the forward process (i.e., Eq. (7)) often causes deviations, especially when $x _ { t }$ contains few tokens. In contrast, in Eq. (14), the fraction of masked tokens in $x _ { l }$ is deterministically $\frac { l } { L }$ . While a theoretical analysis depends on the data distribution, empirical results show that Eq. (12) requires over 1000 Monte Carlo estimates for stable results, whereas Eq. (14) achieves stability with only 128 estimates. In addition, we can simply modify Eq. (14) to its conditional version (i.e., Eq. (6)) based on Eq. (5).

Any-order autoregressive models (AO-ARM) [59, 101, 102] characterize the joint distribution autoregressively for all possible orders $\pi$ of the $L$ variables. To learn such a distribution, an AO-ARM utilizes a weight-sharing neural network to model all univariate conditionals and employs mask tokens to represent absent variables. During training, the expected negative log-likelihood over the uniform distribution of all orders $U _ { \pi }$ is minimized:

$$
- \mathbb {E} _ {x _ {0}, \pi \sim U _ {\pi}} \left[ \sum_ {i = 1} ^ {L} \log p _ {\theta} \left(x _ {0} ^ {\pi (i)} \mid x _ {0} ^ {\pi (<   i)}; \pi\right) \right]. \tag {15}
$$

Intuitively, xπ0 $x _ { 0 } ^ { \pi ( < i ) }$ can be understood as a masked token $x _ { t }$ with index in $\pi ( \geq i )$ being masked. It can be further proved that Eq. (15) is equivalent to Eq. (12). This connection explains the bidirectional reasoning capabilities of LLaDA, even though it was never used explicitly in the inference procedure.

In addition, Nie et al. [27] introduces unsupervised classifier-free guidance (CFG), a plug-and-play technique that balances alignment with prompts and text diversity. Specifically, unsupervised CFG employs the following modified mask predictor for inference:

$$
\tilde {p} _ {\theta} \left(r _ {0} \mid p _ {0}, r _ {t}\right) \propto \frac {p _ {\theta} \left(r _ {0} \mid p _ {0} , r _ {t}\right) ^ {1 + w}}{p _ {\theta} \left(r _ {0} \mid m , r _ {t}\right) ^ {w}}, \tag {16}
$$

where $m$ is a mask sequence of the same length as $p _ { 0 }$ and $w$ is a tunable hyperparameter that controls the strength of $p _ { 0 }$ . To ensure a fair comparison with ARMs, we do not apply CFG to LLaDA in the main text. However, as demonstrated in Appendix B.3, LLaDA is fully compatible with CFG and consistently exhibits improved performance when it is applied.

# A.3 Algorithms

In this section, we present the training and inference algorithms. Specifically, we introduce the pre-training and supervised fine-tuning algorithms in Algorithm 1 and Algorithm 2, respectively. In addition, the likelihood evaluation algorithm is provided in Algorithm 3. Finally, we present the reverse generation process in Algorithm 4 and Algorithm 5, which correspond to the random remasking and the low-confidence [23] remasking strategy, respectively.

# B Experiments

# B.1 Data Collection and Preprocessing

In this section, we first introduce the data collection and filtering processes for both pre-training and SFT. We then describe how LLaDA leverages these datasets during training.

Our pre-training corpus is constructed from diverse publicly available sources, including web data, books, academic papers, social media, encyclopedias, mathematics, and code, with approximately $11 \%$ Chinese, $61 \%$ English, and $28 \%$ code. The data cleaning process involves PDF text extraction, deduplication, and harmful content filtering. To further ensure quality, we fine-tune a BERT [22] model for automated data quality annotation, enabling the selection of higher-quality samples. Our SFT dataset consists of 1 million human-annotated samples and 3.5 million synthetic samples, generated using methods similar to those proposed in Xu et al. [103], Wei et al. [104].

We concatenate the collected documents in the pre-training corpus and segment the text into fixedlength sequences according to the predefined sequence length.

For SFT, a dynamic sequence length strategy is employed, where $| \mathrm { E O S } |$ tokens are appended to the end of shorter pairs to ensure uniform sequence lengths across all samples within each mini-batch. Notably, the padding |EOS| tokens are treated as part of the response, i.e., masked and included in the training objective. The $| \mathrm { E O S } |$ tokens are removed from the generated outputs during sampling. This strategy ensures that the model learns to control the length of its responses by generating |EOS|, enabling the response length to align effectively with the given prompt.

pairs, i.e., (p00, r00 ), (p00r00 p10, r10 ), . . . , (p00r00 p10r10 . . . pn−10 , rn−10 ) and randomly sample one. This data In addition, for n-turn dialogues (p00, r00, p10, r10, . . . , pn−0 $( p _ { 0 } ^ { 0 } , r _ { 0 } ^ { 0 } ) , ( p _ { 0 } ^ { 0 } r _ { 0 } ^ { 0 } p _ { 0 } ^ { 1 } , r _ { 0 } ^ { 1 } ) , \ldots , ( p _ { 0 } ^ { 0 } r _ { 0 } ^ { 0 } p _ { 0 } ^ { 1 } r _ { 0 } ^ { 1 } \ldots p _ { 0 } ^ { n - 1 } , r _ { 0 } ^ { n - 1 } )$ $n$ $( p _ { 0 } ^ { 0 } , r _ { 0 } ^ { 0 } , p _ { 0 } ^ { 1 } , r _ { 0 } ^ { 1 } , \ldots , p _ { 0 } ^ { n - 1 } , r _ { 0 } ^ { n - 1 } )$ , rn−10 ), we treat it as n single-turn dialogue $n$ partitioning strategy not only equips LLaDA with multi-turn dialogue capabilities but also aligns with the above |EOS| padding strategy.

Table 5: Model Architecture. We report the architectural configurations for our 1B and 7B ARM baselines, the 1B and 8B LLaDA models, and the 8B LLaMA3 model.   

<table><tr><td></td><td>Our ARM Baseline 1B</td><td>LLaDA 1B</td><td>Our ARM Baseline 7B</td><td>LLaDA 8B</td><td>LLaMA3 8B</td></tr><tr><td>Layers</td><td>22</td><td>22</td><td>28</td><td>32</td><td>32</td></tr><tr><td>Model dimension</td><td>2048</td><td>2048</td><td>4096</td><td>4096</td><td>4096</td></tr><tr><td>Attention heads</td><td>32</td><td>32</td><td>32</td><td>32</td><td>32</td></tr><tr><td>Vocabulary size</td><td>126,464</td><td>126,464</td><td>126,464</td><td>126,464</td><td>128,000</td></tr><tr><td>FFN dimension</td><td>5634</td><td>5634</td><td>13,440</td><td>12,288</td><td>14,336</td></tr><tr><td>Key/Value heads</td><td>4</td><td>4</td><td>8</td><td>32</td><td>8</td></tr><tr><td>Total parameters</td><td>1.49 B</td><td>1.49 B</td><td>6.83 B</td><td>8.02 B</td><td>8.03 B</td></tr><tr><td>Non-embedding parameters</td><td>0.97 B</td><td>0.97 B</td><td>5.80 B</td><td>6.98 B</td><td>6.98 B</td></tr></table>

Table 6: Ablation on CFG. CFG consistently improves the performance of LLaDA.   

<table><tr><td></td><td>ARC-C</td><td>Hellaswag</td><td>TruthfulQA</td><td>WinoGrande</td><td>GPQA</td><td>PIQA</td></tr><tr><td>w/o CFG</td><td>45.9</td><td>70.5</td><td>46.1</td><td>74.8</td><td>25.2</td><td>73.6</td></tr><tr><td>w/ CFG</td><td>47.9</td><td>72.5</td><td>46.4</td><td>74.8</td><td>26.1</td><td>74.4</td></tr></table>

# B.2 Details about Model Training

This section provides the training details of LLaDA and the corresponding ARM baselines.

Firstly, for efficiency, we trained an ARM and an MDM, both with 1.5B parameters and identical architectures. Additionally, we scaled the MDM to 8B parameters. Due to computational resource constraints, we did not train an 8B autoregressive model with the same architecture. Instead, we utilized our previously trained 7B autoregressive model for comparison. These four models are utilized in the scalability analysis in Sec. 3.1.

We adopted a Transformer architecture similar to LLaMA [6, 21] for the ARMs and MDMs we trained. Specifically, we employ RMSNorm [105] to stabilize training, use SwiGLU [106] as the activation function to enhance non-linearity, and integrate RoPE [107] for more expressive positional encoding. Tab. 5 provides an overview of the model architectures.

For the 1B and 7B ARM baselines, as well as the 1B and 8B LLaDA models, we utilized the AdamW optimizer [29] with a weight decay of 0.1 and adopted the Warmup-Stable-Decay [28] learning rate scheduler. The learning rate was linearly increased from 0 to the maximum value over the first 2000 iterations and then held constant. For LLaDA 8B, to ensure stable training, the learning rate was reduced once during pre-training, as detailed in Sec. 2.2. For the 1B ARM baseline and both the 1B and 8B LLaDA models, the maximum learning rate is set to $4 \times 1 0 ^ { - 4 }$ with a batch size of 1280, without any hyperparameter tuning. For the 7B ARM baseline, the maximum learning rate is set to $4 . 2 \times 1 0 ^ { - \bar { 4 } }$ with a batch size of 4224, both selected via grid search.

Additionally, we employ the widely used $6 N D$ formulation [108, 109] to calculate the training FLOPs in Fig. 3, where $N$ represents the number of non-embedding parameters, and $D$ denotes the total number of training tokens. The detailed results corresponding to Fig. 3 are provided in Tab. 18 and Tab. 19.

# B.3 Ablation on Classifier-free Guidance

This section presents an ablation study on classifier-free guidance (CFG). Theoretical details about CFG can be found in the Appendix A.2.

For simplicity, we select six representative benchmarks, including ARC-C, HellaSwag, TruthfulQA, WinoGrande, PIQA, and GPQA, and conduct experiments using LLaDA 8B Base. We search the CFG scale in $\{ 0 . 5 , 1 , 1 . 5 , 2 \}$ for each task and report the best result. As shown in Tab. 6, CFG consistently improves the performance of LLaDA. We emphasize that, to ensure a fair comparison with ARMs, CFG is not used in the main results reported in the paper.

![](images/8894a1c5c75490bb360fcf0eeeccc9761453ac918960bc93125ef1009239c00b.jpg)  
(a) Autoregressive.

![](images/51dab35d5fe92b78956f6e361d0fce0ea02ab47bdd0d49350933bacb1889d631.jpg)  
(b) Block Diffusion.

![](images/5e78a70d030fe781aaa25f965dfe7f7ce793f27416144f8ed4d6ad7cf115db61.jpg)  
(c) Block Diffusion LLaDA.   
Figure 4: Flexible Sampling Strategies Supported by LLaDA. Colored squares depict non-masked tokens, while squares marked with $\times$ denote masked tokens. In this illustration, the block length for both block diffusion and block diffusion LLaDA sampling is 4.

Table 7: Ablation on Sampling Strategies for LLaDA 8B Base. $L ^ { \prime }$ is the block length. Pure diffusion sampling achieves the best overall performance.   

<table><tr><td></td><td></td><td>BBH</td><td>GSM8K</td><td>Math</td><td>HumanEval</td><td>MBPP</td></tr><tr><td>Autoregressive</td><td></td><td>38.1</td><td>63.1</td><td>23.6</td><td>18.3</td><td>33.4</td></tr><tr><td rowspan="4">Block Diffusion</td><td>L&#x27; = 2</td><td>37.3</td><td>62.6</td><td>25.2</td><td>14.6</td><td>33.6</td></tr><tr><td>L&#x27; = 4</td><td>40.0</td><td>65.7</td><td>26.6</td><td>15.9</td><td>36.0</td></tr><tr><td>L&#x27; = 8</td><td>42.0</td><td>68.2</td><td>27.7</td><td>19.5</td><td>39.2</td></tr><tr><td>L&#x27; = 32</td><td>45.7</td><td>68.6</td><td>29.7</td><td>29.9</td><td>37.4</td></tr><tr><td rowspan="4">Block Diffusion LLaDA</td><td>L&#x27; = 2</td><td>48.0</td><td>70.0</td><td>30.8</td><td>26.2</td><td>40.0</td></tr><tr><td>L&#x27; = 4</td><td>48.5</td><td>70.3</td><td>31.3</td><td>27.4</td><td>38.8</td></tr><tr><td>L&#x27; = 8</td><td>48.6</td><td>70.2</td><td>30.9</td><td>31.1</td><td>39.0</td></tr><tr><td>L&#x27; = 32</td><td>48.3</td><td>70.3</td><td>31.2</td><td>32.3</td><td>40.0</td></tr><tr><td>Pure Diffusion</td><td></td><td>49.7</td><td>70.3</td><td>31.4</td><td>35.4</td><td>40.0</td></tr></table>

Table 8: Ablation on Sampling Strategies for LLaDA 8B Instruct. The block length is set to 32 for efficiency. Pure diffusion sampling achieves the best overall performance.   

<table><tr><td></td><td>GSM8K</td><td>Math</td><td>HumanEval</td><td>MBPP</td><td>GPQA</td><td>MMLU-Pro</td><td>ARC-C</td></tr><tr><td>Autoregressive</td><td>0</td><td>9.5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>84.4</td></tr><tr><td>Block Diffusion</td><td>24.6</td><td>23.5</td><td>17.1</td><td>21.2</td><td>29.3</td><td>32.5</td><td>88.1</td></tr><tr><td>Block Difusion LLaDA</td><td>77.5</td><td>42.2</td><td>46.3</td><td>34.2</td><td>31.3</td><td>34.8</td><td>85.4</td></tr><tr><td>Pure Diffusion</td><td>69.4</td><td>31.9</td><td>49.4</td><td>41.0</td><td>33.3</td><td>37.0</td><td>88.5</td></tr></table>

# B.4 Details and Ablation on Sampling Strategies

In this section, we first introduce the different sampling strategies supported by LLaDA. We then present ablation studies to evaluate the performance of these sampling strategies.

Flexible Sampling Strategies. In Sec. 2.4, Fig. 2 (c) illustrates the reverse generation process of LLaDA. As shown in Fig. 4, in addition to the reverse generation process, LLaDA also supports autoregressive and block diffusion [31] sampling directly after the pre-training or SFT stages, without requiring any further modifications or retraining. Block diffusion sampling applies the origin reverse process within each block and the autoregressive sampling across blocks. In the original block diffusion process, the sequence length varies dynamically. As shown in Fig. 4 (c), LLaDA can also adopt a fixed-length block diffusion strategy, which we refer to as block diffusion LLaDA, also known as semi-autoregressive remasking.

Experimental Setup. We evaluate different sampling strategies using both LLaDA 8B Base and LLaDA 8B Instruct for comprehensive analysis. For LLaDA 8B Base, we use the five benchmarks in Tab. 1 that are evaluated based on sampling rather than likelihood estimation. For LLaDA 8B Instruct, we use the seven metrics in Tab. 2, excluding MMLU and HellaSwag, since these two tasks only require the model to generate a single token (i.e., A, B, C, or D). In all settings, one token is generated

Table 9: Analysis on Random and Low-confidence Remasking Strategies. The low-confidence remasking consistently outperforms the random remasking.   

<table><tr><td>Length</td><td>BBH</td><td>GSM8K</td><td>Math</td><td>HumanEval</td><td>MBPP</td></tr><tr><td>Random Remasking</td><td>32.1</td><td>21.3</td><td>9.2</td><td>11.6</td><td>21.0</td></tr><tr><td>Low-confidence Remasking</td><td>45.0</td><td>70.0</td><td>30.3</td><td>32.9</td><td>40.2</td></tr></table>

Table 10: Ablation on Generation Length. The results are not sensitive to the length hyperparameter.   

<table><tr><td>Length</td><td>BBH</td><td>GSM8K</td><td>Math</td><td>HumanEval</td><td>MBPP</td></tr><tr><td>256</td><td>45.0</td><td>70.0</td><td>30.3</td><td>32.9</td><td>40.2</td></tr><tr><td>512</td><td>50.4</td><td>70.8</td><td>30.9</td><td>32.9</td><td>39.2</td></tr><tr><td>1024</td><td>49.7</td><td>70.3</td><td>31.4</td><td>35.4</td><td>40.0</td></tr></table>

per sampling step. For autoregressive and block diffusion sampling, generation terminates when the |EOS| token is produced. For block diffusion LLaDA (i.e., semi-autoregressive remasking) and pure diffusion sampling, the generation length is fixed at 1024 for LLaDA 8B Base, while for LLaDA 8B Instruct, it is tuned from {64, 256, 512} to balance efficiency and performance. Low-confidence remasking is applied to intra-block diffusion sampling in both block diffusion and block diffusion LLaDA, as well as to pure diffusion sampling. We also test different block lengths for LLaDA 8B Base. For LLaDA 8B Instruct, we only evaluate block length 32 for efficiency, as it yields the best results on LLaDA 8B Base.

Additionally, for LLaDA 8B Instruct, due to heavy padding of $| \mathrm { E O S } |$ tokens in the SFT data (as detailed in Sec. B.1), we observe that under pure diffusion sampling, the proportion of $| \mathrm { E O S } |$ tokens in model outputs becomes very high. This leads to extremely short generations and degrades model performance. To mitigate this issue, for HumanEval, MBPP, GSM8K, Math, and GPQA, we set the confidence score of the $| \mathrm { E O S } |$ token to zero during pure diffusion sampling. This adjustment helps maintain an appropriate ratio of $| \mathrm { E O S } |$ tokens during generation.

Finally, we conduct ablation studies to analyze the effects of random and low-confidence remasking strategies using the pure diffusion sampling. For efficiency, we use LLaDA 8B Base with generation length and sampling steps set to 256 in this analysis.

Results. As shown in Tab. 7, for block diffusion sampling, overall performance improves as the block length increases. Moreover, both Tab. 7 and Tab. 8 show that block diffusion sampling consistently outperforms autoregressive sampling, and block diffusion LLaDA sampling further improves upon standard block diffusion sampling. Finally, pure diffusion sampling achieves the best overall performance.

In addition, Tab. 9 shows that the low-confidence remasking strategy consistently outperforms the random remasking strategy. We hypothesize that low-confidence remasking functions similarly to the annealed sampling method used by default in ARMs, improving accuracy by reducing the diversity of generated sentences.

We discover that autoregressive sampling leads to very poor performance for LLaDA 8B Instruct. This is because each SFT data is a complete sentence, so given a sequence length, LLaDA 8B Instruct tends to generate a full sentence within that length. In contrast, LLaDA 8B Base does not suffer from this issue, as the pre-training data consists of truncated documents (as detailed in Appendix B.1) and the model is trained with random sequence lengths (as detailed in Sec. 2.2). As a result, when given a short sequence length, LLaDA 8B Base tends to generate only part of a sentence, which can then be used as a prefix to continue generation.

Setting the block length to 8 in Tab. 8 further improves the GSM8K score from 77.5 to 78.6.

# B.5 Ablation on Generated Length

In this section, we conduct ablation studies on the generated length.

![](images/280a6e083df29d3bb84503d9a43e0c33417bc481d47406c7e3a49a2198f90aae.jpg)

![](images/7e9080596a09001d1408443a81da2d7d674bb6688634e954b485ed879f0f8267.jpg)

![](images/d42cffe585530732184e5822d930f996dab7a48c96a772644bed6a312b193c03.jpg)

![](images/cf2a94b8765f8bc3d87cd87196cffc93bdf3a353be31d23f85fa1c86fbf4251f.jpg)  
Figure 5: Analysis of Sampling Efficiency. The generation length for LLaDA is set to 256, with sampling steps set to 32, 64, 128, and 256 across the figures. This corresponds to decoding 8, 4, 2, and 1 token(s) per forward pass, respectively. LLaDA enables a flexible trade-off between generation quality and sampling speed.

To ensure fairness, for each setting, we set the number of sampling steps equal to the generated length, ensuring that in each sampling step, just one tokens are transferred from the mask to the text. We conduct experiments using LLaDA 8B Base.

As reported in Tab. 10, the results are not sensitive to the length hyperparameter.

# B.6 Standard Benchmarks and Evaluation Details

In this section, we introduce the benchmarks we used and present the details of our evaluation process.

Following standard LLM [25, 26] evaluation practices, we assess LLaDA across four dimensions:

General ability: MMLU [110], BBH [111], ARC-C [112], Hellaswag [113], TruthfulQA [114], WinoGrande [115] and PIQA [116].

Math and science ability: GSM8K [117], Math [118] and GPQA [119].

Code generation: HumanEval [120], HumanEval-FIM [121] and MBPP [122].

Chinese understanding: CMMLU [123] and C-Eval [124].

For all the aforementioned benchmarks, we follow the widely adopted evaluation process [125] used in LLM assessments, primarily employing conditional likelihood estimation and conditional generation. Specifically, for certain benchmarks, a prompt and multiple candidate answers are provided, and the model is required to compute each candidate’s conditional likelihood. The candidate with the highest likelihood is then selected as the model’s final answer, and accuracy is used as the evaluation metric. For the remaining benchmarks, the model generates responses based on the given prompt, and performance is evaluated using metrics such as exact match and other relevant criteria.

For the base model, we use conditional likelihood estimation for MMLU, CMMLU, C-Eval, ARC-C, Hellaswag, TruthfulQA, WinoGrande, PIQA, and GPQA, while the remaining benchmarks are evaluated using conditional generation. For the instruct model, we evaluate all benchmarks using conditional generation.

Table 11: Analysis of Memory Consumption. Memory is measured in GB. Without any inference optimization techniques (e.g., KV Cache), LLaDA has memory usage comparable to LLaMA3, and slightly higher than LLaMA3 when the latter uses KV Cache.   

<table><tr><td>Input Length</td><td>Output Length</td><td>LLaDA 8B</td><td>LLaMA3 8B w/o KV-Cache</td><td>LLaMA3 8B w/ KV-Cache</td></tr><tr><td rowspan="3">512</td><td>512</td><td>17.03</td><td>16.70</td><td>16.32</td></tr><tr><td>1024</td><td>17.53</td><td>17.49</td><td>16.43</td></tr><tr><td>2048</td><td>18.52</td><td>20.00</td><td>16.73</td></tr><tr><td rowspan="3">1024</td><td>512</td><td>17.53</td><td>17.16</td><td>16.36</td></tr><tr><td>1024</td><td>18.01</td><td>18.26</td><td>16.41</td></tr><tr><td>2048</td><td>19.02</td><td>21.39</td><td>16.74</td></tr></table>

For the base model, we use the widely adopted open-source evaluation framework lm-evaluationharness [125], except for the HumanEval-FIM metric, which is not supported by the framework. For HumanEval-FIM on the base model and all evaluation metrics on the instruct model, we use an internal evaluation library. We choose the internal library as lm-evaluation-harness shows greater deviation from the LLaMA3 results reported by Yang et al. [25], relative to our internal evaluation.

For benchmarks evaluated via conditional likelihood estimation, we use Monte Carlo estimation to approximate Eq. (6) for LLaDA. Since MMLU, CMMLU, and C-EVAL only require the likelihood of a single token, a single Monte Carlo estimate is sufficient for these benchmarks. For all other benchmarks, we find that 128 Monte Carlo samples are adequate to produce stable results.

For benchmarks evaluated using conditional generation, we apply pure diffusion sampling with a low-confidence remasking strategy to both LLaDA Base and LLaDA Instruct. For LLaDA Base, we set both the generation length and the number of sampling steps to 1024. For LLaDA Instruct, the number of sampling steps is set equal to the answer length, which is configured as follows: 3 for MMLU and HellaSwag, 64 for GPQA, 256 for MBPP and MMLU-Pro, and 512 for HumanEval, GSM8K, Math, and ARC-C. As detailed in Appendix B.4, for HumanEval, MBPP, GSM8K, Math, and GPQA, we set the confidence of the |EOS| token to zero during sampling for LLaDA Instruct.

# B.7 Analysis of Sampling Efficiency

In this section, we first analyze the sampling efficiency of LLaDA, including both sampling speed and memory consumption. We then discuss potential optimizations to further improve its efficiency.

We use four representative open-ended generation benchmarks for sampling speed analysis: GSM8K, Math, HumanEval, and MBPP. We use the widely adopted throughput metric to measure generation speed, defined as the number of tokens generated per second. We compare LLaDA 8B Base and LLaMA3 8B Base, both using bfloat16 precision. All experiments in this section were conducted on a single A100-80GB GPU with a batch size of 1. For LLaDA, the output length is fixed to 256 tokens across all four benchmarks.

Fig. 5 shows that LLaDA enables a flexible trade-off between generation quality and speed by adjusting the number of sampling steps. Specifically, on the GSM8K and Math datasets, LLaDA 8B Base achieves comparable performance to LLaMA3 8B Base while delivering 1.5 and 1.8 times higher throughput, even though LLaMA3 uses KV Cache and LLaDA operates without any inference optimization techniques. For the HumanEval benchmark, LLaDA 8B Base performs comparably to LLaMA3 8B Base when the throughput is matched. On the MBPP benchmark, LLaDA 8B Base lags behind LLaMA3 8B Base.

For LLaMA3, the acceleration benefit provided by KV caching is notably weaker on the HumanEval dataset, which can be attributed to its relatively short prompt lengths. Specifically, the average prompt lengths for GSM8K, Math, MBPP, and HumanEval are 894, 680, 628, and 132 tokens, respectively.

Tab. 11 compares of memory consumption between LLaDA 8B Base and LLaMA3 8B Base. To avoid variations in generation length caused by differences in training data, we fix both the input and output token lengths during the memory analysis. For LLaDA, memory usage remains constant regardless of the number of sampling steps. Its memory consumption is comparable to LLaMA3 8B Base without KV cache, but slightly higher than with KV cache.

Table 12: Comparison on iGSM Dataset.   

<table><tr><td></td><td>4 steps</td><td>5 steps</td><td>6 steps</td></tr><tr><td>LLaMA3 8B Base</td><td>38.0</td><td>35.0</td><td>34.0</td></tr><tr><td>LLaDA 8B Base</td><td>64.0</td><td>41.0</td><td>44.0</td></tr></table>

We emphasize that the goal of this study is not to propose a model that is faster than ARMs. Instead, we aim to show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities such as scalability, in-context learning, and instruction-following are inherently depend on ARMs. A substantial body of research [31, 79–87] has focused on improving the generation efficiency of MDMs through algorithmic or architectural innovations. We leave similar efficiency-oriented exploration for LLaDA to future work.

# B.8 Evaluation on iGSM Dataset

To further assess the mathematical capabilities of LLaDA, we test its performance on iGSM [34], an infinite, synthetic GSM8K-like dataset. iGSM is generated via specific rules, with parameters that control the difficulty of problems (i.e., the number of solution steps). For evaluation consistency, we append "#### $answer" to the final solution, adhering to the GSM8K format. Below is an example with solution steps set to 4:

(Question) The number of each North Star Elementary’s Cultural Studies Classroom equals 1. The number of each Westridge Elementary’s Dance Studio equals 3 times as much as the sum of each North Star Elementary’s Classroom and each North Star Elementary’s Cultural Studies Classroom. How many Dance Studio does Westridge Elementary have?

(Solution) Define North Star Elementary’s Cultural Studies Classroom as x; so $\mathbf { x } = 1$

Define North Star Elementary’s Classroom as m; so $\mathbf { m } = \mathbf { x } = 1$

Define Westridge Elementary’s Dance Studio as n; $\mathbf { w } = \mathbf { m } + \mathbf { x } = 1 + 1 = 2$ ;

so $\mathbf { \partial } \mathbf { n } = 3 \ ^ { * } \mathbf { w } = 3 \ ^ { * } 2 = 1$ #### 1

Since there are slight differences between GSM8K and iGSM (e.g., the use of a mod 5 algorithmic system), we follow [34] and provide a system prompt along with four-shot question-answer pairs for each problem.

(Prompt) You’re an expert at solving elementary math problems involving addition, subtraction, and multiplication. You solve all the problems in a uniform format. All calculations are done modulo 5. For example, $4 + 4$ equals 3, $2 + 4$ equals 1, $3 + 3 + 3$ equals 4, $3 \ast 3$ equals 4, and $2 ^ { \ast } 2$ equals 4. When providing your solution, please end with ’#### $\mathbf { X }$ .’ where $\mathbf { X }$ is your final answer, an integer between 0 and 4. You must solve all the problems using the same solution format. Our scenarios involve up to four categories of objects: schools, classrooms, backpacks and stationeries. Each school may contain classrooms, each classroom may contain backpacks, and each backpack may contain stationeries. We can specify quantities, such as ¨the number of dance studios at each Lakeshore High.Ässume that every entity with the same name has an identical configuration; for example, each Lakeshore High contains the same number of dance studios. Another guiding principle is that what is not mentioned does not exist: when we refer to classrooms at Lakeshore High, we are only discussing the classrooms explicitly mentioned in our scenario. Furthermore, if Lakeshore High is not even mentioned, any classroom within it is automatically considered to be non-existent (i.e. 0).

For solution steps ranging from 4 to 6, we generate 100 questions for each case and report the corresponding accuracy in Table 12. As shown in the table, LLaDA 8B Base demonstrates significant and consistent advantages over LLaMA3 8B Base on unseen mathematical problems, aligning with the results in Table 1.

Table 13: Visualization of the Block Diffusion LLaDA Sampling Process. In the response of LLaDA, darker colors indicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.   

<table><tr><td rowspan="6">User 
LLaDA</td><td colspan="13">What are the benefits of regular exercise for physical and mental health?</td><td></td></tr><tr><td>Regular</td><td>exercise</td><td>can</td><td>improve</td><td>physical</td><td>health</td><td>by</td><td>strengthening</td><td>muscles</td><td>and</td><td>bones</td><td>,</td><td></td><td></td></tr><tr><td>improving</td><td colspan="2">cardiovascular</td><td>health</td><td>,</td><td>and</td><td>reducing</td><td>the</td><td>risk</td><td>of</td><td>chronic</td><td>diseases</td><td>such</td><td>as</td></tr><tr><td>diabetes</td><td>and</td><td>heart</td><td>disease</td><td>.</td><td>lt</td><td>can</td><td>also</td><td>improve</td><td>mental</td><td>health</td><td>by</td><td>reducing</td><td>stress</td></tr><tr><td>and</td><td>anxiety</td><td>,</td><td>improving</td><td>mood</td><td>,</td><td>and</td><td>enhancing</td><td>cognitive</td><td>function</td><td>.</td><td>Additionally</td><td>,</td><td></td></tr><tr><td>exercise</td><td>can</td><td>improve</td><td>sleep</td><td>quality</td><td>and</td><td>increase</td><td>energy</td><td>levels</td><td>.</td><td></td><td></td><td></td><td></td></tr></table>

# B.9 Poem Completion Tasks

In this section, we present examples from our poem completion dataset as follows.

Example 1:

Prompt: 淑 的下一句是什么？直接输出句子即可。

窈窕 女Answer: 君子 逑。

Example 2:

Prompt: 不 一格降人 的上一句是什么？直接输出句子即可。

拘 才Answer: 劝 公 。

# B.10 More Case Studies

In this section, we present additional case studies of LLaDA 8B Instruct. First, Tab. 13 shows the sampling process of the block diffusion LLaDA sampling, while Tab. 14 depicts the sampling process for multi-turn dialogues with random remasking. Additionally, Tab. 15 and Tab. 16 provide further examples of single-turn and multi-turn dialogues. Finally, Tab. 17 presents examples of poem reversal completions where the LLaDA 8B Instruct model succeeds, in contrast to the failure of GPT-4o.

# C Impact Statement

Our work shows the promise of diffusion models for language modeling at scale and challenges the common assumption that core LLM capabilities such as scalability, in-context learning, and instruction-following are inherently dependent on ARMs. Our findings open new avenues for exploring alternative probabilistic paradigms in natural language processing, with potential applications in conversational AI, code generation, and complex reasoning tasks.

However, diffusion models, like traditional LLMs, raise similar societal concerns. These include the environmental impact of large-scale training, the potential misuse for generating harmful content, and the amplification of biases present in training data. Addressing these challenges is critical to ensuring the responsible development and deployment of diffusion language models.

Table 14: Visualization of the Multi-turn Dialogue. We employ random remasking strategy. In the response of LLaDA, darker colors indicate tokens predicted in the later stages of sampling, while lighter colors correspond to earlier predictions.   

<table><tr><td>User</td><td>I want to have a pet, but I don&#x27;t know whether to get a cat or a dog. Do you have any suggestions?</td></tr><tr><td>LLaDA</td><td>It really depends on your preferences , lifestyle , and the type of pet you want . If you want an independent pet and have a busy schedule , a cat is a good choice . If you prefer a companion and have a more active lifestyle , a dog might be a better fit . Have you considered your options ?</td></tr><tr><td>User</td><td>I&#x27;m very busy at work, so it seems like a cat would be a more suitable option. Do you have any suggestions for getting a cat?</td></tr><tr><td>LLaDA</td><td>Sure ! First , make sure you have enough space in your home and provide food , water , and toys for your cat . Also , research different breeds and personalities , and choose cats that are suitable for your lifestyle . Lastly , make sure you have the necessary vaccinations and insurance in place before you get a pet .</td></tr></table>

Table 15: Single-turn Dialogue Cases of LLaDA 8B Instruct.   

<table><tr><td>Daily life
User
LLaDA</td><td>What&#x27;s a piece of advice you&#x27;ve received that you&#x27;ve found helpful?
One helpful piece of advice I&#x27;ve received is to always be open to learning and growth. I&#x27;ve found that being willing to explore new ideas and experiences has helped me become more resilient and adaptable. By embracing a growth mindset, I&#x27;ve been able to identify opportunities for improvement and development, both personally and professionally.</td></tr><tr><td>Daily life
User
LLaDA</td><td>Please recommend me three famous movies
Certainly! Here are three famous movies that have achieved widespread popularity and critical acclaim:
1. **The Shawshank Redemption** (1994) - Inspired by a novella and directed by Frank Darabont, this film tells the story of Andy Dufresne, a man convicted of murdering his wife, dealing with his harsh life in Shawshank Prison and forming an unlikely friendship with a fellow inmate.
2. **The Godfather** (1972) - Directed by Francis Ford Coppola, this film is based on Mario Puzo&#x27;s novel of the same name. It follows the story of the Corleone family, particularly Michael Corleone, who rises to power and takes over the family&#x27;s criminal empire.
3. **The Empire Strikes Back** (1980) - Directed by George Lucas, this film continues the story of a young Skywalker in the Star Wars saga. It follows Luke Skywalker as he helps the Rebel Alliance fight against the Galactic Empire and destroy the powerful weapon, the Death Star.
All of these films are celebrated for their cinematic achievements, powerful storytelling, and memorable characters.</td></tr><tr><td>Translation
User
LLaDA</td><td>Please help me translate into Chinese: &#x27;What is now proved was once only imagined&#x27;
现在被证明的曾经只是想象</td></tr><tr><td>Translation
User
LLaDA</td><td>Please help me translate into German: &#x27;What is now proved was once only imagined&#x27;
Wer heute bestätig ist, war einst nur immaginiert.</td></tr><tr><td>Math
User
LLaDA</td><td>Emma decides to run 4 sprints 5 times a week. She runs 75 meters each sprint. How many total meters does she run a week?
Emma runs 4 sprints each time, so she runs a total of 4 x 5 = 20 sprints a week. Each sprint is 75 meters long, so Emma runs a total of 20 x 75 = 1500 meters a week. The final result is 1500</td></tr><tr><td>Code
User
LLaDA</td><td>I am writing python code, I have an array, please help me sort it from largest to smallest
Certainly! You can sort an array from the largest to smallest in Python using the &#x27;sort()
method or the &#x27;sorted()&#x27; function. Here&#x27;s an example:
&gt;&gt;&gt; Using &#x27;sort()
method:
my_array = [3, 1, 4, 1, 5, 2]
my_array.sort(reverse=True)
print(my_array)</td></tr></table>

Table 16: Multi-turn Dialogue Cases of LLaDA 8B Instruct.   

<table><tr><td colspan="2">Daily life</td></tr><tr><td>User</td><td>How do I prepare for a job interview?</td></tr><tr><td>LLaDA</td><td>To prepare for a job interview, you should research the company and the job, practice common interview questions, and dress appropriately.</td></tr><tr><td>User</td><td>What are some common interview questions?</td></tr><tr><td>LLaDA</td><td>Some common interview questions include “Tell me about yourself” and “Why do you want to work for us”?</td></tr><tr><td colspan="2">Daily life</td></tr><tr><td>User</td><td>How do I start learning programming?</td></tr><tr><td>LLaDA</td><td>To learn programming, you can start by choosing a programming language that you are interested in. Some popular languages include Python, Java, and C++.</td></tr><tr><td>User</td><td>Which programming language should I learn first?</td></tr><tr><td>LLaDA</td><td>It depends on your interests and goals. Python is a great language for beginners for its simplicity and versatility.</td></tr><tr><td>User</td><td>How long does it take to learn Python?</td></tr><tr><td>LLaDA</td><td>It can take anywhere from a few weeks to several months to learn Python.</td></tr><tr><td>User</td><td>What are some good Python projects for beginners?</td></tr><tr><td>LLaDA</td><td>Some good Python projects for beginners include building a simple calculator, a game, and a web application.</td></tr><tr><td colspan="2">Math</td></tr><tr><td>User</td><td>Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?</td></tr><tr><td>LLaDA</td><td>Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Altogether, Natalia sold 48 + 24 = 72 clips in April and May. The final result is 72.</td></tr><tr><td>User</td><td>Natalia sold three times as many items in June as she did in May. How many clips did Natalia sell Altogether in these three months?</td></tr><tr><td>LLaDA</td><td>Natalia sold clips to 48 friends in April. She sold half as many clips in May, so she sold 48/2 = 24 clips in May. Natalia sold three times as many items in June as she did in May, so she sold 24 * 3 = 72 clips in June. Altogether, Natalia sold 48 + 24 + 72 = 144 clips in these three months. The final result is 144.</td></tr></table>

Table 17: Poem Reversal Completion Cases where LLaDA 8B Instruct Succeeds but GPT-4o Fails.   

<table><tr><td rowspan="2">Question1
User
GPT-4o</td><td>“但闻人语响”的上一句是什么?</td></tr><tr><td>“但闻人语响”的上一句是“无可奈何花落去”,这两句出自唐代词人李清照的《武陵春,春晚:无可奈何花落去,但闻人语响。这首词表达了作者在春天时节的孤独与感伤,借花落和人语声的描写,反映出词人内心的无亲与失落。</td></tr><tr><td>LLaDA</td><td>“但闻人语响”的上一句是“空山不见人”</td></tr><tr><td rowspan="2">Question2
User
GPT-4o</td><td>“夜静春山空”的上一句是什么?</td></tr><tr><td>“夜静春山空”的上一句是“空山新雨后”.这两句出自唐代诗人王维的《山居秋膜》:空山新雨后,夜静春山空。这首诗描绘了春天山中雨后的寂静与空灵,表现出自然的宁静与和谐美。</td></tr><tr><td>LLaDA</td><td>“夜静春山空”的上一句是“人闲桂花落”</td></tr></table>

Table 18: Detailed results of LLaDA in Fig. 3. "-" indicates missing values, which do not affect the observations regarding the scalability of LLaDA. These missing values are due to hardware failures.   

<table><tr><td>Model</td><td>Training Tokens</td><td>FLOPs</td><td>MMLU</td><td>CMMLU</td><td>ARC-C</td><td>PIQA</td><td>GSM8K</td><td>HumanEval</td></tr><tr><td>LLaDA 1B</td><td>37.75B</td><td>2.20e20</td><td>25.52</td><td>25.95</td><td>25.17</td><td>59.41</td><td>1.82</td><td>0.00</td></tr><tr><td>LLaDA 1B</td><td>88.08B</td><td>5.13e20</td><td>27.11</td><td>26.52</td><td>26.96</td><td>61.86</td><td>3.03</td><td>1.83</td></tr><tr><td>LLaDA 1B</td><td>138.41B</td><td>8.06e20</td><td>29.32</td><td>27.13</td><td>30.20</td><td>63.38</td><td>2.35</td><td>0.00</td></tr><tr><td>LLaDA 1B</td><td>239.08B</td><td>1.39e21</td><td>31.48</td><td>30.77</td><td>27.99</td><td>63.11</td><td>3.26</td><td>1.22</td></tr><tr><td>LLaDA 1B</td><td>352.32B</td><td>2.05e21</td><td>35.86</td><td>34.35</td><td>31.31</td><td>65.34</td><td>3.64</td><td>3.05</td></tr><tr><td>LLaDA 1B</td><td>461.37B</td><td>2.69e21</td><td>31.86</td><td>30.98</td><td>30.12</td><td>65.51</td><td>2.35</td><td>0.61</td></tr><tr><td>LLaDA 8B</td><td>62.91B</td><td>2.63e21</td><td>32.22</td><td>28.5</td><td>30.20</td><td>63.82</td><td>3.87</td><td>2.44</td></tr><tr><td>LLaDA 8B</td><td>125.83B</td><td>5.27e21</td><td>33.39</td><td>33.9</td><td>34.64</td><td>66.54</td><td>8.72</td><td>3.66</td></tr><tr><td>LLaDA 8B</td><td>251.66B</td><td>1.05e22</td><td>42.84</td><td>40.59</td><td>40.10</td><td>69.04</td><td>15.31</td><td>3.66</td></tr><tr><td>LLaDA 8B</td><td>377.49B</td><td>1.58e22</td><td>45.11</td><td>43.99</td><td>39.25</td><td>68.61</td><td>25.40</td><td>9.76</td></tr><tr><td>LLaDA 8B</td><td>503.32B</td><td>2.11e22</td><td>43.57</td><td>41.38</td><td>42.06</td><td>70.24</td><td>27.52</td><td>9.76</td></tr><tr><td>LLaDA 8B</td><td>629.14B</td><td>2.63e22</td><td>48.80</td><td>47.13</td><td>42.24</td><td>72.09</td><td>30.10</td><td>12.80</td></tr><tr><td>LLaDA 8B</td><td>679.48B</td><td>2.85e22</td><td>49.61</td><td>48.19</td><td>41.30</td><td>70.84</td><td>26.31</td><td>8.54</td></tr><tr><td>LLaDA 8B</td><td>792.72B</td><td>3.31e22</td><td>50.88</td><td>49.01</td><td>42.58</td><td>70.51</td><td>31.99</td><td>6.10</td></tr><tr><td>LLaDA 8B</td><td>981.47B</td><td>4.11e22</td><td>49.47</td><td>48.10</td><td>40.27</td><td>71.38</td><td>-</td><td>6.10</td></tr><tr><td>LLaDA 8B</td><td>1107.30B</td><td>4.64e22</td><td>51.13</td><td>47.57</td><td>41.13</td><td>69.26</td><td>36.69</td><td>10.37</td></tr><tr><td>LLaDA 8B</td><td>1233.13B</td><td>5.16e22</td><td>50.52</td><td>49.72</td><td>45.05</td><td>71.49</td><td>38.97</td><td>9.76</td></tr><tr><td>LLaDA 8B</td><td>1358.95B</td><td>5.69e22</td><td>54.61</td><td>53.97</td><td>49.40</td><td>74.05</td><td>48.14</td><td>17.68</td></tr><tr><td>LLaDA 8B</td><td>1547.70B</td><td>6.48e22</td><td>57.38</td><td>56.04</td><td>49.49</td><td>74.59</td><td>53.30</td><td>20.73</td></tr><tr><td>LLaDA 8B</td><td>1975.52B</td><td>8.27e22</td><td>58.52</td><td>57.87</td><td>50.68</td><td>75.35</td><td>-</td><td>19.51</td></tr></table>

Table 19: Detailed results of the autoregressive baelines in Fig. 3.   

<table><tr><td>Model</td><td>Training Tokens</td><td>FLOPs</td><td>MMLU</td><td>CMMLU</td><td>ARC-C</td><td>PIQA</td><td>GSM8K</td><td>HumanEval</td></tr><tr><td>ARM 1B</td><td>37.75B</td><td>2.20e20</td><td>25.47</td><td>25.38</td><td>30.20</td><td>67.36</td><td>2.20</td><td>4.88</td></tr><tr><td>ARM 1B</td><td>88.08B</td><td>5.13e20</td><td>24.67</td><td>25.23</td><td>33.96</td><td>70.02</td><td>7.51</td><td>10.37</td></tr><tr><td>ARM 1B</td><td>138.41B</td><td>8.06e20</td><td>29.25</td><td>27.48</td><td>33.45</td><td>70.29</td><td>8.34</td><td>9.76</td></tr><tr><td>ARM 7B</td><td>17.30B</td><td>6.02e20</td><td>26.92</td><td>25.18</td><td>21.02</td><td>57.18</td><td>1.29</td><td>1.22</td></tr><tr><td>ARM 7B</td><td>34.60B</td><td>1.20e21</td><td>25.83</td><td>25.38</td><td>24.07</td><td>62.84</td><td>1.59</td><td>2.44</td></tr><tr><td>ARM 7B</td><td>86.50B</td><td>3.01e21</td><td>24.41</td><td>24.90</td><td>25.42</td><td>71.11</td><td>2.88</td><td>7.93</td></tr><tr><td>ARM 7B</td><td>173.02B</td><td>6.02e21</td><td>26.20</td><td>24.78</td><td>26.10</td><td>74.27</td><td>6.67</td><td>9.15</td></tr><tr><td>ARM 7B</td><td>207.62B</td><td>7.23e21</td><td>30.36</td><td>28.86</td><td>31.86</td><td>74.48</td><td>8.57</td><td>12.80</td></tr><tr><td>ARM 7B</td><td>224.92B</td><td>7.83e21</td><td>29.49</td><td>32.26</td><td>31.19</td><td>74.37</td><td>8.95</td><td>8.54</td></tr><tr><td>ARM 7B</td><td>242.22B</td><td>8.43e21</td><td>33.62</td><td>31.38</td><td>34.92</td><td>75.41</td><td>10.84</td><td>9.15</td></tr><tr><td>ARM 7B</td><td>259.52B</td><td>9.03e21</td><td>34.11</td><td>34.20</td><td>32.88</td><td>75.19</td><td>9.33</td><td>10.98</td></tr><tr><td>ARM 7B</td><td>311.43B</td><td>1.08e22</td><td>35.66</td><td>35.49</td><td>36.61</td><td>75.14</td><td>11.30</td><td>10.37</td></tr><tr><td>ARM 7B</td><td>363.33B</td><td>1.26e22</td><td>34.54</td><td>37.67</td><td>34.58</td><td>76.55</td><td>12.28</td><td>14.02</td></tr><tr><td>ARM 7B</td><td>415.24B</td><td>1.45e22</td><td>35.37</td><td>38.37</td><td>35.25</td><td>76.39</td><td>14.40</td><td>12.80</td></tr><tr><td>ARM 7B</td><td>449.84B</td><td>1.57e22</td><td>39.51</td><td>39.24</td><td>34.92</td><td>76.82</td><td>14.94</td><td>14.63</td></tr><tr><td>ARM 7B</td><td>519.09B</td><td>1.81e22</td><td>40.30</td><td>40.69</td><td>37.29</td><td>77.15</td><td>14.03</td><td>14.63</td></tr><tr><td>ARM 7B</td><td>778.57B</td><td>2.71e22</td><td>43.33</td><td>43.50</td><td>38.31</td><td>77.53</td><td>17.59</td><td>14.63</td></tr><tr><td>ARM 7B</td><td>1038.09B</td><td>3.61e22</td><td>45.06</td><td>46.12</td><td>41.69</td><td>77.86</td><td>20.02</td><td>15.85</td></tr><tr><td>ARM 7B</td><td>1384.12B</td><td>4.82e22</td><td>47.63</td><td>48.18</td><td>47.80</td><td>76.93</td><td>22.82</td><td>15.24</td></tr><tr><td>ARM 7B</td><td>2076.18B</td><td>7.23e22</td><td>47.68</td><td>50.85</td><td>44.07</td><td>77.37</td><td>24.79</td><td>14.63</td></tr><tr><td>ARM 7B</td><td>2214.59B</td><td>7.71e22</td><td>49.26</td><td>52.08</td><td>53.56</td><td>77.69</td><td>27.37</td><td>17.07</td></tr></table>