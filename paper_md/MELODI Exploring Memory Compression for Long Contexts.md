# MELODI: EXPLORING MEMORY COMPRESSION FOR LONG CONTEXTS

Yinpeng Chen, DeLesley Hutchins, Aren Jansen, Andrey Zhmoginov,

David Racz, Jesper Andersen

Google DeepMind

{yinpengc,delesley,arenjansen,azhmogin,dracz,jespera}@google.com

# ABSTRACT

We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both network layers and context windows. Specifically, the short-term memory is achieved through recurrent compression of context windows across multiple layers, ensuring smooth transitions between windows. In contrast, the long-term memory performs further compression within a single middle layer and aggregates information across context windows, effectively consolidating crucial information from the entire history. Compared to a strong baseline - the Memorizing Transformer employing dense attention over a large long-term memory (64K key-value pairs) - our method demonstrates superior performance on various long-context datasets while remarkably reducing the memory footprint by a factor of 8.

# 1 INTRODUCTION

![](images/be4c08fd0c43f38d1aa96b431dbd63026e1c909f6d43535ee7f712dbd72ac101.jpg)  
Figure 1: Overview of MELODI. MELODI employs a hierarchical memory representation, incorporating both short-term and long-term compression mechanisms, integrated with a transformer-based language model. It utilizes a stack of short-term layers to recurrently compress each context window $x _ { k } ^ { 0 }$ into short-term memory tokens $\{ z _ { k } ^ { l } \}$ , and inserts a long-term layer to store compressed key-value pairs within a long-term memory $m _ { 1 : k }$ . Both short-term and long-term layers leverage modified transformer blocks. In this illustration, we assume a total of $N$ layers, with $M$ short-term layers preceding 1 long-term layer and $N - M - 1$ short-term layers following it.

Long-context language models, exemplified by Gemini (Gemini-Team, 2024) and GPT (OpenAI, 2024), showcase remarkable capabilities across diverse modalities (e.g., text, images, audio, code, video) and seamlessly integrate various machine learning techniques, including many-shot incontext learning (Agarwal et al., 2024), chain-of-thought prompting (Wei et al., 2022b), and the

incorporation of explicit instructions (Chung et al., 2024; Wei et al., 2022a). However, the quadratic complexity of attention mechanisms within transformer models necessitates significant computational resources to handle long contexts effectively. This has spurred the development of efficient solutions (Dai et al., 2019; Wu et al., 2022; Bulatov et al., 2022) that process long contexts via short context windows, much like how humans process information by reading a book chapter by chapter. A central question underlying these solutions is: how can we effectively model and manage memory to bridge the gaps between these short context windows over long context?

Memory fundamentally revolves around compressing and storing information for future utilization, all within the constraints of limited capacity. The Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) tackles this by recurrently compressing historical information into hidden states after processing each token. With the rise of Transformer models (Vaswani et al., 2017) dominating the language modeling landscape, recent memory designs have shifted towards utilizing Transformers to process a context window, thereby moving the focus of memory management from the token level to the context window level.

Transformer-XL (Dai et al., 2019) employs a caching mechanism to store multi-layer key-value (KV) pairs from the preceding window as memory. Memorizing Transformer (Wu et al., 2022) builds upon this foundation by incorporating a dedicated layer to memorize all KV pairs from that layer across all prior windows. Meanwhile, Block Recurrent Transformer (Hutchins et al., 2022) and Recurrent Memory Transformer (Bulatov et al., 2022) introduce distinct recurrent compression mechanisms, implemented in a middle layer and at the output, respectively.

In this paper, we introduce MELODI (short for “MEmory with LOw DImension”), an efficient memory architecture designed to handle long contexts despite operating on short context windows (e.g., 512 tokens per window). MELODI integrates both short-term and long-term memory through a compression-based approach. The short-term memory, recurrent in nature and possessing low capacity, spans multiple network layers, progressively compressing context tokens and prior memory at each layer. In contrast, the long-term memory, incremental and high-capacity, resides within a single network layer. It maintains a record of the entire history by further compressing each context window and stacking them. Both short-term and long-term memory are seamlessly incorporated into a multi-layer transformer model using a “sandwich” structure (see Figure 1), incurring negligible additional parameters.

MELODI demonstrates strong performance on various long-context datasets. For instance, utilizing a 13-layer transformer network with 1024 embedding dimensions and 512-token context windows, MELODI achieves perplexity scores of 10.44 and 2.11 on PG-19 (T5 vocabulary) and arXiv Math (Meena vocabulary), respectively. This represents a clear improvement over the Memorizing Transformer (10.62 on PG-19, 2.14 on arXiv) with dense attention (as opposed to top-k attention), while significantly reducing memory usage by a factor of 8. Furthermore, ablation studies confirm the complementary nature of short-term and long-term memory in MELODI, highlighting their synergistic contribution to an efficient and effective memory architecture.

# 2 OUR METHOD: MELODI

MELODI focuses on efficiently comprehending long contexts through the utilization of short context windows, thus circumventing the quadratic complexity associated with attention mechanisms over long sequences. This approach necessitates a memory design that not only ensures smooth transitions between windows but also preserves crucial information from all preceding windows.

# 2.1 ARCHITECTURE OVERVIEW

Design principle: The core principle behind MELODI is to represent short-term and long-term memory through a hierarchical compression scheme. Specifically, the short-term memory recurrently compresses context tokens across multiple network layers (e.g., condensing a 512-token context window into 128 memory tokens). This process not only facilitates seamless transitions between context windows but also aggregates information across them, effectively functioning as a fixed-size multi-layer long short-term memory (LSTM) mechanism (Hochreiter & Schmidhuber, 1997). Furthermore, each context window undergoes additional compression within a middle layer and is then stacked into a long-term memory. This long-term memory retains essential information from the

entire history, thus compensating for any potential forgetting in the short-term memory. Both shortterm and long-term memory are seamlessly integrated into a transformer-based language model, enabling the comprehension of long contexts even under the constraint of short context windows.

Sandwich architecture: MELODI’s network architecture (Figure 1) features a ”sandwich” structure, with a long-term compression layer inserted between multiple recurrent short-term compression layers. Both layer types utilize a standard transformer block (attention and feed-forward network) with tailored compression modifications. The short-term layers recurrently compress the current context window and update short-term memory, while the long-term layer further compresses information and appends it to long-term memory.

Terminology: In the remainder of this paper, we adopt the following notation. We use $k$ to index context windows and $l$ to index network layers. Within the ${ { l } ^ { t h } }$ layer of the $k ^ { t h }$ context window, the input context tokens are represented by $x _ { k } ^ { i - 1 }$ . The output context tokens, denoted as $x _ { k } ^ { l }$ , serve as input for the subsequent layer. The input and output of the short-term memory are $z _ { k - 1 } ^ { l }$ and $z _ { k } ^ { l }$ , respectively. The long-term memory preceding window $k$ is denoted as $m _ { 1 : k - 1 }$ . Note that we omit the subscript $l$ for the long-term memory since it resides within a single long-term layer.

Next, we will discuss both short-term and long-term layers in detail.

# 2.2 SHORT-TERM MEMORY: MULTI-LAYER RECURRENT COMPRESSION

The short-term memory is distributed across multiple short-term layers (see Figure 1). This subsection delves into the specifics of the short-term layer, using the ${ { l } ^ { t h } }$ layer as an illustrative example for processing the $k ^ { t { \tilde { h } } }$ context window. The shortterm layer serves a dual purpose: (a) transforming context tokens $x _ { k } ^ { l - 1 }$ via a transformer block (yielding output $x _ { k } ^ { l } .$ ), and (b) recurrently compressing the current context window into the short-term memory $z _ { k } ^ { l }$ . It accomplishes this by updating both context tokens and short-term memory through a shared transformer block, albeit along separate pathways. As visualized in Figure 2, context tokens traverse vertically across layers (from $x _ { k } ^ { l - 1 }$ to $x _ { k } ^ { l } .$ ), whereas short-term memory flows horizontally across context windows (from $z _ { k - 1 } ^ { l }$ to $z _ { k } ^ { l } .$ ). To enable inter-layer communication within the shortterm memory, we introduce summary tokens $u _ { k } ^ { l }$ that propagate through the layers. We elaborate on the key components below.

Short-term memory $z _ { k } ^ { l }$ : The short-term memory (illustrated in Figure 2) is implemented as a sequence of vectors with length $S$ , each having the same dimensionality as context tokens (e.g., 1024 channels). Notably, the number of shortterm memory vectors is substantially smaller than the length of the context window (e.g., 128 memory tokens per window of 512 context tokens). Within each context window, the short-term mem-

ory serves initially as a previous context for auto-regressive prediction of subsequent context tokens (except for the first window in which the short-term memory is empty). It is then updated by compressing and incorporating information from the context tokens within the current window.

![](images/3400323484283e7fb43594acb907ea7d00ce643c334b7652e9d29f41463ade60.jpg)  
Figure 2: Short-term layer. The figure illustrates the processing of the $k ^ { t h }$ context window at the $l ^ { t h }$ short-term layer. It takes the memory from the previous window $z _ { k - 1 } ^ { l }$ and the current context/summary $( x _ { k } ^ { l - 1 } , u _ { k } ^ { l - 1 } )$ ul−1) from the previous layer as input. The short-term layer adds two linear token mixers (Tolstikhin et al., 2021) on top of a standard transformer layer (including attention and FFN) to separate the summary for the next layer $u _ { k } ^ { l }$ and the memory for the next window $z _ { k } ^ { l }$ . Best viewed in color.

Transforming context tokens: The context tokens $x _ { k } ^ { l }$ are generated through causal attention to both (a) the preceding short-term memory $z _ { k - 1 } ^ { l }$ and (b) preceding tokens within the current context window. This attention mechanism is followed by the application of a feed-forward network (FFN). Relative position embeddings are employed for both the context tokens $x _ { k } ^ { l - 1 }$ and the preceding

short-term memory $z _ { k - 1 } ^ { l }$ . Mathematically, this can be represented as:

$$
x _ {k} ^ {l} = \mathcal {T} \left(x _ {k} ^ {l - 1} \mid z _ {k - 1} ^ {l}\right), \tag {1}
$$

where $\tau ( x | z )$ indicates applying a transformer block on $x$ for a given preceding context $z$

Recurrent compression: Beyond transforming context tokens, the short-term layer also recurrently compresses the current context window into short-term memory. Similar to the approach in RMT (Bulatov et al., 2022) and AutoCompressors (Chevalier et al., 2023), this compression is achieved by appending summary tokens $u$ after context tokens $x$ and passing the combined sequence through the transformer block. Consequently, the resulting summary tokens compresses both the as: preceding short-term memory and the current context window via attentional pooling, expressed $\hat { u } _ { k } ^ { l } = \mathcal { T } ( u _ { k } ^ { l - 1 } | z _ { k - 1 } ^ { l } , x _ { k } ^ { l - 1 } )$ , where the input summary tokens $u _ { k } ^ { l - 1 }$ originate from the previous layer (refer to Figure 2). We use $\hat { u } _ { k } ^ { l }$ (instead of $u _ { k } ^ { l } )$ to denote an intermediate result that is further processed in the subsequent summary branching step. In practice, both context and summary tokens can be updated simultaneously within a single transformer operation: $x _ { k } ^ { l } , \hat { u } _ { k } ^ { l } = \mathcal { T } ( x _ { k } ^ { l - 1 } , u _ { k } ^ { l - 1 } | z _ { k - 1 } ^ { l } )$ = T (xl−k . Relatmary osition embeddings are applied on the short-term memory , while a causal mask is applied on the combined sequence $z _ { k - 1 } ^ { l }$ onteand $x _ { k } ^ { l - 1 }$ , and sum-$u _ { k } ^ { l - 1 }$ $x _ { k } ^ { l - 1 }$ ul−k $u _ { k } ^ { l - 1 }$

Summary tokens (containing $U$ tokens) are initialized from learnable embeddings (prior to the first layer) and set to the same length as the short-term memory $\mathbf { \Sigma } ( U \mathbf { \Sigma } = S$ ). Propagating through all layers, they facilitate inter-layer communication within the short-term memory. Moreover, branching summary tokens both upwards to the next layer and rightwards to the next window improves performance, a strategy we will discuss in more detail subsequently.

Summary branching: We employ distinct linear token mixers (Tolstikhin et al., 2021) on the context and summary tokens to generate separate summary tokens for the subsequent layer and shortterm memory tokens for the next window. Unlike channel mixing, a linear token mixer linearly combines the $M _ { i }$ input tokens across each channel to produce $M _ { o }$ output tokens with the same dimensionality by using an $M _ { i } \times M _ { o }$ matrix. This is mathematically represented as follows:

$$
u _ {k} ^ {l} = \mathcal {M} _ {\uparrow} \left(x _ {k} ^ {l}, \hat {u} _ {k} ^ {l}\right), \quad z _ {k} ^ {l} = \mathcal {M} _ {\rightarrow} \left(x _ {k} ^ {l}, \hat {u} _ {k} ^ {l}\right), \quad \text {w h e r e} x _ {k} ^ {l}, \hat {u} _ {k} ^ {l} = \mathcal {T} \left(x _ {k} ^ {l - 1}, u _ {k} ^ {l - 1} \mid z _ {k - 1} ^ {l}\right), \tag {2}
$$

where $\mathcal { M } _ { \uparrow }$ and $\mathcal { M } _ {  }$ denote the two linear token mixers towards the subsequent layer and the next window respectively. The resulting summary and memory tokens exhibit distinct linear combination of context $\hat { x _ { k } ^ { l } }$ and summary tokens $\hat { u } _ { k } ^ { l }$ , implying divergent compression flows across layers and windows. Since the summary and short-term memory share the same number of tokens $( S )$ , each mixer comprises $( W + S ) \times S$ parameters, where $W$ represent the number of context tokens per window. This parameter count is negligible for short context windows. For instance, with a context window of 512 tokens and 128 summary tokens, the two mixers collectively require $( 5 1 2 + 1 2 8 ) \times 1 2 8 \times 2 = 1 6 4 8$ parameters, constituting a mere $1 . 3 \%$ of a transformer block with 1024 dimensions.

Summary: The short-term memory layer can be succinctly represented as a function $z _ { k } ^ { l } , x _ { k } ^ { l } , u _ { k } ^ { l } =$ $h ( z _ { k - 1 } ^ { l } , x _ { k } ^ { l - 1 } , u _ { k } ^ { l - 1 } )$ l− . This function transforms context tokens $x _ { k } ^ { l - 1 }$ 1 and summary tokens ul−k $u _ { k } ^ { l - 1 }$ upward to the next layer (from $l - 1$ to l) while simultaneously propagating short-term memory $z _ { k - 1 } ^ { l }$ rightward to the next window (from $k - 1$ to $k$ ). Built upon a standard transformer block, this layer introduces negligible additional parameters through the summary branching mechanism.

Relation to Block Recurrent Transformer (BRT) (Hutchins et al., 2022): While BRT represents short-term memory by combining multi-layer KV caching from Transformer XL with a dedicated recurrent memory layer, Melodi models short-term memory as a consistent recurrent compression mechanism across multiple layers. Moreover, MELODI updates the short-term tokens by adding residual connections (cross-attending to the previous memory) over the previous layer, instead of over the previous memory tokens directly as in BRT.

While short-term memory facilitates smooth transitions between context windows, the inherent limitation of its capacity can lead to the inevitable loss of information, particularly for contexts located further back in the sequence. In the following subsection, we will demonstrate how long-term memory can be leveraged to mitigate this forgetting issue.

![](images/9c30892498ffbe684fd9ec5bf6f04fa340c5a49d450d637f58004f3854bbe858.jpg)  
Figure 3: Long-term layer. The long-term layer adds three components on top of the short-term layer (see Figure 2). Firstly, it introduces a long-term memory $m _ { 1 : k - 1 }$ by caching the compressed key-value pairs and allows the current context/summary $( x _ { k } ^ { l - 1 } , u _ { k } ^ { l - 1 } )$ to cross attend to them. Secondly, the self-attention and cross-attention are integrated via gating. Finally, the linear token mixing output additional compressed tokens and appends their key-value pairs $m _ { k }$ into the long-term memory (as $m _ { 1 : k }$ ) for the next window. Best viewed in color.

# 2.3 LONG-TERM MEMORY: SINGLE-LAYER MEMORIZING COMPRESSED KEY-VALUE PAIRS

In this subsection, we utilize long-term memory to retain information from all previous windows, thereby alleviating the forgetting inherent in short-term memory. The key idea involves further compressing the context window and storing the compressed representations across the entire history.

Further compression: In contrast to the short-term memory, which compresses information across multiple layers, the long-term memory achieves a higher compression rate within a single layer. Specifically, it compresses a context window into $L$ long-term tokens at a designated middle layer (refer to Figure 1), where $L$ is less than the number of short-term tokens per layer $( L < S )$ ). Illustratively, we might compress a context window of 512 tokens into $S = 1 2 8$ short-term tokens at each layer, but further condense it into $L = 6 4$ long-term tokens at a single layer.

Storing long-term memory: The key-value (KV) pairs of long-term tokens are sequentially stored in a first-in-first-out (FIFO) queue with a maximum capacity of $Q _ { m a x }$ windows. Consequently, the long-term memory can hold up to $L \times Q _ { m a x }$ KV pairs. For contexts shorter than $Q _ { m a x }$ windows, a compressed representation of the entire prior history is preserved. For longer documents exceeding $Q _ { m a x }$ windows, a substantial portion of the recent history $Q _ { m a x }$ windows) is still retained. We opt to store KV pairs (rather than the tokens themselves) because they are repeatedly utilized in cross-attention mechanisms for subsequent context windows, a point we will elaborate on later.

Long-term layer: Figure 3 illustrates the implementation of a long-term layer, which builds upon a short-term layer but incorporates three key additions. First, it introduces a long-term memory component (denoted as $m _ { 1 : k - 1 }$ prior to the $k ^ { \tilde { t h } }$ context window) and enables the current context tokens $x _ { k }$ and summary tokens $u _ { k }$ to cross-attend to it. Second, the cross-attention shares parameters with the self-attention and their results (cross attention: $\mathcal { A } _ { x }$ , self-attention: $\mathcal { A } _ { s }$ ) are combined through a gating mechanism using a learnable scalar $\alpha$ per attention head, formulated as $\alpha \mathcal { A } _ { x } + ( 1 - \alpha ) \mathcal { A } _ { s }$ . Finally, an additional linear token mixer is introduced to generate long-term tokens for the current window, and their key-value (KV) pairs $m _ { k }$ are appended to the long-term memory. This token mixer comprises $( W + U ) \times L$ parameters, where $W$ , $U$ and $L$ represent the number of context, summary and long-term memory tokens, respectively. It is notably smaller than the mixers in the short-term layer because: (a) $L$ is less than $S$ (the number of short-term memory tokens), and (b) it is present in only one layer.

Table 1: Long-term vs. short-term memory. In our notation, $L$ and $S$ represent the number of long-term and short-term tokens per network layer, respectively. Note that long-term has fewer tokens $( L < S )$ . $Q _ { m a x }$ denotes the maximum number of windows encompassed by the long-term memory queue. $N$ signifies the total number of network layers.   

<table><tr><td>PROPERTY</td><td>LONG</td><td>SHORT</td></tr><tr><td>Number of layers</td><td>single</td><td>multiple</td></tr><tr><td>Update per window</td><td>incremental</td><td>recurrent</td></tr><tr><td>Capacity</td><td>L × Qmax</td><td>S × N</td></tr></table>

Table 2: Baseline implementations. Our reimplementations utilize a cosine decay learning rate schedule (replacing inverse square root decay) and dense cross-attention for the Memorizing Transformer (replacing top-k attention). This results in improved performance compared to prior reported results.   

<table><tr><td rowspan="2">Baseline 
Method</td><td colspan="2">PG-19 (T5)</td><td>arXiv</td><td>(Meena)</td></tr><tr><td>prior</td><td>our</td><td>prior</td><td>our</td></tr><tr><td>Transformer XL</td><td>11.96</td><td>11.54</td><td>2.67</td><td>2.61</td></tr><tr><td>Block Recurrent</td><td>11.55</td><td>10.98</td><td>2.36</td><td>2.26</td></tr><tr><td>Memorizing Trans.</td><td>11.62</td><td>10.74</td><td>2.31</td><td>2.15</td></tr></table>

Long-term vs short-term memory: Table 1 provides a comparative overview of the long-term and short-term memory mechanisms. In contrast to the short-term memory, which operates recurrently across multiple layers, the long-term memory functions incrementally within a single layer. The short-term memory spans $N$ network layers, with each layer utilizing $S$ tokens, while the longterm memory queue spans $Q _ { m a x }$ context windows, with each window represented by $L$ tokens. To illustrate, consider a 12-layer transformer model processing context windows of 512 tokens each, with embedding dimension 1024. If we employ $S  { \mathrm { ~ \ r ~ { ~ \vert ~ { ~ \ - ~ 1 2 8 } ~ } ~ } }$ short-term tokens per layer and $L = 6 4$ long-term tokens per window, with a maximum capacity of $Q _ { m a x } = 1 2 8$ windows for the long-term memory, the short-term and long-term memory caches would have capacities of 1.6M $( 1 2 8 ^ { t o k e n s } \times 1 0 2 4 ~ ^ { d i m } \times 1 2 ^ { \mathit { l a y e r s } } )$ and 16.8M $\bar { ( 6 4 ^ { p a i r s } \times 2 ^ { t o k \bar { e } n s / p a i r } \times 1 0 2 4 ^ { d i m } \times 1 2 \bar { 8 } ^ { w i n d o w s } ) }$ floats, respectively. It’s worth noting that, for computational efficiency, we store key-value (KV) pairs in the long-term memory, whereas tokens are stored directly in the short-term memory.

Relation to Memorizing Transformer (MT) (Wu et al., 2022): Similar to Memorizing Transformer, MELODI incorporates key-value pairs from a middle layer into its long-term memory. Experiments corroborate the finding in MT that employing additional long-term layers yields only incremental gains. However, a key distinction lies in the fact that MELODI stores compressed KV pairs, rather than the KV pairs of context tokens directly, as in MT. This modification substantially reduces the size of the long-term memory. For instance, when compressing a context window of 512 tokens into 64 long-term tokens, MELODI achieves an 8-fold reduction in long-term memory size.

# 3 EXPERIMENTAL RESULTS

We evaluate MELODI on three long-context datasets: PG19 (Rae et al., 2019), arXiv Math (Wu et al., 2022), and C4 (Raffel et al., 2020a), using the standard auto-regressive language modeling task, where the objective is to predict the next token in a sequence. All models are trained from scratch, and we report the average perplexity on the respective test sets as our evaluation metric.

# 3.1 DATASETS

PG19: The PG19 dataset (Rae et al., 2019) consists of 28,752 English books published before 1919, averaging around 68,972 tokens per book. We utilize three 32k vocabularies: (a) Meena (Thoppilan et al., 2022), (b) T5 (Raffel et al., 2020b) and (c) a custom SentencePiece vocabulary (Kudo & Richardson, 2018) trained specifically on PG19.

arXiv Math: The arXiv dataset (Wu et al., 2022) comprises technical math papers from arXiv, with token counts comparable to PG19 due to LaTeX’s use of special characters, resulting in smaller subwords. We use a 32k Meena vocabulary (Thoppilan et al., 2022) and a 32k custom vocabulary.

$\mathbf { C 4 } ( 4 \mathbf { K } + )$ : The C4 dataset (Raffel et al., 2020a) is a large collection of internet documents. To emphasize long documents where memory is crucial, we filter out those with fewer than 4,096 tokens and utilize a 32k custom vocabulary.

Table 3: Comparisons with baselines on three datasets. The table reports average token-level perplexities for various models on three datasets. All models were trained under the same settings (e.g. segment length 4096, context window 512, 500k training steps). Three MELODI configurations were used: $S _ { 1 9 2 } + L _ { 3 2 }$ , $S _ { 1 2 8 } + L _ { 6 4 }$ , and $S _ { 1 9 2 } + L _ { 9 6 }$ . For instance, $S _ { 1 9 2 } + L _ { 3 2 }$ indicates ${ \bar { S } } = 1 9 2$ short-term and $L = 3 2$ long-term tokens per context window. All MELODI models utilized a longterm memory spanning 128 context windows. MELODI $S _ { 1 9 2 } + L _ { 3 2 }$ outperformed Transformer XL and Block Recurrent Transformer while consuming less memory. Notably, MELODI $S _ { 1 9 2 } + L _ { 9 6 }$ clearly surpassed Memorizing Transformer, using only a fifth of its memory.   

<table><tr><td rowspan="2">MODEL</td><td colspan="3">MEMORY</td><td colspan="3">PG19</td><td colspan="2">arXiv</td><td>C4(4K+)</td></tr><tr><td>All</td><td>Short</td><td>Long</td><td>Meena</td><td>T5</td><td>Custom</td><td>Meena</td><td>Custom</td><td>Custom</td></tr><tr><td>Transformer XL</td><td>13.6M</td><td>13.6M</td><td>0M</td><td>8.65</td><td>11.41</td><td>12.42</td><td>2.60</td><td>3.22</td><td>18.22</td></tr><tr><td>Block Recurrent</td><td>13.1M</td><td>13.1M</td><td>0M</td><td>8.30</td><td>10.98</td><td>11.90</td><td>2.26</td><td>2.70</td><td>17.82</td></tr><tr><td>\( MELODI S_{192}+L_{32} \)</td><td>11.0M</td><td>2.6M</td><td>8.4M</td><td>8.08</td><td>10.51</td><td>11.47</td><td>2.12</td><td>2.54</td><td>17.55</td></tr><tr><td>Memorizing Trans.</td><td>147.8M</td><td>13.6M</td><td>134.2M</td><td>8.07</td><td>10.62</td><td>11.53</td><td>2.14</td><td>2.56</td><td>17.37</td></tr><tr><td>\( MELODI S_{128}+L_{64} \)</td><td>18.5M</td><td>1.7M</td><td>16.8M</td><td>8.06</td><td>10.44</td><td>11.42</td><td>2.11</td><td>2.52</td><td>17.53</td></tr><tr><td>\( MELODI S_{192}+L_{96} \)</td><td>27.8M</td><td>2.6M</td><td>25.2M</td><td>7.91</td><td>10.29</td><td>11.27</td><td>2.09</td><td>2.49</td><td>17.25</td></tr></table>

# 3.2 SETUP

We utilized a decoder-only transformer architecture (with 12 or 13 layers), incorporating both shortterm and long-term memory caches. The model had an embedding size of 1024, 8 attention heads with a dimensionality of 128 each, and an FFN hidden layer of size 4096. All models were implemented in JAX and Flax and trained from scratch for 500k steps on 32 TPU cores. Further training details are provided in the Appendix B.

During training, each long document was segmented into 4096-token chunks to facilitate batch processing. These chunks were then organized into training batches, each comprising 8 context windows of 512 tokens. In our ablation study, MELODI’s default configuration compressed each context window into $S { = } 1 2 8$ short-term memory tokens per layer and $L { = } 6 4$ long-term memory tokens.

# 3.3 COMPARISON WITH BASELINES

Baselines: We benchmark MELODI against three well-established prior works: Transformer XL (Dai et al., 2019), Block Recurrent Transformer (Hutchins et al., 2022), and Memorizing Transformer (Wu et al., 2022). To ensure a fair comparison, we re-implement these baselines within our framework and evaluate them under identical settings.

Our re-implementations of the baseline models achieve superior performance compared to the results reported in their original papers (see Table 2). This improvement can be primarily attributed to two key factors: (a) using cosine decay learning rate schedule (Hoffmann et al., 2022) instead of the inverse square root decay, (b) using dense cross-attention instead of top-k attention for the Memorizing Transformer. Our re-implementations provide stronger baselines against which to evaluate MELODI’s effectiveness.

Comparisons: Table 3 presents a comparison of MELODI against three baseline models (Transformer XL, Block Recurrent Transformer, and Memorizing Transformer) across three datasets. We evaluate three MELODI configurations: $S _ { 1 9 2 } + L _ { 3 2 }$ , $S _ { 1 2 8 } + L _ { 6 4 }$ , and $S _ { 1 9 2 } + L _ { 9 6 }$ , where $S$ and $L$ denote the number of short-term and long-term tokens per context window, respectively. All models (MELODI and baselines) utilize a 13-layer transformer architecture, except for Block Recurrent Transformer, which inserts a block recurrent layer into a 12-layer transformer, ensuring a similar parameter count for all models.

MELODI $S _ { 1 9 2 + L _ { 3 2 } }$ consistently outperforms both Transformer XL and Block Recurrent Transformer across all datasets while utilizing less memory. For instance, it achieves a perplexity of 10.51 on PG-19 (T5 vocabulary), surpassing Transformer XL (11.41) and Block Recurrent Transformer (10.98) while consuming only 11.0M memory compared to their 13.6M and 13.1M, respectively.

In comparison to the Memorizing Transformer, MELODI $S _ { 1 2 8 } + L _ { 6 4 }$ exhibits slightly improved performance while significantly reducing memory consumption by approximately 8 times. MELODI

![](images/485577cd12bae83565b9499b2321bea8cc4704e77029f5f7d3900685e23a67df.jpg)  
Figure 4: Ablation of memory size on PG-19. The token perplexity is reported for various combinations of short-term and long-term memory sizes. Each curve represents a fixed size of long-term memory, with points along the curve indicating different short-term memory sizes. For example, the blue curve $( L _ { 9 6 } )$ , uses 96 long-term tokens per window over 128 windows, totaling 12,288 tokens. Each point on this curve represents a different short-term memory size (e.g. $S _ { 8 }$ denotes 8 short-term tokens per context window). Memory size is measured by the number of floating-point numbers (floats). For instance, $\boldsymbol { L } _ { 9 6 }$ stores 12,288 long-term key-value (KV) pairs, each with 1024 dimensions, resulting in a total of $1 2 , 2 8 8 \times 1 0 2 4 \times 2 { = } 2 5 . 2$ million floats. The table on the right provides the perplexity results for each point on the left plot, using matching colors. These results highlight that long-term and short-term memories play complementary roles, and increasing either type’s capacity improves performance. Notably, MELODI achieves superior performance compared to baselines like Transformer XL, Block Recurrrent Transformer and Memorizing Transformer while utilizing fewer memory resources. Best viewed in color.

<table><tr><td></td><td>L0
0.0M</td><td>L8
2.1M</td><td>L16
4.2M</td><td>L32
8.4M</td><td>L64
16.8M</td><td>L96
25.2M</td></tr><tr><td>S8
0.1M</td><td>11.81</td><td>11.69</td><td>11.59</td><td>11.42</td><td>11.34</td><td>11.15</td></tr><tr><td>S16
0.2M</td><td>11.75</td><td>11.63</td><td>11.47</td><td>11.35</td><td>11.24</td><td>11.10</td></tr><tr><td>S32
0.4M</td><td>11.60</td><td>11.57</td><td>11.40</td><td>11.25</td><td>11.11</td><td>11.07</td></tr><tr><td>S64
0.9M</td><td>11.52</td><td>11.43</td><td>11.36</td><td>11.23</td><td>11.08</td><td>10.98</td></tr><tr><td>S128
1.7M</td><td>11.39</td><td>11.41</td><td>11.27</td><td>11.08</td><td>10.95</td><td>10.90</td></tr><tr><td>S192
2.6M</td><td>11.28</td><td>11.25</td><td>11.14</td><td>11.02</td><td>10.89</td><td>10.83</td></tr><tr><td>S256
3.4M</td><td>11.26</td><td>11.23</td><td>11.11</td><td>10.93</td><td>10.83</td><td>10.77</td></tr></table>

$S _ { 1 9 2 } + L _ { 9 6 }$ further improves perplexity across all datasets, achieving a substantial reduction in memory usage exceeding a factor of 5. These trends remain consistent across different network depths (12-layer and 13-layer), as shown in Table 5 in Appendix C. These results collectively highlight MELODI’s efficacy and efficiency as a memory architecture for language models.

# 3.4 ABLATIONS

We conduct an ablation study of MELODI using the default configuration $( S _ { 1 2 8 } small + L _ { 6 4 } )$ with 128 short-term and 64 long-term tokens per context window. The long-term memory spans 128 context windows, and the transformer architecture consists of 13 layers. All models are trained on the PG-19 dataset with the T5 vocabulary for 200k steps.

Complementary roles of short-term and long-term memory: Figure 4 illustrates how the sizes of both short-term and long-term memory jointly influence perplexity. Each curve represents a fixed long-term memory size, with varying short-term memory sizes depicted by points along the curve. With the exception of the black curve, which solely utilizes short-term memory, all other curves incorporate long-term memory spanning 128 context windows. The figure demonstrates that increasing either short-term or long-term memory capacity leads to improved perplexity, highlighting their complementary roles in performance. Notably, by judiciously selecting memory sizes (e.g., $S _ { 1 9 2 }$ for short-term and $L _ { 3 2 }$ for long-term), we can outperform the Memorizing Transformer while utilizing less memory than Transformer XL and Block Recurrent Transformer.

Impact of long-term memory coverage: In contrast to the previous ablation, we now maintain a fixed number of short-term $S = 1 2 8 $ ) and long-term tokens $L = 6 4$ ) per context window while varying the number of windows encompassed by the long-term memory. Figure 5 demonstrates that performance improves as the long-term memory covers a wider range of context windows.

Interestingly, incorporating long-term memory for only the preceding 2 or 4 windows yields marginal perplexity improvements, suggesting that recent context is already effectively captured by the short-term memory. However, performance gains accelerate as the long-term memory ex-

![](images/ef5edae2cd81df984d7b31bc017fde582fe98f485decfebe149f30e0493156f7.jpg)  
Figure 5: Long-term memory coverage. The coverage metric indicates the number of preceding context windows spanned by the long-term memory. All data points use $L { = } 6 4$ long-term and $S { = } 1 2 8$ short-term tokens per window. However, they vary in long-term memory capacity. For instance, ‘32’ denotes covering 32 context windows, totaling $3 2 \times 6 4 = 2 0 4 8$ long-term tokens. Perplexity improves marginally with long-term memory coverage of 2-4 windows, then accelerates until 32 windows, after which it levels off.

![](images/b07d9eb31489cc24cb0e45cc701b9ad71bc925a44c389279c947d3a0f2b2527a.jpg)

![](images/8a843cfeb3ce5c88f705a147ddd1763ac450640b443550c071efde255e654222.jpg)  
Figure 6: Shorter context windows. (Left): Perplexity with varying context window sizes (512, 256, 128 tokens). The number of short-term and long-term memory tokens in MELODI is proportionally adjusted $( \times \frac { 1 } { 2 } , \times \frac { 1 } { 4 }$ , respectively) to ensure consistent long-term memory coverage. Even with smaller windows, MELODI with appropriate memory allocation (e.g. $S _ { 1 2 8 } { + } L _ { 6 4 } )$ consistently outperforms baselines. (Right): Perplexity increase due to window size reduction. The $x$ and $y$ axes represent the perplexity increment when reducing the window size from 512 to 256 and 128 tokens, respectively. Models with long-term memory (i.e. MELODI variants with long-term memory and Memorizing Transformer) exhibit significantly less performance degradation (smaller perplexity increments) compared to those relying solely on short-term memory (MELODI with only short-term memory, Transformer XL, and Block Recurrent Transformer).

pands to encompass up to 32 windows, after which the improvements level off. This observation indicates that while the middle and distant history are beneficial for language modeling, they are not adequately retained in the short-term memory. These findings further underscore the complementary nature of short-term and long-term memory mechanisms.

Performance with shorter context windows: Figure 6 examines the impact of reducing context window size on model performance. The plot on the left displays perplexity scores for context window sizes of 512, 256, and 128 tokens. The number of short-term and long-term memory tokens in MELODI is proportionally adjusted $( \times \frac { 1 } { 2 } , \times \frac { 1 } { 4 }$ , respectively) to ensure consistent long-term memory coverage across different window sizes. Even with shorter context windows, MELODI with appropriate memory allocation (e.g., $S _ { 1 2 8 } + L _ { 6 4 } )$ consistently outperforms the baseline models.

The plot on the right illustrates the increase in perplexity resulting from reducing the context window size. Notably, models incorporating long-term memory (MELODI variants with long-term memory and Memorizing Transformer) exhibit significantly less performance degradation (smaller perplexity increments) compared to models relying solely on short-term memory (MELODI variant with only short-term memory, Transformer XL, and Block Recurrent Transformer). This highlights the greater robustness of long-term memory mechanisms to reductions in context window size.

Number of short-term layers: We investigate the effect of varying the number of short-term layers on model performance. For a given layer count (e.g., 4 layers), the short-term layers are uniformly distributed throughout the network (e.g., layers 1, 5, 9, and 13). To disable short-term memory within a layer, we remove (a) the attention mechanism to the preceding short-term memory and (b) the linear token mixer responsible for updating the short-term memory.

Figure 7 shows that perplexity improves rapidly as the number of short-term layers increases from 1 to 4, after which the gains diminish. This observation supports our utilization of multiple layers for effective short-term memory modeling. However, it also suggests that disabling short-term memory in half of the layers offers a more efficient approach with negligible performance degradation.

Number of long-term layers: Figure 8 illustrates the effect of varying the number of long-term layers on perplexity. We consider two configurations: $L _ { 6 4 }$ and $L _ { 3 2 }$ , which compress each context window into 64 and 32 long-term tokens, respectively. With $L _ { 6 4 }$ , introducing the first long-term

Table 4: Summary branching. Summary branching provides a consistent gain of approximately 0.3 in perplexity, both with (column $\mathrm { S T + L T } )$ ) and without (column ST) long-term memory.   

<table><tr><td>Branching</td><td>ST</td><td>ST+LT</td></tr><tr><td>No</td><td>11.68</td><td>11.24</td></tr><tr><td>Yes</td><td>11.39</td><td>10.95</td></tr></table>

![](images/e7cb7c5aa763d76d17283f34a8b900239d88f92c3c3d5c929cf7342ae3e1eb68.jpg)  
Figure 7: Number of shortterm layers. Perplexity improves as the number of shortterm layers increases.

![](images/e9316cc84f77c12c87445e7943332d3d5ca5a4d9966f3c5dc16210af0816debd.jpg)  
Figure 8: Number of longterm layers. A single layer with sufficient long-term tokens is adequate.

layer yields a substantial perplexity improvement, after which performance plateaus. In contrast, with $L _ { 3 2 }$ , while adding a second long-term layer provides a notable gain, the resulting performance remains inferior to that of a single $L _ { 6 4 }$ layer, even though both configurations utilize the same total number of long-term tokens. These findings validate our design choice of employing a single longterm memory layer with a sufficient number of long-term tokens.

Summary branching: Table 4 examines the effect of summary branching on perplexity, both with $( { \mathrm { S T } } { + } \mathrm { L T } )$ and without (ST) long-term memory. Summary branching consistently yields a perplexity improvement of approximately 0.3, indicating distinct summary information flow across network layers and context windows.

# 4 RELATED WORK

Memory in language models: While LSTMs (Hochreiter & Schmidhuber, 1997) achieve longrange memory through recurrent compression at the token level, the advent of Transformers (Vaswani et al., 2017) has shifted the focus to memory mechanisms operating at the context window level. Transformer-XL (Dai et al., 2019) introduces a caching mechanism to store key-value (KV) pairs from the preceding context window as a form of short-term memory. Block Recurrent Transformer (Hutchins et al., 2022) and RMT (Bulatov et al., 2022), integrate recurrent mechanisms inspired by LSTMs into Transformer architectures at the window level. Munkhdalai et al. (2024) explore the use of additional memory like Hopfield Networks (Hopfield, 1982; Ramsauer et al., 2021). Memorizing Transformer (Wu et al., 2022) introduces a dedicated memory layer to store KV pairs for long-term memory, whereas MemoryLLM (Wang et al., 2024) incorporates long-term memory in every layer, incurring a substantial memory overhead. In contrast to these approaches, MELODI integrates both short-term and long-term memory into a transformer model via compression.

Extending context length: Recent research has demonstrated promising progress in scaling the context length of language models. To mitigate the cost of attention mechanisms over long contexts, LongLoRA (Chen et al., 2024), PCW (Ratner et al., 2023), and Landmark-Attention (Mohtashami & Jaggi, 2023) employ sparse local attention for efficient fine-tuning. Position-Interpolation (Chen et al., 2023) and YaRN (Peng et al., 2023) extend context size by modifying the RoPE embedding scheme (Su et al., 2023). Xiong et al. (2023) provide a practical recipe for extending LLAMA2 (Touvron et al., 2023) to handle up to 32,768 tokens, while Fu et al. (2024) further explore datacentric approaches for extending context length through lightweight continual pretraining.

Compression: Recent work has demonstrated the effectiveness of Transformer models in compressing input sequences by appending summary tokens (Rae et al., 2019; Bulatov et al., 2022; Chevalier et al., 2023; Ge et al., 2024). For instance, RMT (Bulatov et al., 2022) utilizes the output of summary tokens recurrently as short-term memory. AutoCompressor (Chevalier et al., 2023) aggregates summary tokens across segments to generate a summary representation for long documents used in retrieval tasks. GISTING (Mu et al., 2024) applies this technique to compress long prompts. ICAE (Ge et al., 2024) further incorporates LoRA fine-tuning (Hu et al., 2021) for context compression, while TransformerFAM (Hwang et al., 2024) introduces feedback attention to enhance performance.

# 5 CONCLUSION

In this work, we have introduced MELODI, a novel memory architecture designed to address the challenges of long document processing within the constraints of short context windows. The core innovation of MELODI lies in its hierarchical compression approach, wherein short-term memory facilitates smooth transitions between context windows through recurrent compression across multiple layers, and long-term memory preserves crucial information from the entire history by performing further compression and aggregation within a single middle layer. Our empirical evaluations on multiple long-context datasets have validated MELODI as an efficient and effective solution. The success of MELODI underscores the potential of hierarchical memory compression for tackling the complexities of long document processing. We anticipate that further research in this direction will enhance long context understanding and generation over multiple modalities.

# REFERENCES

Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning, 2024. URL https://arxiv.org/abs/2404.11018.   
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 11079–11091. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/47e288629a6996a17ce50b90a056a0e1-Paper-Conference.pdf.   
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation, 2023. URL https://arxiv.org/ abs/2306.15595.   
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models, 2024. URL https://arxiv. org/abs/2309.12307.   
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 3829–3846, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 232. URL https://aclanthology.org/2023.emnlp-main.232.   
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):1–53, 2024. URL http://jmlr.org/papers/v25/23-0870.html.   
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Anna Korhonen, David Traum, and Llu´ıs Marquez (eds.), ` Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978–2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https:// aclanthology.org/P19-1285.   
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context, 2024. URL https://arxiv. org/abs/2402.10171.   
Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model, 2024. URL https://arxiv.org/abs/ 2307.06945.   
Gemini-Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530.   
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. ¨ Neural Computation, 9(8): 1735–1780, 1997.   
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/ 2203.15556.

J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the National Academy of Sciences of the United States of America, 79(8): 2554–2558, April 1982.   
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685.   
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Block-recurrent transformers. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 33248–33261. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ d6e0bbb9fc3f4c10950052ec2359355c-Paper-Conference.pdf.   
Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, and Pedro Moreno Mengibar. Transformerfam: Feedback attention is working memory, 2024. URL https://arxiv.org/ abs/2404.09173.   
Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-2012. URL https://aclanthology.org/ D18-2012.   
Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for transformers. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 54567–54585. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/ file/ab05dc8bf36a9f66edbff6992ec86f56-Paper-Conference.pdf.   
Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens, 2024. URL https://arxiv.org/abs/2304.08467.   
Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention, 2024. URL https://arxiv.org/abs/ 2404.07143.   
OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774.   
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models, 2023. URL https://arxiv.org/abs/2309.00071.   
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.   
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020a. URL http: //jmlr.org/papers/v21/20-074.html.   
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-totext transformer. Journal of Machine Learning Research, 21(140):1–67, 2020b. URL http: //jmlr.org/papers/v21/20-074.html.   
Hubert Ramsauer, Bernhard Schafl, Johannes Lehner, Philipp Seidl, Michael Widrich, Thomas ¨ Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlovic, Geir Kjetil Sandve, Victor Greiff,´ David Kreil, Michael Kopp, Gunter Klambauer, Johannes Brandstetter, and Sepp Hochreiter.¨ Hopfield networks is all you need, 2021. URL https://arxiv.org/abs/2008.02217.

Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large language models, 2023.   
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In Proceedings of the 35th International Conference on Machine Learning, pp. 4603–4611. PMLR, 2018.   
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864.   
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, and et al. Lamda: Language models for dialog applications, 2022. URL https://arxiv.org/abs/2201.08239.   
Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.   
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.   
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.   
Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian McAuley. Memoryllm: Towards self-updatable large language models, 2024. URL https://arxiv.org/abs/2402.04624.   
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022a. URL https://openreview. net/forum?id=gEZrGCozdqR.   
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 24824–24837. Curran Associates, Inc., 2022b. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.   
Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=TrjbxzRcnf-.   
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma. Effective long-context scaling of foundation models, 2023. URL https://arxiv.org/abs/2309.16039.

# A LIMITATIONS

A primary limitation of our current method is its focus on training from scratch, without addressing the fine-tuning of pre-trained models with fixed context window sizes. In future work, we plan to explore adapting MELODI (short-term and long-term memory) to enhance the long-context capabilities of pre-trained models through techniques like LoRA fine-tuning.

# B TRAINING DETAILS

We use Adafactor optimizer (Shazeer & Stern, 2018) with a learning rate schedule that employs a linear warmup for the first 1000 steps, followed by cosine decay. The maximum and minimum learning rates are set to 0.01 and 0.001, respectively, as recommended in Hoffmann et al. (2022). A dropout rate of 0.05 is applied. All models are trained for $5 0 0 \mathrm { k }$ steps $2 0 0 \mathrm { k }$ for ablations) on 32 TPU cores with a batch size of 32 (1 per core).

# C MORE COMPARISON WITH BASELINES

Comparison with baselines using a fixed context window size: Table 5 presents a comparison of MELODI against three baseline models (Transformer XL, Block Recurrent Transformer, and Memorizing Transformer) across three datasets, using a consistent segment length of 4096 tokens and a context window size of 512. The evaluation includes both 12-layer and 13-layer transformer architectures to assess the impact of model depth on performance.

Notably, even with fewer layers, MELODI $S _ { 1 9 2 + L _ { 3 2 } }$ consistently outperforms both Transformer XL and Block Recurrent Transformer across all datasets while consuming less memory. For instance, the 12-layer variant of MELODI $S _ { 1 9 2 } { + } L _ { 3 2 }$ achieves a perplexity of 10.66 on PG-19 (T5 vocabulary), surpassing the 13-layer variants of Transformer XL (11.41) and Block Recurrent Transformer (10.98).

Table 5: Comparisons with baselines on three datasets. The table reports average token-level perplexities for various models on three datasets. All models were trained under the same settings (e.g. segment length 4096, context window 512, 500k training steps). Three MELODI configurations were used: $S _ { 1 9 2 } + L _ { 3 2 }$ , $S _ { 1 2 8 } + L _ { 6 4 }$ , and $S _ { 1 9 2 } + L _ { 9 6 }$ . For instance, $S _ { 1 9 2 } + L _ { 3 2 }$ indicates $S = 1 9 2$ short-term and $L = 3 2$ long-term tokens per context window. All MELODI models utilized a longterm memory spanning 128 context windows. MELODI $S _ { 1 9 2 } + L _ { 3 2 }$ outperformed Transformer XL and Block Recurrent Transformer while consuming less memory. Notably, MELODI $S _ { 1 9 2 } + L _ { 9 6 }$ clearly surpassed Memorizing Transformer, using only a fifth of its memory.

<table><tr><td rowspan="2">MODEL</td><td colspan="3">MEMORY</td><td colspan="3">PG19</td><td colspan="2">arXiv</td><td>C4(4K+)</td></tr><tr><td>All</td><td>Short</td><td>Long</td><td>Meena</td><td>T5</td><td>Custom</td><td>Meena</td><td>Custom</td><td>Custom</td></tr><tr><td colspan="10">12 LAYERS</td></tr><tr><td>Transformer XL</td><td>12.6M</td><td>12.6M</td><td>0M</td><td>8.76</td><td>11.54</td><td>12.63</td><td>2.61</td><td>3.23</td><td>18.61</td></tr><tr><td>Block Recurrent</td><td>12.1M</td><td>12.1M</td><td>0M</td><td>8.47</td><td>11.12</td><td>12.11</td><td>2.27</td><td>2.73</td><td>18.27</td></tr><tr><td>\( MELODI S_{192}+L_{32} \)</td><td>10.8M</td><td>2.4M</td><td>8.4M</td><td>8.22</td><td>10.66</td><td>11.66</td><td>2.13</td><td>2.55</td><td>18.03</td></tr><tr><td>Memorizing Trans.</td><td>146.8M</td><td>12.6M</td><td>134.2M</td><td>8.15</td><td>10.74</td><td>11.68</td><td>2.15</td><td>2.57</td><td>17.88</td></tr><tr><td>\( MELODI S_{128}+L_{64} \)</td><td>18.4M</td><td>1.6M</td><td>16.8M</td><td>8.16</td><td>10.61</td><td>11.66</td><td>2.13</td><td>2.55</td><td>18.01</td></tr><tr><td>\( MELODI S_{192}+L_{96} \)</td><td>27.6M</td><td>2.4M</td><td>25.2M</td><td>8.08</td><td>10.48</td><td>11.47</td><td>2.11</td><td>2.51</td><td>17.75</td></tr><tr><td colspan="10">13 LAYERS</td></tr><tr><td>Transformer XL</td><td>13.6M</td><td>13.6M</td><td>0M</td><td>8.65</td><td>11.41</td><td>12.42</td><td>2.60</td><td>3.22</td><td>18.22</td></tr><tr><td>Block Recurrent</td><td>13.1M</td><td>13.1M</td><td>0M</td><td>8.30</td><td>10.98</td><td>11.90</td><td>2.26</td><td>2.70</td><td>17.82</td></tr><tr><td>\( MELODI S_{192}+L_{32} \)</td><td>11.0M</td><td>2.6M</td><td>8.4M</td><td>8.08</td><td>10.51</td><td>11.47</td><td>2.12</td><td>2.54</td><td>17.55</td></tr><tr><td>Memorizing Trans.</td><td>147.8M</td><td>13.6M</td><td>134.2M</td><td>8.07</td><td>10.62</td><td>11.53</td><td>2.14</td><td>2.56</td><td>17.37</td></tr><tr><td>\( MELODI S_{128}+L_{64} \)</td><td>18.5M</td><td>1.7M</td><td>16.8M</td><td>8.06</td><td>10.44</td><td>11.42</td><td>2.11</td><td>2.52</td><td>17.53</td></tr><tr><td>\( MELODI S_{192}+L_{96} \)</td><td>27.8M</td><td>2.6M</td><td>25.2M</td><td>7.91</td><td>10.29</td><td>11.27</td><td>2.09</td><td>2.49</td><td>17.25</td></tr></table>

Table 6: Comparisons with baselines that utilize longer context windows. MELODI, with a context window size of 512, outperforms three baselines (i.e., Transformer XL, Block Recurrent Transformer and Memorizing Transformer) even when they utilize longer context windows (1024 and 2048 tokens). This highlights MELODI’s effectiveness in processing long sequences despite using shorter context windows.   

<table><tr><td rowspan="2">MODEL</td><td rowspan="2">Segment Length</td><td rowspan="2">Window Length</td><td colspan="3">MEMORY</td><td rowspan="2">PG19 T5</td></tr><tr><td>All</td><td>Short</td><td>Long</td></tr><tr><td>Transformer XL</td><td>4096</td><td>512</td><td>13.6M</td><td>13.6M</td><td>0M</td><td>11.41</td></tr><tr><td>Transformer XL</td><td>4096</td><td>1024</td><td>27.3M</td><td>27.3M</td><td>0M</td><td>11.11</td></tr><tr><td>Transformer XL</td><td>4096</td><td>2048</td><td>54.5M</td><td>54.5M</td><td>0M</td><td>10.99</td></tr><tr><td>Transformer XL</td><td>6144</td><td>3072</td><td>81.8M</td><td>81.8M</td><td>0M</td><td>10.96</td></tr><tr><td>Block Recurrent</td><td>4096</td><td>512</td><td>13.1M</td><td>13.1M</td><td>0M</td><td>10.98</td></tr><tr><td>Block Recurrent</td><td>4096</td><td>1024</td><td>25.7M</td><td>25.7M</td><td>0M</td><td>10.91</td></tr><tr><td>Block Recurrent</td><td>4096</td><td>2048</td><td>50.9M</td><td>50.9M</td><td>0M</td><td>10.88</td></tr><tr><td>Memorizing Trans.</td><td>4096</td><td>512</td><td>147.8M</td><td>13.6M</td><td>134.2M</td><td>10.62</td></tr><tr><td>Memorizing Trans.</td><td>4096</td><td>1024</td><td>161.5M</td><td>27.3M</td><td>134.2M</td><td>10.53</td></tr><tr><td>Memorizing Trans.</td><td>4096</td><td>2048</td><td>188.7M</td><td>54.5M</td><td>134.2M</td><td>not stable</td></tr><tr><td>MELODI S192+L32</td><td>4096</td><td>512</td><td>11.0M</td><td>2.6M</td><td>8.4M</td><td>10.51</td></tr><tr><td>MELODI S128+L64</td><td>4096</td><td>512</td><td>18.5M</td><td>1.7M</td><td>16.8M</td><td>10.44</td></tr><tr><td>MELODI S192+L96</td><td>4096</td><td>512</td><td>27.8M</td><td>2.6M</td><td>25.2M</td><td>10.29</td></tr></table>

Table 7: Directly copying short-term tokens to long-term memory: Here, we force the short-term and long-term to share the same number of tokens (e.g. $S _ { 6 4 } { + } L _ { 6 4 } )$ ), and examine the impact of directly using shortterm tokens as long-term tokens, bypassing the linear token mixer. This approach results in performance degradation compared to generating distinct long-term tokens.   

<table><tr><td>Copying</td><td>S96+L96</td><td>S64+L64</td><td>S32+L32</td></tr><tr><td>Yes</td><td>11.00</td><td>11.36</td><td>11.42</td></tr><tr><td>No</td><td>10.92</td><td>11.08</td><td>11.25</td></tr></table>

Table 8: Position of long-term layer. Here, we use a 13-layer MELODI model, where layers are indexed from 0 to 12. The model uses 128 shortterm and 64 long-term tokens per context window. While the default position of the long-term layer is at layer 8, placing it at different layers between 5 and 11 yields consistent perplexity scores.   

<table><tr><td>Layer</td><td>5</td><td>6</td><td>7</td><td>8*</td></tr><tr><td>Perplexity</td><td>11.00</td><td>11.01</td><td>11.03</td><td>10.95</td></tr><tr><td>Layer</td><td>9</td><td>10</td><td>11</td><td></td></tr><tr><td>Perplexity</td><td>10.96</td><td>10.95</td><td>10.94</td><td></td></tr></table>

In comparison to the Memorizing Transformer, MELODI $S _ { 1 2 8 } + L _ { 6 4 }$ exhibits slightly improved performance while dramatically reducing memory consumption by a factor of 8. Furthermore, MELODI $S _ { 1 9 2 } + L _ { 9 6 }$ achieves even better perplexity scores across all datasets with a substantial reduction in memory usage exceeding a factor of 5. The improvement is consistent for using both 12-layer and 13-layer transformer architectures. These results collectively highlight MELODI’s efficacy and efficiency as a memory architecture for language models.

Comparison with baselines that utilize longer context windows: Table 6 demonstrates that on PG-19 (T5 vocabulary), MELODI (with a context window size of 512 tokens) outperforms three baselines even when they utilize longer context windows (1024 and 2048 tokens), further highlighting its effectiveness and efficiency in processing long sequences. While the baselines generally exhibit improved performance with increasing context window size, Memorizing Transformer shows instability with a 2048-token window. We hypothesize that this instability stems from the limited backpropagation through time (BPTT) depth (two windows) within each segment (4096 tokens).

# D MORE ABLATIONS

In this section, we present additional ablation studies.

Directly copying short-term memory to long-term memory: This ablation experiment, conducted at the long-term layer, explores directly copying short-term memory tokens as long-term tokens. Instead of generating long-term tokens using the linear token mixer, this approach utilizes the shortterm tokens present at the long-term layer and stores them directly in the long-term memory. Here, we force the short-term and long-term to share the same number of tokens $( S = L )$ ). The results in Table 7 indicate that this direct copying method leads to a performance degradation.

Effect of long-term layer position: We investigate the impact of varying the position of the longterm layer within a 13-layer MELODI model $( S _ { 1 2 8 } \small { + } L _ { 6 4 } )$ . Layers are indexed from 0 to 12, with the default long-term layer position at layer 8. Results in Table 8 reveal consistent perplexity scores when the long-term layer is positioned between layers 5 and 11.