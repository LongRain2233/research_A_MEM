# VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models

Xinlei Yu1 Chengming $\mathrm { { X u ^ { 2 } } }$ Guibin Zhang1 Zhangquan Chen3 Yudong Zhang5 Yongbo He4 Peng-Tao Jiang6 Jiangning Zhang4 Xiaobin $\mathrm { H u ^ { 1 * } }$ Shuicheng Yan1

1National University of Singapore 2Fudan University 3Tsinghua University 4Zhejiang University

5University of Science and Technology of China 6vivo

# Abstract

Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a “visual processing bottleneck”: a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and longterm semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for finegrained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of $1 1 . 0 \%$ relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.

# 1. Introduction

Visual-Language Models (VLMs) have demonstrated impressive capabilities in visual understanding, reasoning and generation [31, 50]. Latest flagship models, both closedsourced [2, 11, 39] and open-sourced [1, 4, 55, 56, 63], represent a significant leap towards a general-purpose intelligent model that can both perceive and think about the visual world. Despite their success, VLMs still face significant inherent challenges when tackling complicated tasks that require advanced visual abilities, such as fine-grained perception, multi-step reasoning, or maintaining fidelity over long generative sequences [17, 25]. A fundamental limitation stems from the pervasive propensity, exhibited dur-

ing deep autoregressive decoding, toward a deficit in visual memory, which prioritizes accumulated textual context over the initial visual evidence and lacks visual semantic knowledge [52, 90]. It manifests as a “visual processing bottleneck” that impairs performance in fine-grained visual understanding, efficient reasoning, and robust generation.

Prior efforts to overcome this limitation have explored several distinct strategic axes, which can be primarily categorized into four paradigms, as illustrated in Fig. 1. One intuitive paradigm is the (a) direct training paradigm, which optimizes model parameters via fine-tuning or reinforcement learning [26, 35, 44, 66]. This relatively brute-force approach often sacrifices generalization for task-specific performance, leading to catastrophic forgetting. Another axis concerns the representation space of the intervention, (b) image-level paradigm, operating in the pixel space by explicitly synthesizing new visual inputs, which offers image-level thinking but at a prohibitive computational cost [13, 24, 29, 48, 49, 87]. Conversely, (c) token-level paradigm constrains operations to visual tokens, which is more efficient but fundamentally non-generative, limiting the model to merely re-surfacing what it has already encoded [8, 16, 28, 75]. Recently, a promising direction lies in the (d) latent space paradigm, which introduces continuous latent contexts in the sequential inference process. Unfortunately, existing latent space methods either rely solely on the language space [21, 30, 47, 68, 81] or require auxiliary visual data [70], limiting their application in VLMs.

To overcome this problem, we resort to cognitive psychology, specifically the Dennis Norris Theory [38]:

Short-term memory and long-term memory are two distinct storage systems that can be modeled on their neural underpinnings, the former is governed by vision, while the latter holds sway over abstract semantics.

While this cognitive theory reveals the essence of human cognition, it can be smoothly translated into an architectural principle of VLMs: short-term memory is visually-dominant, enhancing perception of the current

![](images/2de3be7e750e236775ad61d60fadff832c726e6e7e78f8da4090cbcd69e78499.jpg)

![](images/de9791bc11be8ed33f2168efaa7c8d599aec9371d33361f8f63237245d6ef933.jpg)

![](images/68174a90f1fe72e4a981badc164cd50d2153b07304aa3251f052d5585eec1327.jpg)

![](images/540a01b47f3174cfc5c2880e4c990871d4298599f88530cbaa5c4dea37862166.jpg)  
Figure 1. Four primary paradigms for enhancing visual capabilities: (a) the direct training paradigm, (b) the image-level paradigm, (c) the token-level paradigm, and (d) the latent space paradigm. Our VisMem belongs to the last one, featuring latent vision memory.

visual scenes, while long-term memory is semanticallydominant, providing generalized knowledge and contextualized semantic, completing the full cognitive chain.

Based on such inspiration, we propose VisMem, a novel and cognitively-aligned framework that systematically incorporates short- and long-term latent vision memory into VLMs. VisMem functions by non-intrusively extending the vocabulary of VLMs with special tokens that trigger ondemand latent vision memory invocation during autoregressive generation. Upon generating an invocation token, a lightweight query builder assesses the hidden states, which contains the current multi-modal cognition, to formulate a contextual-aware query which is then dispatched to one of two specialized, lightweight memory formers: short-term memory former that generates latent tokens encoding finegrained, perceptual evidences of current visual inputs; longterm memory former that synthesizes tokens representing abstract, high-level semantic knowledge. These generated latent memory tokens are seamlessly inserted into the generation stream, enriching the contexts and enabling it to output with a seamless integration of detailed visual information and generalized semantic knowledge.

With a two-stage training paradigm based on reinforcement learning tailored for our proposed framework, the model learns to first generate effective memory contents, based on which the optimal patterns for invoking the memory is then learned. Our extensive experiments across a wide range of benchmarks spanning visual understanding, reasoning, and generation demonstrate that our approach can substantially enhance the comprehensive visual capabilities on various base models, while also improving crossdomain generalization and mitigating the problem of catastrophic forgetting. Our contributions are listed as follows:

• We propose a novel paradigm to proactively harness vision memory, alleviating the “visual processing bottleneck” and augmenting advanced visual capabilities.   
• We propose a short- and long-term latent vision memory system with distinct purposes and mechanisms, which is

analogous to the cognitive psychology.

• We propose a dynamic memory invocation mechanism for seamlessly invoking and inserting latent memory tokens into the autoregressive inference process.   
• We evaluate the framework on extensive benchmarks, showcasing significant improvements in advanced visual capacities, cross-domain generalization, catastrophic forgetting mitigation, and compatibility across base models.

# 2. Related Work

# 2.1. Visual Capacities Enhancement

As demonstrated in Fig. 1, existing methods to alleviate “visual processing bottleneck” of VLMs broadly fall into four main categories: (a) direct training paradigm, which directly optimizes model parameters for target visual tasks, as in SFT, Visual-RFT [35], VLM-R1 [44], Vision-R1 [26], and PAPO [66]. Nonetheless, these methods suffer from catastrophic forgetting, specifically manifested as the degradation of general capabilities and overspecialization in specific visual cognition tasks [74, 89]; (b) image-level paradigm, which either leverages bounding boxes to denote visual evidence, represented by methods as Visual CoT [42], DeepEyes [87], SpatialVTS [33], VGR [58], and GRIT [13], or externally generate the iterative visual inputs via predefined tools, as seen in Sketchpad [24], VPRL [69], PyVision [85], OpenAI o3 [40], Pixel-Reasoner [48], MVoT [29], and OpenThinkImg [49]. Nevertheless, modifying visual inputs incurs extremely high computational costs, accompanied by high latency and reliance on external tools and concretized images; (c) tokenlevel paradigm, which select original representations and cannot modify visual evidences, thus restricted by insufficiently refined information and suboptimal selection strategies, as in ICoT [16], MINT-CoT [8], SCAFFOLD [28], LLaVA-AURORA [6], VPT [75], Chameleon [54], (d) latent space paradigm, which employs latent states to optimize autoregressive generation, but its focus remains on pure language models, e.g., Coconut [21], MemGen [81],

LatentSeek [30], SoftCoT [68], CODI [47]. Although Mirage [70] attempts to construct a latent vision space, requiring substantial manually labeled images. Our VisMem also belongs to this paradigm, but differs from existing methods by integrating latent vision memory within generation processes, characterized by a short and long memory system.

# 2.2. Memory Empowerment

Another mechanism closely tied to our approach involves endowing models with memory functionality. One intuitive strategy entails directly optimize models on prior trajectories, exemplified by [14, 45, 80], or to store them into the external memory repositories [53, 61]. Besides, some models inject persistently stored, retrieval-augmented knowledge from external environments, such as Expel [83] and MemoryBank [88], others, such as SkillWeaver [86] and Alita [41], distill prior knowledge as reusable tools. Currently, latent memory, as an implicit memory representation with better cross-domain generalization, efficiently encodes deep semantic associations, including $\mathbf { M } +$ [65] and Mem-Gen [81]. Nevertheless, these memory paradigms fail to ideally accommodate visual information, which manifests as a continuous, high-dimensional perceptual input. Consequently, the exploration of efficient visual memory mechanisms remains a largely uncharted territory. Thus, we propose a more human-aligned latent vision memory paradigm.

# 3. Methodology

# 3.1. Preliminary

Problem Formulation. Based on the interaction process of VLMs, we formulate the problem and introduce the notations used. We first define a policy model $\mathcal { P }$ , which is powered by a base VLM. Given a visual task to be solved, feeding a instruction-vision pair $( I , V )$ sampled from a task distribution $\mathcal { D }$ , the policy model unfolds a corresponding trajectory $\tau$ at a timestep $t$ , including pairs of current state $s _ { t }$ of the environment and the action $a _ { t }$ performed by the model. Here, the state of the environment includes textual contexts and visual observations. Internally, the action is generated sequentially by the token-by-token autoregressive decoding of the model, yielding the output token sequence $\{ x _ { t , 1 } , x _ { t , 2 } , \ldots , x _ { t , l } \}$ . The generation of i-th output token $\boldsymbol { x } _ { t , i }$ could be presented as:

$$
x _ {t, i} \sim \mathcal {P} (\cdot \mid s _ {t}, x _ {<   i}), \tag {1}
$$

where the prediction is conditioned on the current environment state and previously generated tokens. To endow the model with vision memory, a vision memory system $\mathcal { M }$ is adhered to the policy model, thus, the objective is to optimize the memory-enhanced model jointly and to maximize its expected performance:

$$
\max  _ {\mathcal {P}, \mathcal {M}} \mathbb {E} _ {(I, V) \sim \mathcal {D}, \tau \sim (\mathcal {P}, \mathcal {M})} [ S (\tau) ], \tag {2}
$$

where $S \left( \cdot \right)$ denotes the quantifiable performance results, e.g., accuracy or signal from a reward model.

Motivation. Building on the Dennis Norris Theory [38], which aligns with contemporary models of human memory, the coordinated operation of short- and long-term visual memories surmounts the “visual processing bottleneck”. Short-term latent visual memory maintains fine-grained detail for immediate use and is thus visually dominant; by contrast, long-term latent visual memory abstracts across experiences to enable flexible reuse and is therefore semantically dominant. Taking the task illustrated in Fig. 2 as a case in point, “find the classic Lay’s on the shelf” entails the deployment of short-term vision memory, retaining visual details for immediate perceptual demands, while “get in the promotion” triggers generalized semantic knowledge about the “promotion label” acquired from historical scenarios, which is grounded in long-term latent memory, to facilitate the comprehension of the task-based sight. Existing paradigms for enhancing visual capabilities fail to adequately consider vision memory, thus, our VisMem proposes a latent memory method to bridge this gap. More theoretical foundations are in Appendix 6.

Memory System. Based on previous contents, the task could be further disassembles into two main interactive parts: memory invocation (Sec. 3.2): related to “where and how to invoke the short- or long-term vision memory”; memory formation (Sec. 3.3): related to “what content should the short- or long-term vision memory convey”. Additionally, these two decomposed processes interact closely with each other, with distinct priorities and objectives, requiring a meticulously designed training recipe (Sec. 3.4).

# 3.2. Memory Invocation

As illustrated in Fig. 2, our latent vision memory invocation strategy largely aligns with the standard generation pipeline of VLMs, thereby preserving their robust fundamental visual capabilities. Typically, VLMs generate rationales and answers; however, such pure text sequences lack the granularity to capture fine-grained visual perceptions and semantics, which poses challenges to accurate visual understanding, reasoning, and generation. This limitation arises because during inference, VLMs tend to prioritize accumulated textual context over visual evidence, a phenomenon particularly pronounced in long sequences [17, 25, 72, 78]. To address this, we extend the vocabulary $\nu$ of VLMs by incorporating four additional memory-operation tokens, resulting in ${ \mathcal { V } } ^ { \bar { \prime } } = { \mathcal { V } } \cup \left\{ < m _ { I } ^ { s } > , < m _ { E } ^ { s } > , \dot { < } m _ { I } ^ { l } > , < m _ { E } ^ { l } > \right\}$ . Here, $< m _ { I } >$ and $< m _ { E } >$ form paired invocation and end tokens, where the superscripts $s$ and $l$ denote short- or long-term memory, respectively. Specifically, we register these as indivisible special tokens in the tokenizer and enlarge the embedding matrix from $\mathbb { R } ^ { | \nu | \times d }$ to $\mathbb { R } ^ { ( | \nu | + 4 ) \times d }$ , where $d$ is the dimension of the model. Furthermore, we initialize the em-

![](images/153851a83ef9972c032f731f5ab73d4933e15e30729d5c4045a9f97b2d217d38.jpg)  
Figure 2. The overview of our proposed VisMem.

beddings of the invocation tokens $( < m _ { I } ^ { s } >$ and $< m _ { I } ^ { l } >$ ) using the embedding vector of a delimiter token with small perturbations, and update these embeddings during training to facilitate faster convergence. The two end tokens $( < m _ { E } ^ { s } >$ and $< m _ { E } ^ { l } >$ ) are treated as structural markers; they are initialized analogously with a lower learning rate. In practice, we also employ constrained decoding to encourage wellformed invocation-end pairs.

Specifically, the latent vision memory invocation tokens function as triggers for initiating memory insertion, based on the continuous internal cognitive states. During autoregressive generation (see Eq. (4)), upon the output of an invocation token, the memory former immediately initiates the latent vision memory formation procedure:

$$
x _ {t, i} \rightarrow \left\{\begin{array}{l l}\text {i n v o c a t i o n},&x _ {t, i} \in \left\{\langle m _ {I} ^ {s} \rangle , \langle m _ {I} ^ {l} \rangle \right\}\\\text {c o n t i n u e},&\text {o t h e r w i s e}\end{array}. \right. \tag {3}
$$

The resulting latent vision memory, whether short- or longterm as dictated by the specific token type, is subsequently inserted right after the already output invocation token. Following this insertion, the corresponding end token for short $( < m _ { E } ^ { s } > )$ or long memory $( < m _ { E } ^ { l } > )$ is automatically appended to resume token-by-token decoding:

$$
x _ {t, i} \sim \mathcal {P} (\cdot \mid s _ {t}, x _ {t, <   i}, \{m _ {I}, m _ {1}, \dots , m _ {N}, m _ {E} \}). \tag {4}
$$

# 3.3. Memory Formation

To activate the vision memory capability of VLMs, we integrate two memory components: short-term vision memory, which encodes rich visual evidence, and long-term vision memory, which primarily encodes high-level, knowledgebased visual pertinent semantics, without modifying the

core VLM and damaging general abilities. This integration leverages short-term memory to enhance advanced visual perception and comprehension, while long-term memory enables the generalization of semantic experiences during reasoning, thus comprehensively enhancing the overall visual performance. As illustrated in Fig. 2, the memory formation process hinges on two core components: a query builder $\boldsymbol { B }$ , which is responsible for generating queries to hook memory; and memory formers $\mathcal { F } _ { s }$ and $\mathcal { F } _ { l }$ , which are dedicated to constructing latent visual memories.

Query Builder. Through this process, we transform hidden states incorporating current cognition into a more efficient and accurate memory query. Initially, we instantiate a lightweight transformer encoder denoted as $\boldsymbol { B }$ and a learnable memory query $\mathbf { Q } _ { i n i t } = \{ q _ { 1 } , . . . , q _ { K } \}$ , where $K$ represents the length of the query sequence and each $q \in \mathbb { R } ^ { d }$ . Given the state at a particular time, $\boldsymbol { B }$ encodes the query sequence based on internal visual and contextual hidden states to retrieve the corresponding latent memory contents. During each invocation, as the policy model generates the current output token sequence, i.e., the token sequence starting from the initial position or from the end of the previous invocation, it accordingly produces a sequence of hidden state vectors $\{ h _ { 1 } , \ldots , h _ { z } \}$ . Similarly, visual encoder produces visual hidden state vectors $\{ v _ { 1 } , \ldots , v _ { y } \}$ . Thus, the combination of them $\mathbf { H } = \{ v _ { 1 } , \ldots , v _ { y } , h _ { 1 } , \ldots , h _ { z } \} \in \mathbb { R } ^ { ( y + z ) \times d }$ , characterizing the multi-modal cognitive state at the time, where $y$ and $z$ denote the lengths. Subsequently, we concatenate the initialized memory query to the rear of these hidden states to update the queried semantic information:

$$
\mathbf {Q} = \mathcal {B} ([ \mathbf {H}, \mathbf {Q} _ {\text {i n i t}} ]) [ - K: ], \tag {5}
$$

where we select the output of the last layer of the encoder (see Eq. (10)), and take the last $K$ encoded vectors as the memory query $\mathbf { Q } \in \mathbb { R } ^ { K \times d }$ to hook latent memory. Furthermore, we employ a masked attention to exclusively enable attention propagation from the query to the hidden states H, while suppressing attention in the reverse direction, i.e., from H to $\mathbf { Q }$ (see Eq. (11)). Here, both short- and long-term memory share the same query builder $\boldsymbol { B }$ .

Latent Memory Former. Distinct from many existing paradigms [26, 44, 70], we internalize the latent vision memory into lightweight formers, preserving the general abilities of base VLMs and ensuring the compatibility of our paradigm. We initialize two lightweight LoRA adapters, which are respectively designated as the short-term memory former $\mathcal { F } _ { s }$ and long-term memory former $\mathcal { F } _ { l }$ , attached to the vision encoder and the final language model of the VLM, without directly tampering with the core parameters. More precisely, we first append the generated memory query $\mathbf { Q }$ along with a set of learnable memory tokens after the corresponding target token sequence X. Then we process it by short-term or long-term memory former, which contextualizes and embeds the latent memory information:

$$
\mathbf {M} _ {s / l} = \mathcal {F} _ {s / l} \left(\left[ \mathbf {X}, \mathbf {Q}, \mathbf {M} _ {\text {i n i t}} \right]\right) \left[ - N _ {s / l}: \right], \tag {6}
$$

where short- and long-term latent vision memory $\mathbf { M } _ { s / l } \in$ $\mathbb { R } ^ { N _ { s / l } \times d }$ , while $N _ { s }$ and $N _ { l }$ are the predetermined lengths of memory tokens, which can be taken from $\{ 2 , 4 , 8 , 1 6 , 3 2 \}$ . For the short-term pathway, the resultant memory representation is concatenated with the visual token stream, and pass through the original projector to align it with the representation space of the language model. The two memory formers serve as dedicated memory carriers, exclusively storing visual evidences and semantic knowledge within themselves. When the policy model executes a memory invocation, the incoming memory query triggers externalization of useful short- or long-term memory. These memories are seamlessly inserted into the token generation process alongside the invocation and end signals and barely interfere with the original generation, as specified in Eq. (4).

# 3.4. Training Recipe

We design a two-stage training procedure based on GRPO [43], whose optimization objectives are to optimize the effective formation and invocation of latent memory. The first stage enhances the utility of memory, while the second stage maximizes the reward of each invocation, thereby accelerating the convergence of different components steadily. More detailed algorithms and implementations are present in Appendix 7.2 and 8.3.

Stage I: Memory Formation Optimization. In this stage, we update the query builder $\boldsymbol { B }$ , and memory formers $\mathcal { F } _ { s / l }$ while keeping the policy model $\mathcal { P }$ frozen. Initially, during the autoregressive generation process, we randomly invoke

either short- or long-term memory upon detecting the delimiter, thereby acquiring initial memory capabilities. Then, the scope of memory invocations is extended to the intervals between delimiters, this not only provides a richer trajectory of memory interactions but also enables memory invocation at arbitrary positions within the generation sequence. The core objective is to maximize the performance improvement relative to trajectory without memory integration $\Delta S ( \tau ) = S ( \tau ) - S ( \tau _ { b a s e } )$ , thereby enhancing the quality of the memory formation (full function in Eq. (14)):

$$
\max  _ {\mathcal {F} _ {s / l}, \mathcal {B}} \mathbb {E} _ {\tau \sim \mathcal {P}} \left(\cdot | x, \mathbf {M} _ {s / l}\right), \mathbf {M} _ {s / l} \sim \mathcal {F} _ {s / l} (\mathbf {Q}), \mathbf {Q} \sim \mathcal {B} (\mathbf {H}) [ \Delta S (\tau) ]. \tag {7}
$$

Stage II: Memory Invocation Optimization. In this process, we update part parameters $\theta$ of the policy model $\mathcal { P }$ , and keeps all the memory formation components frozen. At this stage, the policy model $\mathcal { P }$ is required to invoke memory efficiently and accurately, which entails two core requirements: selecting the correct memory type and avoiding invalid invocations. Thus, we add two penalties to the objective, which could be optimized by (full function in Eq. (15)):

$$
\max  _ {\theta} \mathbb {E} _ {\tau \sim \mathcal {P} \left(\cdot | x, \mathbf {M} _ {s / l}\right)} [ \Delta S (\tau) - \alpha \left(p _ {t y p e} + p _ {n e g}\right) ], \tag {8}
$$

where $\alpha$ denotes the penalty intensity. The type penalty, $p _ { \mathrm { t y p e } } = \operatorname* { m a x } \left( 0 , S ( \tau _ { \mathrm { r e v } } ) - S ( \tau ) \right)$ , serves to penalize the erroneous selection of memory types, where $\tau _ { \mathrm { r e v } }$ represents the invocation of an alternative memory type. In parallel, the negative penalty $p _ { \mathrm { n e g } } = \operatorname* { m a x } \left( 0 , \overline { { S } } - \dot { S } ( \tau ) \right)$ is designed to penalize invocations with negative returns, aiming to enhance efficiency. Here, $\overline { S }$ denotes the mean of quantifiable scores across candidate trajectories.

# 4. Experiments

# 4.1. Settings

Benchmarks. We select 12 benchmarks to comprehensively evaluate three main abilities of VLMs, i.e., understanding, reasoning and generation [31]. These benchmarks include: (1) understanding: MMStar [7], MMVet [76], MMT [73], BLINK [15], MuirBench [57]; (2) reasoning: MMMU [79], LogicVista [67], MathVista [37], MV-Math [62]; (3) generation: HallBench [19], Multi-Trust [82], MMVU [34]. Details are in Appendix 8.2.

Baselines. We compare our VisMem against 15 baselines, falling into four categories: (a) direct training methods: SFT, Visual-RFT [35], VLM-R1 [44], Vision-R1 [26] and PAPO [66]; (b) image-level methods: GRIT [13], Sketchpad [24], MVoT [29], OpenThinkImg [49] and Deep-Eyes [87]; (c) token-level methods: Scaffold [28], MINT-CoT [8], ICoT [16], and VPT [75]; (d) latent space methods: Mirage [70]. Details are in Appendix 8.3.

Implementation Details. All experiments (except for Tab. 2) are implemented on Qwen2.5-VL-7B [4] based on 8

Table 1. Results on 12 benchmarks to evaluate visual understanding, reasoning and generation abilities. The best and second best values are emphasized, and the average values are calculated for both specific capabilities and overall results.   

<table><tr><td>Method</td><td>MM Star</td><td>MM Vet</td><td>MMT</td><td>BLINK</td><td>Muir Bench</td><td>Avg.</td><td>MMMU</td><td>Logic Vista</td><td>Math Vista</td><td>MV -Math</td><td>Avg.</td><td>Hall Bench</td><td>Multi Trust</td><td>MMVU</td><td>Avg.</td><td>Avg.</td></tr><tr><td>Vanilla [4]</td><td>62.6</td><td>66.0</td><td>54.0</td><td>55.4</td><td>57.4</td><td>59.3</td><td>56.0</td><td>43.5</td><td>67.8</td><td>18.9</td><td>46.6</td><td>52.3</td><td>64.8</td><td>55.4</td><td>57.7</td><td>54.5</td></tr><tr><td>SFT</td><td>64.7</td><td>67.5</td><td>56.8</td><td>54.5</td><td>58.7</td><td>60.3</td><td>57.7</td><td>46.1</td><td>69.5</td><td>22.8</td><td>49.0</td><td>53.6</td><td>67.0</td><td>59.1</td><td>59.9</td><td>56.5</td></tr><tr><td>Visual-RFT [35]</td><td>65.6</td><td>70.5</td><td>59.1</td><td>58.0</td><td>62.9</td><td>63.6</td><td>62.4</td><td>51.7</td><td>71.6</td><td>26.5</td><td>53.0</td><td>55.8</td><td>70.7</td><td>63.2</td><td>63.2</td><td>59.8</td></tr><tr><td>VLM-R1 [44]</td><td>66.3</td><td>73.0</td><td>59.4</td><td>60.6</td><td>63.8</td><td>64.6</td><td>63.4</td><td>53.0</td><td>75.9</td><td>34.6</td><td>56.7</td><td>54.2</td><td>69.9</td><td>61.7</td><td>61.9</td><td>61.3</td></tr><tr><td>Vision-R1 [26]</td><td>67.1</td><td>71.7</td><td>60.2</td><td>60.8</td><td>64.0</td><td>65.0</td><td>63.2</td><td>53.9</td><td>77.2</td><td>38.7</td><td>58.2</td><td>56.4</td><td>72.6</td><td>63.6</td><td>64.2</td><td>62.5</td></tr><tr><td>PAPO [66]</td><td>64.2</td><td>69.8</td><td>57.9</td><td>53.3</td><td>56.7</td><td>60.4</td><td>61.2</td><td>52.5</td><td>73.3</td><td>34.8</td><td>55.5</td><td>50.3</td><td>67.7</td><td>56.5</td><td>58.2</td><td>58.2</td></tr><tr><td>Sketchpad [24]</td><td>62.1</td><td>64.5</td><td>57.0</td><td>54.9</td><td>52.8</td><td>58.3</td><td>57.9</td><td>47.4</td><td>68.4</td><td>24.6</td><td>49.6</td><td>52.1</td><td>66.2</td><td>57.2</td><td>58.5</td><td>55.4</td></tr><tr><td>GRIT [13]</td><td>65.8</td><td>67.8</td><td>57.9</td><td>52.5</td><td>51.0</td><td>59.0</td><td>59.4</td><td>51.6</td><td>68.1</td><td>22.4</td><td>50.4</td><td>53.7</td><td>67.3</td><td>60.1</td><td>60.4</td><td>56.5</td></tr><tr><td>PixelReasoner [48]</td><td>65.3</td><td>67.1</td><td>58.7</td><td>56.8</td><td>60.5</td><td>61.7</td><td>58.9</td><td>49.3</td><td>69.6</td><td>25.9</td><td>50.9</td><td>55.9</td><td>69.9</td><td>61.5</td><td>62.4</td><td>58.3</td></tr><tr><td>DeepEyes [87]</td><td>66.4</td><td>70.5</td><td>60.3</td><td>60.4</td><td>63.0</td><td>64.1</td><td>60.3</td><td>49.1</td><td>70.8</td><td>31.5</td><td>52.9</td><td>57.4</td><td>72.6</td><td>64.6</td><td>64.9</td><td>60.5</td></tr><tr><td>OpenThinkImg [49]</td><td>66.0</td><td>71.6</td><td>60.8</td><td>59.2</td><td>61.7</td><td>63.9</td><td>61.4</td><td>52.8</td><td>73.0</td><td>28.0</td><td>53.8</td><td>54.9</td><td>74.0</td><td>64.3</td><td>64.4</td><td>60.6</td></tr><tr><td>Scaffold [28]</td><td>63.9</td><td>67.0</td><td>58.5</td><td>52.5</td><td>52.9</td><td>59.0</td><td>58.1</td><td>51.0</td><td>64.7</td><td>21.0</td><td>48.7</td><td>54.8</td><td>68.5</td><td>60.6</td><td>61.3</td><td>56.1</td></tr><tr><td>ICoT [16]</td><td>65.6</td><td>67.9</td><td>60.5</td><td>54.3</td><td>57.0</td><td>61.1</td><td>58.6</td><td>49.8</td><td>76.7</td><td>30.8</td><td>54.0</td><td>57.0</td><td>69.1</td><td>62.0</td><td>62.7</td><td>59.1</td></tr><tr><td>MINT-CoT [8]</td><td>66.2</td><td>69.5</td><td>57.3</td><td>55.4</td><td>58.9</td><td>61.5</td><td>57.7</td><td>51.5</td><td>77.4</td><td>39.2</td><td>56.5</td><td>56.7</td><td>71.4</td><td>60.8</td><td>63.0</td><td>60.2</td></tr><tr><td>VPT [75]</td><td>64.2</td><td>70.8</td><td>59.0</td><td>58.6</td><td>63.5</td><td>63.2</td><td>59.1</td><td>53.0</td><td>72.3</td><td>34.7</td><td>54.8</td><td>52.3</td><td>64.7</td><td>61.4</td><td>59.5</td><td>59.5</td></tr><tr><td>Mirage [70]</td><td>64.5</td><td>71.8</td><td>56.1</td><td>56.3</td><td>59.0</td><td>61.5</td><td>59.4</td><td>50.6</td><td>70.3</td><td>35.4</td><td>53.9</td><td>50.9</td><td>66.1</td><td>60.3</td><td>59.1</td><td>58.4</td></tr><tr><td>VisMem (Ours)</td><td>68.9</td><td>75.1</td><td>62.5</td><td>64.5</td><td>69.8</td><td>68.2</td><td>63.9</td><td>55.7</td><td>79.8</td><td>41.4</td><td>60.2</td><td>59.6</td><td>77.0</td><td>68.2</td><td>68.3</td><td>65.5</td></tr></table>

NVIDIA H200 141G GPUs. The length of memory query $K$ is set to 8, and the lengths of short-term $N _ { s }$ and longterm latent vision memory $N _ { l }$ are 8 and 16, respectively. More implementation details are listed in Appendix 8.4.

# 4.2. Main Results

The main experimental results demonstrate that our proposed memory system VisMem unlocks the untapped potentials with three key enhancements: [Enh.1] advanced visual capabilities, [Enh.2] cross-domain generalization, [Enh.3] catastrophic forgetting alleviation.

[Enh.1] VisMem enables advanced and comprehensive visual capabilities. As presented in Tab. 1, our proposed method demonstrates distinct superiority over other baseline models. Compared with the vanilla model, Vis-Mem achieves a notable average improvement of $1 1 . 0 \%$ across all benchmarks. When compared with the top three baselines (i.e., Vision-R1 [26], VLM-R1 [44], and Open-ThinkImg [49]), our method still maintains improvements of $3 . 0 \%$ , $4 . 2 \%$ , and $4 . 9 \%$ , respectively. Furthermore, it consistently enhances performance across the three core domains of visual tasks, namely, understanding, reasoning, and generation. Our latent vision memory mechanism yields comprehensive enhancements in visual capabilities, with specific gains of $+ 8 . 9 \%$ in visual understanding, $+ 1 4 . 4 \%$ in reasoning, and $+ 1 0 . 6 \%$ in generation, relative to the vanilla model. It is also noteworthy that direct RL-based methods (e.g., VLM-R1 [44] and Vision-R1 [26]) also achieve relatively better performance than most other paradigms. However, this approach of directly modifying parameters relies on incremental parameter updates, which may lead to the overwriting of prior general knowledge and

![](images/f0acfed4ea9479d4ae3cbd2ede4f7bf594c542bf225ac40e6f9a7298d458df7d.jpg)  
Figure 3. Results of the cross-domain generalization study. Models are only trained on Visual CoT [42] and Mulberry [71]. Dashed bar indicates the results with full training data.

result in catastrophic forgetting.

As illustrated in Tab. 5 and 6, we conduct additional evaluations on selected subsets of MuirBench [57] and LogicVista [67]. Endowed with short- and long-term vision memory, our VisMem outperforms all baseline methods by a substantial margin in tasks demanding fine-grained visual evidence, such as counting $( + 7 . 0 \% )$ , visual retrieval $( + 9 . 4 \% )$ , and grounding $( 1 3 . 1 \% )$ , while also yielding notable improvements in visual reasoning tasks, including inductive $( + 5 . 7 \% )$ and deductive $( + 7 . 1 \% )$ learning.

[Enh.2] VisMem showcases great cross-domain generalization. To evaluate the cross-domain generalization capability of our model, specifically whether its stored latent visual memory can transfer across diverse unseen tasks, we exclusively train our VisMem and comparative baseline models on two datasets: Visual CoT [42] and Mulberry [71], then subsequently assess their performance on four unseen target benchmarks. As demonstrated in Fig. 3, 7, and Tab. 7, VisMem not only consistently achieves significant performance gains on out-of-domain tasks $( + 6 . 9 \%$ on MMVet [76], $+ 9 . 1 \%$ on MuirBench [57], $+ 2 0 . 2 \%$ on MV-Math [62], and $+ 9 . 9 \%$ on MultiTrust [82]), but also main-

Table 2. Results on nine base models with various sizes and sources, including Qwen2.5-VL-3B/7B/32B [4], LLaVA-OV-1.5-4B/8B [1], InternVL-3.5-4B/8B/14B/38B [63]. $\uparrow$ indicates the performance enhancement compared with the base model.   

<table><tr><td>Base Model</td><td>MM Star</td><td>MM Vet</td><td>MMT</td><td>BLINK</td><td>Muir Bench</td><td>MMMU</td><td>Logic Vista</td><td>Math Vista</td><td>MV -Math</td><td>Hall Bench</td><td>Multi Trust</td><td>MMVU</td></tr><tr><td>Qwen2.5-VL-3B [4]</td><td>52.9</td><td>61.5</td><td>49.8</td><td>46.0</td><td>46.1</td><td>52.6</td><td>39.7</td><td>61.0</td><td>13.2</td><td>46.3</td><td>56.9</td><td>48.4</td></tr><tr><td>+ VisMem (Ours)</td><td>61.0↑8.1</td><td>72.5↑11.0</td><td>59.3↑9.5</td><td>58.6↑12.6</td><td>64.4↑18.3</td><td>61.9↑9.3</td><td>53.1↑13.4</td><td>70.4↑9.4</td><td>31.7↑18.5</td><td>58.0↑11.7</td><td>70.3↑13.4</td><td>60.6↑12.2</td></tr><tr><td>Qwen2.5-VL-7B [4]</td><td>62.6</td><td>66.0</td><td>54.0</td><td>55.4</td><td>57.4</td><td>56.0</td><td>43.5</td><td>67.8</td><td>18.9</td><td>52.3</td><td>64.8</td><td>55.4</td></tr><tr><td>+ VisMem (Ours)</td><td>68.9↑6.3</td><td>75.1↑9.1</td><td>62.5↑8.5</td><td>64.5↑9.1</td><td>69.8↑11.4</td><td>63.9↑7.9</td><td>55.7↑12.2</td><td>79.8↑12.0</td><td>41.4↑22.5</td><td>59.6↑7.3</td><td>77.0↑12.2</td><td>68.2↑12.8</td></tr><tr><td>Qwen2.5-VL-32B [4]</td><td>67.1</td><td>68.7</td><td>64.7</td><td>59.9</td><td>63.5</td><td>70.6</td><td>47.9</td><td>72.7</td><td>29.0</td><td>53.6</td><td>64.5</td><td>55.5</td></tr><tr><td>+ VisMem (Ours)</td><td>73.9↑6.8</td><td>77.9↑9.2</td><td>72.0↑7.3</td><td>68.6↑8.7</td><td>73.3↑9.8</td><td>75.9↑5.3</td><td>63.5↑15.6</td><td>83.5↑10.8</td><td>54.9↑25.9</td><td>60.2↑6.6</td><td>77.7↑13.2</td><td>68.4↑12.9</td></tr><tr><td>LLaVA-OV-1.5-4B [1]</td><td>62.5</td><td>60.4</td><td>54.4</td><td>38.2</td><td>42.6</td><td>49.4</td><td>39.3</td><td>66.5</td><td>11.0</td><td>41.8</td><td>47.5</td><td>44.2</td></tr><tr><td>+ VisMem (Ours)</td><td>69.0↑6.5</td><td>70.1↑9.7</td><td>62.7↑8.3</td><td>56.9↑18.7</td><td>59.6↑17.0</td><td>59.7↑10.3</td><td>53.7↑14.4</td><td>79.0↑12.5</td><td>27.2↑16.2</td><td>52.8↑11.0</td><td>66.4↑18.9</td><td>61.9↑17.7</td></tr><tr><td>LLaVA-OV-1.5-8B [1]</td><td>65.3</td><td>67.1</td><td>57.8</td><td>49.8</td><td>50.5</td><td>55.3</td><td>46.5</td><td>68.3</td><td>15.7</td><td>50.1</td><td>54.7</td><td>50.6</td></tr><tr><td>+ VisMem (Ours)</td><td>70.8↑5.5</td><td>75.7↑8.6</td><td>64.7↑6.9</td><td>61.0↑11.2</td><td>62.6↑12.1</td><td>63.0↑7.7</td><td>59.5↑13.0</td><td>80.0↑11.7</td><td>34.5↑18.8</td><td>55.5↑5.4</td><td>69.4↑14.7</td><td>67.0↑16.4</td></tr><tr><td>InternVL-3.5-4B [63]</td><td>62.8</td><td>73.1</td><td>62.7</td><td>57.1</td><td>52.8</td><td>59.9</td><td>53.2</td><td>76.3</td><td>17.5</td><td>43.0</td><td>56.2</td><td>44.7</td></tr><tr><td>+ VisMem (Ours)</td><td>70.2↑7.4</td><td>80.3↑7.2</td><td>69.0↑6.3</td><td>65.2↑8.1</td><td>63.9↑11.1</td><td>68.5↑8.6</td><td>64.6↑11.4</td><td>82.5↑6.2</td><td>30.8↑13.3</td><td>52.4↑9.4</td><td>70.4↑14.2</td><td>61.9↑17.2</td></tr><tr><td>InternVL-3.5-8B [63]</td><td>67.0</td><td>80.1</td><td>64.6</td><td>58.4</td><td>55.7</td><td>68.6</td><td>54.8</td><td>77.5</td><td>27.1</td><td>54.5</td><td>65.9</td><td>52.3</td></tr><tr><td>+ VisMem (Ours)</td><td>71.8↑4.8</td><td>85.4↑5.3</td><td>69.5↑4.9</td><td>66.1↑7.7</td><td>65.3↑9.6</td><td>73.3↑4.7</td><td>64.7↑9.9</td><td>82.9↑5.4</td><td>44.7↑17.6</td><td>60.9↑6.4</td><td>78.5↑12.6</td><td>68.3↑16.0</td></tr><tr><td>InternVL-3.5-14B [63]</td><td>67.3</td><td>79.0</td><td>66.1</td><td>56.9</td><td>57.7</td><td>68.8</td><td>55.9</td><td>79.1</td><td>29.4</td><td>54.1</td><td>69.6</td><td>54.8</td></tr><tr><td>+ VisMem (Ours)</td><td>72.4↑5.1</td><td>85.7↑6.7</td><td>70.6↑4.5</td><td>66.1↑9.2</td><td>67.0↑9.3</td><td>73.8↑5.0</td><td>65.5↑9.6</td><td>85.1↑6.0</td><td>46.6↑17.2</td><td>60.5↑6.4</td><td>77.8↑8.2</td><td>68.3↑13.5</td></tr><tr><td>InternVL-3.5-38B [63]</td><td>72.2</td><td>79.7</td><td>70.5</td><td>60.3</td><td>64.0</td><td>72.1</td><td>61.1</td><td>80.2</td><td>35.7</td><td>60.2</td><td>71.5</td><td>58.0</td></tr><tr><td>+ VisMem (Ours)</td><td>75.1↑2.9</td><td>86.4↑6.7</td><td>73.3↑2.8</td><td>67.5↑7.2</td><td>69.9↑5.9</td><td>75.8↑3.7</td><td>68.7↑7.6</td><td>85.4↑5.2</td><td>56.9↑21.2</td><td>65.8↑5.6</td><td>79.0↑7.5</td><td>69.9↑11.9</td></tr></table>

![](images/18ab4a28ed06cabead348619d1b1d5c9508c4e7d106a61686e419a741fbb1435.jpg)  
Figure 4. Results of four-stage continual learning on MMVet [76]. Stage 0 only includes itself, while stage 1, 2, 3 sequentially train models on different additional training data combinations.

tains leading performance relative to all baselines. Notably, our method outperforms the second-ranked model by a substantial margin of $2 . 7 \mathrm { - } 6 . 8 \%$ across all four benchmarks, while narrowing the performance gap relative to results obtained with full training data. This observation underscores its robust cross-domain knowledge transfer capability.

[Enh.3] VisMem alleviates catastrophic forgetting. As illustrated in Fig. 4, 8, and Tab. 8, we conduct sequential training of the models across four stages, with performance assessed on MMVet [76] after each stage. At stage 0, the model was trained exclusively on the base task, and in subsequent stages, we incrementally incorporated selected benchmarks into the training process. From the continual learning results, our VisMem demonstrates significantly stronger knowledge retention capabilities. Although direct training paradigms yield relatively excellent overall performance in offline learning tasks with once-off training, they suffer from severe catastrophic forgetting. For instance, SFT exhibits over $10 \%$ performance degradation throughout the training process, the highest among all baselines. Additionally, at stage 0, VLM-R1 [44] and Vision-R1 [35] achieve performance improvements of $1 1 . 8 \%$ and $1 0 . 9 \%$ respectively compared to the vanilla model, however, these

improvements are retained by less than $0 . 5 \%$ at stage 4. In contrast, our method effectively mitigates catastrophic forgetting, exhibiting the smallest performance gap relative to original full-data training among all baselines. It is further worth noting that our latent vision memory enhances performance at stages 1 and 3 without any degradation, reflecting superior cross-task generalization.

# 4.3. Additional Analyses

Through additional analyses, we derive three key research observations pertaining to VisMem: [Obs.1] compatibility across base models, [Obs.2] dynamic and adaptive memory invocation, [Obs.3] relatively low inference latency.

[Obs.1] VisMem is robustly compatible across various base models. As detailed in Tab. 2 and Fig. 11, to evaluate the generalizability of our approach across diverse base models, we assess nine widely used base models, encompassing Qwen2.5-VL-3B/32B [4], LLaVA-OV-1.5- 4B/8B [1], InternVL-3.5-4B/8B/14B/38B [63], with parameter scales ranging from 3B to 38B. The results indicate that our latent vision memory paradigm exhibits strong compatibility across various models, yielding significant performance improvements across most visual tasks.

[Obs.2] The memory invocations are dynamic and selfadaptive. To elaborate on the effectiveness of our dual latent memory system, we characterize the properties of the short- and long-term memories it forms. As illustrated in Fig. 5, we first analyze the type-specific invocation ratios and their relative positions within the output sequence across four benchmarks. In summary, invocation ratios are self-adaptive across tasks, while both memory types exhibit a dynamic downward trend in invocation frequency throughout the output sequence. Task-specific comparisons

![](images/e5069a1f5cecb35c78c07ecd4971035a41e05aaa807552ef3d03fa10557b26d1.jpg)  
(a) MMVet [76]

![](images/d31370001a18163f008d0a4cf6b09bb0aec3dcff703737a41ada600adcb517ae.jpg)  
(b) MuirBench [57]

![](images/4cb658406671e712496da055df5074ee1baac925bdfb3f11d1f378b4f66b6725.jpg)  
(c) MV-Math [62]

![](images/f4dd340d21e2c7cbcfd4ddc2ddd03175ed228186085ca670dd9051c104525f7f.jpg)  
(d) MultiTrust [82]   
Figure 5. Results of memory invocation ratio and invocation relative position across four benchmarks.

in Fig. 9 further reveal that short-term latent memories are invoked more frequently to retrieve fine-grained details during visual information acquisition and understanding, particularly in multi-image scenarios, such as MuirBench [57]. Conversely, long-term latent vision memories play a more critical role in reasoning, e.g., in MV-Math [62], by providing abstract semantic knowledge relevant to the current task. Furthermore, Tab. 5 and 6, which detail the sub-task performance of MuirBench [57] and LogicVista [67] respectively, further illustrate that short-term and long-term latent visual memories are complementary. Their dynamic invocation yields superior performance compared to relying on a single memory type or the absence of vision memory.

[Obs.3] VisMem incurs minimal inference latency while yielding substantial performance gains. As showcased in Fig. 6 and Tab. 12, we compare the average inference time and task performance on four benchmarks to quantify the efficiency-performance trade-off of our method. Our Vis-Mem, by harnessing the capabilities of dual vision memory, attains the best performance while incurring insignificant inference latency. Notably, image-level paradigms significantly elevate inference latency, particularly for tasks involving long thinking paths. In contrast, our VisMem exhibits remarkable effectiveness while maintaining average inference latency comparable to that of direct training optimization and token-level methods.

Ablation Study and Sensitivity Analysis. As reported in Tab. 3, we conduct ablative studies on the memory invocation and dual memory formation. The results reveal that both short-term and long-term memory components contribute to performance across diverse visual tasks, while their complementarity synergistically drives the optimal performance. Additionally, as detailed in Tab. 9, our design achieves a favorable balance between effectiveness and efficiency, with accurate and non-redundant memory invoca-

![](images/804718d1bf6d35cffc0f9d4a1506f1c0d8261edfac6dd39e89e16dfb0f39d05f.jpg)  
(a) MMVet [76]

![](images/438c105fc9e2d2aa7877aea8771a7aae7058b513c94eabc186c8819b0cc9154b.jpg)  
(b) MuirBench [57]

![](images/c54b8fa09fbbeb7243d4a6a67266e6f200448bad35159e06aff359f123da182c.jpg)  
(c) MV-Math [62]

![](images/6792899521b5b413832ec16ddbc739af5db2cac2070e2f44ba573f72157e8485.jpg)  
(d) MultiTrust [82]   
Figure 6. Results of average inference time and performance across four benchmarks. The size is proportional to its y-value.

Table 3. Ablations of latent vision memory invocation and dual latent vision memory formation.   

<table><tr><td>Ablation</td><td>MM Vet</td><td>MuirBench</td><td>MV-Math</td><td>MultiTrust</td></tr><tr><td>Vanilla</td><td>66.0</td><td>57.4</td><td>18.9</td><td>64.8</td></tr><tr><td>Random Invocation (25%)</td><td>69.2</td><td>59.4</td><td>29.8</td><td>69.4</td></tr><tr><td>Random Invocation (50%)</td><td>71.9</td><td>63.2</td><td>26.1</td><td>68.5</td></tr><tr><td>Random Invocation (75%)</td><td>73.6</td><td>62.7</td><td>21.9</td><td>63.7</td></tr><tr><td>Full Invocation (100%)</td><td>73.4</td><td>56.0</td><td>17.5</td><td>62.6</td></tr><tr><td>Short-term Memory</td><td>71.5</td><td>65.6</td><td>29.6</td><td>73.6</td></tr><tr><td>Long-term Memory</td><td>69.4</td><td>60.2</td><td>36.1</td><td>69.8</td></tr><tr><td>Complete VisMem (Ours)</td><td>75.1</td><td>69.8</td><td>41.4</td><td>77.0</td></tr></table>

tion. As shown in Fig. 10 and Tab. 10, 11, we conduct sensitivity analyses of the sequence lengths of the memory query $K$ , short-term $N _ { s }$ and long-term $N _ { l }$ latent memory tokens. As observed, performance generally improves with increasing sequence lengths within a reasonable range. Notably, our selected hyper-parameters achieve a favorable balance between performance and computational efficiency.

# 5. Conclusion

To address “visual processing bottleneck” of VLMs that impairs advanced visual capacities, we propose VisMem in this work, a cognitively inspired framework embedding dynamic latent vision memory, which integrates dual specialized memory formers guided by human patterns, with a non-intrusive memory invocation mechanism. Extensive experiments validate VisMem achieves an obvious performance improvement across various benchmarks, and exhibits strong cross-domain generalization, catastrophic forgetting mitigation, compatibility, and efficient inference, unlocking comprehensive and advanced visual potentials.

# References

[1] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. 1, 7, 4   
[2] Anthropic. Introducing claude haiku 4.5, 2025. 1   
[3] Alan Baddeley. Working memory: Theories, models, and controversies. Annual review of psychology, 63(1):1–29, 2012. 1   
[4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 5, 6, 7, 2, 3, 4, 8   
[5] Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. LLaVA steering: Visual instruction tuning with 500x fewer parameters through modality linear representation-steering. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL: Long Papers), pages 15230–15250, Vienna, Austria, 2025. Association for Computational Linguistics.   
[6] Mahtab Bigverdi, Zelun Luo, Cheng-Yu Hsieh, Ethan Shen, Dongping Chen, Linda G Shapiro, and Ranjay Krishna. Perception tokens enhance visual reasoning in multimodal language models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 3836–3845, 2025. 2   
[7] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems (NeurIPS), 37:27056–27087, 2024. 5, 2   
[8] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-ofthought reasoning. arXiv preprint arXiv:2506.05331, 2025. 1, 2, 5, 6, 4, 8   
[9] Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning. arXiv preprint arXiv:2505.16782, 2025.   
[10] Zhangquan Chen, Ruihui Zhao, Chuwei Luo, Mingze Sun, Xinlei Yu, Yangyang Kang, and Ruqi Huang. Sifthinker: Spatially-aware image focus for visual reasoning. arXiv preprint arXiv:2508.06259, 2025.   
[11] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1   
[12] Yuhao Dong, Zuyan Liu, Hai-Long Sun, Jingkang Yang, Winston Hu, Yongming Rao, and Ziwei Liu. Insight-v: Exploring long-chain visual reasoning with multimodal large

language models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 9062– 9072, 2025.   
[13] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. 1, 2, 5, 6, 8   
[14] Dayuan Fu, Keqing He, Yejie Wang, Wentao Hong, Zhuoma GongQue, Weihao Zeng, Wei Wang, Jingang Wang, Xunliang Cai, and Weiran Xu. Agentrefine: Enhancing agent generalization through refinement tuning. In International Conference on Learning Representations (ICLR), 2025. 3   
[15] Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision (ECCV), pages 148–166. Springer, 2024. 5, 2, 6   
[16] Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Li. Interleaved-modal chain-of-thought. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 19520–19529, 2025. 1, 2, 5, 6, 8   
[17] Akash Ghosh, Arkadeep Acharya, Sriparna Saha, Vinija Jain, and Aman Chadha. Exploring the frontier of visionlanguage models: A survey of current methodologies and future directions. arXiv preprint arXiv:2404.07214, 2024. 1, 3   
[18] Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning. arXiv preprint arXiv:2510.27492, 2025.   
[19] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 14375–14385, 2024. 5, 2   
[20] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2   
[21] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 1, 2   
[22] Liqi He, Zuchao Li, Xiantao Cai, and Ping Wang. Multimodal latent space learning for chain-of-thought reasoning in language models. In Proceedings of the AAAI conference on artificial intelligence, pages 18180–18187, 2024.   
[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. 3

[24] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, and Ranjay Krishna. Visual sketchpad: Sketching as a visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems (NeurIPS), 37:139348–139379, 2024. 1, 2, 5, 6, 8   
[25] Jen-Tse Huang, Dasen Dai, Jen-Yuan Huang, Youliang Yuan, Xiaoyuan Liu, Wenxuan Wang, Wenxiang Jiao, Pinjia He, and Zhaopeng Tu. Visfactor: Benchmarking fundamental visual cognition in multimodal large language models. arXiv preprint arXiv:2502.16435, 2025. 1, 3   
[26] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 1, 2, 5, 6, 3, 4, 8   
[27] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2   
[28] Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, and Yang Liu. Scaffolding coordinates to promote vision-language coordination in large multi-modal models. arXiv preprint arXiv:2402.12058, 2024. 1, 2, 5, 6, 8   
[29] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imag- ´ ine while reasoning in space: Multimodal visualization-ofthought. In International Conference on Machine Learning (ICML), 2025. 1, 2, 5   
[30] Hengli Li, Chenxi Li, Tong Wu, Xuekai Zhu, Yuxuan Wang, Zhaoxin Yu, Eric Hanchen Jiang, Song-Chun Zhu, Zixia Jia, Ying Nian Wu, et al. Seek in the dark: Reasoning via testtime instance-level policy gradient in latent space. arXiv preprint arXiv:2505.13308, 2025. 1, 3   
[31] Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, and Long Chen. A survey on multimodal benchmarks: In the era of large ai models. arXiv preprint arXiv:2409.18142, 2024. 1, 5   
[32] Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, and Liqiang Nie. Finecir: Explicit parsing of finegrained modification semantics for composed image retrieval. arXiv preprint arXiv:2503.21309, 2025.   
[33] Xun Liang, Xin Guo, Zhongming Jin, Weihang Pan, Penghui Shang, Deng Cai, Binbin Lin, and Jieping Ye. Enhancing spatial reasoning through visual and textual thinking. arXiv preprint arXiv:2507.20529, 2025. 2   
[34] Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang, Muyang He, Jian Li, Zheng Liu, Harry Yang, Sernam Lim, et al. Seeing clearly, answering incorrectly: A multimodal robustness benchmark for evaluating mllms on leading questions. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 9087– 9097, 2025. 5, 2, 6   
[35] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visualrft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. 1, 2, 5, 6, 7, 8

[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 3   
[37] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. 5   
[38] Dennis Norris. Short-term memory and long-term memory are still different. Psychological bulletin, 143(9):992, 2017. 1, 3   
[39] OpenAI. Gpt 5, 2025. 1   
[40] OpenAI. Think with image, 2025. 2   
[41] Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, et al. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286, 2025. 3   
[42] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems (NeurIPS), 37:8612–8642, 2024. 2, 6, 4, 5   
[43] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 5, 1   
[44] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. 1, 2, 5, 6, 7, 3, 4, 8   
[45] Qianli Shen, Yezhen Wang, Zhouhao Yang, Xiang Li, Haonan Wang, Yang Zhang, Jonathan Scarlett, Zhanxing Zhu, and Kenji Kawaguchi. Memory-efficient gradient unrolling for large-scale bi-level optimization. Advances in Neural Information Processing Systems (NeurIPS), 37:90934–90964, 2024. 3   
[46] Ruolin Shen, Xiaozhong Ji, Kai Wu, Jiangning Zhang, Yijun He, HaiHua Yang, Xiaobin Hu, and Xiaoyu Sun. Align and surpass human camouflaged perception: Visual refocus reinforcement fine-tuning. arXiv preprint arXiv:2505.19611, 2025.   
[47] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. 1, 3   
[48] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 1, 2, 6, 8   
[49] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 1, 2, 5, 6, 4, 8

[50] Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, et al. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918, 2025. 1   
[51] Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. Latent chain-of-thought for visual reasoning. arXiv preprint arXiv:2510.23925, 2025.   
[52] Hai-Long Sun, Zhun Sun, Houwen Peng, and Han-Jia Ye. Mitigating visual forgetting via take-along visual conditioning for multi-modal long CoT reasoning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL: Long Papers), pages 5158–5171, Vienna, Austria, 2025. Association for Computational Linguistics. 1   
[53] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard Schwarz. Online adaptation of language models with a memory of amortized contexts. Advances in Neural Information Processing Systems (NeurIPS), 37:130109–130135, 2024. 3   
[54] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2   
[55] GLM-V Team. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025. 1   
[56] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 1   
[57] Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li, Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, and Muhao Chen. Muirbench: A comprehensive benchmark for robust multi-image understanding. In International Conference on Learning Representations (ICLR), 2025. 5, 6, 8, 2, 3   
[58] Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025. 2   
[59] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems (NeurIPS), 37:95095–95169, 2024. 2, 5, 6   
[60] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model’s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2   
[61] Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. Advances in Neural Information Processing Systems (NeurIPS), 37:53764–53797, 2024. 3

[62] Peijie Wang, Zhong-Zhi Li, Fei Yin, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating multimodal math reasoning in multi-visual contexts. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 19541–19551, 2025. 5, 6, 8, 2   
[63] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1, 7, 4   
[64] Yujun Wang, Jinhe Bi, Soeren Pirk, Yunpu Ma, et al. Ascd: Attention-steerable contrastive decoding for reducing hallucination in mllm. arXiv preprint arXiv:2506.14766, 2025.   
[65] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogerio Feris, and Zexue He. $\mathbf { M } +$ : Extending memoryllm with scalable long-term memory. arXiv preprint arXiv:2502.00592, 2025. 3   
[66] Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025. 1, 2, 5, 6, 8   
[67] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. 5, 6, 8, 2, 4   
[68] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025. 1, 3   
[69] Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Let’s think ´ only with images. arXiv preprint arXiv:2505.11409, 2025. 2   
[70] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. 1, 3, 5, 6, 2, 4, 8   
[71] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. 6, 2, 4, 5   
[72] Hao Yin, Guangzong Si, and Zilei Wang. Clearsight: Visual signal enhancement for object hallucination mitigation in multimodal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 14625–14634, 2025. 3   
[73] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006, 2024. 5, 2   
[74] Jiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting continual learning of vision-language models via mixture-of-experts adapters. In

Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 23219–23230, 2024. 2   
[75] Runpeng Yu, Xinyin Ma, and Xinchao Wang. Introducing visual perception token into multimodal large language model. arXiv preprint arXiv:2502.17425, 2025. 1, 2, 5, 6, 8   
[76] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In International Conference on Machine Learning (ICML), 2024. 5, 6, 7, 8, 2, 4   
[77] Xinlei Yu, Zhangquan Chen, Yudong Zhang, Shilin Lu, Ruolin Shen, Jiangning Zhang, Xiaobin Hu, Yanwei Fu, and Shuicheng Yan. Visual document understanding and question answering: A multi-agent collaboration framework with test-time scaling. arXiv preprint arXiv:2508.03404, 2025.   
[78] Xinlei Yu, Chengming Xu, Guibin Zhang, Yongbo He, Zhangquan Chen, Zhucun Xue, Jiangning Zhang, Yue Liao, Xiaobin Hu, Yu-Gang Jiang, et al. Visual multi-agent system: Mitigating hallucination snowballing via visual flow. arXiv preprint arXiv:2509.21789, 2025. 3   
[79] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9556–9567, 2024. 5, 2   
[80] Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory: Tracing hierarchical memory for multi-agent systems. arXiv preprint arXiv:2506.07398, 2025. 3   
[81] Guibin Zhang, Muxin Fu, and Shuicheng Yan. Memgen: Weaving generative latent memory for self-evolving agents. arXiv preprint arXiv:2509.24704, 2025. 1, 2, 3, 6   
[82] Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, and Jun Zhu. Multitrust: A comprehensive benchmark towards trustworthy multimodal large language models. In The Conference on Neural Information Processing Systems (NeurIPS), 2024. 5, 6, 8, 2   
[83] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19632–19642, 2024. 3   
[84] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 1702–1713, 2025.   
[85] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Qilong Wu, Kaipeng Zhang, and Chen Wei. Pyvision: Agentic vision with dynamic tooling. arXiv preprint arXiv:2507.07998, 2025. 2   
[86] Boyuan Zheng, Michael Y Fatemi, Xiaolong Jin, Zora Zhiruo Wang, Apurva Gandhi, Yueqi Song, Yu

Gu, Jayanth Srinivasa, Gaowen Liu, Graham Neubig, et al. Skillweaver: Web agents can self-improve by discovering and honing skills. arXiv preprint arXiv:2504.07079, 2025. 3   
[87] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing” thinking with images” via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. 1, 2, 5, 6, 4, 8   
[88] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19724–19731, 2024. 3   
[89] Da-Wei Zhou, Yuanhan Zhang, Yan Wang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Learning without forgetting for vision-language models. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 47(6):4489–4504, 2025. 2   
[90] Yucheng Zhou, Zhi Rao, Jun Wan, and Jianbing Shen. Rethinking visual dependency in long-context reasoning for large vision-language models. arXiv preprint arXiv:2410.19732, 2024. 1

# VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models Supplementary Material

# 6. Theoretical Foundations

As the mainstream position in anthropological cognitive psychology since the 20th century, short-term memory and long-term memory are two distinct storage systems that can be differentiated based on their functional and neural underpinnings [3, 38]. Specifically, the Dennis Norris Theory [38] proposes that short-term memory requires processing new visual information, temporarily storing multiple tokens, and enabling variable signals. It relies neurologically on vision-specific brain regions, e.g., the visual cortex and the posterior superior temporal lobe associated with verbal short-term memory), exhibiting visual dominance; longterm memory, however, centers on abstract semantic representations and relies on semantic-related brain regions like the medial temporal lobe and mid-temporal lobe.

Thus, we propose a framework termed VisMem to invoke dual short and long latent memory during the tokenby-token autoregressive generation. Aligned with Dennis Norris Theory [38], we instantiate these roles in a VLM backbone via latent vision memory invocation and latent vision memory formation, which together produce distinct short and long latent memory tokens and integrate them into the generation stream of the model.

# 7. Methodology Details

# 7.1. Query Builder

As described in Sec. 3.3, the we initialize a lightweight transformer-based encoder as memory builder $\boldsymbol { B }$ . We feed the concatenated memory query $\mathbf { Q }$ and hidden states of vision and output H into the builder to encoder query as memory hook (see Eq. (5)). The transformer-based builder has $L$ layers of encoders, the output process of the $\ell$ layer could be summarized as:

$$
\operatorname {S A} (x) = \operatorname {S M} \left(\frac {\left(x W _ {q}\right) \left(x W _ {k}\right) ^ {\top}}{\sqrt {d _ {k}}} + M\right) \left(x W _ {v}\right), \tag {9}
$$

$$
x ^ {\ell} = \operatorname {F F} \left(\operatorname {L N} \left(x ^ {\ell - 1} + \operatorname {S A} \left(\operatorname {L N} \left(x ^ {\ell - 1}\right)\right)\right)\right) + x ^ {\ell - 1}, \tag {10}
$$

where we simplify the input sequence to $x$ , and SM, MHA, FF, LN denote the softmax, multi-head self-attention, feedforward layer, layer normalization operations, respectively. In addition, $M$ is the mask which only allows attention from memory query $\mathbf { Q }$ to hidden states H, and blocks the reverse direction:

$$
M _ {i j} = \left\{ \begin{array}{l l} - C, & i <   K \text {a n d} j \geq K \\ 0, & \text {o t h e r w i s e} \end{array} , \right. \tag {11}
$$

where $C \gg 0$ is constant, thus the attention is close to $- \infty$ .

# 7.2. Training Recipe

As mentioned in Sec. 3.4, we design a two-stage training pipeline: at the first stage, the main objective is to optimize the memory formation process (see Eq. (7)); at the second stage, the main objective is to optimize the memory invocation (see Eq. (8)). We update the models based on reinforcement learning, i.e., GRPO strategy [43]. Specifically, for each instruction-vision pair $( I , V )$ , the policy model $\mathcal { P }$ generates a group of $G$ distinct candidate trajectories, termed as $\mathcal { T } = \{ \tau _ { 1 } , \dots , \tau _ { G } \}$ . For each trajectory, we utilize a $S \left( \cdot \right)$ to quantify the performance. Then, a group-relative baseline is calculated via averaging and standardizing all trajectories within the candidate group $G$ :

$$
\bar {S} = \frac {1}{G} \sum_ {i = 1} ^ {G} S (\tau_ {i}), \hat {S} = \sqrt {\frac {1}{G} \sum_ {i = 1} ^ {G} \left(S (\tau_ {i}) - \bar {S}\right) ^ {2}}. \tag {12}
$$

Consequently, the group-relative advantage of each trajectory could be formulated as:

$$
\hat {A} = \frac {S (\tau) - \bar {S}}{\hat {S} + \epsilon}. \tag {13}
$$

At the Stage I, the reinforcement learning optimizes the memory formation process, whose final objective function is:

$$
\begin{array}{l} \mathcal {J} _ {G R P O} ^ {s t a g e 1} (\phi) = \mathbb {E} _ {\tau , \mathbf {M} _ {s / l}, \mathbf {Q}} \left[ \frac {1}{G} \sum_ {i = 1} ^ {G} \right. \\ \left. \left. \min  \left(\rho_ {i} (\phi) \hat {A} _ {i}, \operatorname {c l i p} \left(\rho_ {i} (\phi), 1 - \epsilon , 1 + \epsilon\right) \hat {A} _ {i}\right) \right] \right. \tag {14} \\ - \beta D _ {\mathrm {K L}} \left[ \pi_ {\tau} ^ {\phi} \| \pi_ {\mathrm {r e f}} ^ {\phi} \right], \\ \end{array}
$$

where $\epsilon$ controls the group-relative advantage $\hat { A }$ , $\beta$ regulates the KL divergence penalty, and the updated policy parameters $\pi ^ { \phi } = \pi ^ { \phi } \left( \mathbf { Q } \mid \mathbf { H } \right) \cdot \pi ^ { \phi } \left( \mathbf { M } _ { s / l } \mid \mathbf { Q } \right)$ .

At the Stage II, the reinforcement learning optimizes the memory invocation process, whose final objective function is:

$$
\begin{array}{l} \mathcal {J} _ {G R P O} ^ {s t a g e 2} (\theta) = \mathbb {E} _ {\tau , x} \left[ \frac {1}{G} \sum_ {i = 1} ^ {G} \right. \\ \left. \left. \min  \left(\rho_ {i} (\theta) \hat {A} _ {i}, \operatorname {c l i p} \left(\rho_ {i} (\theta), 1 - \epsilon , 1 + \epsilon\right) \hat {A} _ {i}\right) \right] \right. \tag {15} \\ - \beta D _ {\mathrm {K L}} \left[ \pi_ {\tau} ^ {\theta} \| \pi_ {\mathrm {r e f}} ^ {\theta} \right]. \\ \end{array}
$$

# 8. Experiment Details

# 8.1. Training Data

During the two-stage training procedure, we use the same training data to optimize both the memory invocation and memory formation in the latent vision memory system. Initially, we include the training split dataset of the selected benchmarks and retain their original data division. For benchmarks without a training phase, we use them solely for evaluation. Additionally, we incorporate the Visual CoT [42] and Mullberry [71], improving the reasoning abilities.

# 8.2. Benchmarks

To comprehensively evaluate the performance of the selected baselines, we involve 12 benchmarks, consisting of 5 benchmarks of understanding, 4 benchmarks of reasoning, and 3 benchmarks of generation:

• MMStar [7] is a high-quality vision-centric benchmark meticulously curated by human experts. This benchmark assesses 6 core capabilities across 18 detailed axes of visual understanding.   
• MMVet [76] establishes 6 core visual understanding capabilities and investigates 16 critical integrations derived from their combinations. It uses an evaluator tailored for open-ended outputs.   
• MMT [73] consists of carefully curated multi-choice visual questions, covering 32 core meta-tasks and 162 subtasks within the field of visual understanding.   
• BLINK [15] reconstructs 14 classic computer vision tasks into multiple-choice questions. Each question is paired with either single or multiple images and supplemented with visual prompting.   
• MuirBench [57] covers 12 diverse multi-image tasks, which involve 10 categories of multi-image relations. Each standard instance is paired with an unanswerable variant that differs only minimally in semantics.   
• MMMU [79] comprises meticulously curated visual questions sourced from college exams, quizzes, and textbooks spanning 30 subjects and 183 subfields, which focus on advanced reasoning grounded in domain-specific knowledge.   
• LogicVista [67] evaluates general logical cognition abilities across 5 logical reasoning tasks, which encompass 9 distinct capabilities. Each question is annotated with the correct answer and the human-written reasoning behind the selection.   
• MathVista [59] unifies the challenges of heterogeneous mathematical and visual tasks, which are curated from math-oriented multimodal datasets.   
• MV-Math [62] is a dataset comprising mathematical problems, integrating multiple images interleaved with text, and detailed annotations. It features multiple-choice,

free-form, and multi-step questions across 11 subject areas at 3 difficulty levels.

• HallBench [19] consists of images paired with questions, designed by human experts to assess the hallucination level of generation.   
• MultiTrust [82] covers five primary aspects: truthfulness, safety, robustness, fairness, and privacy, evaluating the trustworthiness of generation.   
• MMVU [34] encompasses 12 categories, and designs evaluation metrics that measure the quality and error degree of generation.

# 8.3. Baselines

We select a total of 16 baselines, including the vanilla model [4], 5 direct training paradigms: SFT, Visual-RFT [35], VLM-R1 [44], Vision-R1 [26], and PAPO [66]; 5 image-level paradigms: Sketchpad [24], GRIT [13], PixelReasoner [48], DeepEyes [87], and OpenThinkImg [49]; 4 token-level paradigms: Scaffold [28], ICoT [16], MINT-CoT [8], and VPT [75]; and 1 latent space paradigm: Mirage [70].

Here, VLM-R1 [44] and Vision-R1 [26] follow the main GRPO [20] paradigm based on VLMs. To assess the effectiveness of different methods, our VisMem is trained on Qwen-2.5-VL-7B [4]. For strategies initially implemented on other base models, e.g., GPT-4o [27] and Qwen2- VL [60], we transfer them to Qwen2.5-VL-7B [4] for fair comparison. Besides, we maintain identical training datasets across most counterparts; however, for those three methods with specially curated datasets, we follow their original settings. Namely, Mirage [70] requires additional labeled training images, so we follow its original training dataset; GRIT [13] uses a tailored training process with designed data; and MINT-CoT [8] curates high-quality mathematical samples with grids and annotations.

# 8.4. Implementations

The configurations and implementations of the experiments include three main parts: the core hyperparameters, the parameters of the LoRA adapter, and the parameters we use during training. The configurations and implementations of the experiments are listed in Tab. 4.

# 9. Additional Results

# 9.1. Benchmark Subset Results towards Visual Subcapacities

To precisely identify the capability boundaries and advantages of our VisMem, rather than relying solely on overall scores to judge its quality, we evaluate the results of subsets of MuirBench [57] and LogicVista [67] benchmarks. We select 9 subsets of the former benchmark, including: counting, grounding, matching, scene, difference, cartoon,

Table 4. Configurations of parameters.   

<table><tr><td>Configurations</td><td>Parameters</td><td>Values</td><td></td></tr><tr><td rowspan="3">Core</td><td>K</td><td>8</td><td></td></tr><tr><td>Ns</td><td>4</td><td></td></tr><tr><td>Nl</td><td>8</td><td></td></tr><tr><td rowspan="4">LoRA [23]</td><td>rank</td><td>16</td><td></td></tr><tr><td>α</td><td>32</td><td></td></tr><tr><td>drop_out_rate</td><td>0.1</td><td></td></tr><tr><td>targetModule</td><td>[q-proj, v-proj]</td><td></td></tr><tr><td rowspan="13">Training</td><td></td><td>Stage I</td><td>Stage II</td></tr><tr><td>batch_size</td><td>8</td><td></td></tr><tr><td>epoch</td><td>2</td><td></td></tr><tr><td>warmup_ratio</td><td>0.2</td><td></td></tr><tr><td>num_iteration</td><td>1</td><td></td></tr><tr><td>learning_rate</td><td>5e-5</td><td></td></tr><tr><td>optimizer</td><td>AdamW [36]</td><td></td></tr><tr><td>Scheduler</td><td>Cosine</td><td></td></tr><tr><td>group_size</td><td>16</td><td></td></tr><tr><td>clip_ratio</td><td>0.2</td><td></td></tr><tr><td>kl_penalty_coefficient β</td><td>0.015</td><td></td></tr><tr><td>target_kl_per_token</td><td>0.03</td><td></td></tr><tr><td>penalty_intensity α</td><td>-</td><td></td></tr></table>

Table 5. Results on 9 selected subsets of MuirBench [57]. We compare our VisMem with the second and third best scored counterparts, and separately use the short or long latent memory to assess the improvements of each.   

<table><tr><td>Method</td><td>Counting</td><td>Grounding</td><td>Matching</td><td>Scene</td><td>Difference</td><td>Cartoon</td><td>Diagram</td><td>Geographic</td><td>Retrieval</td></tr><tr><td>Vanilla [4]</td><td>44.1</td><td>34.2</td><td>80.9</td><td>70.5</td><td>53.2</td><td>52.9</td><td>82.4</td><td>53.7</td><td>76.1</td></tr><tr><td>VLM-R1 [44]</td><td>52.5</td><td>38.1</td><td>83.6</td><td>73.5</td><td>58.1</td><td>55.1</td><td>86.8</td><td>56.7</td><td>79.4</td></tr><tr><td>Vision-R1 [26]</td><td>53.8</td><td>39.2</td><td>84.5</td><td>73.1</td><td>57.4</td><td>57.2</td><td>87.4</td><td>57.9</td><td>78.9</td></tr><tr><td>VisMem (Short Memory)</td><td>61.3</td><td>49.4</td><td>82.7</td><td>72.1</td><td>58.9</td><td>54.0</td><td>88.9</td><td>61.8</td><td>87.5</td></tr><tr><td>VisMem (Long Memory)</td><td>46.3</td><td>42.6</td><td>83.2</td><td>74.3</td><td>55.4</td><td>59.4</td><td>87.4</td><td>62.7</td><td>78.3</td></tr><tr><td>VisMem</td><td>60.8</td><td>52.3</td><td>84.0</td><td>76.2</td><td>60.6</td><td>59.7</td><td>90.1</td><td>65.5</td><td>89.8</td></tr></table>

diagram, geographic, and retrieval. While in the latter benchmark, we also select 10 subsets, including 5 reasoning skills: inductive, deductive, numerical, spatial, and mechanical, and 5 capacities: patterns, puzzles, OCR, graphs, and tables. It is worth noting that the selected subsets are only part of the benchmark, thus, the average values of the 10 subsets are not the results of the benchmarks.

As listed in Tab. 5, compared with VLM-R1 [44] and Vision-R1 [26], our VisMem achieves the best results on 7 subsets and ranks second on the remaining two subsets. Specifically, it has a generalized enhancement of at least $5 \%$ over the base model. Besides, VisMem improves the performance the vanilla model by $1 6 . 7 \% / 1 8 . 2 \% / 1 1 . 8 \% / 1 3 . 7 \%$

on the counting, grounding, geographic, and retrieval subtasks, vastly exceeding the second-best counterpart by 7.0- $1 3 . 1 \%$ . These results indicate that our latent vision memory system significantly promote the fine-grained visual cognition and perception of the base VLMs.

As presented in Tab. 6, our VisMem outperforms two baseline models, i.e., VLM-R1 [44] and Vision-R1 [26], by achieving the top performance across 8 subsets. Specifically, it delivers a generalized improvement of no less than $7 \%$ over the base model. Notably, on inductive, deductive, graph-based, and table-based sub-tasks, VisMem surpasses the vanilla model by $1 4 . 8 \%$ , $1 4 . 8 \%$ , $1 8 . 4 \%$ , and $2 1 . 1 \%$ , respectively, which exceeds the second-ranked model by a

Table 6. Results on 10 selected subsets (5 reasoning skills and 5 capabilities) of LogicVista [67]. We compare our VisMem with the second and third best scored counterparts, and separately use the short or long latent memory to assess the improvements of each.   

<table><tr><td>Method</td><td>Inductive</td><td>Deductive</td><td>Numerical</td><td>Spatial</td><td>Mechanical</td><td>Patterns</td><td>Puzzles</td><td>OCR</td><td>Graphs</td><td>Tables</td></tr><tr><td>Vanilla [4]</td><td>44.6</td><td>45.0</td><td>39.7</td><td>37.9</td><td>48.7</td><td>30.1</td><td>32.5</td><td>41.6</td><td>34.4</td><td>36.8</td></tr><tr><td>VLM-R1 [44]</td><td>53.7</td><td>52.7</td><td>45.8</td><td>44.1</td><td>57.3</td><td>35.8</td><td>42.8</td><td>49.0</td><td>46.5</td><td>52.6</td></tr><tr><td>Vision-R1 [26]</td><td>53.5</td><td>51.4</td><td>46.7</td><td>44.8</td><td>58.9</td><td>36.5</td><td>43.6</td><td>49.7</td><td>48.2</td><td>53.8</td></tr><tr><td>VisMem (Short Memory)</td><td>49.8</td><td>50.1</td><td>44.7</td><td>45.2</td><td>54.3</td><td>35.2</td><td>42.0</td><td>47.6</td><td>50.3</td><td>54.1</td></tr><tr><td>VisMem (Long Memory)</td><td>57.5</td><td>58.4</td><td>42.8</td><td>40.0</td><td>52.0</td><td>35.7</td><td>38.0</td><td>47.4</td><td>48.9</td><td>51.3</td></tr><tr><td>VisMem</td><td>59.4</td><td>59.8</td><td>46.9</td><td>47.2</td><td>57.4</td><td>38.9</td><td>44.6</td><td>48.5</td><td>52.8</td><td>57.9</td></tr></table>

substantial margin of $5 . 3 \mathrm { - } 7 . 1 \%$ . These results demonstrate that our latent visual memory system delivers contextualized semantic knowledge, thereby enhancing visual reasoning and robust generation capabilities.

# 9.2. Cross-domain Generalization

To evaluate the cross-domain generalization capability of our model, we train it exclusively on general datasets, namely, Visual CoT [42] and Mullberry [71]), to verify whether latent visual memory can be transferred to unseen domains. As shown in Tab. 7 and Fig. 7, our method demonstrates superior performance, which exhibits a smaller performance drop than the fully trained model across all four selected benchmarks, confirming strong cross-domain generalization. Despite being trained on only two datasets, our method achieves a significant performance improvement of $9 . 1 - 2 0 . 5 \%$ across the four benchmarks, with a mere $2 \%$ performance gap relative to the fully trained model. When compared to other baselines, it still maintains a performance lead of $3 . 4 \% / 6 . 7 \% / 2 . 7 \% / 4 . 7 \%$ $2 . 7 \%$ $4 . 7 \%$ across the four evaluations, respectively.

In general, the image-level, token-level, and latent space paradigms suffer from smaller performance degradation, whereas the direct training paradigm exhibits inferior generalization ability. For example, VLM-R1 [44] experiences a $5 . 3 \%$ performance drop; by contrast, this value is only $2 . 1 \%$ for OpenThinkImg [49], $1 . 1 \%$ for MINT-CoT [8], and $2 . 3 \%$ for our method. These results indicate that while direct training optimizations notably improve performance on specific tasks, they compromise generalization ability to some extent.

# 9.3. Catastrophic Forgetting Mitigation

To assess the extent of catastrophic forgetting, we conducted continual learning experiments with our VisMem and other baselines. As presented in Tab. 8 and Fig. 8, our method effectively mitigates forgetting of earlier tasks. It consistently achieves the best performance at each stage, demonstrating strong robustness against catastrophic forgetting. Following four-stage sequential continual training,

it retains $7 2 . 1 \%$ performance on MMVet [76], outperforming $6 8 . 4 \%$ of DeepEyes [87] and $6 7 . 0 \%$ of Mirage [70].

While the direct training paradigm significantly improves performance on specific tasks, it adapts to new tasks via direct updates to core parameters. This introduces conflicts when parameter update directions contradict the storage of existing knowledge, compounded by a lack of constraints from prior knowledge. Consequently, in stage 3, the performance of most direct training methods even falls below that of the vanilla model. In contrast, methods such as OpenThinkImg [49] and our proposed VisMem exhibit stronger knowledge retention and forward transfer capabilities. For instance, in stage 3, training on additional datasets further improves their performance on MMVet [76].

# 9.4. Versatility across Various Base Models

As presented in Tab. 2 and Fig. 11, we incorporate our latent visual memory paradigm into 9 base models, including Qwen2.5-VL-3B/7B/32B [4], LLaVA-OV-1.5-4B/8B [1], and InternVL-3.5-4B/8B/14B/38B [63]. Our VisMem consistently enhances the visual capabilities of all base models, spanning 3B to 38B parameter sizes across three VLM families. For the widely used medium-sized models (i.e., 7B or 8B parameter models), our latent visual memory delivers substantial performance gains, which brings a $6 . 3 \substack { - 2 3 . 1 \% }$ improvement across all benchmarks for Qwen2.5-VL-7B [4], a $5 . 5 \substack { - 2 0 . 2 \% }$ improvement for LLaVA-OV-1.5-8B [1], and a $4 . 8 \mathrm { - } 1 7 . 6 \%$ improvement for InternVL-3.5-8B [63], respectively.

Furthermore, in most benchmarks, smaller-parameter base models yield greater performance gains than their medium- or large-sized counterparts. This phenomenon may stem from an imbalance in task difficulty, which makes it more challenging for models with higher baseline scores to achieve further improvements. In contrast, larger models exhibit more significant gains in dense reasoning benchmarks: the integration of latent visual memory overcomes bottlenecks in visual reasoning by providing fine-grained visual evidence and semantic knowledge. Notably, this model-agnostic approach, independent of specific model ar-

Table 7. Results of various models with full training datasets and partial datasets (Visual CoT [42] and Mulberry [71]), and evaluated across four benchmarks.   

<table><tr><td rowspan="2">Method</td><td colspan="2">MM Vet</td><td colspan="2">MuirBench</td><td colspan="2">MV-Math</td><td colspan="2">MultiTrust</td></tr><tr><td>Full</td><td>Part</td><td>Full</td><td>Part</td><td>Full</td><td>Part</td><td>Full</td><td>Part</td></tr><tr><td>Vanilla [4]</td><td colspan="2">66.0</td><td colspan="2">57.4</td><td colspan="2">18.9</td><td colspan="2">64.8</td></tr><tr><td>SFT</td><td>67.5</td><td>65.8</td><td>58.7</td><td>57.2</td><td>22.8</td><td>21.2</td><td>67.0</td><td>65.4</td></tr><tr><td>Visual-RFT [35]</td><td>70.5</td><td>65.3</td><td>62.9</td><td>57.8</td><td>26.5</td><td>24.2</td><td>70.7</td><td>66.0</td></tr><tr><td>VLM-R1 [44]</td><td>73.0</td><td>67.7</td><td>63.8</td><td>59.0</td><td>34.6</td><td>32.1</td><td>69.9</td><td>66.1</td></tr><tr><td>Vision-R1 [26]</td><td>71.7</td><td>68.4</td><td>64.0</td><td>59.8</td><td>38.7</td><td>35.6</td><td>72.6</td><td>67.1</td></tr><tr><td>PAPO [66]</td><td>69.8</td><td>68.6</td><td>56.7</td><td>56.4</td><td>34.8</td><td>32.8</td><td>67.7</td><td>66.4</td></tr><tr><td>DeepEyes [87]</td><td>70.5</td><td>67.9</td><td>63.0</td><td>60.6</td><td>31.5</td><td>27.9</td><td>72.6</td><td>68.5</td></tr><tr><td>OpenThinkImg [49]</td><td>71.6</td><td>69.5</td><td>61.7</td><td>59.7</td><td>28.0</td><td>25.9</td><td>74.0</td><td>68.3</td></tr><tr><td>ICoT [16]</td><td>67.9</td><td>67.1</td><td>57.0</td><td>56.4</td><td>30.8</td><td>28.3</td><td>69.1</td><td>68.4</td></tr><tr><td>MINT-CoT [8]</td><td>69.5</td><td>68.4</td><td>58.9</td><td>57.8</td><td>39.2</td><td>36.4</td><td>71.4</td><td>70.2</td></tr><tr><td>Mirage [70]</td><td>71.8</td><td>70.2</td><td>59.0</td><td>57.2</td><td>35.4</td><td>33.1</td><td>66.1</td><td>64.0</td></tr><tr><td>VisMem (Ours)</td><td>75.1</td><td>72.9</td><td>69.8</td><td>66.4</td><td>41.4</td><td>39.1</td><td>77.0</td><td>74.9</td></tr></table>

Table 8. Results of various models on MMVet [76] with four-stage continual learning. Stage 0: MMVet [76]; Stage 1: BLINK [15], and MuirBench [57]; Stage 2: LogicVista [67], and Math-V [59]; Stage 3: MultiTrust [82], and MMVU [34].   

<table><tr><td>Method</td><td>Stage 0</td><td>Stage 1</td><td>Stage 2</td><td>Stage 3</td><td>Original</td></tr><tr><td>Vanilla [4]</td><td></td><td></td><td>66.0</td><td></td><td></td></tr><tr><td>SFT</td><td>71.4</td><td>70.6</td><td>62.3</td><td>60.1</td><td>67.5</td></tr><tr><td>Visual-RFT [35]</td><td>74.0</td><td>72.2</td><td>67.3</td><td>65.7</td><td>70.5</td></tr><tr><td>VLM-R1 [44]</td><td>77.8</td><td>74.1</td><td>66.4</td><td>66.9</td><td>73.0</td></tr><tr><td>Vision-R1 [26]</td><td>76.9</td><td>74.0</td><td>66.1</td><td>66.3</td><td>71.7</td></tr><tr><td>PAPO [66]</td><td>75.0</td><td>74.5</td><td>63.4</td><td>62.9</td><td>69.8</td></tr><tr><td>DeepEyes [87]</td><td>74.1</td><td>74.6</td><td>68.9</td><td>68.4</td><td>70.5</td></tr><tr><td>OpenThinkImg [49]</td><td>76.2</td><td>74.7</td><td>66.5</td><td>67.9</td><td>71.6</td></tr><tr><td>ICoT [16]</td><td>71.9</td><td>71.3</td><td>67.1</td><td>64.7</td><td>67.9</td></tr><tr><td>MINT-CoT [8]</td><td>72.4</td><td>71.8</td><td>65.8</td><td>66.2</td><td>69.5</td></tr><tr><td>Mirage [70]</td><td>79.1</td><td>77.8</td><td>68.7</td><td>67.0</td><td>71.8</td></tr><tr><td>VisMem (Ours)</td><td>78.6</td><td>78.9</td><td>71.3</td><td>72.1</td><td>75.1</td></tr></table>

chitectures or structures, bolsters the prospects for broad practical application.

# 9.5. Ablation Study

The vanilla model establishes a baseline characterized by the shortest inference time and highest speed across all benchmarks, yet exhibits the lowest performance. This confirms that latent vision memory is indispensable for enhancing task performance. For the random memory invocation variants, increasing the invocation probability $( 2 5 \% - 1 0 0 \% )$ results in longer inference time and reduced speed. Performance peaks at a $7 5 \%$ probability before declining, indi-

cating that excessive memory invocation impairs efficiency without yielding additional performance benefits. Ablation studies of the short-term and long-term memory components reveal task-specific advantages: the short-term memory component outperforms on MuirBench [57] and MultiTrust [82], while the long-term component demonstrates superior performance on MV-Math [62]. Notably, the complete VisMem framework achieves the highest performance across all benchmarks, validating the value of integrating dual-component vision memory for balanced and robust visual capacities.

![](images/b657e4eb1567135562d1f962e0436b4e03ffdd7e193b6a4e8a5a7073d3b8b3b1.jpg)  
Figure 7. Results of various models of the cross-domain generalization study. Models are only trained on Visual CoT [42] and Mulberry [71], and are evaluated on four benchmarks.

![](images/a1d1313016a66dc8b5787c72268876e31d7e122dfda296f6acbfe461ed370216.jpg)  
Figure 8. Results of four-stage continual learning on MMVet [76]. The model is sequentially trained on each training data combination (Stage 0 → Stage $1 $ Stage 2 → Stage 3). Stage 0 only includes MMVet [76] as training data, while Stage 1, 2, 3 add data targeting visual understanding [15, 57], reasoning [59, 67], and generation [34, 82].

Table 9. Ablations of latent vision memory invocation and dual vision memory formation. Following [81], “Random Invocation” denotes that the latent memory is inserted into the output sequence with a certain probability when outputting delimiter symbol tokens, and short or long latent memory is inserted with equal probability. When only utilizing short or long latent memory, we directly skip the formation of the specific memory if invocation tokens are predicted and continue the process of decoding.   

<table><tr><td rowspan="2">Ablation</td><td colspan="3">MMVet</td><td colspan="3">MuirBench</td><td colspan="3">MV-Math</td><td colspan="3">MultiTrust</td></tr><tr><td>Time</td><td>Speed</td><td>Perf.</td><td>Time</td><td>Speed</td><td>Perf.</td><td>Time</td><td>Speed</td><td>Perf.</td><td>Time</td><td>Speed</td><td>Perf.</td></tr><tr><td>Vanilla</td><td>0.76</td><td>1.32</td><td>66.0</td><td>3.79</td><td>0.26</td><td>57.4</td><td>5.47</td><td>0.18</td><td>18.9</td><td>3.62</td><td>0.28</td><td>64.8</td></tr><tr><td>Random Invocation (25%)</td><td>0.80</td><td>1.25</td><td>69.2</td><td>3.94</td><td>0.25</td><td>59.4</td><td>8.79</td><td>0.11</td><td>29.8</td><td>6.14</td><td>0.16</td><td>69.4</td></tr><tr><td>Random Invocation (50%)</td><td>0.83</td><td>1.20</td><td>71.9</td><td>4.12</td><td>0.24</td><td>63.2</td><td>11.68</td><td>0.09</td><td>26.1</td><td>8.62</td><td>0.12</td><td>68.5</td></tr><tr><td>Random Invocation (75%)</td><td>0.86</td><td>1.16</td><td>73.6</td><td>4.27</td><td>0.23</td><td>62.7</td><td>14.78</td><td>0.07</td><td>21.9</td><td>10.11</td><td>0.10</td><td>63.7</td></tr><tr><td>Full Invocation (100%)</td><td>0.88</td><td>1.14</td><td>73.4</td><td>4.43</td><td>0.23</td><td>56.0</td><td>17.87</td><td>0.06</td><td>17.5</td><td>13.43</td><td>0.07</td><td>62.6</td></tr><tr><td>Short-term Memory</td><td>0.79</td><td>1.27</td><td>71.5</td><td>4.00</td><td>0.25</td><td>65.6</td><td>7.64</td><td>0.12</td><td>29.6</td><td>4.96</td><td>0.20</td><td>73.6</td></tr><tr><td>Long-term Memory</td><td>0.81</td><td>1.23</td><td>69.4</td><td>3.95</td><td>0.25</td><td>60.2</td><td>7.61</td><td>0.12</td><td>36.1</td><td>4.80</td><td>0.21</td><td>69.8</td></tr><tr><td>Complete VisMem (Ours)</td><td>0.84</td><td>1.19</td><td>75.1</td><td>4.10</td><td>0.24</td><td>69.8</td><td>7.87</td><td>0.13</td><td>41.4</td><td>5.85</td><td>0.17</td><td>77.0</td></tr></table>

# 9.6. Analysis of Latent Vision Memory

We visualize the invocation ratio and relative invocation position, as presented in Fig. 5 and 9: the former illustrates benchmark-specific differences between the two memory components, while the latter depicts type-specific variations across the four benchmarks. In addition, as reported in Tab. 5 and 6, the short- and long-term latent visual mem-

ory components exhibit task-specific advantages for different visual sub-tasks. For instance, the short-term memory provides supplementary visual information to support enhanced visual understanding, such as counting, grounding, and visual retrieval. By contrast, the long-term memory encodes contextualized semantic knowledge, which strengthens complex visual reasoning. These results reveal that our

![](images/148930c372588163d5d80820872929eb61e4c90eeb19a951833b64ed10eed2a3.jpg)

![](images/439e7f23db620b4eaa4d23a903a1ec095b1d9d4856648e1dc48f5f709a7cdd60.jpg)  
Figure 9. Results of memory invocation ratio and relative position across four benchmarks. The former denotes the proportion of invoked samples to all samples, while the relative position denotes the position in the whole output sequence when the invocation occurred. We apply gaussian smoothing to the curves to highlight their main trends.

![](images/4b96d2a28445b796f59d62d3de572dd24bef821b0eef5a177b90931f4ea2c5fe.jpg)  
Figure 10. Results of sensitivity analysis on the sequence length of memory query $K$ , short- and long-term memory $N _ { s }$ and $N _ { l }$ .

Table 10. Results of different length of memory query $K$ .   

<table><tr><td>K</td><td>MMVet</td><td>MuirBench</td><td>MV-Math</td><td>MultiTrust</td></tr><tr><td>Vanilla</td><td>66.0</td><td>57.4</td><td>18.9</td><td>64.8</td></tr><tr><td>2</td><td>69.6</td><td>66.0</td><td>34.7</td><td>71.9</td></tr><tr><td>4</td><td>72.5</td><td>68.9</td><td>40.6</td><td>74.8</td></tr><tr><td>8</td><td>73.1</td><td>69.8</td><td>41.1</td><td>77.0</td></tr><tr><td>16</td><td>73.3</td><td>70.0</td><td>41.4</td><td>77.7</td></tr><tr><td>32</td><td>74.5</td><td>70.3</td><td>40.9</td><td>78.2</td></tr></table>

proposed VisMem dynamically adjusts invocation position and frequency according to task characteristics, thereby balancing efficiency and performance.

# 9.7. Sensitive Analysis of Sequence Lengths

We conduct an analysis on MMVet [76] focused on the lengths of three key sequences: the memory query $K$ , the short-term latent visual memory $N _ { s }$ , and the long-term latent visual memory $N _ { l }$ . It is observed that as the lengths of these three sequences increase from 2 to 32, model performance improves accordingly, but this is accompanied by increased computational costs.

Table 11. Results of different length of short latent vision memory $N _ { s }$ and the length of long latent vision memory $N _ { l }$ across four benchmarks.   

<table><tr><td>Ns</td><td>Nl</td><td>MMVet</td><td>MuirBench</td><td>MV-Math</td><td>MultiTrust</td></tr><tr><td colspan="2">Vanilla</td><td>66.0</td><td>57.4</td><td>18.9</td><td>64.8</td></tr><tr><td>2</td><td>-</td><td>67.2</td><td>63.7</td><td>28.2</td><td>69.3</td></tr><tr><td>4</td><td>-</td><td>69.9</td><td>64.6</td><td>31.5</td><td>71.4</td></tr><tr><td>8</td><td>-</td><td>71.8</td><td>65.2</td><td>33.8</td><td>73.4</td></tr><tr><td>16</td><td>-</td><td>71.1</td><td>67.8</td><td>34.0</td><td>73.3</td></tr><tr><td>32</td><td>-</td><td>73.0</td><td>69.1</td><td>34.4</td><td>72.7</td></tr><tr><td>-</td><td>2</td><td>66.4</td><td>60.3</td><td>29.3</td><td>71.0</td></tr><tr><td>-</td><td>4</td><td>68.4</td><td>61.8</td><td>32.4</td><td>72.8</td></tr><tr><td>-</td><td>8</td><td>69.7</td><td>63.0</td><td>33.5</td><td>74.2</td></tr><tr><td>-</td><td>16</td><td>70.3</td><td>63.4</td><td>34.8</td><td>74.9</td></tr><tr><td>-</td><td>32</td><td>70.8</td><td>63.1</td><td>35.5</td><td>75.3</td></tr><tr><td>8</td><td>16</td><td>75.1</td><td>69.8</td><td>41.1</td><td>77.0</td></tr></table>

# 9.8. Inference Efficiency

As presented in Tab. 12 and the bubble plots in Fig. 6, we compare the average inference time, average inference speed, and task performance across the four benchmarks. Our approach achieves an optimal performance-efficiency balance, with minimal additional time overhead. For instance, image-level paradigms exhibit nearly twice the inference time of the vanilla model, resulting in significant latency and substantial inference overhead. In contrast, our VisMem introduces only controllable computational latency increments, ranging from $8 . 2 \%$ to $4 3 . 8 \%$ relative to the vanilla model, which are on par with those of other direct training and token-level paradigms.

Table 12. Average inference time per sample (seconds), average inference speed (samples / seconds), and task performances across four benchmarks on various methods. Perf. indicates Performance.   

<table><tr><td rowspan="2">Method</td><td colspan="3">MMVet</td><td colspan="3">MuirBench</td><td colspan="3">MV-Math</td><td colspan="3">MultiTrust</td></tr><tr><td>Time</td><td>Speed</td><td>Perf.</td><td>Time</td><td>Speed</td><td>Perf.</td><td>Time</td><td>Speed</td><td>Perf.</td><td>Time</td><td>Speed</td><td>Perf.</td></tr><tr><td>Vanilla [4]</td><td>0.76</td><td>1.32</td><td>66.0</td><td>3.79</td><td>0.26</td><td>57.4</td><td>5.47</td><td>0.18</td><td>18.9</td><td>3.62</td><td>0.28</td><td>64.8</td></tr><tr><td>SFT</td><td>0.75</td><td>1.33</td><td>67.5</td><td>3.82</td><td>0.26</td><td>58.7</td><td>6.35</td><td>0.16</td><td>22.8</td><td>3.68</td><td>0.27</td><td>67.0</td></tr><tr><td>Visual-RFT [35]</td><td>0.76</td><td>1.32</td><td>70.5</td><td>3.81</td><td>0.26</td><td>62.9</td><td>5.66</td><td>0.17</td><td>26.5</td><td>3.65</td><td>0.27</td><td>70.7</td></tr><tr><td>VLM-R1 [44]</td><td>0.77</td><td>1.30</td><td>73.0</td><td>3.83</td><td>0.26</td><td>63.8</td><td>7.88</td><td>0.13</td><td>34.6</td><td>3.69</td><td>0.27</td><td>69.9</td></tr><tr><td>Vision-R1 [26]</td><td>0.77</td><td>1.30</td><td>71.7</td><td>3.83</td><td>0.26</td><td>64.0</td><td>8.42</td><td>0.12</td><td>38.7</td><td>3.71</td><td>0.27</td><td>72.6</td></tr><tr><td>PAPO [66]</td><td>0.76</td><td>1.32</td><td>69.8</td><td>3.81</td><td>0.26</td><td>56.7</td><td>6.74</td><td>0.15</td><td>34.8</td><td>3.68</td><td>0.27</td><td>67.7</td></tr><tr><td>Sketchpad [24]</td><td>2.39</td><td>0.42</td><td>64.5</td><td>8.90</td><td>0.11</td><td>52.8</td><td>9.10</td><td>0.11</td><td>24.6</td><td>5.47</td><td>0.18</td><td>66.2</td></tr><tr><td>GRIT [13]</td><td>0.80</td><td>1.25</td><td>67.8</td><td>4.07</td><td>0.25</td><td>51.0</td><td>8.45</td><td>0.12</td><td>22.4</td><td>4.06</td><td>0.25</td><td>67.3</td></tr><tr><td>PixelReasoner [48]</td><td>1.45</td><td>0.69</td><td>67.1</td><td>7.34</td><td>0.14</td><td>60.5</td><td>9.96</td><td>0.10</td><td>25.9</td><td>5.60</td><td>0.18</td><td>69.9</td></tr><tr><td>DeepEyes [87]</td><td>3.21</td><td>0.31</td><td>70.5</td><td>8.46</td><td>0.12</td><td>63.0</td><td>11.72</td><td>0.09</td><td>31.5</td><td>6.14</td><td>0.16</td><td>72.6</td></tr><tr><td>OpenThinkImg [49]</td><td>3.68</td><td>0.27</td><td>71.6</td><td>8.69</td><td>0.12</td><td>61.7</td><td>10.38</td><td>0.10</td><td>28.0</td><td>6.43</td><td>0.16</td><td>74.0</td></tr><tr><td>Scaffold [28]</td><td>0.83</td><td>1.20</td><td>67.0</td><td>4.35</td><td>0.23</td><td>52.9</td><td>7.01</td><td>0.14</td><td>21.0</td><td>3.88</td><td>0.26</td><td>68.5</td></tr><tr><td>ICoT [16]</td><td>0.97</td><td>1.15</td><td>67.9</td><td>4.57</td><td>0.22</td><td>57.0</td><td>8.94</td><td>0.11</td><td>30.8</td><td>4.20</td><td>0.24</td><td>69.1</td></tr><tr><td>MINT-CoT [8]</td><td>0.81</td><td>1.23</td><td>69.5</td><td>4.18</td><td>0.24</td><td>58.9</td><td>7.89</td><td>0.13</td><td>39.2</td><td>4.03</td><td>0.25</td><td>71.4</td></tr><tr><td>VPT [75]</td><td>2.98</td><td>0.34</td><td>70.8</td><td>9.63</td><td>0.10</td><td>63.5</td><td>9.59</td><td>0.10</td><td>34.7</td><td>5.79</td><td>0.17</td><td>64.7</td></tr><tr><td>Mirage [70]</td><td>0.86</td><td>1.16</td><td>71.8</td><td>4.02</td><td>0.25</td><td>59.0</td><td>7.71</td><td>0.13</td><td>35.4</td><td>3.82</td><td>0.26</td><td>66.1</td></tr><tr><td>VisMem (Ours)</td><td>0.84</td><td>1.19</td><td>75.1</td><td>4.10</td><td>0.24</td><td>69.8</td><td>7.87</td><td>0.13</td><td>41.4</td><td>3.85</td><td>0.26</td><td>77.0</td></tr></table>

![](images/402471391fcf06a0eabd09bc4b2b9b3b851b8ca1fba79e91095ec6985dda7a61.jpg)  
(a) Qwen2.5-VL-3B

![](images/bae786c575feaa03f339031bc8216efc0288a63fe60add7dbab366ac2683daaa.jpg)  
(b) Qwen2.5-VL-7B

![](images/4b674614130b4d19220325f3fc748d8fded1a7aebc461ab05066bcfd22f7bc95.jpg)  
(c) Qwen2.5-VL-32B

![](images/18d6ecaa6ecf417ea76051b3db209e449ef339cf55f89af0d0bae60ac1783f74.jpg)  
(d) LLaVA-OV-1.5-4B

![](images/f9e0a047a0546058d2e2d7ae8c173954b467ea69ceb5579cf1785d1132bcbe1e.jpg)  
(e) LLaVA-OV-1.5-8B

![](images/f8a05c34b955e0e3d7a222e29a8e64dfcf260c10c13e2563e7ab93ee1e35a6f1.jpg)  
(f) InternVL-3.5-4B

![](images/03aa97550fe4f005c36d81c8809f371a1ae572a140270c2faba0a46042abd4b8.jpg)  
(g) InternVL-3.5-8B

![](images/9a6456aa80db08adf4039e93653d3ec1266bd1f2f9a0c78a33c4f9db350d440d.jpg)  
(h) InternVL-3.5-14B

![](images/c337b61c1e1be4e3ea1e61b145bd727d034c19b17d3d1e1eb95fd91e57c401be.jpg)  
(i) InternVL-3.5-38B   
Figure 11. Results on different base models.