![](images/60c895f2c77785d56e6c93f2087a095a1cef4a1b3650bca4a55bddaceeb77ba3.jpg)

# OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs

NJU-LINK Team

Full author list in Contributions

# Abstract

Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBencha, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.

ahttps://github.com/NJU-LINK/OmniVideoBench

# 1 Introduction

Multimodal large language models (MLLMs) have recently made impressive progress in bridging vision, language, and audio (Yin et al., 2024; Song et al., 2025; Cheng et al., 2025). While early benchmarks primarily focused on image-text alignment or visual reasoning (Xu et al., 2025a; Chen et al., 2024a; Yue et al., 2024a), the integration of video and audio presents a quite different challenge: models must jointly process long temporal sequences, dynamic scene transitions, and complementary acoustic cues. Despite rapid advances, evaluation of MLLMs on audio-visual reasoning remains underdeveloped. Existing benchmarks (Li et al., 2024a; Hong et al., 2025) often (i) focus on short video clips that underrepresent long-term temporal dependencies, (ii) emphasize a single modality (e.g., vision) while treating audio as auxiliary or optional. As a result, current evaluations fail to capture the challenges inherent to comprehensive video understanding, where audio and vision must be integrated consistently and logically to support robust inference.

To address these limitations, we introduce OmniVideoBench, a high-quality benchmark designed for evaluating audio-visual reasoning abilities in MLLMs. Specifically, first, we collect 628 diverse videos spanning up to 30 minutes across 8 major categories and 68 subcategories, covering realistic contexts such as news, sports, documentaries, vlogs, and ego-centric recordings. Then, we construct 1,000 high-quality question-answer pairs based on these videos, and each pair is annotated with step-by-step reasoning chains as shown in Figure 1, where these reasoning steps explicitly indicate modality and evidence information. This design not only strengthens the reliability of the evaluation but also provides a unique signal for analyzing how models reason, rather than just the final answers.

Based on our OmniVideoBench, we conduct extensive evaluations of both closed-source and open-source MLLMs, and several insightful findings are as follows:

• OmniVideoBench poses significant challenges for Omni-Modal Language Models. Current MLLMs have not achieved a passing score $( \bar { < } 6 0 \% )$ on OmniVideoBench. The best-performing model, Gemini-2.0-Pro, only achieves an accuracy of $5 8 . 9 0 \%$ . Except for the newly proposed Qwen3-Omni, the performance of open-source models is close to random.

![](images/1d10c7206481a1982d85f30e625c998f537c6c36ea1ae3cd7c9886c67b11b6cf.jpg)  
Figure 1: Examples in OmniVideoBench (“V” presents vision and $^ { \prime \prime } { \mathrm { A } } ^ { \prime \prime }$ presents audio), and we present the atomic reasoning traces for these examples.

• Omni-understanding abilities on long videos have significant improvement room. Although some leading models (such as Gemini-2.5-pro) demonstrate relatively robust performance on long videos, other models (e.g., Gemini-2.0-Flash, Qwen3-Omni-30B-A3B) still struggle on long video understanding.   
• Performance varies a lot for videos with different audio signals. Gemini-2.5-Pro only achieves $3 8 . 4 6 \%$ accuracy on videos with music signal, while the results on sound and speech are $\dot { 5 } 7 . 7 2 \%$ and $6 1 . 6 6 \%$ , respectively.   
• Performance on different task types differs a lot. For example, Gemini-2.5-Pro achieves accuracy below $5 0 \%$ on the background and music understanding task, which requires low-semantic acoustic cues (e.g., musical style, tempo changes), and the accuracy results on the relationship reasoning and summarization tasks are more than $\mathrm { \bar { 8 0 \% } }$ .

# 2 OmniVideoBench

# 2.1 Overview

OmniVideoBench is a benchmark for evaluating the audio-visual collaborative reasoning of MLLMs. The main task in the evaluation requires a model to process a video, its audio, and associated text to generate a textual answer supported by explicit reasoning steps. This process assesses the model’s ability to synthesize information across modalities, from recognizing objects to comprehending complex scene dynamics and context. This section details the benchmark’s design principles, annotation protocols, and dataset statistics.

![](images/7e26d9fa1b5eb6e27f17d9bcb67574ed2db3987b787d37f811baaef53512deac.jpg)

![](images/f805975584871b5393527988ab2f314593cef78d7ceceba4f734efb29ce27ed8.jpg)

![](images/8d4703e454d55eafaab78a0cd15abf2bcdfed7bb4778c0f263e8db496834ab3e.jpg)  
Figure 2: The complete pipeline of data collection, annotation, and refinement, where filtering and refinement serve as two key processes for quality assurance.

# 2.2 Video Collection

OmniVideoBench is composed of real-world videos sourced from YouTube1 and Bilibili2. These videos feature rich audiovisual content. Therefore, a comprehensive understanding necessitates the accurate processing and integration of both audio and visual modalities for reasoning.

Regarding video richness, we primarily focus on two dimensions: type and duration. For type diversity, we categorize videos into eight broad classes: Vlog, News, Cartoon, Sports, Documentary, TV, Ego, and Others. Each class is further subdivided into nearly seventy fine-grained subcategories, which facilitates video retrieval and ensures broad coverage. Video categories are unevenly distributed. News and documentary videos have dense audio that nearly covers visual content, making them unsuitable for audio-visual reasoning tasks; thus, we manually controlled the video type distribution. For duration diversity, we restrict video lengths to the range of several seconds to 30 minutes, so as to evaluate reasoning across varying temporal scales.

Building upon this foundation, we established a set of rigorous video collection criteria that not only ensure the quality of the videos themselves, like resolution, but also guarantee the richness and diversity of their audio and visual content. To further avoid data overlap with existing training sets (e.g., popular TV shows), we restrict the selection to recent publications. The detailed collection principles are provided in Appendix B.

# 2.3 Data Annotation

After collecting high-quality videos, we carried out manual annotation. Compared with automated annotation, automated methods cap the evaluation ceiling by the capabilities of the annotating model, whereas manual annotation produces questions that are closer to real-world needs.

In Figure 2, we first designed multiple-choice questions consisting of the question stem, the correct answer, and several distractors, to facilitate convenient evaluation of model performance. At this stage, we obtained approximately 2,500 QA pairs. We categorize the tasks into 13 types: Fine-grained Perception, Spatial Reasoning, Attribute Comparison, Background & Music Understanding, Counting, Temporal Understanding, Summarization, Sentiment Analysis, Causal Reasoning, Relationship Reasoning, Reference Reasoning, Ego Reasoning, and Hypothetical Reasoning. In this design, each question requires reliance on audio-visual reasoning, and the answer must be both correct and unique with no alternative plausible interpretations in the video. Moreover, we require that questions should not depend on video resolution or frame rate. Cases where the target object is extremely small, blurred, and barely recognizable to the human eye, or where the relevant event occurs only within an instant, are excluded. In addition, we established the following rules to minimize the interference caused by extraneous textual information.

• Questions should avoid redundant information. We minimize unnecessary details in the question text, such as the gender, clothing, or exact speech of characters, as long as doing so does not affect the correctness or uniqueness of the answer. This serves two purposes: reducing textual cues that the model could exploit and increasing question difficulty to better test its audio-visual understanding.   
• The length of answers is capped. To prevent the answer text itself from providing excessive cues to

Table 1: Dataset statistics divided into video-level and annotation-level information.   

<table><tr><td colspan="2">Video Statistics</td><td colspan="2">Annotation Statistics</td></tr><tr><td>#Major Categories</td><td>8</td><td>#Task Types</td><td>13</td></tr><tr><td>#Subcategories</td><td>68</td><td>Avg. Question Len.</td><td>14.68 words</td></tr><tr><td>Avg. Duration</td><td>384.24 s</td><td>Avg. Answer Len.</td><td>4.92 words</td></tr><tr><td>Min. Resolution</td><td>480p</td><td>Avg. Reasoning Steps</td><td>5.68</td></tr><tr><td>Max. Resolution</td><td>1080p</td><td>Audio Types (Sp:So:Mu)</td><td>762:147:91</td></tr></table>

the model, which could reduce the extent to which the evaluation reflects its understanding and reasoning over audio and visual modalities, we impose a limit on answer length. This constraint ensures that the results more faithfully capture the model’s multimodal comprehension and reasoning capabilities.

• The format of options must be consistent. Here, “format” refers to aspects such as length, tone, style, and variation patterns. If these features are inconsistent, they may provide the model with unintended cues for reasoning. For instance, when three options are considerably longer than the remaining one, when three options adopt a casual tone while the other is markedly formal, such discrepancies undermine the assumption that each option should have an equal probability of being chosen, thereby compromising the fairness of the evaluation.   
• Negative options must be relevant to the question. We require that all distractors appear in the video and maintain relevance to the question. Without this constraint, the model could easily eliminate distractors, greatly reducing the need for reasoning.   
• Options should maintain a consistent semantic distance. We formalize semantic distance as the number of differing semantic units between options. Let an option $o _ { i }$ be represented as a set of semantic units $S _ { i }$ . The semantic distance between two options $o _ { i }$ and $o _ { j }$ is defined as:

$$
d \left(o _ {i}, o _ {j}\right) = \left| S _ {i} \triangle S _ {j} \right| \tag {1}
$$

where $\triangle$ denotes the symmetric difference, capturing the distinct semantic units between two options. To prevent models from exploiting unbalanced textual cues rather than performing genuine audio-visual reasoning, we require that all distractors have consistent distances from one another and from the correct option.

# 2.4 Quality Assurance

We employed an advanced MLLM (i.e., Gemini 2.0 Flash), with strong audiovisual perception and comprehension capabilities, as well as long-context processing ability, to filter out questions that could be resolved using only a single modality. If the model successfully selected the correct answer with a plausible explanation while relying solely on unimodal information, the corresponding question was removed. After this filtering stage, approximately 1,500 questions were retained.

Subsequently, we employed a large language model, DeepSeek-V3.1 (Liu et al., 2024a), with strong reasoning capabilities to filter out questions that could be answered solely based on textual information. Such cases primarily fall into two categories: first, questions that involve classical, well-known, or universally shared knowledge or objects, which can be answered without reference to the video content; and second, questions where the phrasing of the question, options, or answers provides unintended textual cues. For the former, we directly discarded the questions. For the latter, our annotators reviewed the reasoning process generated by the model and revised the textual formulations to eliminate such biases. After this stage of refinement, 1103 questions were retained.

Another group of annotators conducts the final refinement stage, thoroughly reviewing all questions to identify and remove those with incorrect, non-unique, or mismatched answers. After this validation, annotators enriched each question with step-by-step reasoning chains, where each step consists of three elements: modality, evidence, and inference. The modality specifies whether the step relies on audio or visual information; the evidence denotes the specific information extracted from the video; and the inference describes the reasoning derived from that information. We required each step to be atomic, meaning that it should involve only one modality and capture a minimal unit of evidence, such as a spoken sentence, an action, or the appearance of a character. This design ensures that the reasoning process is both detailed and comprehensive. Through this process, we obtained 1000 high-quality QA pairs with explicit step-by-step reasoning chains, forming a robust dataset for multimodal audio-visual reasoning.

![](images/c8ee5737b52480b6e4ed5d6c8ecf9cf4c3ffefb40bd44617b3fc4a1721758e9a.jpg)  
(a) Video Categories

![](images/7934a7a19bd15c6dbef0ce28aa3b1d2e2b270fed54b92b93fbcecd288a03cb31.jpg)  
(b) QA Task Distribution

![](images/9e8d10cdb4cf278e54996b98415fd0e40f8d13c6bcc5b0ea1cdc09df65d1188c.jpg)  
(d) Audio Type Distribution   
Figure 3: (a) OmniVideoBench covers 8 major categories and 68 subcategories. (b) OmniVideoBench comprises 13 task types. The above part shows the video duration distribution across different tasks, while the durations are categorized into four groups: “Short” for less than 1 minute, “Medium” for 1–5 minutes, $\bf { \mu } ^ { \prime \prime } ( L o n g ^ { \prime \prime }$ for 5–10 minutes, and “Ultralong” for more than 10 minutes. The lower part illustrates the distribution of three types of audio (i.e., Speech, Sound and Music). (c) Distribution of video durations across four time intervals. (d) Distribution of three audio types.

Table 2: Comparisons between different benchmarks and datasets. V, I, A for modality represent video, image and audio. Qwen2.5-Omni represents the performance of Qwen2.5-Omni-7B on these benchmarks. Multiple Domains signifies whether the video includes diverse domains. R and S in Video Type denote real-world and synthetic data. Sp, So, and Mu represent Speech, Sound, and Music for Audio Type, respectively. Video Duration represents the duration in seconds. MC, CLS for Answer Type indicate Multiple Choice and Classification from fixed vocabulary, respectively.   

<table><tr><td>Benchmark</td><td>Modality</td><td>Qwen2.5-Omni</td><td>Multiple Domains</td><td>Video Type</td><td>Audio Type</td><td>Video Duration</td><td>Answer Type</td></tr><tr><td>AVQA (Yang et al., 2022)</td><td>V+A</td><td>/</td><td>X</td><td>R</td><td>So</td><td>10</td><td>MC</td></tr><tr><td>Music-AVQA (Li et al., 2022)</td><td>V+A</td><td>/</td><td>X</td><td>R+S</td><td>Mu</td><td>60</td><td>CLS</td></tr><tr><td>AVTRUSTBENCH (Chowdhury et al., 2025)</td><td>V+A</td><td>/</td><td>✓</td><td>R+S</td><td>Sp+So+Mu</td><td>10\60</td><td>MC</td></tr><tr><td>MMAU (Sakshi et al., 2024)</td><td>A</td><td>71.0</td><td>✓</td><td>/</td><td>Sp+So+Mu</td><td>/</td><td>MC</td></tr><tr><td>DAVE (Radevski et al., 2025)</td><td>V+A</td><td>31.0</td><td>✓</td><td>R+S</td><td>So</td><td>≤60</td><td>MC</td></tr><tr><td>AV-Odyssey (Gong et al., 2024)</td><td>I+A</td><td>/</td><td>✓</td><td>R</td><td>Sp+So+Mu</td><td>/</td><td>MC</td></tr><tr><td>AVHbench (Judgement) (Sung-Bin et al., 2024)</td><td>V+A</td><td>74.7</td><td>✓</td><td>R+S</td><td>So</td><td>10</td><td>CLS</td></tr><tr><td>OmniBench (Li et al., 2024b)</td><td>I+A</td><td>56.1</td><td>✓</td><td>R</td><td>Sp+So+Mu</td><td>/</td><td>MC</td></tr><tr><td>Daily-Omi (Zhou et al., 2025)</td><td>V+A</td><td>47.5</td><td>X</td><td>R</td><td>Sp+So+Mu</td><td>30-60</td><td>MC</td></tr><tr><td>WorldSense (Hong et al., 2025)</td><td>V+A</td><td>48.3</td><td>✓</td><td>R</td><td>Sp+So+Mu</td><td>15-656</td><td>MC</td></tr><tr><td>OmniVideoBench (Ours)</td><td>V+A</td><td>29.3</td><td>✓</td><td>R</td><td>Sp+So+Mu</td><td>4-1955</td><td>MC</td></tr></table>

# 2.5 Dataset Statistics

As shown in Table 1, our OmniVideoBench dataset consists of 628 real-world videos with audio tracks, spanning 8 major categories and 68 subcategories. The videos are of high quality and diverse in content, with an average duration of 384.6 seconds, an average resolution of $4 8 0 \mathrm { p } .$ , about 2k ASRtranscribed tokens per video, and roughly three speakers per video. On the annotation side, OmniVideoBench contains 1000 audio–visual reasoning QA pairs across 13 task types, with an average question length of 14.68 words and an average answer length of 4.92 words. Each QA pair is annotated with step-by-step reasoning chains averaging 5.68 steps. The reasoning process covers both modalities, with $5 4 \%$ of steps grounded in vision and $4 6 \%$ in audio. There are 762, 147, 91 QA pairs related to Speech, Sound and Music, respectively, highlighting the complementarity of modalities in multi-step reasoning. Moreover, we provide more detailed statistics in Figure 3.

![](images/d8add9fa68065b6c5726a5f433ac29789249f30d10cd881a455cea1d1f860505.jpg)  
Figure 4: Performance comparison of selected models on OmniVideoBench and Daily-Omni. “Red line” denotes random guessing.

Table 3: Results of different models. The table reports accuracy on videos across three audio types and four duration ranges. Boldface highlights the best performance within each column.   

<table><tr><td rowspan="2">Models</td><td colspan="3">Audio Type</td><td colspan="4">Video Duration</td><td rowspan="2">Avg.</td></tr><tr><td>Music</td><td>Sound</td><td>Speech</td><td>(0,1] min</td><td>(1,5] min</td><td>(5,10] min</td><td>(10,30] min</td></tr><tr><td colspan="9">Omni-Modal Language Models (With Visual and Audio)</td></tr><tr><td>Gemini-3.0-Pro</td><td>52.81</td><td>55.17</td><td>64.13</td><td>62.42</td><td>66.18</td><td>57.02</td><td>59.76</td><td>61.79</td></tr><tr><td>Gemini-2.5-Pro</td><td>38.46</td><td>57.72</td><td>61.66</td><td>57.83</td><td>64.43</td><td>55.02</td><td>55.94</td><td>58.90</td></tr><tr><td>Gemini-3.0-Flash</td><td>49.45</td><td>50.34</td><td>56.69</td><td>58.43</td><td>55.10</td><td>55.90</td><td>52.29</td><td>55.10</td></tr><tr><td>Gemini-2.5-Flash</td><td>39.56</td><td>57.04</td><td>53.17</td><td>55.42</td><td>55.10</td><td>47.37</td><td>52.11</td><td>52.40</td></tr><tr><td>Gemini-2.0-Flash</td><td>29.67</td><td>40.27</td><td>43.21</td><td>49.40</td><td>43.15</td><td>41.05</td><td>34.87</td><td>41.50</td></tr><tr><td>Qwen3-Omni-30B-A3B</td><td>37.36</td><td>34.67</td><td>39.26</td><td>45.78</td><td>37.03</td><td>38.86</td><td>35.11</td><td>38.40</td></tr><tr><td>OmniVinci-9B</td><td>30.77</td><td>32.67</td><td>32.15</td><td>38.55</td><td>34.11</td><td>30.13</td><td>27.10</td><td>32.10</td></tr><tr><td>Baichuan-Omni-1.5</td><td>24.18</td><td>31.33</td><td>31.36</td><td>28.92</td><td>31.78</td><td>28.38</td><td>32.44</td><td>30.70</td></tr><tr><td>HumanOmni-7B</td><td>20.87</td><td>31.08</td><td>31.61</td><td>36.57</td><td>29.36</td><td>29.60</td><td>29.25</td><td>30.50</td></tr><tr><td>VITA-1.5-7B</td><td>25.27</td><td>28.57</td><td>31.49</td><td>31.33</td><td>27.41</td><td>30.57</td><td>33.97</td><td>30.50</td></tr><tr><td>MiniCPM-o</td><td>27.47</td><td>28.57</td><td>30.24</td><td>31.43</td><td>28.49</td><td>34.53</td><td>26.15</td><td>29.70</td></tr><tr><td>Qwen2.5-Omni-7B</td><td>23.07</td><td>25.33</td><td>30.70</td><td>41.57</td><td>27.41</td><td>25.33</td><td>26.72</td><td>29.30</td></tr><tr><td>VideoLLaMA2-7B</td><td>26.37</td><td>30.67</td><td>29.25</td><td>32.00</td><td>28.20</td><td>29.60</td><td>28.29</td><td>29.20</td></tr><tr><td colspan="9">Omni-Modal Language Models (Visual Only)</td></tr><tr><td>Gemini-2.0-Flash</td><td>25.27</td><td>36.67</td><td>30.99</td><td>33.73</td><td>35.86</td><td>32.75</td><td>22.48</td><td>31.30</td></tr><tr><td>Qwen2.5-Omni-7B</td><td>27.47</td><td>26.67</td><td>26.22</td><td>28.31</td><td>27.11</td><td>24.45</td><td>25.95</td><td>26.40</td></tr><tr><td colspan="9">Visual Language Models (Visual Only)</td></tr><tr><td>Qwen2.5-VL-32B</td><td>32.97</td><td>32.00</td><td>31.49</td><td>38.55</td><td>31.20</td><td>29.26</td><td>30.53</td><td>31.80</td></tr><tr><td>Qwen2.5-VL-7B</td><td>29.67</td><td>31.33</td><td>29.51</td><td>25.90</td><td>30.03</td><td>31.88</td><td>30.15</td><td>29.80</td></tr><tr><td>Qwen2.5-VL-72B</td><td>26.37</td><td>29.33</td><td>29.91</td><td>33.13</td><td>30.03</td><td>31.88</td><td>24.43</td><td>29.50</td></tr><tr><td colspan="9">Baseline LLMs</td></tr><tr><td>DeepSeek-V3.1</td><td>28.57</td><td>26.17</td><td>27.28</td><td>30.91</td><td>27.57</td><td>25.00</td><td>26.44</td><td>27.60</td></tr></table>

# 2.6 Dataset Comparison

In Table 2, we compare OmniVideoBench with representative audio-video benchmarks. While AV-Odyssey (Gong et al., 2024) and OmniBench (Li et al., 2024b) operate on single images, OmniVideoBench targets substantially more challenging videos with durations ranging from a few seconds to 30 minutes. Recent benchmarks such as AVTrustBench (Chowdhury et al., 2025), DAVE (Radevski et al., 2025), and MMAU (Sakshi et al., 2024) begin emphasizing audio–video coordination, but typically focus on specific capabilities or short clips. AVHBench (Sung-Bin et al., 2024) also evaluates audiovisual consistency, yet its tasks remain centered on shorter videos and hallucination detection. OmniVideoBench, by contrast, expands the scope to diverse video types, broader temporal spans, and fine-grained cross-modal reasoning, capturing richer dependencies between audio and vision. Compared to Daily-Omni (Zhou et al., 2025) and WorldSense (Hong et al., 2025), which also utilize multi-domain videos, OmniVideoBench places greater emphasis on explicit audiovisual collaboration. For instance, disabling audio causes Gemini-2.0-Flash’s performance to plummet to random levels, indicating that visual-only cues are insufficient. Furthermore, Figure 4 shows that widely used models such as Qwen2.5-Omni-7B perform closer to random guessing on our benchmark, indicating that OmniVideoBench presents significantly greater challenges than existing multimodal datasets.

# 3 Experiments

# 3.1 Baseline Models

We evaluate open-source MLLMs (i.e., Qwen3-Omni series (Xu et al., 2025b), Qwen2.5-Omni series (Xu et al., 2025c), Baichuan-Omni-1.5 (Li et al., 2025), HumanOmni (Zhao et al., 2025), MiniCPM-o (Yao et al., 2024), VideoLLaMA2 (Cheng et al., 2024)), VITA-1.5-7B (Fu et al., 2025), OmniVinci-9B (Ye et al., 2025) and various closed-source MLLMs (i.e., Gemini-2.5-Pro, Gemini-2.5-Flash (Comanici et al., 2025), and Gemini-2.0-Flash). We also evaluate the Qwen2.5-VL series (Bai et al., 2025) and DeepSeek-V3.1 (Liu et al., 2024a).

![](images/b39b230e795755908eb1bddc5ecec60cb4b2049757a2b3d2ce6ea2b81325b839.jpg)  
Figure 5: Performance Comparison of some Open-Source and Closed-Source Omni Models on 13 Tasks in OmniVideoBench. Here, “Attr”: Attribute Comparison, “Bac&Mu”: Background and Music Understanding, “Caus”: Cause and Effect Reasoning, “Coun”: Counting, “Ego”: Ego Reasoning, “Fine”: Fine-grained Perception, “Hypo”: Hypothetical Reasoning, “Ref”: Referential Reasoning, “Rela”: Relationship Reasoning, “Senti”: Sentiment Analysis, “Spati”: Spatial Reasoning, “Summ”: Summarization, “Tempo”: Temporal Sequencing Understanding.

# 3.2 Main Results

In Table 3, we present evaluation results on OmniVideoBench and have the following observations:

• Open-source models still lag significantly behind closed-source models. Gemini-2.5-Pro achieves the best performance across most tasks. This underscores the urgent need for current open-source models to improve in multiple areas, including fine-grained perception, cross-modal reasoning, and speech awareness.   
• MLLMs show a performance degradation when dealing with music-related audio. We observe that models exhibit lower accuracy in responding to music-dominated videos compared to those containing human voices or ambient sounds, a phenomenon particularly pronounced in opensource models. Unlike human voices conveying explicit semantic content or ambient sounds often corresponding to specific visual events, music primarily encodes abstract emotional and atmospheric information. Current MLLMs demonstrate limited capability to translate such implicit cues into effective reasoning, indicating that cross-modal alignment for emotional and atmospheric understanding remains an urgent challenge to be addressed.   
• Current MLLMs still have room for improvement in long videos. Although some leading models like Gemini-2.5-Pro demonstrate relatively robust performance on long videos, most MLLMs (e.g., Gemini-2.0-Flash, Qwen3-Omni) still struggle in long videos, which highlights the widespread challenge in understanding long videos.

# 3.3 Further Analysis

Performance of Models on Tasks across Different Types. Figure 5 presents a fine-grained comparison of model accuracy on the 13 reasoning categories in OmniVideoBench. Several consistent patterns emerge. (1). Closed-source MLLMs demonstrate superior performance across nearly all task types. Gemini-2.5-Pro achieves the highest accuracy on 11 out of 13 tasks, demonstrating particularly strong performance in Relationship Reasoning, Spatial Reasoning, Referential Reasoning, and Cause and Effect Reasoning. These tasks require long-term sequence integration and multi-step cross-modal reasoning, highlighting Gemini’s strengths in long-context modeling and multimodal fusion. (2). MLLMs’ understanding of audio

![](images/4bef3fa92be84acd003d6496a3e9eb0815202b524d195174e2cdc0b9c6271250.jpg)  
(a) Accuracy rates of selected MLLMs under different inputs.

![](images/570bc5a90b435f66067a111f2347a01012ea1d006a9a1cf38729eef7fdc4970a.jpg)  
(b) Accuracy of Gemini-2.0-Flash on videos with different audio types.   
Figure 6: Accuracy comparison of MLLMs with and without ASR transcripts on OmniVideoBench.

remains limited to relatively superficial surface-level information. Whether open-source or closed-source models, Background and Music Understanding remains the most challenging task, with even Gemini-2.5-Pro achieving accuracy below $5 0 \%$ . This is probably because such tasks require linking low-semantic acoustic cues (e.g., musical style, tempo changes) with high-level reasoning, while current models struggle to master the capability. In contrast, Relationship Reasoning and Summarization are relatively easier. This may be because they rely more on recognizing language within audio and visual observation capabilities, and less on cross-modal abstraction abilities.

Effect of ASR Transcripts for Visual Only MLLMs. To further investigate the role of audio information in MLLMs’ reasoning performance, we evaluate several models using both the automatic speech recognition (ASR) transcripts generated by the Voxtral-Mini-3B model (Liu et al., 2025a) and silent video frames as inputs. The results are shown in Figure 6. The observations are as follows: (1). Open-source models demonstrate weaker integration capabilities for audio information compared to their understanding of textual information. In Figure 6a, all tested models demonstrate significantly improved accuracy after extracting ASR text information compared to receiving only visual inputs. However, the Qwen2.5-Omni-7B model, which processes both visual and audio inputs simultaneously, performed even worse than the Qwen2.5-VL-7B model with equivalent parameters. This highlights a common challenge faced by most open-source Omni-Modal Language Models: insufficient cross-modal reasoning capabilities for audio-visual information. (2). In cross-modal video reasoning, audio comprehension capabilities remain irreplaceable by ASR. In Figure 6b, although ASR can help MLLMs achieve decent performance on certain tasks requiring speech recognition capabilities, its effectiveness is extremely limited for tasks demanding deeper and more abstract audio comprehension such as the videos whose audio type is Music or Sound.

Effect of Different Numbers of Frames. We conduct experiments on Qwen2.5-Omni-7B and Qwen3-Omni-30B-A3B with total frame counts fixed at 32, 64, 128, and 256, respectively, and observe that both models benefit from more frequent time sampling. In Figure 7a, as the total frame counts increase, accuracy steadily improves, likely because richer temporal coverage provides more complete motion cues and reduces the risk of missing key events. As shown in Figure 7b,

this improvement becomes more pronounced for longer videos. The consistent gains across different video durations further indicate that dense frame sampling not only captures fine-grained visual dynamics but also strengthens cross-modal alignment. This highlights the importance of dense temporal information and long-context processing for achieving robust audiovisual reasoning.

Table 4: Comparison of performance on Open-ended Question Answering (QA) and Multiple- Choice Questions (MCQ) across various models.   

<table><tr><td>Models</td><td>Open-ended QA</td><td>MCQ</td></tr><tr><td>Gemini-2.0-Flash</td><td>27.06</td><td>41.50</td></tr><tr><td>Qwen2.5-Omni-7B</td><td>17.25</td><td>29.30</td></tr></table>

Open-ended QA vs. MCQ. To investigate whether the multiple-choice question (MCQ) format overstates model performance, we additionally evaluated several representative models on open-ended question-answering (QA) tasks, where no predefined answer options are provided. In this setting, models must directly generate textual responses, eliminating both the possibility of random guessing and any lexical cues potentially present in candidate options. In Table 4, the accuracy of all models drops significantly compared to their performance on multiple-choice questions. For instance, the Gemini-2.5-Pro, which leads in MCQ benchmarks, experiences a relative accuracy decline exceeding 14 percent in open-ended

![](images/5139304f33383b91f5dd63d84734f7fdc65ca8f78c5168ed3c2bbc955fbcb6e4.jpg)

![](images/68f2d67cbc0ddf27a840e76e602cde94ba76cd4d1506af8fb439f88a997cdfa8.jpg)  
(a) Performance of Qwen2.5-Omni-7B and Qwen3-Omni-30B-A3B at different numbers of frames.

Figure 7: Performance of selected models when inputting videos with different numbers of frames.   
![](images/5914c0fb54141e154bac72d266a4d617cbdff90ae5c11f7158623ee28a4bdb01.jpg)  
(b) Accuracy of Qwen3-Omni-30B-A3B on questions with videos of varying durations across different numbers of frames.

scenarios, while open-source models exhibit even steeper drops.

# 4 Related Works

Omni-Understanding MLLMs. The development of MLLMs (Chen et al., 2022; Awadalla et al., 2023; Liu et al., 2023; Peng et al., 2025; Yang et al., 2023) began with a foundational focus on integrating the two primary modalities of vision and language. A recent paradigm shift aims to develop Omni-modal MLLMs capable of processing and generating information across an arbitrary combination of modalities (“Any-to-Any"). This approach positions the LLM as a central cognitive engine, unifying diverse data types like audio, video, and text within its semantic space (Liu et al., 2024b; Yuan et al., 2025). This has driven a move from integrating pre-trained unimodal components towards developing “natively multimodal" architectures trained from the ground up, as exemplified by models like GPT-4o (Hurst et al., 2024). This ambition is showcased by state-of-the-art models (Xu et al., 2025c; Zhao et al., 2025; Li et al., 2024c; 2025; Yao et al., 2024; Sun et al., 2025; Liu et al., 2025b; Wu et al., 2025a), which pioneer end-to-end streaming capabilities for simultaneously processing video and audio to generate text and speech. At the forefront of this paradigm, proprietary models like Gemini series (Team, 2024; Comanici et al., 2025) demonstrate pinnacle performance, powered by a natively multimodal design and a massive context window that together unlock superior understanding of complex, interwoven data streams.

MLLM Benchmarks. The landscape of MLLM evaluation has matured significantly, evolving from foundational perception benchmarks (Liu et al., 2024c; Li et al., 2024a; Yu et al., 2024a;b; Chen et al., 2024b; Jiang et al., 2025) to more sophisticated frameworks (He et al., 2025; Du et al., 2025; Wu et al., 2025b). Recent efforts probe deeper cognitive abilities, with MLLM-Bench (Ge et al., 2025) assessing a hierarchy of cognitive skills. MMMU (Yue et al., 2023) and MMMU-Pro (Yue et al., 2024b) challenging models with expert-level, multi-disciplinary reasoning under stricter protocols like vision-only inputs. Simultaneously, evaluation has specialized into high-stakes domains such as finance (Gan et al., 2024) and medicine (Chen et al., 2024c). For video, some benchmarks (Wang et al., 2019; Li et al., 2021; 2023; Fang et al., 2024; Wu et al., 2024) now focus on the critical challenge of long-context temporal understanding (Liu et al., 2025c), revealing key limitations in current models.

# 5 Conclusion

We presented OmniVideoBench, a large-scale benchmark for evaluating audio-visual collaborative reasoning in MLLMs, with diverse videos, carefully verified QA pairs, and explicit reasoning annotations. Experiments show that both open- and closed-source models still struggle with modality complementarity, long-form temporal reasoning, and music understanding, underscoring a large gap from human-level performance. We hope this benchmark will drive future research toward more robust and generalizable multimodal reasoning systems.

# 6 Contributions

Our team members contribute to the development of OmniVideoBench from the following perspectives:

• Data Annotation Management

• Model Evaluation

• Data Annotation

• Result Analysis

• Data Quality Inspection

• Paper Writing

# Co-First Authors

• Caorui Li, Southeast University, caoruili@seu.edu.cn   
• Yu Chen, Southeast University, yu_chen@seu.edu.cn   
• Yiyan Ji, Nanjing University, jiyiiiyyy@gmail.com

# Core Contributors

• Jin Xu, Alibaba Group   
• Zhenyu Cui, Southeast University   
• Shihao Li, Nanjing University   
• Yuanxing Zhang, Kuaishou Technology   
• Wentao Wang, Nanjing University   
• Zhenghao Song, M-A-P   
• Dingling Zhang, Nanjing University   
• Ying He, University of Science and Technology Beijing   
• Haoxiang Liu, University of Science and Technology Beijing   
• Yuxuan Wang, Alibaba Group   
• Qiufeng Wang, Southeast University

# Contributors

• Jiafu Tang, Nanjing University   
• Zhenhe Wu, M-A-P   
• Jiehui Luo, Central Conservatory of Music   
• Zhiyu Pan, Nanjing University   
• Weihao Xie, Huazhong University of Science and Technology   
• Chenchen Zhang, M-A-P   
• Zhaohui Wang, Nanjing University   
• Jiayi Tian, Alibaba Group   
• Yanghai Wang, Nanjing University   
• Zhe Cao, Nanjing University   
• Minxin Dai, Nanjing University   
• Ke Wang, BUPT   
• Runzhe Wen, Nanjing University   
• Yinghao Ma, Queen Mary University of London   
• Yaning Pan, Fudan University   
• Sungkyun Chang, Queen Mary University of London   
• Termeh Taheri, Queen Mary University of London   
• Haiwen Xia, Peking University   
• Christos Plachouras, Queen Mary University of London   
• Emmanouil Benetos, Queen Mary University of London   
• Yizhi Li, University of Manchester

• Ge Zhang, M-A-P   
• Jian Yang, M-A-P   
• Tianhao Peng, M-A-P   
• Zili Wang, M-A-P   
• Minghao Liu, 2077AI   
• Junran Peng, University of Science and Technology Beijing   
• Zhaoxiang Zhang, Chinese Academy of Sciences

# Corresponding Author

• Jiaheng Liu, Nanjing University, liujiaheng@nju.edu.cn

# References

Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models. National Science Review, 11(12):nwae403, November 2024. ISSN 2095-5138. doi: 10.1093/nsr/nwae403. URL https://doi.org/10.1093/nsr/nwae403. _eprint: https://academic.oup.com/nsr/article-pdf/11/12/nwae403/61201557/nwae403.pdf.   
Shezheng Song, Xiaopeng Li, Shasha Li, Shan Zhao, Jie Yu, Jun Ma, Xiaoguang Mao, Weimin Zhang, and Meng Wang. How to bridge the gap between modalities: Survey on multimodal large language model. IEEE Transactions on Knowledge and Data Engineering, 37(9):5311–5329, 2025. doi: 10.1109/TKDE.2025. 3527978.   
Junhao Cheng, Yuying Ge, Teng Wang, Yixiao Ge, Jing Liao, and Ying Shan. Video-holmes: Can mllm think like holmes for complex video reasoning? ArXiv, abs/2505.21374, 2025.   
Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, et al. Visulogic: A benchmark for evaluating visual reasoning in multi-modal large language models. arXiv preprint arXiv:2504.15279, 2025a.   
Qiguang Chen, Libo Qin, Jin Zhang, Zhi Chen, Xiao Xu, and Wanxiang Che. M 3 cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought. arXiv preprint arXiv:2405.16473, 2024a.   
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9556–9567, 2024a.   
Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seedbench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13299–13308, 2024a.   
Jack Hong, Shilin Yan, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Weidi Xie. Worldsense: Evaluating real-world omnimodal understanding for multimodal llms. ArXiv, abs/2502.04326, 2025.   
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024a.   
Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. Avqa: A dataset for audio-visual question answering on videos. In Proceedings of the 30th ACM International Conference on Multimedia, 2022.   
Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, Ji rong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19086–19096, 2022.   
Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, and Dinesh Manocha. Avtrustbench: Assessing and enhancing reliability and robustness in audio-visual llms. arXiv preprint arXiv:2501.02135, 2025.   
S Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. Mmau: A massive multi-task audio understanding and reasoning benchmark. arXiv preprint arXiv:2410.19168, 2024.   
Gorjan Radevski, Teodora Popordanoska, Matthew B Blaschko, and Tinne Tuytelaars. Dave: Diagnostic benchmark for audio visual evaluation. arXiv preprint arXiv:2503.09321, 2025.   
Kaixiong Gong, Kaituo Feng, Bohao Li, Yibing Wang, Mofan Cheng, Shijia Yang, Jiaming Han, Benyou Wang, Yutong Bai, Zhuoran Yang, and Xiangyu Yue. Av-odyssey bench: Can your multimodal llms really understand audio-visual information? ArXiv, abs/2412.02611, 2024.   
Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, Arda Senocak, Joon Son Chung, and Tae-Hyun Oh. Avhbench: A cross-modal hallucination benchmark for audio-visual large language models. arXiv preprint arXiv:2410.18325, 2024.   
Yizhi Li, Ge Zhang, Yi Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, Yiming Liang, Jiaheng Liu, Jian Yang, Siwei Wu, Xingwei Qu, Jinjie Shi, Xinyue Zhang, Zhen Yang, Xiangzhou Wang, Zhaoxiang Zhang, Zachary Liu, Emmanouil Benetos, Wenhao Huang, and Chenghua Lin. Omnibench: Towards the future of universal omni-language models. ArXiv, abs/2409.15272, 2024b.

Ziwei Zhou, Rui Wang, and Zuxuan Wu. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. ArXiv, abs/2505.17862, 2025.   
Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, Yuanjun Lv, Yongqi Wang, Dake Guo, He Wang, Linhan Ma, Pei Zhang, Xinyu Zhang, Hongkun Hao, Zishan Guo, Baosong Yang, Bin Zhang, Ziyang Ma, Xipin Wei, Shuai Bai, Keqin Chen, Xuejing Liu, Peng Wang, Mingkun Yang, Dayiheng Liu, Xingzhang Ren, Bo Zheng, Rui Men, Fan Zhou, Bowen Yu, Jianxin Yang, Le Yu, Jingren Zhou, and Junyang Lin. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b.   
Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025c.   
Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Dawei Pan, Chong Li, Yuanbo Fang, Dong-Ling Kuang, Mingrui Wang, Chenglin Zhu, Youwei Zhang, Hongyu Guo, Fengyu Zhang, Yuran Wang, Bowen Ding, Wei Song, Xu Li, Yuqiu Huo, Zheng Liang, Shusen Zhang, Xin Wu, Shuai Zhao, Lin-Xiao Xiong, Yozhen Wu, Jia-Reng Ye, Wenhao Lu, Bowen Li, Yan Zhang, Yaqi Zhou, Xin Chen, Lei Su, Hongda Zhang, Fuzhong Chen, Xu Dong, Na Nie, Zhiying Wu, Bin Xiao, Ting Li, Shunya Dang, Ping Zhang, Yijia Sun, Jincheng Wu, Jinjie Yang, Xionghai Lin, Zhi-Xing Ma, Ke-Ye Wu, Jia Li, Ai-Min Yang, Hui Liu, Jianqiang Zhang, Xiaoxi Chen, Guangwei Ai, Wentao Zhang, Yicong Chen, Xiaoqin Huang, Kun Li, Wenjing Luo, Yi qun Duan, Lingling Zhu, Ran Xiao, Zhengquan Su, Jiani Pu, Dian Wang, Xu Jia, Tianyu Zhang, Mengyu Ai, Mang Wang, Yu Qiao, Lei Zhang, Yanjun Shen, Fan Yang, Miao Zhen, Yijie Zhou, Mingyang Chen, Fei Li, Chenzheng Zhu, Keer Lu, Yaqi Zhao, Hao Liang, Youquan Li, Yanzhao Qin, Lin-Lin Sun, Jianhua Xu, Haoze Sun, Mingan Lin, Zenan Zhou, and Weipeng Chen. Baichuan-omni-1.5 technical report. ArXiv, abs/2501.15368, 2025.   
Jiaxin Zhao, Qize Yang, Yi-Xing Peng, Detao Bai, Shimin Yao, Boyuan Sun, Xiang Chen, Shenghao Fu, Weixuan chen, Xihan Wei, and Liefeng Bo. Humanomni: A large vision-speech language model for human-centric video understanding. ArXiv, abs/2501.15111, 2025.   
Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qi-An Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: A gpt-4v level mllm on your phone. ArXiv, abs/2408.01800, 2024.   
Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024.   
Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, et al. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025.   
Hanrong Ye, Chao-Han Huck Yang, Arushi Goel, Wei Huang, Ligeng Zhu, Yuanhang Su, Sean Lin, An-Chieh Cheng, Zhen Wan, Jinchuan Tian, et al. Omnivinci: Enhancing architecture and data for omni-modal understanding llm. arXiv preprint arXiv:2510.15870, 2025.   
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025.   
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.   
Alexander H Liu, Andy Ehrenberg, Andy Lo, Clément Denoix, Corentin Barreau, Guillaume Lample, Jean-Malo Delignon, Khyathi Raghavi Chandu, Patrick von Platen, Pavankumar Reddy Muddireddy, et al. Voxtral. arXiv preprint arXiv:2507.13264, 2025a.   
Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: A survey on vision-language pre-training. Machine Intelligence Research, 20:38–56, 2022.   
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani S. Marathe, Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive vision-language models. ArXiv, abs/2308.01390, 2023.

Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. URL https://api.semanticscholar.org/CorpusID:258179774.   
Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl, 2025. URL https://arxiv.org/abs/2503.07536.   
Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-context configurations for image captioning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 804b5e300c9ed4e3ea3b073f186f4adc-Abstract-Conference.html.   
Jiaheng Liu, Zehao Ni, Haoran Que, Tao Sun, Zekun Wang, Jian Yang, Jiakai Wang, Hongcheng Guo, Zhongyuan Peng, Ge Zhang, et al. Roleagent: Building, interacting, and benchmarking high-quality role-playing agents from scripts. Advances in Neural Information Processing Systems, 37:49403–49428, 2024b.   
Ruibin Yuan, Hanfeng Lin, Shuyue Guo, Ge Zhang, Jiahao Pan, Yongyi Zang, Haohe Liu, Yiming Liang, Wenye Ma, Xingjian Du, et al. Yue: Scaling open foundation models for long-form music generation. arXiv preprint arXiv:2503.08638, 2025.   
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024.   
Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Dawei Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Keer Lu, Yaqi Zhao, Yan-Bin Shen, Fan Yang, Kaicheng yu, Tao Lin, Jianhua Xu, Zenan Zhou, and Weipeng Chen. Baichuan-omni technical report. ArXiv, abs/2410.08565, 2024c.   
Wei Sun, Linhan Cao, Yu Shan Cao, Weixia Zhang, Wen Wen, Kaiwei Zhang, Zijian Chen, Fangfang Lu, Xiongkuo Min, and Guangtao Zhai. Engagement prediction of short videos with large multimodal models. ArXiv, abs/2508.02516, 2025.   
Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. ArXiv, abs/2502.04328, 2025b.   
Yongliang Wu, Xinting Hu, Yuyang Sun, Yizhou Zhou, Wenbo Zhu, Fengyun Rao, Bernt Schiele, and Xu Yang. Number it: Temporal grounding videos like flipping manga, 2025a. URL https://arxiv. org/abs/2411.10332.   
Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv, abs/2403.05530, 2024.   
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In Computer Vision – ECCV 2024: 18th European Conference, Milan, Italy, September 29–October 4, 2024, Proceedings, Part VI, page 216–233, Berlin, Heidelberg, 2024c. Springer-Verlag. ISBN 978-3-031-72657-6. doi: 10.1007/978-3-031-72658-3_13.   
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: evaluating large multimodal models for integrated capabilities. In Proceedings of the 41st International Conference on Machine Learning, ICML’24. JMLR.org, 2024a.   
Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Qinghong Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet v2: A challenging benchmark to evaluate large multimodal models for integrated capabilities. ArXiv, abs/2408.00765, 2024b.   
Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain. ArXiv, abs/2402.15527, 2024b.   
Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. ArXiv, abs/2502.09621, 2025.

Yancheng He, Shilong Li, Jiaheng Liu, Weixun Wang, Xingyuan Bu, Ge Zhang, Zhongyuan Peng, Zhaoxiang Zhang, Zhicheng Zheng, Wenbo Su, et al. Can large language models detect errors in long chain-of-thought reasoning? arXiv preprint arXiv:2502.19361, 2025.   
Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025.   
Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, and Xu Yang. On the generalization of sft: A reinforcement learning perspective with reward rectification, 2025b. URL https://arxiv.org/abs/2508.05629.   
Wentao Ge, Shunian Chen, Hardy Chen, Nuo Chen, Junying Chen, Zhihong Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, Dingjie Song, Xidong Wang, Anningzhe Gao, Zhang Zhiyi, Jianquan Li, Xiang Wan, and Benyou Wang. MLLM-bench: Evaluating multimodal LLMs with per-sample criteria. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 4951–4974, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/2025.naacl-long.256. URL https://aclanthology.org/2025.naacl-long.256/.   
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9556–9567, 2023.   
Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark. In Annual Meeting of the Association for Computational Linguistics, 2024b.   
Ziliang Gan, Yu Lu, Dong Zhang, Haohan Li, Che Liu, Jian Liu, Ji Liu, Haipang Wu, Chaoyou Fu, Zenglin Xu, Rongjunchen Zhang, and Yong Dai. Mme-finance: A multimodal finance benchmark for expert-level understanding and reasoning. ArXiv, abs/2411.03314, 2024.   
Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tian-Xin Li, Haodong Duan, Ziyan Huang, Yan-Cheng Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J. Seibel, Junjun He, and Yu Qiao. Gmai-mmbench: A comprehensive multimodal evaluation benchmark towards general medical ai. ArXiv, abs/2408.03361, 2024c.   
Xin Eric Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4580–4590, 2019.   
Linjie Li, Jie Lei, Zhe Gan, Licheng Yu, Yen-Chun Chen, Rohith Krishnan Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara L. Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, and Zicheng Liu. Value: A multi-task benchmark for video-and-language understanding evaluation. ArXiv, abs/2106.04632, 2021.   
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: A comprehensive multi-modal video understanding benchmark. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 22195–22206, 2023.   
Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbenchvideo: A long-form multi-shot benchmark for holistic video understanding. ArXiv, abs/2406.14515, 2024.   
Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context interleaved video-language understanding. ArXiv, abs/2407.15754, 2024.   
Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He, Huanxuan Liao, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, et al. A comprehensive survey on long context language modeling. arXiv preprint arXiv:2503.17407, 2025c.

# A Full Video Category Taxonomy

Table 5 shows that videos in OmniVideoBench span 8 major categories and 68 subcategories.

Table 5: Full taxonomy of the video dataset.   

<table><tr><td>Main Category</td><td>Subcategories</td></tr><tr><td>Vlog</td><td>Cooking &amp; Cuisine; Travel &amp; Outdoor; Art; Animals; Daily Life at Home; DIY &amp; Handcraft; Gardening; Fitness; Sports; Interviews; Party Games; Makeup &amp; Beauty; Fashion &amp; Styling; Hiking &amp; Trekking</td></tr><tr><td>News</td><td>Politics; Economy; Society; Technology; Education; Healthcare; Military; Law &amp; Justice; Sports; Culture; Entertainment; Weather; Disaster; Transportation</td></tr><tr><td>Cartoon</td><td>2D Animation; 3D Animation</td></tr><tr><td>Sports</td><td>Basketball; Football (Soccer); Volleyball; Badminton; Table Tennis; Swimming; Figure Skating; Skiing; Gymnastics; Wrestling &amp; judo; Track &amp; Field; Esports; Others</td></tr><tr><td>Documentary</td><td>Nature &amp; Wildlife; History &amp; Archaeology; Society &amp; Humanity; Politics &amp; Military; Science &amp; Engineering; Medicine &amp; Health; Crime &amp; Law; Art &amp; Culture; Education &amp; Growth; Economy; Environment &amp; Climate; Food &amp; Culinary Culture; Religion &amp; Belief</td></tr><tr><td>TV</td><td>Short; Dramas &amp; Web Series; Variety; Stage Plays; Dance; Mime; Movies</td></tr><tr><td>Others</td><td>Live; Advertisement; Course Replay; Short Video</td></tr><tr><td>Ego</td><td>First-person: People; First-person:Pets</td></tr></table>

# B Detailed Principles of Video Collection

To ensure an objective and reliable evaluation of MLLMs, the videos included in the benchmark must satisfy multiple requirements, ensuring diversity in both type and duration. The content should provide rich information across audio and visual modalities, while maintaining complementarity between the two. In other words, the benchmark avoids cases where the visual content can be fully inferred from the audio alone, or where the audio is redundant given the visual stream. Furthermore, since many existing video training datasets overlap with the sources of our benchmark—for example, clips from Friends—evaluation may otherwise reduce to simple “answer memorization.” To mitigate this unfairness, we additionally consider the publication year of videos when constructing the dataset. The detailed principles for video collection are as follows:

• Video publication date. Given that most existing training datasets are constructed from YouTube videos, similar to ours, or contain overlapping content such as identical TV shows, we restrict our selection to videos published after June 2024. We use the most recent videos possible to mitigate unfairness and potential overestimation issues arising from the model having already been exposed to similar content during training.   
• Rich dynamic visual information. The distinguishing feature of videos compared to images lies in their rich dynamic visual information. A prerequisite for evaluating a model’s ability to understand visual information in videos is that the videos themselves contain sufficient dynamic content to be captured and analyzed. Consequently, videos lacking diverse dynamic visual information are excluded, such as those consisting of only several static scenes or perspectives throughout, or those that remain largely static with minimal motion confined to a small corner of the frame.   
• Effective audio information. In some videos, the audio is completely unrelated to the visual content, such as when only an independent background track is added. We consider such audio to be invalid. To fairly evaluate the model’s capability in audio-visual collaborative reasoning, the audio—whether speech, environmental sound, or music—must align with the visual content.   
• Absence of subtitle. We excluded videos with embedded subtitles, as such practices convey most of the audio information visually, enabling models to “cheat” through vision alone. Likewise, videos containing large text overlays were regarded as undesirable, since these overlays often

directly reveal information about characters’ speech, mental states, or ongoing events, thereby undermining the assessment of the model’s genuine understanding and reasoning abilities.

• Video resolution. To ensure video quality, we require a minimum resolution of $4 8 0 \mathrm { p } .$ , and the visual content must be free from issues such as distortion or blurriness that would hinder comprehension.

# C Prompts Used in This Work

# C.1 Prompt for Overall Evaluation

# Instruction: You are given a video. Based on the content of the video, answer the following question:

# Question: {Question}

# Options:

A: {Option A} B: {Option B} C: {Option C} D: {Option D}

# Task:

Answer with the option’s letter directly(e.g., A, B, C, or D).

If your access to the video content is limited, at least one option that is more likely than the others must be chosen.

Mustn’t give any other reason for can not choose!

# C.2 Prompt to select questions that can be answered without relying on options

# Role: You are an impartial judge.

# Instruction: Your task is NOT to answer the question, but to determine whether the question is inherently DEPENDENT on the multiple-choice options in order to be answered.

# Task:

We aim to convert this multiple-choice question into an open-ended question.

The video content is NOT provided here, but you should assume you have fully watched the video and know everything about it.

Your job is ONLY to decide whether the question itself *requires* the options to be answerable.

# Guidelines:

- If the question can still be reasonably answered **without needing the options** (even if the exact wording might change slightly), return $\mathbf { \Delta ^ { \prime \prime } N _ { 0 } } { \mathbf { \prime \prime } }$ .   
- If the question cannot be answered at all without the options (e.g., it explicitly asks “Which of the following. . . ” ), return “Yes”.

# Question: {Question}

# Answer: {Answer}

Respond ONLY with “Yes” or $\mathbf { \Delta ^ { \prime \prime } N _ { 0 } } { \mathbf { \prime \prime } }$ .

# C.3 Prompt for multiple-choice questions with step-by-step reasoning

# Instruction: You are given a video. Based on the content of the video, answer the following question:

# Question: {Question}

# Options:

A: {Option A} B: {Option B} C: {Option C} D: {Option D}

# Task:

Note that you should first reason step by step, and then you should give your final choice in A, B, C, or D.

Your answer format should be as follows:

Step X: [Reasoning step X]

The final choice is:

\bbox{{Answer with the option’s letter directly(A, B, C, or D).}}.