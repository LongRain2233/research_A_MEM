# SEDM: Scalable Self-Evolving Distributed Memory for Agents

Haoran $\mathrm { X u } ^ { * 1 , 2 }$ , Jiacong $\mathrm { H u } ^ { * 1 , 3 }$ , Ke Zhang1,4, Lei $\mathrm { Y u } ^ { 1 , 5 }$ , Yuxin Tang1,6, Xinyuan Song1,7, Yiqun Duan1,8, Lynn Ai1, and Bill $\mathrm { S h i ^ { \dag 1 } }$

1Gradient

2Zhejiang University

3South China University of Technology

4Waseda University

5University of Toronto

6Rice University

7Emory University

8University of Technology Sydney

# Abstract

In long-term multi-agent systems, the accumulation of trajectories and historical interactions makes efficient memory management a particularly challenging task, with significant implications for both performance and scalability. Existing memory management methods typically depend on vector retrieval and hierarchical storage, yet they are prone to noise accumulation, uncontrolled memory expansion, and limited generalization across domains. To address these challenges, we present SEDM (Self-Evolving Distributed Memory), a verifiable and adaptive framework that transforms memory from a passive repository into an active, self-optimizing component. SEDM integrates verifiable write admission based on reproducible replay, a self-scheduling memory controller that dynamically ranks and consolidates entries according to empirical utility, and cross-domain knowledge diffusion that abstracts reusable insights to support transfer across heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM improves reasoning accuracy while reducing token overhead compared with strong memory baselines, and further enables knowledge distilled from fact verification to enhance multi-hop reasoning. The results highlight SEDM as a scalable and sustainable memory management design for open-ended multi-agent collaboration. The code will be released upon acceptance of this paper.

# 1 Introduction

In recent years, the rapid development of large-scale multi-agent systems (MAS) [58, 30, 12, 60, 3] has expanded their application in diverse domains, including collaborative reasoning, decisionmaking, and autonomous planning [51]. A fundamental challenge in open-ended, long-term tasks lies in enabling agents to effectively manage, interpret, and reutilize information accumulated through continuous interactions with both peers and their environment [60]. In the absence of effective memory management design, the sheer scale of historical interactions can easily overwhelm computational resources and compromise decision quality [13].

In open-ended and long-term multi-agent tasks, each agent relies on its past memories, the observed states of other agents, and the current environment to make decisions for subsequent actions or responses [10]. During continuous interaction between agents and their environment, the MAS gradually accumulates extensive logs of interactions, invocation trajectories, and high-level policy memories [38]. Such overwhelming amounts of information directly impact the efficiency and cost of decision-making, often leading to higher monetary costs and longer contextual requirements for inference [52]. Therefore, designing an efficient and sustainable memory mechanism has become a critical issue for modern long-term multi-agent systems.

Current methods primarily adopt vector retrieval and hierarchical memory structures to manage storage and retrieval efficiently [18]. Vector retrieval [19, 22, 17, 28, 16] leverages semantic similarity to identify relevant entries, while hierarchical organization arranges information in layered structures according to abstraction levels [45]. These approaches have shown promise in improving retrieval accuracy and managing memory scalability [7]. However, in complex collaborative multi-agent tasks, their effectiveness diminishes, as the underlying assumptions of stability and linear growth do not hold [56]. This gap between theoretical promise and practical performance highlights several critical limitations that hinder their long-term applicability.

One major challenge is the inevitable accumulation of noise, which severely degrades retrieval quality [11]. As the memory size expands without constraint, the system faces exponentially increasing computational costs in both retrieval and context construction [17]. This not only reduces overall efficiency but also amplifies the interference caused by redundant information [28]. In particular, the presence of low-value or semantically irrelevant entries dilutes the contribution of high-quality information in retrieval results, impairing downstream task performance and leading to measurable declines in metrics [5]. In addition, the cumulative noise effect increases response latency and accelerates the nonlinear consumption of computational and storage resources [22], ultimately threatening both scalability and stability in long-term MAS operations [43].

To overcome these limitations, we introduce Scalable Self-Evolving Distributed Memory (SEDM), a framework that transforms memory from a passive repository into an adaptive, self-optimizing, and verifiable component for multi-agent systems. Unlike conventional designs that treat memory as a static store, SEDM continually refines knowledge to enhance learning and decision-making efficiency in dynamic task environments. It operationalizes memory as an active mechanism by integrating verifiability and continuous self-improvement into the memory lifecycle. At its core, memory items undergo a rigorous admission process based on self-contained execution contexts (SCECs), such as Docker and ReproZip [34, 9], which package all necessary information for environment-free replay and offline validation. This mechanism provides empirical evidence for utility at write time, ensuring that only useful, high-quality experiences enter the memory repository. Once admitted, memory items are dynamically managed by a self-scheduling controller and enhanced through crossdomain knowledge diffusion. The controller leverages admission-derived weights, combined with semantic similarity, to schedule retrieval-time usage without costly reranking, while consolidation and progressive evolution continuously refine the repository by promoting stable items, merging redundancies, and pruning harmful ones. Beyond single-task settings, SEDM abstracts reusable insights into general forms, enabling knowledge distilled in one domain to be safely transferred and re-validated in others. Together, these components establish a scalable and auditable memory mechanism that enhances reasoning accuracy, reduces overhead, and supports sustainable long-term multi-agent collaboration.

We evaluate SEDM on two representative benchmarks, FEVER [53] for fact verification and HotpotQA [61] for multi-hop reasoning, comparing against no-memory and G-Memory baselines [63]. The results show that SEDM consistently improves task accuracy while significantly reducing token overhead, thereby achieving a better balance between performance and efficiency. Ablation studies confirm that both the verifiable admission mechanism and the self-scheduling controller contribute progressively to this gain, with the latter playing a key role in constraining prompt growth without sacrificing accuracy. Furthermore, cross-domain evaluation demonstrates that memory distilled from one dataset can transfer to another, with factual knowledge from FEVER notably boosting performance on HotpotQA. These findings highlight SEDM as a scalable, adaptive, and generalizable memory framework for long-term multi-agent reasoning.

Our contributions are summarized as follows:

• We propose Self-Evolving Distributed Memory (SEDM), a novel framework that transforms memory from a passive repository into an adaptive, verifiable, and continuously improving component, introducing self-contained execution contexts (SCECs) for reproducible admission and utility-based memory weighting.   
• We design a self-scheduling memory controller that selectively manages memory at retrieval time and continuously refines the repository through consolidation, redundancy suppression, and progressive evolution, thereby balancing accuracy and efficiency.   
• We conduct extensive evaluations on LoCoMo, FEVER, and HotpotQA benchmarks, demonstrating that SEDM consistently improves task accuracy while significantly reducing token overhead.

# 2 Related Work

Self-Evolving Agents. Recent efforts in building self-evolving agents have focused on enabling systems to improve their reasoning or behavior over time without explicit retraining. Approaches such as Reflexion [50] and Voyager [55] allow agents to iteratively refine their strategies by leveraging self-reflection and accumulated trajectories. Similarly, MEMIT [33] demonstrates the feasibility of localized knowledge editing within large language models, suggesting a pathway for agents to evolve by continuously updating their internal representations. These studies highlight the importance of mechanisms that support autonomous adaptation and progressive self-improvement in dynamic environments.

Agent Memory. In parallel, research on agent memory has investigated how to store, retrieve, and utilize knowledge efficiently across long-horizon interactions. Episodic memory systems, such as those proposed by Park et al. [42], emulate human-like memory consolidation to support consistent long-term behavior in simulated social environments. Memory-augmented neural networks [15] and differentiable neural dictionaries [21] further demonstrate how structured memory access can enhance reasoning and generalization. More recently, retrieval-augmented generation frameworks tailored for interactive agents [35] have shown that dynamically grounding responses in curated external memories improves both interpretability and task success. Together, these works underscore the need for memory systems that are not only scalable but also adaptive to the agent’s evolving operational context.

# 3 Methodology

# 3.1 System Overview

Figure 1 illustrates the differences between no memory, fixed memory, and our proposed SEDM framework, highlighting how SEDM achieves verifiable admission, adaptive scheduling, and sustainable knowledge evolution.

Figure 2 gives an end-to-end view of SEDM. The system introduces verifiability and self-improvement into the memory life cycle and consists of three tightly integrated modules. (i) SCEC-based Verifiable Write Admission packages each run into a Self-Contained Execution Context (SCEC) and performs environment-free A/B replay to estimate the marginal utility of a candidate memory item; only items with positive evidence are admitted and assigned an initial weight. (ii) Self-Scheduling in the Memory Controller uses admission-derived weights together with semantic similarity to score candidates at retrieval time, while also maintaining the repository by updating weights from observed outcomes, merging near duplicates, and pruning harmful entries. (iii) Cross-Domain Knowledge Diffusion abstracts admitted items into conservative general forms and re-validates them in other tasks, allowing knowledge to transfer safely across domains.

# 3.2 SCEC-based Verifiable Write Admission

We formulate write admission as a verifiable, environment-free procedure that assigns an initial utility weight to each candidate memory item before it enters the repository. The process is based on a Self-Contained Execution Context (SCEC), a minimal and standardized package that enables validation, parallel replay, and offline auditing. By placing admission behind paired A/B evaluations within SCECs, the system produces reproducible evidence for weight initialization while filtering out negative or noisy experiences.

![](images/dc683253525ac437139230f7e9b6b8b13703b95ba84f06370b697b938894f469.jpg)

![](images/ec290c9ee05227728ac2dd3abd6b63a2d42b7cb4d07b4b091c6f2039dfa44514.jpg)

![](images/b04192f78a78faff771b6b06d82b1287ccf1d9a14d65130c96c5cce8cb211070.jpg)  
Figure 1: Illustration of different memory strategies. No Memory: the agent interacts with the environment without retaining past information. Fixed Memory: the agent retrieves from a static memory pool, which may grow excessively. SEDM: introduces verifiable write admission, parallel simulation, and adaptive scheduling to build high-quality, self-evolving memory that supports efficient and transferable knowledge use.

# 3.2.1 Self-Contained Packaging and Distributed Replay

Each task execution is encapsulated into an SCEC to support reproducible validation and analysis without requiring the original environment. An SCEC includes all necessary inputs, outputs, tool summaries, seeds, and configuration hashes, ensuring (i) self-contained representation, (ii) environment-free replay by summarizing external tool calls, (iii) deterministic reproduction across model versions and seeds, and (iv) minimal sufficiency by storing only essential information.

Treating an SCEC as an independent job enables large-scale distributed A/B replay on arbitrary workers. Only aggregated statistics, along with integrity hashes and version stamps, are uploaded, preserving auditability while controlling cost. This environment-free design eliminates the need to reconstruct complex environments or interact with real agents during validation, thereby allowing memory effectiveness to be tested through parallel replay at scale. As a result, admission decisions can be made rapidly and consistently, significantly reducing computational overhead while ensuring that only high-quality experiences enter the memory repository.

# 3.2.2 SCEC-grounded A/B Test for Memory Item Initialization

From each SCEC, we extract one candidate memory item $m$ , represented as a concise, independently injectable snippet. The extraction process identifies decisive reasoning or corrective steps, performs deduplication and canonicalization, and attaches provenance information.

To evaluate its utility, we conduct a paired A/B test within the same SCEC. The control condition (A) uses the original prompt, while the treatment condition (B) augments the prompt with the candidate memory $m$ . This setup isolates the marginal effect of $m$ and provides empirical evidence for its contribution. For a query $q$ , the constructed prompts are defined as

$$
I _ {A} = f (q), \quad I _ {B} = f (q; m), \tag {1}
$$

![](images/c5dc48f2a72f813cfa6db84d8bb7a6a2dbe4f24d32df599e5f3b1998f5eaccfd.jpg)  
Figure 2: SEDM architecture. Left: task execution generates traces that are packaged into a Self-Contained Execution Context (SCEC) with inputs, outputs, tool summaries, seeds, and hashes. Bottom: from each SCEC, a candidate memory is extracted and evaluated via paired A/B replay (Original vs. Injected); distributed verification computes ∆Reward, ∆Latency, and $\Delta$ Tokens, and an admission gate accepts the item and assigns its initial weight if the score is positive, else discards it. Right: the memory controller performs (a) memory scheduling using $s ( q , \bar { m ) } = \sin ( q , m ) \times w ( m )$ for retrieval and injection, (b) consolidation and evolution by updating weights from outcomes and merging near-duplicate items $( m _ { \mathrm { m e r g e d } } = \mathrm { M e r g e } ( m _ { i } , m _ { j } ) )$ , and (c) knowledge diffusion by abstracting reusable insights $( m _ { \mathrm { g e n e r a l } } = \mathrm { A b s t r a c t } ( m _ { \mathrm { s p e c i f i c } } ) )$ . Linked trajectory, query, and insight graphs track the vertical evolution of memory and preserve provenance. The dashed loop indicates retrieval and injection during inference, closing the self-improving cycle.

where $f ( \cdot )$ denotes prompt construction and $I _ { B }$ injects $m$ into the SCEC’s dedicated slot together with summarized tool feedback. The model execution inside the SCEC is denoted by $\mathcal { F }$ :

$$
o _ {A} = \mathcal {F} \left(I _ {A}\right), \quad o _ {B} = \mathcal {F} \left(I _ {B}\right). \tag {2}
$$

We then measure the deltas in reward, latency, and token usage:

$$
\Delta R = R \left(o _ {B}\right) - R \left(o _ {A}\right), \quad \Delta L = L \left(o _ {B}\right) - L \left(o _ {A}\right), \quad \Delta T = T \left(o _ {B}\right) - T \left(o _ {A}\right), \tag {3}
$$

where $R ( \cdot )$ is the task-specific reward, and $L ( \cdot )$ and $T ( \cdot )$ denote latency and token overhead, respectively. A composite admission score balances utility and cost:

$$
S = \Delta R - \lambda_ {L} \Delta L - \lambda_ {T} \Delta T, \tag {4}
$$

with $\lambda _ { L } , \lambda _ { T } \geq 0$ controlling the trade-offs.

The admission decision and initial weight are then defined as

$$
\operatorname {a c c e p t} (m) \Longleftrightarrow S \geq \eta , \quad w _ {0} (m) = \max  \{0, S \}, \tag {5}
$$

where $\eta$ is the acceptance threshold. Multiple runs may be averaged to mitigate variance.

Accepted items are stored together with their initial weights and full provenance (hashes, seeds, versions, and $\mathrm { A } / \mathrm { B }$ fingerprints), while rejected or ambiguous items are excluded. This procedure yields a compact, auditable admission signal that can be validated offline and efficiently executed in parallel without dependence on the original environment.

# 3.3 Self-Scheduling in the Memory Controller

The memory controller manages and optimizes the repository through a self-scheduling policy. Unlike traditional systems that depend on costly per-query reranking [37, 49, 36], our approach establishes an evidence-based mechanism for both selecting memory items during retrieval and continuously refining the repository. It comprises two core functions: retrieval-time scheduling, which determines how to use memories effectively for an incoming query, and consolidation and progressive evolution, which curates a compact, high-quality memory set. Together, these components ensure that memory usage is grounded in verifiable utility signals, improving both efficiency and performance.

# 3.3.1 Retrieval-time Scheduling

The controller’s scheduling policy relies on a ranking signal aligned with realized utility, avoiding the instability and computational cost of on-the-fly large language model reranking [44]. Prior approaches typically use vector similarity or ad-hoc prompt-based scoring, but semantic similarity alone does not guarantee actual task benefit, and repeated reranking adds latency and variance [22, 17] or ad-hoc prompt-based scoring [37, 44].

In our design, we incorporate evidence collected at write time via A/B validation on Self-Contained Execution Contexts (SCECs). These statistics, particularly the measured changes in reward and latency, are mapped into a stable admission-derived weight $w ( m )$ for each memory item. At retrieval time, this weight is combined with semantic similarity to form a utility-aligned score:

$$
s (q, m) = \sin (q, m) \times w (m), \tag {6}
$$

where $\sin ( q , m )$ denotes the semantic similarity between query $q$ and memory $m$ , and $w ( m )$ reflects its empirically validated utility.

This coupling of semantic relevance with admission-grounded evidence stabilizes selection and reduces overhead, ensuring that memory items are injected into prompts not only because they are similar, but because they have demonstrated measurable benefit. As a result, retrieval decisions are both efficient and aligned with the system’s long-term objectives.

# 3.3.2 Consolidation and Progressive Evolution

The consolidation and progressive evolution module maintains a compact yet effective memory repository by suppressing redundancy, preserving items with stable gains, and eliminating or recycling items that show conflicts or sustained negative contributions. While retrieval-time scheduling focuses on selecting memories for specific queries, consolidation and evolution aim to improve the repository itself so that subsequent scheduling operates on a cleaner and more reliable basis.

Progressive evolution is achieved by tracking usage and outcome signals and applying conservative updates to utility weights over time. Items that are rarely retrieved or consistently fail to provide positive utility are gradually decayed, reducing their influence in future selections. Conversely, items that repeatedly yield positive gains across related contexts are promoted. Promotion increases their weights and, when consistency is observed across multiple items, may trigger abstraction into higher-level insights. Such abstractions enable representative entries to replace families of consistent low-level experiences [14, 41]. If observed outcomes diverge significantly from admission-time evidence, items are demoted or queued for cleanup. All updates are logged with provenance to ensure the evolution process remains auditable [2, 26, 9].

The weight update for a memory item $m$ depends on its current weight $w ( m )$ , usage frequency $f _ { u s e } ( m )$ , and average realized utility $\bar { U } ( m )$ since the last update:

$$
w _ {t + 1} (m) = w _ {t} (m) + \alpha \cdot \bar {U} _ {t} (m) - \beta \cdot f _ {\text {u s e}, t} (m), \tag {7}
$$

where $\alpha$ and $\beta$ control the influence of observed utility and usage. This ensures weights evolve from admission-derived values toward refined, usage-aware estimates.

Conflict detection is integral to this loop. A memory is marked as conflicting when repeated injections consistently reduce task reward or when its implications contradict other rules. Such items undergo progressive weight reduction, and if their weight falls below a threshold, they are demoted or removed. All decisions retain version traces and evidence chains to support rollback and inspection.

Semantic consolidation addresses redundancy among items from different SCECs. When two or more items, $m _ { i }$ and $m _ { j }$ , show high semantic similarity without conflicting applicability, they are merged:

$$
m _ {\text {m e r g e d}} = \operatorname {M e r g e} \left(m _ {i}, m _ {j}\right). \tag {8}
$$

The merged entry preserves essential content while aggregating evidence from the originals, and its weight $w ( m _ { \mathrm { m e r g e d } } )$ is reconciled to reflect combined support without double counting. Contributing items are archived or soft-deleted, preserving provenance if needed. By collapsing near-duplicates into single representatives, the repository reduces retrieval noise and strengthens utility signals [1, 32, 4, 64].

Through these routines, the controller maintains a concise but reliable set of memories. Redundant entries are consolidated [32], stable positives are promoted [20], abstractions generalize recurring insights [14], and harmful items are isolated or removed [54]. As a result, the repository remains aligned with realized utility, ensuring that retrieval decisions exploit empirical evidence rather than ad-hoc reranking.

# 3.4 Cross-Domain Knowledge Diffusion

This component exploits the environment-free and verifiable properties of the SCEC method, treating memory entries as portable and re-verifiable assets across domains. Tasks from diverse domains continuously supply evidence to refine memory weights, thereby improving both universality and robustness of the repository. The process follows a loop of migrate, re-validate, and re-incorporate: transfer is initiated through retrieval and weighted injection; subsequent usage allows re-estimation of weights via SCEC-compatible procedures; and the updates inform later scheduling and admission decisions. Throughout, retrieval signals remain unchanged: similarity-based relevance, $\sin ( q , m )$ for query $q$ and memory item $m$ , combined with the admission-derived weight $w ( m )$ . This design avoids evidence-free cold starts and eliminates the need for additional scoring components at runtime.

Immediately after admission, each specific entry mspecific generates a conservative general form mgeneral through a lightweight abstraction operator:

$$
m _ {\text {g e n e r a l}} = \operatorname {A b s t r a c t} \left(m _ {\text {s p e c i f i c}}\right). \tag {9}
$$

This yields a dual-linked pair in which the general form strips domain-specific features while preserving transferable essence. The general form serves as a low-risk candidate for cross-domain retrieval, while the specific form remains primary within its source domain.

The abstraction process is rule-governed and minimal. Entities and domain-specific terms are replaced with typed placeholders, retaining actionable task–action structures while removing non-essential detail [14, 24, 29]. The result is a compact snippet suitable for direct injection and controlled comparisons. To minimize orchestration cost, abstraction is generated alongside the SCEC-based A/B assessment within the distributed pipeline [9].

For weight inheritance, the general form is initialized conservatively as a scaled version of the specific weight, $w _ { \mathrm { g e n e r a l } } = \alpha \cdot w _ { \mathrm { s p e c i f i c } }$ with $\alpha < 1$ . This prior encodes caution against over-abstraction while preserving provenance for later auditing and weight updates.

At retrieval, both forms compete in the candidate set with a unified score:

$$
s (q, m) = \operatorname {s i m} (q, m) \times w (m). \tag {10}
$$

In-domain queries typically favor specific forms due to higher semantic match and weight, while cross-domain queries benefit from the more stable similarity of general forms. This mechanism enables knowledge diffusion across domains without introducing extra runtime complexity, while maintaining auditability, portability, and stability. Subsequent use further refines the weights, allowing knowledge to propagate adaptively across diverse tasks.

# 4 Experiment

# 4.1 Experimental Setup

# 4.1.1 Dataset and Model

Evaluation is conducted on the LoCoMo benchmark [31], which consists of two components: (i) multi-turn dialogues (approximately 600 turns per dialogue, averaging 26,000 tokens) and (ii)

question-answer (QA) pairs grounded in those dialogues. Each dialogue contains roughly 200 questions spanning single-hop [46], multi-hop [62], open-domain [23], and temporal reasoning [6]. All experiments are carried out with gpt-4o-mini to ensure consistency and comparability across evaluations.

Performance is measured using two complementary metrics: Token-level F1 Score (F1) [46], which captures the overlap between predicted and ground-truth answers, and BLEU-1 (B1) [40], which evaluates unigram-level lexical similarity.

# 4.1.2 Baselines

To evaluate the effectiveness of our framework SEDM, we compare it with several representative baselines for multi-session dialogue reasoning: LoCoMo [31], a benchmark for assessing longrange retrieval and reasoning in multi-session conversations; ReadAgent [27], a human-in-theloop agent for long-context reading; MemoryBank [65], a memory-augmented retrieval model; MemoryGPT [39], an LLM operating system with hierarchical memory modules; A-Mem [59], a dynamic agentic memory system that creates, links, and updates structured memories; Zep [47], a retrieval-based agent with structured memory access for temporally extended queries; LangMem [57], an open-source framework connecting memory chains across sessions; Mem0 [8], a modular memory system with explicit in-context memory operations; and G-memory [63], which leverages hierarchical tracing for memory retrieval.

# 4.2 Main results

Table 1: Evaluation results on the LoCoMo benchmark dataset comparing SEDM with other memoryenabled systems. Models are evaluated on F1 [46] and BLEU-1 (B1) [40] across Single Hop [46], Multi-Hop [62], Open Domain [23], Temporal, and Adversial questions [6]. Higher is better. The top two best results are marked in Bold.   

<table><tr><td rowspan="2">Method</td><td colspan="2">Single Hop</td><td colspan="2">Multi-Hop</td><td colspan="2">Open Domain</td><td colspan="2">Temporal</td><td colspan="2">Adversal</td></tr><tr><td>F1↑</td><td>B1↑</td><td>F1↑</td><td>B1↑</td><td>F1↑</td><td>B1↑</td><td>F1↑</td><td>B1↑</td><td>F1↑</td><td>B1↑</td></tr><tr><td>LoCoMo [31]</td><td>25.0</td><td>19.8</td><td>18.4</td><td>14.8</td><td>40.4</td><td>29.1</td><td>12.0</td><td>11.2</td><td>69.2</td><td>68.8</td></tr><tr><td>ReadAgent [27]</td><td>9.2</td><td>6.5</td><td>12.6</td><td>8.9</td><td>9.7</td><td>7.7</td><td>5.3</td><td>5.1</td><td>9.8</td><td>9.0</td></tr><tr><td>MemoryBank [65]</td><td>5.0</td><td>4.8</td><td>9.7</td><td>7.0</td><td>6.6</td><td>5.2</td><td>5.6</td><td>5.9</td><td>7.4</td><td>6.5</td></tr><tr><td>MemoryGPT [39]</td><td>26.7</td><td>17.7</td><td>25.5</td><td>19.4</td><td>41.0</td><td>34.3</td><td>9.2</td><td>7.4</td><td>43.2</td><td>42.7</td></tr><tr><td>A-Mem [59]</td><td>27.0</td><td>20.1</td><td>45.9</td><td>36.7</td><td>44.7</td><td>37.1</td><td>12.1</td><td>12.0</td><td>50.0</td><td>49.5</td></tr><tr><td>Zep [47]</td><td>30.2</td><td>17.2</td><td>15.0</td><td>11.6</td><td>26.7</td><td>18.4</td><td>3.5</td><td>2.7</td><td>22.6</td><td>15.1</td></tr><tr><td>LangMem [25]</td><td>22.4</td><td>15.2</td><td>18.7</td><td>16.0</td><td>31.6</td><td>23.9</td><td>27.8</td><td>21.5</td><td>28.3</td><td>21.3</td></tr><tr><td>Mem0 [8]</td><td>27.3</td><td>18.6</td><td>18.6</td><td>13.9</td><td>34.0</td><td>24.8</td><td>26.9</td><td>21.1</td><td>30.4</td><td>22.2</td></tr><tr><td>G-memory [63]</td><td>34.6</td><td>26.6</td><td>9.05</td><td>7.2</td><td>53.5</td><td>44.0</td><td>32.4</td><td>25.6</td><td>11.3</td><td>9.3</td></tr><tr><td>SEDM (Ours)</td><td>33.5</td><td>24.4</td><td>12.1</td><td>9.2</td><td>51.7</td><td>37.0</td><td>47.5</td><td>33.1</td><td>12.1</td><td>9.3</td></tr></table>

Table 1 presents the results of SEDM compared with strong baselines, including LoCoMo, ReadAgent, MemoryBank, MemoryGPT, A-Mem, Zep, LangMem, Mem0, and G-memory, on the LoCoMo benchmark across Single Hop, Multi-Hop, Open Domain, Temporal, and Adversarial reasoning tasks.

Overall, the results show that SEDM delivers strong and consistent improvements in challenging reasoning scenarios. In particular, SEDM achieves the highest F1 and BLEU-1 scores on the Open Domain and Temporal settings, surpassing all other baselines by a substantial margin. For instance, SEDM improves Temporal reasoning by more than 15 F1 points compared with the strongest baseline (G-memory), demonstrating its ability to capture and utilize temporally structured information effectively.

On the Single Hop setting, SEDM remains highly competitive, ranking among the top two models alongside G-memory, while showing significantly better balance across different question types. Although A-Mem and MemoryGPT achieve strong results on Multi-Hop and Adversarial tasks

respectively, their performance drops markedly on other reasoning categories, whereas SEDM maintains stable and robust performance across the board.

These results highlight that the learned selective memory mechanism in SEDM provides clear advantages over heuristic, retrieval-based, and hierarchical memory systems. By selectively integrating context across sessions, SEDM enables LLMs to reason more effectively over long, multi-session dialogues and demonstrates strong generalization across diverse reasoning categories.

# 4.3 Efficient analysis on Fever and HotpotQA

We further conduct Experiments on two widely used benchmark datasets: FEVER[53] and HotpotQA[61].HotpotQA is a large-scale question-answering benchmark designed to evaluate the ability of systems to perform multi-hop reasoning across diverse natural language inputs. FEVER is a fact-checking dataset that provides human-written claims about Wikipedia entities, each labeled as Supported, Refuted, or NotEnoughInfo. Both datasets present significant challenges for testing long-term reasoning and memory utilization in language agents.

The proposed SEDM is compared with the following baselines: (1) No Memory: the model only relies on the query input without any memory augmentation, serving as the basic performance reference; (2) G-Memory[63]: a memory-augmented method that stores all past information in a global memory pool and retrieves by similarity search. Although effective, it incurs high inference cost due to the large number of prompt tokens; (3) SEDM (ours): our scalable self-evolving distributed memory, which introduces memory scheduling and selection mechanisms to maintain a compact and adaptive working set, thereby balancing performance and efficiency.

All experiments run on the same backbone LLM (GPT-4o-mini) [38]. Dense retrieval is handled by ALL-MINILM-L6-V2 [48], which embeds both knowledge snippets and queries for similarity search. On the evaluation side, we use FEVER accuracy for fact-checking and HotpotQA exact-match (EM) for multi-hop QA. Efficiency is tracked by counting prompt and completion tokens consumed during inference. To ensure fair comparisons, every method is granted the same memory budget; the proposed SEDM does not expand this budget, but instead adaptively schedules which entries are kept in memory.

The overall performance on FEVER and HotpotQA is summarized in Table 2. In the FEVER dataset, the baseline model achieved only 57 without memory, reflecting limited reasoning ability in the absence of external knowledge and prior memory. G-Memory improved the score to 62, but this gain came at the cost of a dramatic increase in the number of prompt tokens, leading to significantly higher inference costs. In contrast, SEDM achieved the highest score of 66 while consuming far fewer tokens than G-Memory. This demonstrates that our method successfully balances the trade-off between performance and efficiency through its memory selection and scheduling mechanisms.

In the HotpotQA dataset, the trend is similar to that observed in FEVER. The no-memory baseline scored only 34, while G-Memory increased the score to 38. SEDM further improved performance, reaching a score of 39 while simultaneously reducing computational overhead, confirming its effectiveness in multi-hop reasoning tasks.

Moreover, we evaluate the transfer ability of SEDM between FEVER and HotpotQA, two distinct downstream tasks. Specifically, the agent collects experience on the HotpotQA task using SEDM and then evaluates it on FEVER to measure knowledge transfer and prompting effects. Under this setting, the score on FEVER reached 64. Compared with G-Memory, which scored 62, and the no-memory baseline, which scored 57, our results demonstrate that SEDM enables adaptive memory selection that leverages previously collected experiences to improve performance across tasks.

# 4.4 Ablation Study

To evaluate the contribution of individual SEDM components, we conduct ablation studies on both HotpotQA and FEVER. Table 3 reports results under three configurations: (i) the baseline without memory, (ii) the addition of SCEC-based verifiable write admission $( + S C E C )$ , and (iii) the full SEDM with the memory controller’s self-scheduling mechanism (+SCEC + Self-Scheduling).

On HotpotQA, introducing $+ S C E C$ improves the score from 34 to 37, but also increases prompt tokens by $43 \%$ $2 . 4 6 \mathrm { M }  3 . 5 2 \mathrm { M } )$ and completion tokens from 29K to 52K. With +Self-Scheduling,

Table 2: Performance comparison on FEVER (fact verification) and HotpotQA (multi-hop reasoning). We report task accuracy (Score) along with efficiency metrics (Prompt Tokens and Completion Tokens). SEDM achieves the best accuracy on both benchmarks while substantially reducing token consumption compared with G-Memory, highlighting its ability to balance effectiveness and efficiency.   

<table><tr><td rowspan="2">Method</td><td colspan="3">FEVER</td><td colspan="3">HotpotQA</td></tr><tr><td>Score</td><td>Prompt Tokens</td><td>Completion Tokens</td><td>Score</td><td>Prompt Tokens</td><td>Completion Tokens</td></tr><tr><td>No Memory</td><td>57</td><td>1.65M</td><td>24K</td><td>34</td><td>2.46M</td><td>29K</td></tr><tr><td>G-Memory</td><td>62</td><td>3.62M</td><td>109K</td><td>38</td><td>4.63M</td><td>114K</td></tr><tr><td>SEDM (Ours)</td><td>66</td><td>2.47M</td><td>53K</td><td>39</td><td>3.88M</td><td>55K</td></tr></table>

Table 3: Ablation study on HotpotQA and FEVER, showing the progressive contribution of SEDM components.   

<table><tr><td>Dataset</td><td>Setting</td><td>Score</td><td>Prompt tokens</td><td>Completion tokens</td></tr><tr><td rowspan="3">HotpotQA</td><td>No Memory</td><td>34</td><td>2.46M</td><td>29K</td></tr><tr><td>+ SCEC</td><td>37</td><td>3.52M</td><td>52K</td></tr><tr><td>+ SCEC + Self-Scheduling</td><td>39</td><td>3.88M</td><td>55K</td></tr><tr><td rowspan="3">FEVER</td><td>No Memory</td><td>57</td><td>1.65M</td><td>24K</td></tr><tr><td>+ SCEC</td><td>64</td><td>2.19M</td><td>53K</td></tr><tr><td>+ SCEC + Self-Scheduling</td><td>66</td><td>2.47M</td><td>53K</td></tr></table>

the score further rises to 39, while prompt tokens grow only by $10 \%$ ( $3 . 5 2 \mathrm { M }  3 . 8 8 \mathrm { M } \mathrm { \Omega }$ , showing that scheduling effectively controls token overhead relative to the accuracy gain. On FEVER, the baseline achieves 57. Adding $+ S C E C$ raises the score to 64, accompanied by an increase in prompt tokens from 1.65M to 2.19M $( + 3 3 \% )$ and completion tokens from 24K to 53K. With scheduling, performance improves to 66, while prompt tokens rise only to 2.47M $( + 1 3 \% )$ , and completion tokens remain unchanged, confirming that the controller filters relevant memory without inflating responses.

In summary, across both datasets, SCEC consistently yields substantial accuracy gains at the cost of increased token usage, while the self-scheduling mechanism provides further improvements with relatively minor overhead. This demonstrates that SEDM not only enhances reasoning accuracy but also achieves a more favorable trade-off between performance and efficiency.

# 4.5 Cross-Domain Evaluation

To further assess the generalization ability of SEDM across domains, we conduct a cross-domain experiment in which memory is collected on one dataset and evaluated on another. Table 4 reports the results on FEVER, HotpotQA, and LoCoMo.

Table 4: Cross-domain evaluation of SEDM. Rows indicate the dataset used for memory collection, and columns indicate the dataset used for testing.   

<table><tr><td>Collect ↓/ Test →</td><td>FEVER</td><td>HotpotQA</td><td>LoCoMo</td></tr><tr><td>FEVER</td><td>66</td><td>41</td><td>38.1</td></tr><tr><td>HotpotQA</td><td>64</td><td>39</td><td>38.6</td></tr><tr><td>LoCoMo</td><td>65</td><td>34</td><td>37.6</td></tr></table>

FEVER as target: The best score is achieved by FEVER itself (66); memory from HotpotQA and LoCoMo yields slightly lower scores (64 and 65), indicating that fact-verification benefits most from in-domain memory.

HotpotQA as target: The highest score is obtained by FEVER HotpotQA (41), 2 points above the in-domain result (39). Memory from LoCoMo drops to 34, the lowest cross-domain score, suggesting that dialogue-grounded knowledge is least useful for multi-hop reasoning.

LoCoMo as target: All three sources perform within 1 point of each other (37.6–38.6). Thus, no single source dominates, and dialogue-grounded evaluation is remarkably robust to the origin of memory.

Overall, SEDM exhibits task-dependent transfer: factual-verification memory transfers surprisingly well to HotpotQA, whereas dialogue memory transfers poorly to the other two tasks. In-domain memory is not universally optimal, and the benefit of domain alignment varies significantly by task pair.

# 5 Conclusion

This paper introduces SEDM, Scalable Self-Evolving Distributed Memory, which transforms memory in multi-agent systems from a passive repository into an adaptive and verifiable component by integrating SCEC-based admission, self-scheduling refinement, and cross-domain knowledge diffusion. Through this principled design, SEDM addresses the challenges of noise accumulation, uncontrolled growth, and weak generalization that limit existing methods. Experiments on LoCoMo, FEVER, and HotpotQA confirm that SEDM improves reasoning accuracy while reducing computational and token overhead, demonstrating its potential as a scalable and sustainable memory mechanism for long-term multi-agent collaboration.

# References

[1] Andrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. On the resemblance and containment of documents. In Proceedings of the Compression and Complexity of Sequences (SEQUENCES), pages 21–29, 1997.   
[2] Peter Buneman, Sanjeev Khanna, and Wang-Chiew Tan. Why and where: A characterization of data provenance. In Proceedings of the 8th International Conference on Database Theory (ICDT), pages 316–330, 2001.   
[3] Lucian Bu¸soniu, Robert Babuška, and Bart De Schutter. A comprehensive survey of multiagent reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156–172, 2008.   
[4] Moses Charikar. Similarity estimation techniques from rounding algorithms. Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC), pages 380–388, 2002.   
[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. In ACL, pages 1870–1879, 2017.   
[6] Jiaan Chen, Shuaichen Chen, Yifan He, Qingyao Wang, Zheng Zhang, Yue Tang, Bing Qin, and Ting Liu. Timeqa: A question answering benchmark for temporal reasoning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8217–8234. Association for Computational Linguistics, 2021.   
[7] Xie Chen, Percy Liang, and Le Song. Hierarchical memory networks. arXiv preprint arXiv:1605.07427, 2016.   
[8] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory, 2025.   
[9] Fernando Chirigati, Rémi Rampin, Dennis Shasha, and Juliana Freire. Reprozip: Computational reproducibility with ease, 2016.   
[10] Abhishek Das, Satwik Kottur, José M. F. Moura, Stefan Lee, and Dhruv Batra. Learning cooperative visual dialog agents with deep reinforcement learning. In ICCV, pages 2951–2960, 2017.   
[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint arXiv:1805.04833, 2018.

[12] Jakob Foerster, Yannis M. Assael, Nando de Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. In NeurIPS, pages 2137–2145, 2016.   
[13] Jakob Foerster, Richard Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In AAMAS, pages 122–130, 2018.   
[14] Anirudh Goyal, Yoshua Bengio, Aaron Courville, and Shakir Mohamed. Abstraction in reinforcement learning: A state of the art survey. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pages 4394–4401, 2019.   
[15] Alex Graves, Greg Wayne, and Ivo Danihelka. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471–476, 2016.   
[16] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. A deep relevance matching model for ad-hoc retrieval. In CIKM, pages 55–64, 2016.   
[17] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In EACL, pages 874–880, 2021.   
[18] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus. In IEEE Transactions on Big Data, volume 7, pages 535–547, 2019.   
[19] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus, 2019.   
[20] Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. Reinforcement learning: A survey. Journal of Artificial Intelligence Research, 4:237–285, 1996.   
[21] Łukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio. Learning to remember rare events. In International Conference on Learning Representations (ICLR), 2017.   
[22] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP, pages 6769–6781, 2020.   
[23] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics (TACL), 7:452–466, 2019.   
[24] Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning through probabilistic program induction. In Science, volume 350, pages 1332–1338, 2015.   
[25] LangChain. Langmem: Memory framework in langchain for long-context reasoning. https: //www.langchain.com/, 2024.   
[26] Timothy Lebo, Satya Sahoo, Deborah McGuinness, Khalid Belhajjame, James Cheney, Daniel Garijo, Simon Miles, Stian Soiland-Reyes, Stephan Zednik, and Jun Zhao. Prov-o: The prov ontology. W3C Recommendation, 2013.   
[27] Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. A human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727, 2024.   
[28] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, 2020.   
[29] Chen Liang, Jonathan Berant, Quoc Le, Kenneth Forbus, and Ni Lao. Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), Volume 1 (Long Papers), pages 23–33, 2017.

[30] Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NeurIPS), pages 6379–6390, 2017.   
[31] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. Introduces the LoCoMo dataset with multi-turn dialogues (about 600 turns per dialogue, averaging 26,000 tokens) and question–answer (QA) pairs grounded in those dialogues.   
[32] Gurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. Detecting near-duplicates for web crawling. In Proceedings of the 16th International Conference on World Wide Web (WWW), pages 141–150, 2007.   
[33] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 17359–17372, 2022.   
[34] Dirk Merkel. Docker: Lightweight linux containers for consistent development and deployment, 2014.   
[35] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christos Nalmpantis, Alexandre Ramé, Vivek Jayaram, Ugur Dogan, Yi Tay Wang, Thomas Scialom, Timo Schick, Roberta Raileanu, Bap- ˘ tiste Roziere, Xavier Bresson, Hervé Jegou, Hugo Touvron, Edouard Grave, Armand Joulin, Guillaume Lample, and Koustuv Sinha. Augmented language models: a survey. arXiv preprint arXiv:2302.07842, 2023.   
[36] Jianmo Ni, Chenghao Lu, Jing Ma, Bo Huang, Adam McLean, Zeyu Xu, Eric Wallace, and Wentau Yih. Large dual encoders are generalizable retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9844–9855, 2022.   
[37] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert, 2019.   
[38] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.   
[39] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems, 2024.   
[40] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318. Association for Computational Linguistics, 2002.   
[41] Emilio Parisotto, Jack Song, and Yann Dauphin. Stabilizing transformers for reinforcement learning. arXiv preprint arXiv:1910.06764, 2019.   
[42] Joon Sung Park, Carrie J O’Brien, Carrie J Cai, Meredith Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST), pages 1–22, 2023.   
[43] Ofir Press, Noah A. Smith, and Omer Levy. Improving transformer models by reordering their sublayers. In ACL, pages 2996–3005, 2020.   
[44] Zhen Qin, Rolf Jagerman, Kai Hui, Chenyan Xiong, Bhaskar Mitra, Fernando Diaz, and Nick Craswell. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint arXiv:2306.17563, 2023.   
[45] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.   
[46] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: $1 0 0 { , } 0 0 0 { + }$ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2383–2392. Association for Computational Linguistics, 2016.

[47] Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, and Daniel Chalef. Zep: A temporal knowledge graph architecture for agent memory, 2025.   
[48] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3982–3992, 2019.   
[49] Shuhuai Ren, Yuchen Qu, Jing Liu, Wayne Xin Zhao, Qi She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2825–2835, 2021.   
[50] Noah Shinn, Brandon Labash, and Ashwin Gopinath. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.   
[51] Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent systems: A survey. Foundations and Trends in Artificial Intelligence, 1(1–2):1–122, 2007.   
[52] Kurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Douwe Kiela, and Jason Weston. The dialogue dodecathlon: Open-domain knowledge and image grounding for multi-task dialogue systems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 2453–2470, 2020.   
[53] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERification. In Marilyn Walker, Heng Ji, and Amanda Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.   
[54] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network learning. In International Conference on Learning Representations (ICLR), 2019.   
[55] Guanzhi Wang, Shunyu Wang, Yuxiang Wang, Xufeng Liu, Chuanqi Chen, Yunfan Ling, Jiaming Wu, Yutong Li, Han Yu, Zhiyu Dai, Zhiwei Liu, Xin Wang, Jiajun Li, Yizhou Wu, Leonidas Guibas, Li Fei-Fei, and Yuke Zhu. Voyager: An open-ended embodied agent with large language models, 2023.   
[56] Tonghan Wang, Jianhao Zhang, Yi Wu, and Chongjie Wang. Influence-based multi-agent exploration. In ICLR, 2020.   
[57] Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Augmenting language models with long-term memory, 2023.   
[58] Michael Wooldridge. An Introduction to MultiAgent Systems. John Wiley & Sons, 2 edition, 2009.   
[59] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents, 2025.   
[60] Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, and Weinan Zhang. Mean field multi-agent reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning (ICML), pages 5571–5580, 2018.   
[61] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018.   
[62] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2369–2380. Association for Computational Linguistics, 2018.

[63] Guibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. Gmemory: Tracing hierarchical memory for multi-agent systems, 2025.   
[64] Zhao Zhang, Ruichu Cai, Ying Xu, Kun Zhang, Shoujin Wang, and Qiang Yang. Duplicate question detection with deep learning. In Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI), pages 5469–5475, 2019.   
[65] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724–19731, 2024.