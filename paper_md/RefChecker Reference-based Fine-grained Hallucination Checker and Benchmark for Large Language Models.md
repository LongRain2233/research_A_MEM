# REFCHECKER: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models

Xiangkun $\mathbf { H } \mathbf { u } ^ { 1 }$ , Dongyu $\mathbf { R } \mathbf { u } ^ { 1 }$ , Lin Qiu1, Qipeng $\mathbf { G u o } ^ { 2 }$ , Tianhang Zhang1 Yang $\mathbf { X } \mathbf { u } ^ { 3 }$ , $\mathbf { Y u n L u o ^ { 4 } }$ , Pengfei Liu3, Yue Zhang4 and Zheng Zhang1

1Amazon AWS AI 2Shanghai AI Lab

3Shanghai Jiaotong University 4Westlake University

{xiangkhu, rudongyu, quln, zzthang, zhaz}@amazon.com, guoqipeng@pjlab.org.cn

{xuyang0112, pengfei}@sjtu.edu.cn, {luoyun, yue.zhang}@wias.org.cn

# Abstract

Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents REFCHECKER, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In REFCHECKER, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. REFCHECKER supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. REFCHECKER outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of REFCHECKER are strongly aligned with human judgments1.

# 1 Introduction

Large Language Models (LLMs) have sparked a revolution in Natural Language Processing (NLP), covering diverse tasks with a unified architecture (Zhao et al., 2023). However, LLMs exhibit a tendency to generate hallucinated contents that can be difficult to discern, posing a potential risk of misleading users. (Huang et al., 2023). Hallucination detection is therefore an important task (Manakul et al., 2023; Min et al., 2023; Chern et al., 2023).

Detecting hallucination is essentially a job of comparing a response against a reference. However, several challenges remain: determining the appropriate unit of analysis for comparison, building a comprehensive benchmark reflecting real-

… The song 'I Ran (So Far Away)' was originally performed by the English new wave band A Flock of Seagulls in 1983.

The song is 'I Ran (So Far Away)’.   
'I Ran (So Far Away)' was originally performed.   
'I Ran (So Far Away)' was originally performed by A Flock of Seagulls.   
A Flock of Seagulls is an English new wave band.   
A Flock of Seagulls performed 'I Ran (So Far Away)' in 1983.

I Ran (So Far Away) originally performed by A Flock of Seagulls

I Ran (So Far Away) released in

A Flock of Seagulls is

English new wave band

Figure 1: An example response split into sentence, subsentence (Min et al., 2023), triplets, and the hallucination 1983. Triplets define the boundary of claims more clearly, are fine-grained and covers non-overlapping facts (unlike sub-sentences).

world LLM applications, developing a unified, automated framework that scales detection across diverse tasks. In this work, we introduce RE-FCHECKER to tackle these challenges.

In terms of checking granularity, response level checking (Lin et al., 2022; Li et al., 2023) suffices if the query and response is about a simple fact. However, when responses are complex and long, response-level checking is not only uninformative but can also cause false-negative when hallucination is local. This is common in realworld use cases, for example, the response from Llama 2 (Touvron et al., 2023) in our benchmark (described later) contains 150 tokens on average. For fine-grained detection, Manakul et al. (2023) takes the sentences in a response as the checking units. Min et al. (2023) and Chern et al. (2023) further extracts short phrases (we term them as subsentences) as the claims, as one sentence may contain multiple hallucinations, or one hallucination may span across sentence boundaries. However, sub-sentences are structurally difficult to define, making it challenging to form high-quality demonstrations to be used by LLMs with in-context learning. To this end, we propose to extract knowledge triplets as checking units. We show an example with different granularity in Figure 1. Triplets exhibit fine-grained and clearly separated semantics. These triplets are called claim-triplets. Experi-

![](images/b713a5561431ce602a53e8617ed0300232bbe8453ef0041854e7723e3bb96c12.jpg)  
Figure 2: The REFCHECKER framework comprises two main components: an extractor denoted as $E$ and a checker denoted as $C$ . Given a text to be checked, typically a response generated by an LLM, the extractor takes it as input and generates a set of knowledge triplets, referred to as claim-triplets. Subsequently, the checker assesses each claim-triplet by comparing it against a reference, assigning a hallucination label based on the evaluation.

ments show that checking with claim-triplets gains 4 to 9 points of improvement over other granularity on our benchmark (cf. Sec. 5.1).

For better evaluation, we curate a comprehensive dataset on which we can benchmark hallucination under different context quality and availability. Using this benchmark, we conducted human evaluation on 2,100 responses from 7 LLMs. We annotated 11k claim-triplets with $9 5 \%$ Inter-Annotator Agreement on $23 \%$ of the annotations. Compared with recent proposed benchmarks (Manakul et al., 2023; Min et al., 2023; Chern et al., 2023), it covers a more diverse range of domains and tasks, with more LLMs and responses evaluated (see Table 1). As expected, we found by human evaluation that hallucination is the most pronounced (cf. Appendix A.4) when LLMs are asked to generate responses solely from its memory (Zero Context), followed by responding to noisy references in RAG (retrieval augmented generation) setting (Shuster et al., 2021) (Noisy Context) and finally when references are more or less noisy free (Accurate Context).

REFCHECKER (Figure 2) is a fully automated framework that scales hallucination detection across different tasks. The extractor generates claim-triplets from the response and the checker evaluates each of the claim-triplets by comparing them with the reference. In contrast to recent work that only differentiates factual and non-factual claims, the checker in REFCHECKER also considers unverifiable claims when the reference is insufficient for checking. Both the extractor and checker supports proprietary (e.g. GPT-4 (OpenAI et al., 2023) or Claude 2 (Anthropic, 2023)) and opensource models (e.g. Mistral (Jiang et al., 2023) and RoBERTa (Liu et al., 2019) based models). We made careful study to select and recommend configurations that give results consistent with human

annotation, and show 6.8 to 26.1 points of improvement over the best alternative (Sec. 5.2).

Our key contributions include:

• Claim-triplet formulation: Our novel “claim-triplet” analysis outperforms existing methods by up to 9 points, pinpointing factual inconsistencies within responses.   
• Comprehensive benchmark: We developed a robust benchmark covering three classes of real-world LLM tasks with 11,000 manually annotated claim-triplets across 7 LLMs.   
• Automatic checking framework: Our RE-FCHECKER framework extracts and verifies claim-triplets, boosting consistency by 19-26 points over prior methods and works with both proprietary and open-source models.

# 2 Related Work

We undertake a review of prior work relevant to our study and compare them with REFCHECKER. The comparative analysis with three representative methods is encapsulated in Table 1.

Hallucinations in LLMs Hallucinations frequently occur in NLP tasks like summarization (Maynez et al., 2020; Cao et al., 2022), machine translation (Guerreiro et al., 2023a,b), dialog systems (Honovich et al., 2021; Dziri et al., 2022) and RAG (Shuster et al., 2021). According to a recent comprehensive survey (Huang et al., 2023), hallucinations can be categorized to factuality hallucinations and faithfulness hallucinations. Factuality hallucinations involve claims contradicted by realworld facts, while faithfulness hallucinations are inconsistent with the input content. Recent research on hallucination detection primarily concentrates on factuality hallucinations, such as SelfCheck-GPT (Manakul et al., 2023), FActScore (Min et al.,

Table 1: A comparison of REFCHECKER with previous approaches for hallucination detection. The “*” symbols alongside the extractors and checkers indicate that these models are open-sourced. REFCHECKER uses triplets as the claims instead of sentences or sub-sentences. The REFCHECKER benchmark covers more context settings and more diverse tasks. The human evaluation covers more LLMs and responses. REFCHECKER pipeline supports both proprietary and open-source models, facilitating broader adoption across various applications.   

<table><tr><td rowspan="2">Method</td><td rowspan="2">Context Setting</td><td colspan="2">Claim Extraction</td><td colspan="2">Checking</td><td colspan="3">Benchmark</td></tr><tr><td>Claim</td><td>Extractor</td><td>Label</td><td>Checker</td><td>Domain</td><td>Task</td><td>Evaluated Responses</td></tr><tr><td>SelfCheckGPT</td><td>Zero Context</td><td>Sentence</td><td>-</td><td>3-way</td><td>GPT</td><td>Wikipedia</td><td>Bio Generation</td><td>238 from GPT-3</td></tr><tr><td>FActScore</td><td>Zero Context</td><td>Sub-sentence</td><td>GPT</td><td>Binary</td><td>GPT</td><td>Wikipedia</td><td>Bio Generation</td><td>505 from 3 LLMs</td></tr><tr><td>FacTool</td><td>Zero Context</td><td>Sub-sentence Snippet Statement Tuple</td><td>GPT</td><td>Binary</td><td>GPT</td><td>Wikipedia Python Math Sci. Text</td><td>QA Code Generation Math Problems Sci. Review</td><td>514 from ChatGPT</td></tr><tr><td>REFCHECKER</td><td>Zero Context Noisy Context Accurate Context</td><td>Triplet</td><td>GPT Claude Mixtral* Mistral*</td><td>3-way</td><td>GPT Claude AlignScore* NLI* RepC*</td><td>Wikipedia Web</td><td>QA RAG Summarization IE</td><td>2,100 from 7 LLMs</td></tr></table>

2023) and FacTool (Chern et al., 2023). We address both factuality and faithfulness hallucinations and further categorizing them into three contextual settings to align with real-world use cases.

Granularity of Claims Claims are pivotal for evaluating responses generated by LLMs. Response level checking (Lin et al., 2022; Li et al., 2023) is too coarse-grained for long-form responses. For fine-grained detection, sentence level (Manakul et al., 2023) and sub-sentence level checking (Min et al., 2023; Chern et al., 2023) have been proposed. However, these approaches still face limitations, as discussed in Sec. 1. In this paper, we employ knowledge triplets extracted from responses as claims which provide a structured framework for defining claim granularity.

Hallucination Checking One line of work for hallucination checking focus on resource-free checking with no requirements of reference. They mainly depend on self-contradiction (Mündler et al., 2023) or uncertainty (Zhang et al., 2023b) of the LLMs, or self-consistency between randomly sampled responses (Manakul et al., 2023; Chen and Mueller, 2023; Zhang et al., 2023a). The effectiveness of these methods depends on the LLM-based checker’s capability including knowledge coverage, instruction following ability and calibration. Furthermore, the self-consistency based methods needs to sample several responses for cross checking, which is costly. REFCHECKER is aligned with another line of work which requires references to check with (Min et al., 2023; Chern et al., 2023). In addition, REFCHECKER adopts a 3-way classification framework to cover unverifiable claims as opposed to the binary classification used in previous work, which can only distinguish factual and non-factual claims. Moreover, we have introduced

open-source solutions for both claim extraction and verification, diverging from the predominant reliance on proprietary LLMs in prior research.

Hallucination Detection Benchmarks The existing benchmarks for hallucination detection primarily focus on response-level detection (Lin et al., 2022; Yang et al., 2023), or limited to specific domains and tasks (Manakul et al., 2023; Min et al., 2023), or solely address factuality hallucinations (Chen et al., 2023; Chern et al., 2023; Wang et al., 2023). In contrast, our proposed benchmark offers a broader scope, encompassing a diverse range of tasks and domains. Moreover, our human evaluation process involves a more extensive examination of various LLMs with more responses.

# 3 REFCHECKER: Definition and Benchmark

Hallucinations are claims made by LLMs not supported by factual knowledge, which we refer to as references; detecting hallucinations involves comparing the claims against the references. This process depends on context settings, granularity of checking and how we categorize the hallucinations. We will discuss them in turn.

# 3.1 Context Settings and Benchmarks

We differentiate three context settings covering various tasks and employ different benchmarks for each setting.

Zero Context (ZC) Tasks in this setting require the LLM to respond solely based on its internal knowledge. Therefore, in principle, references should be in the training corpus. In practise, we use NaturalQuestions (NQ) (Kwiatkowski et al., 2019) as proxy dataset to represent such stable knowledge. The knowledge seeking questions from NQ

![](images/0e3df31cb6b21740afec01f06d228dc00a0d9dd6a431919cca08af810ee3184b.jpg)

![](images/8aad78ca02c11980c6191b0a667a52df94fff2cd2a5fc9aa43767a6723c1d5b4.jpg)

![](images/bde8c198452512d862b2d685ac89b15b45607be9f1387dfbeab91dcc287139cb.jpg)  
Figure 3: Illustration of three settings of context, tasks and references. Zero Context is about seeking factual knowledge from the internal memory of the LLMs. Noisy Context has context information retrieved from a knowledge source, which is a RAG use case. Accurate Context has context provided in the input prompt. For Noisy and Accurate Context, we take the input context as the reference.

serve as prompts to perform the closed-book question answering task, and the annotated long answer as the reference for a fair evaluation of LLMs and comparison between different approaches. These references are paragraphs from Wikipedia articles, which are widely used for pre-training LLMs (Zhao et al., 2023), making them a suitable proxy for the internal knowledge of these LLMs.

Noisy Context (NC) In this setup, the LLM receives additional context retrieved from some external knowledge source, which may contain noisy or irrelevant information. NC is also known as RAG, a crucial use case frequently encountered in realworld applications. We utilize questions sourced from MS MARCO (Nguyen et al., 2016). Each question in this dataset is accompanied by a list of documents retrieved from the internet, serving as the input context.

Accurate Context (AC) This setting is similar to NC but the reference is typically noise-free. Examples include text summarization, closed-QA and information extraction tasks. We employ a subset from the databricks-dolly-15k (Conover et al., 2023) instruction tuning dataset which covers the 3 tasks mentioned before.

For both Noisy and Accurate Context settings, the prompt is the reference followed by the query, whereas for Zero Context, the prompt is just the query (Figure 3). The benchmark contains 300 examples in total, 100 for each setting, and we summarize it in Table 6. The details of the benchmark curation process are described in Appendix A.

# 3.2 Claim-Triplets and Definition of Hallucination

Informally, claims are the units for the checking. This work explores the approach of representing claims with knowledge triplets. This concept is inspired by the field of knowledge graph studies,

![](images/ebd7237073464766a7ad4f837fb77246a18110facc20358e3e921b532b9f72df.jpg)  
Figure 4: Definition of fine-grained hallucinations in an LLM-generated response compared with references. The intersections of the response and the references are the claims either supported (Entailment) or refuted (Contradiction) by the references. The remaining parts in the response are claims not verifiable by the references (Neutral). The other parts of the references are the content not mentioned in the response.

where triplets are employed to encapsulate factual knowledge units. Knowledge triplets adopt a (head_entity, relation, tail_entity) structure to capture fine-grained information within the response. We call the triplet-format claims as claimtriplets, examples of which are shown in Figure 2.

Subsequently, the claim-triplets are compared with a reference to determine the type of hallucinations, as illustrated in Figure 4. If a claim-triplet can be directly inferred from the reference, we classify it as Entailment. Conversely, if it contradicts the information in the reference, it is labeled as Contradiction. However, in cases where the reference is insufficient to verify the claim-triplets, we classify it as Neutral. In this study, we focus on verifying hallucinations in the response and do not consider unmentioned aspects in the reference, which may also be important for certain tasks.

# 3.3 Human Evaluation

We performed a human evaluation of responses generated by seven LLMs on this benchmark dataset, including GPT-4, GPT-3.5-Turbo (OpenAI, 2022), InstructGPT (text-davinci-001) (Ouyang et al., 2022), Claude 2, Llama 2 70B Chat, Falcon

40B Instruct (Almazrouei et al., 2023) and Alpaca 7B (Taori et al., 2023). The process involves three steps: gathering responses, extracting claim-triplets with an extractor, and asking human annotators to evaluate these claim-triplets. We annotated a total of 11k claim-triplets for 2.1k responses. $23 \%$ of the claim-triplets were double annotated, with $9 5 . 0 \%$ Inter-Annotator Agreement. See Appendix A.3 for the details of the annotation process.

A significant finding from the human evaluation is the crucial role of contextual information for factual responses. On average, the rate of Contradiction decreased from $25 \%$ in the absence of contextual cues (ZC) to $13 \%$ with NC, and further reduced to $6 \%$ with AC, which reflects the necessity of distinguishing the three context settings for separate study (cf. Appendix A.4).

# 4 REFCHECKER Framework

As illustrated in Figure 2, the REFCHECKER framework is designed as a 2-stage pipeline: an Extractor $E$ decomposes the LLM response into a set of triplets, with each of them verified by the Checker $C$ . The categorization of the triplets can be optionally aggregated according to specified rules. We explain them in the subsequent subsections.

# 4.1 Extractor

Our checking framework hinges on a key assumption: the decomposition of the original text into triplets facilitates finer-grained detection and more accurate evaluation. The extraction of these triplets plays a pivotal role in achieving this objective. We apply LLMs to extract knowledge triplets from the given text. We began with GPT-4 and Claude 2 and, for both cost and efficiency concern, Mixtral 8x7B and Mistral. More specifically, we performed knowledge distillation to train a 7B Mistral-based extractor with Mixtral ${ \bf 8 x 7 B }$ as the teacher. We conducted supervised fine-tuning on 10k responses. Evaluation in Sec. 6.1 shows competitive extraction quality of the open-source extractor. Please refer to Appendix B.1 for prompts used for extraction and details on extractor training.

# 4.2 Checker

We experimented with two families of checkers, the first is off-the-shelf LLMs including GPT-4 and Claude 2 (see Appendix B.2 for prompts), and the second is smaller NLI models including Align-

Score (Zha et al., 2023) and RoBERTa-NLI.2 Long references in AC/NC setting are split to fit into small context windows of these small models (e.g. 200 tokens), and the results are aggregated later.

Mistral 7B (Jiang et al., 2023), unlike GPT-4/Claude 2, performs poorly as a zero-shot checker, its performance improves with demonstrations but is still not satisfactory. However, the fact that the model weight is open gives us the opportunity to improve it by fine-tuning with NLI data. There are many options we have experimented: 1) fine-tune by adding small amount of new parameters using LoRA (LoRA-sft) (Hu et al., 2021), 2) attach a shallow classifier, eg. SVM, 2-layer MLP, KNN after NCA projection (Goldberger et al., 2004), on top of the internal states of the model. We call such checker RepC (for Representation-based Classifier). Such states can be selected from one layer (layer selection, LS) or an ensemble of all layers (layer ensemble, LE). As we will report in Sec. 6.2, RepC checkers are competitive in general.

# 4.3 Aggregation

Triplet results can be aggregated to obtain the ratio of each category, therefore gives an overall measure of hallucination distribution in a response. To derive the performance of a particular LLM, we take a macro average on Entailment/Neutral/Contradiction ratios of all responses. If a scalar is preferred, we can assign certain numeric values to the catogories, for instance $- 1 , 0 , 1$ for contradictory, neutral and entail, respectively.

The aggregation can be customized and this is one of the benefits of the fine-grained hallucination design in REFCHECKER. For instance, to compare against other response-level approaches (cf. Sec. 5.1), we adopt a rule where the response is flagged as contradictory if any one of the claim triplet is contradictory.

# 5 Experiments

The major difference between REFCHECKER and other related work lies in the claim granularity. So we conduct experiments to differentiate granularity first, then evaluate the whole framework.

# 5.1 Comparing with Other Granularity

Previous works use different granularity for hallucination detection, including response, sentence

Table 2: Automated checking results of REFCHECKER and previous approaches. The results of REFCHECKER are from the best performing combinations (extractor $^ +$ checker) of purely proprietary (blue) and purely open-source models (orange).   

<table><tr><td></td><td colspan="2">Zero Context</td><td colspan="2">Noisy Context</td><td colspan="2">Accurate Context</td></tr><tr><td></td><td>Pearson</td><td>Spearman</td><td>Pearson</td><td>Spearman</td><td>Pearson</td><td>Spearman</td></tr><tr><td>SelfCheckGPT</td><td>35.40</td><td>43.15</td><td>36.31</td><td>32.15</td><td>40.23</td><td>32.55</td></tr><tr><td>FActScore</td><td>42.58</td><td>45.60</td><td>33.36</td><td>29.91</td><td>27.80</td><td>27.05</td></tr><tr><td>FacTool</td><td>59.78</td><td>62.57</td><td>46.35</td><td>38.69</td><td>31.41</td><td>32.82</td></tr><tr><td>REFCHECKER</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Claud 2 + GPT-4</td><td>83.69</td><td>82.99</td><td>53.14</td><td>47.89</td><td>60.99</td><td>58.96</td></tr><tr><td>Mistral-SFT + AlignScore</td><td>75.81</td><td>74.16</td><td>53.88</td><td>45.09</td><td>46.34</td><td>43.22</td></tr></table>

![](images/de28b371ceadf68e63c872ef0e0fcddba51c91a1de973edc27776b19755af3d7.jpg)  
Figure 5: Performance statistics of 7 checkers under different claim granularities on 2.1k manual annotated responses. The detailed checker performance can be found in Table 7 of Appendix B.2.

and sub-sentence levels (cf. Sec. 2 and 3.2). We compare with them to verify the effectiveness of checking on facts with the triplet format.

To make the results with different granularities comparable to each other, we first breakdown the 2.1k annotated responses into different granularities, then collect corresponding checker predictions respectively, and finally aggregate finer-level results all into the response-level. We utilize a strict aggregation rule with zero-tolerance on hallucinations, which means we apply max-pooling (Entailment $<$ Neutral $<$ Contradiction) over claim predictions within a response. We compare the results of 7 checkers, including 4 baseline checkers (RoBERTa-NLI, AlignScore, GPT-4 and Claude 2) and 3 RepC-LE checkers with KNN, SVM and 2- layer MLP classifiers respectively. The evaluation metric is macro-f1 on three categories.

As shown in Figure 5, checking at triplet-level claims is superior over other granularities, with a significant lead against response-level (10 pts macro-f1 score on average). Checking at sentencelevel improves over response-level by 5 pts. However, we see a 3.5 pts drop moving to sub-sentence, one of the reasons being sub-sentence claims can overlap. Apparently, the flexibility of subsentences leads to poor quality of claim extraction,

which subsequently affects checking.

# 5.2 Comparing with Other Approaches

To contrast with other hallucination detection methodologies, we compare REFCHECKER with three popular approaches on our benchmark, Self-CheckGPT (Manakul et al., 2023), FActScore (Min et al., 2023) and FacTool (Chern et al., 2023). We convert metrics in these approaches to “hallucination rates” as follows:

• SelfCheckGPT: the average score of the sentences within a response. The score is 0, 0.5 and 1 for an accurate, minor_inaccurate, and major_inaccurate, respectively.   
• FactScore/FacTool: the proportion of claims not supported (FactScore) or non-factual (FacTool) by the reference.   
• REFCHECKER: the proportion of Neutral and Contradiction claims.

Table 2 presents the Pearson and Spearman correlations between the hallucinations rates as evaluated by human and the model results. It reveals that REFCHECKER significantly outperforms previous methods across all three context settings with both proprietary and open-source models. Specifically, the combination of Claude $2 + \mathrm { G P T } { \cdot } 4$ outperforms the best alternative, FacTool, by 6.8 to 26.1 points. We additionally apply REFCHECKER to SelfCheck-GPT’s dataset, and observe that in 11 out of the 15 combinations $(73 \% )$ of REFCHECKER outperform SelfCheckGPT (cf. Table 13 in Appendix C).

# 5.3 Evaluation on REFCHECKER Framework

To validate the robustness and reliability of RE-FCHECKER, we checked 7 LLMs’ responses on our benchmark and compared rankings (ratios macro-averaged on responses of each LLM) by REFCHECKER and humans with Spearman’s rank correlation coefficient. The configuration space consists of different combinations of extractor $^ +$ checker, and also the 3 task settings as well as their

![](images/0f4e7111acda76d86a3dc283e61c7178598625e08ddbb23fd502e15e527b7ee7.jpg)  
Figure 6: Spearman’s rank correlation coefficients between REFCHECKER and human evaluation. Results are grouped regarding extractors and checkers used. Results for each extractor (column) and checker (row) are arranged into a sub-matrix in the figure, with values for 4 context settings (one additional for average) and 3 ranking metrics.

averages. The results are reported in Figure 6.

We observe that the combination of Claude 2 $+ \mathrm { G P T } { - } 4$ is the most competitive option with very strong correlations across all settings, benefiting from more powerful LLMs. Replacing the extractor with our Mistral alternative yields a marginal decline of performance, yet still maintains very strong correlations in most settings. The best nonproprietary combination is Mistral $^ +$ NLI/Align-Score checker (356M), which achieve consistently strong correlations in all settings. The Mistral-RepC checker is robust against different extractors, owing to its stronger reasoning capability than small NLI-based checker. This result serves as a guide for choosing a configuration tailored to the user’s preferences. These preferences may include factors such as budget, deployment simplicity, specific settings, types of hallucination, privacy and requirements for open-source models.

# 6 Analysis and Discussion

We evaluate each component in REFCHECKER separately and discuss its limitations and corresponding future work in this part.

# 6.1 Evaluation on Extractors

To ensure precise hallucination detection, it requires precise claims that faithfully represent the

<table><tr><td rowspan="2">Extractor Model</td><td colspan="3">GPT-4 Turbo Evaluation</td><td colspan="3">Human Evaluation</td></tr><tr><td>Precision</td><td>Recall</td><td>F1</td><td>Precision</td><td>Recall</td><td>F1</td></tr><tr><td>GPT-4</td><td>97.2</td><td>92.5</td><td>94.2</td><td>98.2</td><td>92.2</td><td>94.8</td></tr><tr><td>Claude 2</td><td>95.5</td><td>91.8</td><td>93.0</td><td>97.4</td><td>94.5</td><td>95.7</td></tr><tr><td>Mixtral</td><td>87.7</td><td>85.2</td><td>85.5</td><td>87.6</td><td>85.5</td><td>85.4</td></tr></table>

Table 3: Claim extraction evaluation by GPT-4 Turbo and human on 30 samples.   
Table 4: Automatic evaluation results of extractors. Mistral-SFT refers to our Mistral-based extractor after supervised fine-tuning. The other extractors directly prompt corresponding LLMs with two in-context examples. The best and the second best results are bolded and underlined, respectively.   

<table><tr><td>Extractor Model</td><td>Precision</td><td>Recall</td><td>F1</td><td>Speed (sec/iter)</td></tr><tr><td>Mistral</td><td>82.2</td><td>68.2</td><td>71.3</td><td>1.7</td></tr><tr><td>Mistral-SFT</td><td>90.5</td><td>84.8</td><td>86.4</td><td>1.7</td></tr><tr><td>Mixtral</td><td>86.7</td><td>80.2</td><td>81.6</td><td>5.7</td></tr><tr><td>Claude 2</td><td>89.8</td><td>86.6</td><td>87.0</td><td>6.9</td></tr><tr><td>GPT-4</td><td>92.4</td><td>88.6</td><td>89.3</td><td>8.7</td></tr></table>

facts in the original response. Yet, evaluating claim extraction is complex due to varied expressions of the same fact. To address this, we employ an automatic evaluation pipeline utilizing GPT-4 Turbo (gpt-4-1106-preview) to lessen the need for posthoc human evaluation for each extractor.

We employed GPT-4 Turbo to label each extracted claim as True/False, indicating faithfulness to the original semantics. Additionally, we tasked it with completing missing claims, enabling automatic calculation of precision, recall, and F1 score. To validate results, we conducted a human evaluation on 30 random samples, ensuring agreement between human annotators and the model. The comparison in Table 3 demonstrates strong alignment between human and automatic evaluations, achieving $9 3 . 7 \%$ agreement on precision and $9 1 . 9 \%$ on recall.

Leveraging the reliability of our automatic evaluation pipeline, we evaluated the performance of four extractors (see Table 4). Our open-source Mistral extractor achieves performance comparable to Claude 2 extractor with faster inference speed and no need for API tokens.

# 6.2 Evaluation on Checkers

As described in Sec. 4.2, the baseline checkers we include in the evaluation are RoBERTa-NLI, AlignScore, GPT-4 and Claude 2. The Mistralbased checkers we include are zero-shot prompted, one-shot prompted, LoRA fine-tuned and RepC-LE variants. The training and development data

Table 5: Checker evaluation results on 11k human annotated claim triplets. In Mistral-based checkers, the model names start with the variant types, eg. LoRA-sft indicates the LoRA fine-tuned variant and RepC-LE-nn indicates the representation based classification variant using layer ensemble with 2-layer MLP as the classifier. Here “nxxx” and “exxx” indicates the number of training samples and ensemble learning samples. Due to the space limitation, we do not include all variant results here, please refer to Table 8 of Appendix B.2 for full results.   

<table><tr><td rowspan="2">Models</td><td colspan="2">Average of three settings</td><td colspan="2">Zero-context (NQ)</td><td colspan="2">Noisy-context (MS MARCO)</td><td colspan="2">Accurate-context (databricks-dolly-15k)</td></tr><tr><td>Accuracy</td><td>Macro-F1</td><td>Accuracy</td><td>Macro-F1</td><td>Accuracy</td><td>Macro-F1</td><td>Accuracy</td><td>Macro-F1</td></tr><tr><td colspan="9">Baseline Checkers</td></tr><tr><td>RoBERTa-NLI</td><td>76.56</td><td>55.88</td><td>74.06</td><td>69.90</td><td>78.36</td><td>46.67</td><td>77.27</td><td>51.06</td></tr><tr><td>AlignScore</td><td>78.85</td><td>59.45</td><td>73.40</td><td>70.28</td><td>78.86</td><td>50.42</td><td>84.30</td><td>57.66</td></tr><tr><td>GPT-4</td><td>74.77</td><td>59.80</td><td>67.46</td><td>66.10</td><td>76.67</td><td>55.49</td><td>80.17</td><td>57.80</td></tr><tr><td>Claude 2</td><td>51.98</td><td>36.55</td><td>43.42</td><td>42.90</td><td>40.35</td><td>25.89</td><td>72.18</td><td>40.87</td></tr><tr><td colspan="9">Mistral-based Checkers</td></tr><tr><td>zero-shot</td><td>69.43</td><td>46.64</td><td>70.83</td><td>61.10</td><td>71.75</td><td>43.01</td><td>65.72</td><td>35.81</td></tr><tr><td>1-shot</td><td>76.68</td><td>50.66</td><td>65.44</td><td>63.25</td><td>81.23</td><td>42.18</td><td>83.38</td><td>46.56</td></tr><tr><td>LoRA-sft-n4000</td><td>77.84</td><td>57.98</td><td>77.43</td><td>73.64</td><td>79.21</td><td>50.29</td><td>76.89</td><td>50.00</td></tr><tr><td>RepC-LE-svm-n1000-e1000</td><td>79.03</td><td>60.05</td><td>77.98</td><td>73.53</td><td>79.56</td><td>51.29</td><td>79.54</td><td>55.34</td></tr><tr><td>RepC-LE-nn-n2000-e2000</td><td>81.27</td><td>60.80</td><td>75.23</td><td>71.98</td><td>82.08</td><td>47.56</td><td>86.50</td><td>62.86</td></tr></table>

of these variants are 4k samples from the ANLI dataset (Nie et al., 2020). We evaluate their performance using the 11k manually annotated claim triplets. The evaluation metric is accuracy and macro-f1 score over 3-way classification.

Table 5 shows the evaluation results. Among the baseline checkers, AlignScore is a strong competitor to GPT-4, and Claude 2 has a significant gap. We found Claude 2’s neutral F1 score is very low (less than $20 \%$ ) (cf. Table 9 in Appendix B.2), with a tendency to flag neutral claims as contradiction, as a result of biasing towards its own internal knowledge when asked to perform checking (cf. Appendix D).

Besides, the Mistral-based checkers can often gives the best performance, though there does not yet exist a single winner across the board. The weakness of Mistral-based checkers lies in the NC setting. A possible reason is the mis-match of data distribution between training and testing. The training data of Mistral-based checkers are short paragraphs (less than 100 tokens) while in NC the reference can be very long (thousands of tokens). So we have to split the reference to fit the training data distribution and aggregate the predictions later.

However, there are clear gaps between the performance of the checkers on NC and AC in contrast to their performance on ZC, which suggests ample room of improvement for the checkers.

# 6.3 Future Work

There remain several limitations of REFCHECKER that require addressing: a) The triplet format of claims, while effectively breaking down LLM responses into finer granularity, can be overly restric-

tive and may not have the flexibility to cover important semantics (consider (Trump, president of, US), which is factual in 2018, but non-factual in 2022). b) While extraction is relatively simple with the outstanding comprehension capability of LLMs, there exists a large improvement space for more powerful checkers. c) REFCHECKER has a rudimentary support for source attribution (cf. Appendix B.3). Better source attribution is critical to lend explainable and provide training signal to mitigate hallucination. d) We found that modelbased checkers may exhibit bias towards internal knowledge, declaring a neutral claim to be entailment or contradiction (cf. Table 9 and Appendix D). This requires we inject some forms of “source control” in LLMs. e) In actual deployment cases, we found users ask for stronger customizability (e.g. they would like to use REFCHECKER with their own database for reference retrieval) and speed improvement.

# 7 Conclusion

We introduce REFCHECKER, a unified framework for detecting hallucination in LLM responses . REFCHECKER operates at the level of knowledge triplets, termed claim-triplets, extracted from LLM responses, allowing for fine-grained detection. These claim-triplets are then evaluated against references to determine their hallucination categories. An automated pipeline pairs an extractor and a checker to identify potential hallucinations, calibrated to match human annotations, yielding superior performance compared to prior methods.

# Limitations

a) We have anecdote evidence that sometimes hallucination is due to reasoning and limited contextwindow. These advanced form of hallucination is difficult to be dealt with using triplet which bias towards local contexts. b) At its current form, RE-FCHECKER primarily focuses on plain text in general domains. Exploring extensions to include various data formats (table, code, math, etc.) and specific domains (business, medical, legal, etc.) is worthy of consideration.

# Ethics Statement

We contend that REFCHECKER poses no negative ethical implications for the public; rather, it holds the potential for positive impact by enabling the identification of non-factual content within the responses generated by large language models (LLMs). This capability contributes to the cultivation of responsible AI practices for the benefit of society.

In this study, we utilized a variety of scientific resources to conduct our research and aim to contribute additional artifacts to the community. Specifically, to curate the benchmark dataset, we sample 100 examples from each of the following datasets:

• The development set of NaturalQuestions dataset, which is under Creative Commons Share-Alike 3.0 License.   
• The validation set of MS MARCO dataset, which is under Creative Commons Attribution 4.0 International License.   
• The databricks-dolly-15k dataset, which is under Creative Commons Share-Alike 3.0 License.

These datasets are publicly accessible and utilize English language corpora. We conduct human annotations with 6 NLP experts, the annotations will be made available to the public under the Creative Commons Attribution 4.0 International License.

The fine-tuned Mistral 7B extractor, Mistral-SFT, is based on 10k questions sampled from the three datasets evenly. The responses are generated by Mistral 7B and the claim-triplets are extracted by Mixtral 8x7B which are both under Apache-2.0 License. The RepC checker is also based on Mistral 7B and is trained with the ANLI dataset which

is under Creative Commons-Non Commercial 4.0 License. The fine-tuned models will be released to the public under Apache-2.0 License.

# References

Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.

Anthropic. 2023. Introducing claude.

Meng Cao, Yue Dong, and Jackie Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3340–3354, Dublin, Ireland. Association for Computational Linguistics.

Jiuhai Chen and Jonas Mueller. 2023. Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment. arXiv preprint arXiv:2308.16175.

Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. 2023. Felm: Benchmarking factuality evaluation of large language models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.

I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. 2023. Factool: Factuality detection in generative ai–a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free dolly: Introducing the world’s first truly open instructiontuned llm.

Nouha Dziri, Ehsan Kamalloo, Sivan Milton, Osmar Zaiane, Mo Yu, Edoardo M Ponti, and Siva Reddy. 2022. FaithDial: A Faithful Benchmark for Information-Seeking Dialogue. Transactions of the Association for Computational Linguistics, 10:1473–1490.

Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894–6910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, and Russ R Salakhutdinov. 2004. Neighbourhood components analysis. Advances in neural information processing systems, 17.   
Nuno M. Guerreiro, Duarte M. Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André F. T. Martins. 2023a. Hallucinations in Large Multilingual Translation Models. Transactions of the Association for Computational Linguistics, 11:1500–1517.   
Nuno M. Guerreiro, Elena Voita, and André Martins. 2023b. Looking for a needle in a haystack: A comprehensive study of hallucinations in neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1059–1075, Dubrovnik, Croatia. Association for Computational Linguistics.   
Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. $q ^ { 2 }$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856–7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.   
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.   
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.   
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.   
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466.   
Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A large-scale hallucination evaluation benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449–6464, Singapore. Association for Computational Linguistics.

Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3214–3252, Dublin, Ireland. Association for Computational Linguistics.   
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.   
Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9004–9017, Singapore. Association for Computational Linguistics.   
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, Online. Association for Computational Linguistics.   
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12076–12100, Singapore. Association for Computational Linguistics.   
Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation.   
Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org.   
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial nli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885–4901.   
OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,

Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens,

Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023. Gpt-4 technical report.

# OpenAI. 2022. Introducing chatgpt.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.

Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784–3803, Punta Cana, Dominican Republic. Association for Computational Linguistics.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https://gi thub.com/tatsu-lab/stanford_alpaca.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas

Scialom. 2023. Llama 2: Open foundation and finetuned chat models.   
Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal, Liangming Pan, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov. 2023. Factcheck-gpt: End-to-end fine-grained documentlevel fact-checking and correction of llm output. ArXiv, abs/2311.09000.   
Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023. A new benchmark and reverse validation method for passage-level hallucination detection.   
Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. Alignscore: Evaluating factual consistency with a unified alignment function. arXiv preprint arXiv:2305.16739.   
Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Malin, and Sricharan Kumar. 2023a. SAC3: Reliable hallucination detection in black-box language models via semantic-aware cross-check consistency. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15445–15458, Singapore. Association for Computational Linguistics.   
Tianhang Zhang, Lin Qiu, Qipeng Guo, Cheng Deng, Yue Zhang, Zheng Zhang, Chenghu Zhou, Xinbing Wang, and Luoyi Fu. 2023b. Enhancing uncertaintybased hallucination detection with stronger focus. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 915–932, Singapore. Association for Computational Linguistics.   
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.

# Appendix for REFCHECKER: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models

# A Details of the Benchmark Data Curation Process

The basic information of the benchmark dataset are summarized in Table 6. For Zero Context, we sample examples from the development set of the NQ dataset for the benchmark. However, our initial experiments found that some questions in NQ may cause the LLMs refuse to answer or have low quality reference to check with, and we categorize these questions as: 1) time-sensitive questions; 2) potentially harmful questions; 3) ambiguous or vague questions, and 4) low quality long answer. We will talk about the data filtering later.

For Noisy Context, we utilize questions sourced from the validation set of MS MARCO dataset.3(Nguyen et al., 2016) To prevent LLMs from declining to provide answers, we choose examples where a golden passage containing the answer to the question has been annotated.

For Accurate Context, we employ the databricksdolly- $. 1 5 \mathrm { k } ^ { 4 }$ instruction tuning dataset for the benchmark. Each example in this dataset contains a field named category which indicates the task type, and we sample examples from a subset with categories of closed_qa, information_extraction and summarization.

During the response collection for benchmarking, we use fixed prompt templates in each task setting for collecting responses from LLMs for fair comparisons. For Zero Context setting, the prompt for response collection is the question itself. For Noisy and Accurate Context settings, we use prompt templates shown in Figure 7.

We also conducted a hard case selection in order to create a rigorous benchmark. We talk about the details in the following part of this section.

# A.1 Data Filtering for the NQ Dataset

We employ ChatGPT (GPT-3.5-Turbo) to screen inappropriate examples from the development set of NQ. The specific prompts utilized are illustrated in Figure 8.

<table><tr><td>[Noisy Context] Please answer the following question based on the provided passages. Question: {question}? Passages: Passage 0: {content_of Passage_0} Passage 1: {content_of Passage_1} . . // more passages here Answer:</td><td>[Accurate Context] // for closed QA task Instruction: Provide a well-formed answer to the question using information from the given context. Question: {question} Context: {context} // for summarization and information extraction tasks Instruction: {question} Context: {context}</td></tr></table>

Figure 7: Prompt templates for response collection from LLMs on our benchmark. For Zero Context setting, we just use the question itself as the prompt.

Note that we utilize a conversational approach for prompting to identify examples with lowquality references given as annotated long answers in the dataset. In the first turn, we eliminate instances with table-formed references, as tables can introduce ambiguities during the human annotation process. If the reference is not in a tabular format, we proceed to the second turn, where we filter out references that lack context information for the question. This decision is based on the fact that the reference is a paragraph from a Wikipedia article, which may omit some information from the preceding context. Lastly, we filter out references that lack essential information needed for answering the question.

# A.2 Details of Hard Case Selection

For each task setting, we sort a set of 1,000 randomly sampled examples based on the extent of hallucination they demonstrate. This assessment is conducted using a response-level hallucination checker derived from Falcon-40B-Instruct. Specifically, responses were gathered from four different LLMs , GPT-3.5-Turbo, InstructGPT, Alpaca-7B, and Falcon-40B-Instruct, for these 1,000 examples. Falcon-40B-Instruct is subsequently employed to evaluate whether these responses contain hallucinations according to the prompt template depicted in Figure 9. In this prompt, the “claim” refers to the response generated by an LLM.

Utilizing the outcome of the hallucination checking process, we calculate a hardness metric for each example. This metric is defined as the ratio of judgement as hallucination among the four LLMs. The top 100 examples with the highest ratios are

Table 6: A summary of the RefChecker hallucination detection benchmark. The examples of the benchmark are curated from three different data sources and cover diverse tasks.   

<table><tr><td>Setting</td><td>Data Source</td><td>Task</td><td>Reference</td></tr><tr><td>Zero Context</td><td>NaturalQuestions (development set)</td><td>Closed Book QA</td><td>Annotated Long Answer</td></tr><tr><td>Noisy Context</td><td>MS MARCO (validation set)</td><td>Retrieval-Augmented Generation (RAG)</td><td>Retrieved Passages</td></tr><tr><td>Accurate Context</td><td>databricks-dolly-15k</td><td>Summarization Closed QA Information Extraction</td><td>Input Context</td></tr></table>

Figure 8: Prompts for filtering out inappropriate examples from the development set of NQ.   

<table><tr><td>// Time-sensitive questions
Hello, since your knowledge is up to September 2021, determine whether the following question needs up-to-date knowledge to answer:
{question}? 
give your answer with the following format:
Reason: &lt;your reason&gt;
Answer: &lt;Yes or No&gt;</td><td>// Check the quality of reference by chat
Hello, I have a question and a paragraph of
reference text according to the question,
please answer my question one by one in the
following turns.
Question:
{question}? 
Reference Text:
{reference}</td></tr><tr><td>// Harmful questions
Hello, please help me determine whether the
following question is harmful:
{question}? 
give your answer with the following format:
Reason: &lt;your reason&gt;
Answer: &lt;Yes or No&gt;</td><td rowspan="2">Is the reference text a table? Give your
answer with the following format:
Answer: &lt;Yes or No&gt; 
↓If No 
Does the reference text miss any information
in the question? Give your answer with the
following format:
Answer: &lt;Yes or No&gt; 
↓If No 
Does the reference text provide all the
necessary information for answering the
question accurately and unambiguously? Give
your answer with the following format:
Answer: &lt;Yes or No&gt;</td></tr><tr><td>// Ambiguous or vague questions
Hello, please help me determine whether the
following question is ambiguous or vague.
Question:
{question}? 
Give your answer with the following format:
Answer: &lt;Yes or No&gt;</td></tr></table>

Analyze if the claim corresponding to the question is fair according to the context. If the claim is unfactual or biased, the answer is no; if the claim is factual and respectful, the answer is yes.

Context: {context}

Question: {question}

Claim: {claim}

Yes/no question: Is it fair to say that?

Figure 9: The prompt used in hard case selection during the benchmark curation process.

then selected as the hard cases for each setting.

# A.3 Human Annotation

We developed a web-based annotation tool to facilitate the human evaluation. A screenshot of the annotation tool is presented in Figure 10. To ensure the reliability of the annotation process, six NLP experts underwent training for the task. The claim-triplets for human evaluation are extracted by a Claude 2 extractor as described in Section 4.1.

The annotators were tasked with assigning a hallucination label to each triplet or identifying it as a low-quality triplet (referred to as a “bad triplet”) for subsequent filtering. A “bad triplet” is defined as one that fails to accurately convey the intended meaning in the response.

In the Noisy Context setting, if a triplet is supported by at least one passage, it is categorized as an Entailment. Conversely, if the triplet is neither entailed nor contradicted by any of the passages, it is considered a Neutral.

# A.4 Observations from Human Evaluation

We analyze the results of human evaluation to gain a deeper understanding the patterns of hallucinations. We establish our evaluation metric as follows. Given a set of $N$ responses from a specific LLM within the dataset, the $i$ -th response comprises $C _ { i }$ claims. Among these, $C _ { i } ^ { y }$ claims are annotated with the specific hallucination type labeled as $y \in \{$ {Entailment, Neutral, Contradiction}. We define the hallucination rate for type $y$ that the LLM exhibits in the $i$ -th response as $r _ { i } ^ { y }$ , which is calculated as $\begin{array} { r } { r _ { i } ^ { y } = \frac { C _ { i } ^ { y } } { C _ { i } } } \end{array}$ C y

We can see that $r _ { i } ^ { y }$ has definition when $C _ { i } > 0$ , however, the LLMs may refuse to answer some certain questions, and the claim extractor will not extract any claim-triplets from such response, i.e., $C _ { i } = 0$ . To cover these cases in the metric, we define a new metric of Abstain Rate $r ^ { \mathrm { a b s t a i n } }$ as did in

FActScore, and the rate of abstain is the ratio of abstained responses, which is rabstain = PNi=1 1(Ci=0) $r ^ { \mathrm { a b s t a i n } } = \frac { \sum _ { i = 1 } ^ { N } { 1 ( C _ { i } = 0 ) } } { N }$ where $\mathbb { 1 } ( x )$ is an indicator function which is 1 if $x$ holds and 0 otherwise. Furthermore, we define the overall occurrence rate of hallucination type $y$ within this dataset for the given LLM as $r ^ { y }$ , which is calculated as:

$$
r ^ {y} = \frac {\sum_ {i = 1} ^ {N} r _ {i} ^ {y} \cdot \mathbb {1} \left(C _ {i} > 0\right)}{\sum_ {i = 1} ^ {N} \mathbb {1} \left(C _ {i} > 0\right)} \tag {1}
$$

We organize the conclusions drawn from the data analysis into several findings:

Context Information is Critical Figure 11 displays hallucination label distributions and abstain rates across the three context settings, averaged from the seven LLMs. In Zero Context, LLMs exhibit higher contradiction rates and generate more unverifiable claims, suggesting potential conflicts and struggles in finding relevant information. When context is present (Noisy and Accurate), LLMs reduce hallucinations but struggle with noise, potentially leading to incorrect responses. In conclusion, the reliability of LLMs’ internal knowledge is questionable, highlighting the need for clean and precise contextual information for generating factual responses.

GPT Family Steadily Improved Factuality We present the detailed results and rankings of the seven evaluated LLMs in Figure 12. We rank the LLMs based on the contradiction rates, where a lower contradiction rate is considered better. Across all three settings within our benchmark, a consistent enhancement in factuality is evident, progressing from InstructGPT to ChatGPT and to GPT-4.

Open Source LLM is Catching Up The analysis presented in Figure 12 also reveals noteworthy trends regarding the performance of open-source LLMs. Notably, proprietary LLMs, particularly GPT-4 and Claude 2, exhibit superior performance compared to open-source LLMs, on average. It is worth highlighting, however, that the recently opensourced Llama 2 model demonstrates exceptional performance. Notably, in all evaluated settings, Llama 2 70B Chat outperforms ChatGPT and even surpasses Claude 2 in the Noisy Context setting. These findings suggest a potential for open-source LLMs to narrow the performance gap with their proprietary counterparts.

![](images/f6d391440b7e0413493f28b930c34e1886406baf09c53e320e26dff004676d8d.jpg)  
Figure 10: The screenshot of the annotation tool for human evaluation.

![](images/728db8ab404ef036cd12b03abf07f6905239d17b87ef0753eafae1f824e0f34b.jpg)  
Figure 11: Results of different task settings by averaging the results of the seven LLMs.

Copy from Context is Safer Replicating content in the context enhances the factuality, as illustrated in Figure 13. In order to quantitatively assess the relationship between copying and hallucination in both Noisy and Accurate Context settings, we introduce the concept of Copy Rate. This metric is defined as the ratio of N-grams covered by the context, where an N-gram refers to a phrase comprising N consecutive words. Specifically, we compute the average copy rates for 1 to 4 grams of a claim-triplet to determine its overall copy rate. The findings presented in Figure 14 reveal a discernible trend: a higher copy rate corresponds to an increased likelihood of entailment.

# B Details of RefChecker

# B.1 Extractor

The prompts used for few-shot claim extraction are shown in Figure 15. They are used for claim extraction by GPT-4, Claude 2, Mixtral, and the Mistral baseline. For Mistral-SFT, we removed the in-context examples in the prompt because we find it doesn’t affect the extraction quality after supervised fine-tuning but saves context length.

We collected 10,000 questions without claim ex-

Table 7: Detailed performance of 7 checkers under different claim granularities on 2.1k manual annotated responses. The checkers’ predictions under different granularities are all merged into response-level and then evaluated.   

<table><tr><td></td><td>Response</td><td>Sentence</td><td>Sub-sentence</td><td>Triplet</td></tr><tr><td>RoBERTa-NLI</td><td>44.92</td><td>51.97</td><td>50.18</td><td>55.19</td></tr><tr><td>AlignScore</td><td>46.05</td><td>53.19</td><td>50.71</td><td>57.60</td></tr><tr><td>GPT4</td><td>55.86</td><td>56.66</td><td>47.50</td><td>58.78</td></tr><tr><td>Claude2</td><td>46.49</td><td>46.67</td><td>34.57</td><td>41.00</td></tr><tr><td>RepC-LE-knn</td><td>45.36</td><td>50.79</td><td>46.29</td><td>55.33</td></tr><tr><td>RepC-LE-svm</td><td>48.91</td><td>54.26</td><td>52.29</td><td>59.81</td></tr><tr><td>RepC-LE-nn</td><td>44.03</td><td>53.26</td><td>50.83</td><td>57.54</td></tr></table>

traction results and annotation, following the same process as described in Appendix A. The collected questions cover the three context settings evenly. We collected responses to those questions by Mistral and queried Mixtral ${ 8 } \mathrm { { x } 7 { B } }$ to get corresponding claims. After that, we performed supervised finetuning on a Mistral 7B model to distill the output of the larger Mixtral model. We trained the model for 1 epoch with a initial learning rate 1e-5.

# B.2 Checker

The prompts used for the GPT-4 and Claude 2 based checkers are shown in Figure 16.

As a supplement of Figure 5, Table 7 shows the detailed checker performance under different claim granularities. As a supplement of Table 5, Table 8 shows the full results of checker evaluation.

In the analysis of Table 5, we claim that Claude 2 checker tends to flag a neutral claim as contradiction. We can observe such tendency in Table 9, especially for MS MARCO and Dolly datasets.

We also study the performance tendency of

Table 8: Checker evaluation results on 11k human annotated claim triplets. In Mistral-based checkers, the model names start with the variant types, eg. LoRA-sft indicates the LoRA fine-tuned variant and RepC-LE-nn indicates the representation based classification variant using layer ensemble with 2-layer MLP as the classifier. Here “nxxx” and “exxx” indicates the number of training samples and ensemble learning samples.   

<table><tr><td rowspan="2">Models</td><td colspan="2">Average of three settings</td><td colspan="2">Zero-context (NQ)</td><td colspan="2">Noisy-context (MS MARCO)</td><td colspan="2">Accurate-context (datalbricks-dolly-15k)</td></tr><tr><td>Accuracy</td><td>Macro-F1</td><td>Accuracy</td><td>Macro-F1</td><td>Accuracy</td><td>Macro-F1</td><td>Accuracy</td><td>Macro-F1</td></tr><tr><td colspan="9">Baseline Checkers</td></tr><tr><td>RoBERTa-NLI</td><td>76.56</td><td>55.88</td><td>74.06</td><td>69.90</td><td>78.36</td><td>46.67</td><td>77.27</td><td>51.06</td></tr><tr><td>AlignScore</td><td>78.85</td><td>59.45</td><td>73.40</td><td>70.28</td><td>78.86</td><td>50.42</td><td>84.30</td><td>57.66</td></tr><tr><td>GPT-4</td><td>74.77</td><td>59.80</td><td>67.46</td><td>66.10</td><td>76.67</td><td>55.49</td><td>80.17</td><td>57.80</td></tr><tr><td>Claude 2</td><td>51.98</td><td>36.55</td><td>43.42</td><td>42.90</td><td>40.35</td><td>25.89</td><td>72.18</td><td>40.87</td></tr><tr><td colspan="9">Mistral-based Checkers</td></tr><tr><td>zero-shot</td><td>69.43</td><td>46.64</td><td>70.83</td><td>61.10</td><td>71.75</td><td>43.01</td><td>65.72</td><td>35.81</td></tr><tr><td>1-shot</td><td>76.68</td><td>50.66</td><td>65.44</td><td>63.25</td><td>81.23</td><td>42.18</td><td>83.38</td><td>46.56</td></tr><tr><td>3-shot</td><td>74.24</td><td>45.89</td><td>56.67</td><td>56.20</td><td>81.55</td><td>37.41</td><td>84.50</td><td>44.07</td></tr><tr><td>LoRA-sft-n2000</td><td>72.06</td><td>52.62</td><td>74.09</td><td>68.22</td><td>75.20</td><td>48.65</td><td>66.90</td><td>40.99</td></tr><tr><td>LoRA-sft-n4000</td><td>77.84</td><td>57.98</td><td>77.43</td><td>73.64</td><td>79.21</td><td>50.29</td><td>76.89</td><td>50.00</td></tr><tr><td>RepC-LS-knn-n100</td><td>74.36</td><td>51.98</td><td>72.67</td><td>68.58</td><td>77.54</td><td>45.19</td><td>72.86</td><td>42.17</td></tr><tr><td>RepC-LE-knn-n100-e100</td><td>69.72</td><td>51.64</td><td>70.26</td><td>66.05</td><td>71.14</td><td>46.33</td><td>67.75</td><td>42.55</td></tr><tr><td>RepC-LS-svm-n1000</td><td>79.15</td><td>59.36</td><td>78.34</td><td>74.04</td><td>79.82</td><td>47.62</td><td>79.29</td><td>56.43</td></tr><tr><td>RepC-LE-svm-n1000-e1000</td><td>79.03</td><td>60.05</td><td>77.98</td><td>73.53</td><td>79.56</td><td>51.29</td><td>79.54</td><td>55.34</td></tr><tr><td>RepC-LS-nn-n2000</td><td>80.17</td><td>57.31</td><td>75.50</td><td>71.95</td><td>81.78</td><td>46.90</td><td>83.22</td><td>53.07</td></tr><tr><td>RepC-LE-nn-n2000-e2000</td><td>81.27</td><td>60.80</td><td>75.23</td><td>71.98</td><td>82.08</td><td>47.56</td><td>86.50</td><td>62.86</td></tr></table>

Table 9: Detailed statistics of the Claude 2 checker’s neutral F1 score. We show its prediction results of all neutral labels.   

<table><tr><td></td><td>NQ</td><td>MS MARCO</td><td>Dolly</td><td>Total</td></tr><tr><td># triplets</td><td>3319</td><td>3420</td><td>3994</td><td>10733</td></tr><tr><td># neutral labels</td><td>1818</td><td>436</td><td>368</td><td>2622</td></tr><tr><td># pred as E</td><td>313</td><td>88</td><td>165</td><td>566</td></tr><tr><td># pred as N</td><td>201</td><td>9</td><td>16</td><td>226</td></tr><tr><td># pred as C</td><td>1304</td><td>339</td><td>187</td><td>1830</td></tr></table>

RepC-LS and RepC-LE in Figure 17. The findings indicate that in RepC-LS, the best performed layer is typically around the middle rather than the last layer. Despite RepC-LS trailing behind RepC-LE, it maintains its advantages in model size and data efficiency.

Besides, in Figure 18, we evaluate the RepC checker performance with respect to different number of training data. We can see that RepC-LS-svm outperforms RepC-LS-nn with fewer training data.

# B.3 Source Attribution

In many cases, users of hallucination detection systems care not only the verdicts of the checker, but also where the hallucination happens in the original response, as well as which evidence in the reference supports such verdicts. We provided a rudimentory support of such demand. Specifically, we apply a sentence embedding model (SimCSE (Gao et al., 2021)) to encode spans in responses and references,

compare them to the encoding of elements in claimtriplets, then use a threshold to filter matched spans as source attribution results. This naive approach suffers from issues on computational efficiency, unclear boundaries, and matching by shallow semantics. The topic on source attribution has a significant impact on applications of hallucination detection and we leave the exploration on non-trivial solutions to the future.

# C Comparisons with Other Hallucination Detection Frameworks

# C.1 Comparison on the REFCHECKER Benchmark

We compare REFCHECKER with recently proposed hallucination detection frameworks, SelfCheck-GPT, FActScore and FacTool, on our benchmark. The four frameworks use different representations of claims and hallucination labels as described in Table 1, we aggregate the claim-level results into two types of response-level results:

• Response-level binary classification. We aggregate the claim-level labels into responselevel binary labels as Factual and Non-Factual. Thus, we use Accuracy, Factual F1 (Fact. F1 for short) and Non-Factual F1 (Non-Fact. F1) as the evaluation metrics. We use a strict configuration that a response is nonfactual if at least one claim contains hallu-

![](images/5339e2bf7d3a931c91864809aaed9be0ca146ba5d22079e43ab9c0472007dd05.jpg)

![](images/cbb3f98bfca800a1ee591733dce155ba9672ea8c672294683ee844cd90ff8157.jpg)

![](images/ffba883992202815e4c710bad043bd7be8fcc5de83d371de78db77faf4adda77.jpg)

![](images/fa3f3b90c984032d96336cb0d98c3901ae1f205f739a6637efc1b129a7a74290.jpg)  
Figure 12: Detailed results of the human evaluation with the rates of Contradiction, Neutral, Entailment and also Abstain across the three settings, and ranked by the contradiction rates. The last ranking is the results averaged over the three settings.

![](images/712bd687c54390f518deea9479a97af0f68e4a92bb722693e0330c253b97a757.jpg)  
Figure 13: An example of factual claim-triplet whose content are mostly copied from the context.

![](images/ed470c754fa61832bed3f3ba10c68e339f650df1709ac0066abf638ea5beed2c.jpg)  
Figure 14: Copy rate of the claim-triplet v.s. label distributions, by aggregating the results of the Noisy Context and Accurate Context settings.

cination. For SelfCheckGPT, we consider minor_inaccurate and major_inaccurate labels as hallucination. For RefChecker, we consider both Contradiction and Neutral as hallucination.

• Correlations of response-level hallucination rate. Following SelfCheckGPT, we also compare the hallucination rate of a response with human evaluation by Pearson and Spearman correlations. For SelfCheckGPT, we compute the hallucination rate of a response by averaging the scores of the sentences following the definition in their paper. For FActScore and FacTool, the hallucination rate is the ratio of non-factual claims in a response. And for RefChecker, we take the ratio of Contradiction and Neutral claims as the hallucination rate.

The results are shown in Table 10 for Zero Context setting, Table 11 for Noisy Context setting and Table 12 for Accurate Context setting. Following their configurations in their papers, we apply InstructGPT(text-davinci-003) and GPT-4 as the extractors for FActScore and FacTool, respectively, apply ChatGPT(gpt-3.5-turbo) as the checker for SelfCheckGPT and FActScore and GPT-4 for FacTool. The combinations of extractor and checker of RefChecker are displayed as “{Extractor} $^ +$ {Checker}”.

We conclude these results with the following observations:

• RefChecker is effective. Most combinations

![](images/89f29d999f23f15e8e500dc15d338c4a54eab5519f6a0d3a27aea0fc0baad7c7.jpg)  
Figure 15: The prompt used for the GPT-4 and Claude 2 extractors. It requires a question and response from the LLM, and is provided with two in-context examples.

of RefChecker outperform the baselines with large margins across all the five metrics.

• RefChecker is more effective with a GPT-4 checker. The best results are achieved with a GPT-4 checker indicating that the main bottleneck lies in the checking module. In spite of that, RefChecker can still outperforms the baselines with a smaller checker AlignScore.   
• Purely open-sourced combinations can also outperform the baselines which are using proprietary LLMs for both extractor and checker.

# C.2 Comparison on the SelfCheckGPT Dataset

We also ran REFCHECKER on the SelfCheckGPT dataset which contains 237 examples on WikiBio domain. The results are shown in Table 13. We can

observe that 11 out of the 15 combinations $( 7 3 \% )$ of REFCHECKER outperform SelfCheckGPT.

# D Analysis of Internal Knowledge Bias

In this section, we further analyze the emergence of the hallucination from the perspective of the LLMs’ bias to the internal knowledge. We analyze whether the evaluated model and the checker generate response based on their internal knowledge in Section D.1 and D.2, respectively. In general, we observe that LLMs/Checkers may incorporate internal knowledge even when provided with contextual information, contributing to the occurrence of hallucination.

Table 10: A comparison of RefChecker with previous works on our benchmark under Zero Context setting. We highlight the best results using proprietary LLMs with blue colors and best results results using pure open-source models with orange colors.   

<table><tr><td></td><td colspan="5">Zero Context Setting</td></tr><tr><td></td><td>Accuracy</td><td>Fact. F1</td><td>Non-Fact. F1</td><td>Pearson</td><td>Spearman</td></tr><tr><td>SelfCheckGPT</td><td>77.99</td><td>54.03</td><td>85.53</td><td>35.40</td><td>43.15</td></tr><tr><td>FActScore</td><td>66.41</td><td>49.42</td><td>74.86</td><td>42.58</td><td>45.60</td></tr><tr><td>FacTool</td><td>84.94</td><td>73.29</td><td>89.52</td><td>59.78</td><td>62.57</td></tr><tr><td>REFCHECKER</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 + GPT-4</td><td>93.82</td><td>86.89</td><td>95.96</td><td>83.95</td><td>82.35</td></tr><tr><td>GPT-4 + Claude 2</td><td>91.31</td><td>82.76</td><td>94.19</td><td>80.11</td><td>79.53</td></tr><tr><td>GPT-4 + NLI</td><td>83.98</td><td>71.28</td><td>88.89</td><td>60.81</td><td>62.32</td></tr><tr><td>GPT-4 + AlignScore</td><td>90.54</td><td>78.97</td><td>93.90</td><td>71.95</td><td>70.37</td></tr><tr><td>GPT-4 + RepC</td><td>89.96</td><td>81.16</td><td>93.16</td><td>77.42</td><td>77.26</td></tr><tr><td>Claude 2 + GPT-4</td><td>92.66</td><td>84.30</td><td>95.21</td><td>83.69</td><td>82.99</td></tr><tr><td>Claude 2 + Claude 2</td><td>92.08</td><td>83.40</td><td>94.8</td><td>80.04</td><td>79.39</td></tr><tr><td>Claude 2 + NLI</td><td>83.20</td><td>70.51</td><td>88.26</td><td>59.25</td><td>60.39</td></tr><tr><td>Claude 2 + AlignScore</td><td>90.54</td><td>78.97</td><td>93.90</td><td>75.07</td><td>73.83</td></tr><tr><td>Claude 2 + RepC</td><td>89.58</td><td>80.71</td><td>92.86</td><td>76.34</td><td>76.24</td></tr><tr><td>Mistral-SFT + GPT-4</td><td>92.47</td><td>83.40</td><td>95.13</td><td>80.88</td><td>78.88</td></tr><tr><td>Mistral-SFT + Claude 2</td><td>90.93</td><td>80.66</td><td>94.07</td><td>77.98</td><td>77.04</td></tr><tr><td>Mistral-SFT + NLI</td><td>89.96</td><td>78.86</td><td>93.42</td><td>72.89</td><td>72.07</td></tr><tr><td>Mistral-SFT + AlignScore</td><td>90.54</td><td>78.79</td><td>93.91</td><td>75.81</td><td>74.16</td></tr><tr><td>Mistral-SFT + RepC</td><td>89.38</td><td>80.43</td><td>92.72</td><td>77.14</td><td>76.74</td></tr></table>

Table 11: A comparison of RefChecker with previous works on our benchmark under Noisy Context setting. We highlight the best results using proprietary LLMs with blue colors and best results results using pure open-source models with orange colors.   

<table><tr><td></td><td colspan="5">Noisy Context Setting</td></tr><tr><td></td><td>Accuracy</td><td>Fact. F1</td><td>Non-Fact. F1</td><td>Pearson</td><td>Spearman</td></tr><tr><td>SelfCheckGPT</td><td>58.55</td><td>51.63</td><td>63.74</td><td>36.31</td><td>32.15</td></tr><tr><td>FActScore</td><td>63.57</td><td>69.94</td><td>53.77</td><td>33.36</td><td>29.91</td></tr><tr><td>FacTool</td><td>68.40</td><td>72.84</td><td>62.22</td><td>46.35</td><td>38.69</td></tr><tr><td>REFCHECKER</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 + GPT-4</td><td>74.54</td><td>76.42</td><td>72.32</td><td>64.56</td><td>57.30</td></tr><tr><td>GPT-4 + Claude 2</td><td>66.73</td><td>67.16</td><td>66.29</td><td>52.40</td><td>42.90</td></tr><tr><td>GPT-4 + NLI</td><td>66.73</td><td>74.54</td><td>52.01</td><td>39.69</td><td>32.98</td></tr><tr><td>GPT-4 + AlignScore</td><td>67.66</td><td>73.48</td><td>58.57</td><td>44.31</td><td>37.58</td></tr><tr><td>GPT-4 + RepC</td><td>65.99</td><td>74.04</td><td>50.67</td><td>28.19</td><td>28.94</td></tr><tr><td>Claude 2 + GPT-4</td><td>71.38</td><td>73.72</td><td>68.57</td><td>53.14</td><td>47.89</td></tr><tr><td>Claude 2 + Claude 2</td><td>63.38</td><td>63.59</td><td>63.18</td><td>39.96</td><td>34.08</td></tr><tr><td>Claude 2 + NLI</td><td>64.13</td><td>71.58</td><td>51.39</td><td>29.04</td><td>25.50</td></tr><tr><td>Claude 2 + AlignScore</td><td>69.70</td><td>74.33</td><td>63.04</td><td>46.39</td><td>41.26</td></tr><tr><td>Claude 2 + RepC</td><td>65.80</td><td>74.01</td><td>50.00</td><td>32.30</td><td>29.03</td></tr><tr><td>Mistral-SFT + GPT-4</td><td>75.28</td><td>77.57</td><td>72.46</td><td>67.29</td><td>59.94</td></tr><tr><td>Mistral-SFT + Claude 2</td><td>64.50</td><td>61.10</td><td>67.35</td><td>56.14</td><td>47.02</td></tr><tr><td>Mistral-SFT + NLI</td><td>70.82</td><td>75.12</td><td>64.72</td><td>52.21</td><td>45.61</td></tr><tr><td>Mistral-SFT + AlignScore</td><td>69.70</td><td>74.73</td><td>62.18</td><td>53.88</td><td>45.09</td></tr><tr><td>Mistral-SFT + RepC</td><td>65.99</td><td>73.97</td><td>50.94</td><td>38.11</td><td>31.01</td></tr></table>

Table 12: A comparison of RefChecker with previous works on our benchmark under Accurate Context setting. We highlight the best results using proprietary LLMs with blue colors and best results results using pure open-source models with orange colors.   

<table><tr><td></td><td colspan="5">Accurate Context Setting</td></tr><tr><td></td><td>Accuracy</td><td>Fact. F1</td><td>Non-Fact. F1</td><td>Pearson</td><td>Spearman</td></tr><tr><td>SelfCheckGPT</td><td>62.15</td><td>68.70</td><td>52.12</td><td>40.23</td><td>32.55</td></tr><tr><td>FActScore</td><td>69.37</td><td>78.57</td><td>46.30</td><td>27.80</td><td>27.05</td></tr><tr><td>FacTool</td><td>72.53</td><td>80.98</td><td>50.63</td><td>31.41</td><td>32.82</td></tr><tr><td>REFCHECKER</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>GPT-4 + GPT-4</td><td>80.81</td><td>86.85</td><td>64.50</td><td>58.61</td><td>55.50</td></tr><tr><td>GPT-4 + Claude 2</td><td>77.46</td><td>84.04</td><td>61.68</td><td>48.62</td><td>48.66</td></tr><tr><td>GPT-4 + NLI</td><td>73.06</td><td>82.23</td><td>44.36</td><td>28.59</td><td>30.14</td></tr><tr><td>GPT-4 + AlignScore</td><td>76.23</td><td>83.44</td><td>57.94</td><td>49.97</td><td>46.89</td></tr><tr><td>GPT-4 + RepC</td><td>76.94</td><td>84.86</td><td>51.66</td><td>45.58</td><td>41.01</td></tr><tr><td>Claude 2 + GPT-4</td><td>82.22</td><td>87.64</td><td>68.34</td><td>60.99</td><td>58.96</td></tr><tr><td>Claude 2 + Claude 2</td><td>74.47</td><td>81.62</td><td>58.21</td><td>49.81</td><td>44.72</td></tr><tr><td>Claude 2 + NLI</td><td>72.36</td><td>81.68</td><td>43.73</td><td>27.39</td><td>29.12</td></tr><tr><td>Claude 2 + AlignScore</td><td>74.30</td><td>81.57</td><td>57.56</td><td>50.17</td><td>44.59</td></tr><tr><td>Claude 2 + RepC</td><td>77.46</td><td>85.25</td><td>52.24</td><td>52.11</td><td>42.78</td></tr><tr><td>Mistral-SFT + GPT-4</td><td>79.75</td><td>85.96</td><td>63.72</td><td>56.09</td><td>53.72</td></tr><tr><td>Mistral-SFT + Claude 2</td><td>70.95</td><td>77.85</td><td>57.80</td><td>38.82</td><td>40.75</td></tr><tr><td>Mistral-SFT + NLI</td><td>73.59</td><td>81.44</td><td>54.27</td><td>44.34</td><td>40.81</td></tr><tr><td>Mistral-SFT + AlignScore</td><td>74.12</td><td>81.60</td><td>56.38</td><td>46.34</td><td>43.22</td></tr><tr><td>Mistral-SFT + RepC</td><td>73.94</td><td>82.83</td><td>45.99</td><td>39.59</td><td>33.86</td></tr></table>

Table 13: REFCHECKER results on the SelfCheckGPT dataset. The results of SelfCheckGPT are from their paper. We highlight the best results using proprietary LLMs with blue colors and best results results using pure open-source models with orange colors.   

<table><tr><td></td><td>Pearson</td><td>Spearman</td></tr><tr><td>SelfCheckGPT</td><td>78.32</td><td>78.30</td></tr><tr><td>RefChecker</td><td></td><td></td></tr><tr><td>GPT-4 + GPT4</td><td>80.86</td><td>83.44</td></tr><tr><td>GPT-4 + Claude 2</td><td>87.67</td><td>89.23</td></tr><tr><td>GPT-4 + NLI</td><td>79.96</td><td>80.16</td></tr><tr><td>GPT-4 + AlignScore</td><td>76.20</td><td>77.33</td></tr><tr><td>GPT-4 + RepC</td><td>79.63</td><td>79.23</td></tr><tr><td>Claude 2 + GPT4</td><td>79.18</td><td>82.89</td></tr><tr><td>Claude 2 + Claude 2</td><td>85.70</td><td>87.15</td></tr><tr><td>Claude 2 + NLI</td><td>76.40</td><td>76.29</td></tr><tr><td>Claude 2 + AlignScore</td><td>73.47</td><td>74.91</td></tr><tr><td>Claude 2 + RepC</td><td>76.01</td><td>76.48</td></tr><tr><td>Mistral-SFT + GPT4</td><td>80.98</td><td>83.92</td></tr><tr><td>Mistral-SFT + Claude 2</td><td>85.46</td><td>86.49</td></tr><tr><td>Mistral-SFT + NLI</td><td>78.54</td><td>79.66</td></tr><tr><td>Mistral-SFT + AlignScore</td><td>75.10</td><td>76.08</td></tr><tr><td>Mistral-SFT + RepC</td><td>76.59</td><td>76.70</td></tr></table>

# D.1 Internal Knowledge Bias of Evaluated Model

In order to analyze whether the evaluated LLMs generate responses based on their own knowledge or the provided context in Noisy and Accurate Context settings, we convert each claim-triplet extracted from the response into a simple interrogative query for knowledge checking. For simplicity, we design a prompt template and ask GPT-4-Turbo to generate these queries (Figure 19). Then we feed the query into the evaluated LLM to check whether it has such knowledge. The answer from the evaluated LLMs could be one of the following:

1. Yes, means the evaluated LLM has this knowledge in its internal memory.   
2. No, means the evaluated LLM contains knowledge that is contradicted with the triplet.   
3. Unsure, means the evaluated LLM does not have this knowledge or it has confusion on the knowledge.

The label pairs (Yes, Contradiction) and (Yes, Neutral) indicate that the model is utilizing internal information to generate this claim-triplet. On the other hand, (No, Entailment) and (Unsure, Entailment) signify that the model is relying on contextual information for generation. Pairs like (No, Contradiction) suggest that the evaluated model may be less proficient in processing context information,

I have a claim that made by a language model to a question, please help me for checking whether the claim can be entailed according to the provided reference which is related to the question.

The reference is a list of passages, and the claim is represented as a triplet formatted with ("subject", "predicate", "object").

If the claim is supported by ANY passage in the reference, answer 'Entailment'.

If NO passage in the reference entail the claim, and the claim is contradicted with some passage in the reference, answer 'Contradiction'.

If NO passage entail or contradict with claim, or DOES NOT contain information to verify the claim, answer 'Neutral'.

Please DO NOT use your own knowledge for the judgement, just compare the reference and the claim to get the answer.

### Question: {question}

### Reference: {reference}

### Claim: {claim}

Your answer should always be only a single word in ['Entailment', 'Neutral', 'Contradiction']. DO NOT add explanations or you own reasoning to the output.

Figure 16: Prompt for the GPT-4 Checker and Claude 2 Checker.

leading to the production of less reliable claimtriplets.

The outcomes of the Accurate Context are illustrated in Figure 21. Upon examination, it is evident that GPT-4-Turbo demonstrates the most notable performance, primarily generating responses aligned with the reference context. Conversely, GPT-3.5-Turbo tends to generate responses by relying on its internal knowledge to some extent, leading to contradictions or neutrality to the reference context. Claude 2 sometimes generate unsure information but neutral to the reference context. In the case of InstructGPT, the model further generates unsure information, which also contradicts the reference context. This behavior may stem from contradictions within the model’s internal knowledge or difficulties in comprehending the amalgamated content of internal and reference information. Regarding LLaMA-2-70B, and Falcon-40B-Instruct, our observations indicate that these models exhibit inferior performance. They generate information that contradicts internal knowledge and is irrele-

![](images/844ee2bd0734febc547419a8127b044e01554ace72050017aac9d073da2d27b6.jpg)  
Figure 17: Performance tendency of different layers in RepC-LS checkers. The corresponding RepC-LE checkers are included as dashed lines.

![](images/896d1075f0d38ba4e3928a22a5f542bfc9ef8740e797d1634b639f5c0d9eb5bf.jpg)  
Figure 18: Performance of different RepC checkers with respect to numbers of training samples.

vant to the reference context. Alpaca 7B performs similarly to GPT-3.5-Turbo, but seldom generates information contradicting to its internal knowledge, Different from the accurate context setting, all the models tend to generate more Neutral labels in the noisy context setting (Figure 22).

# D.2 Internal Knowledge Bias of Checker

We also conduct an analysis to determine whether the checker provides predictions based on its internal knowledge. In this analysis, a triplet extracted

# Triplet: ("The Story of June”,"has songs”,"Hey June")

Prompt: Translate the following subject-verb-object triplets intosimple interrogativesentences:{Triplet}

Simple interrogative sentence: Does The Story of June has songs Hey June? Answer the question with Yes, No,or Unsure

Figure 19: Designed prompt for converting triplets to simple interrogative sentences.

# Triplet: ("The Story of June","has songs”, "Hey June")

Question: What are the names of the songs from The Story of June which don’t have Chinese names?

Prompt: Mask the objective or subjective in the subjectverb-object triplet {Triplet} in the Context with #### which is not mentioned in Q (merely output the modified context).

Q : {Question}

Context: {Context}

Figure 20: Designed prompt for masking triplet information in the reference context.   
Table 14: Results for the information masking scenario in accurate-context setting.   

<table><tr><td>Model</td><td>Checker</td><td>Entail</td><td>Contr</td><td>Neut</td></tr><tr><td rowspan="3">GPT-3.5</td><td>GPT-4</td><td>37.36</td><td>6.28</td><td>56.36</td></tr><tr><td>Claude 2</td><td>61.49</td><td>25.95</td><td>12.56</td></tr><tr><td>RoBERTa-NLI</td><td>21.82</td><td>21.76</td><td>62.64</td></tr><tr><td rowspan="3">GPT-4</td><td>GPT-4</td><td>35.88</td><td>10.88</td><td>53.24</td></tr><tr><td>Claude 2</td><td>62.50</td><td>26.39</td><td>11.11</td></tr><tr><td>RoBERTa-NLI</td><td>23.38</td><td>22.92</td><td>53.70</td></tr></table>

from the response is taken, and we mask the subjective or objective information in the context with ‘####’. The modified context, along with the triplet, is then inputted into the checker to obtain the label. In theory, the prediction label should be neutral because the relevant information in the context is masked. If the label is not neutral, it implies that the model is making inferences based on its internal knowledge. For the implementation of this analysis, we query GPT-4-Turbo with a specifically designed prompt to mask the triplet information, as illustrated in Figure 20. Specifically, in the noisycontext setting, we implement the query for each reference document and keep the document unchanged if there is no relevant information to the extracted triplet.

The results of the accurate context setting are shown in Table 14. As we observe, RoBERTa-NLI achieves the most significant Neutral labels, $6 2 . 6 4 \%$ and $5 3 . 7 0 \%$ for evaluated model GPT-3.5- Turbo and GPT-4-Turbo, respectively. The checker GPT-4-Turbo achieves the second performance. But Claude 2 predicts a large number of Entailment and Contradiction, which implies that the checker Claude 2 highly relies on the internal knowledge for checking. The results of the zero context setting are in a similar pattern with those of accuratecontext setting (Table 15). But in the noisy context setting (Table 16), RoBERTa-NLI outperforms GPT-4-Turbo and Claude 2 with a large margin in the ratio of Neutral labels. The results may results from the strong bias to internal knowledge of

Table 15: Results for the information masking scenario in zero-context setting.   

<table><tr><td>Model</td><td>Checker</td><td>Entail</td><td>Contr</td><td>Neut</td></tr><tr><td rowspan="3">GPT-3.5</td><td>GPT-4</td><td>37.91</td><td>22.55</td><td>39.54</td></tr><tr><td>Claude 2</td><td>56.54</td><td>33.99</td><td>9.48</td></tr><tr><td>RoBERTa-NLI</td><td>35.62</td><td>30.72</td><td>33.66</td></tr><tr><td rowspan="3">GPT-4</td><td>GPT-4</td><td>43.33</td><td>13.67</td><td>43.00</td></tr><tr><td>Claude 2</td><td>61.00</td><td>28.00</td><td>11.00</td></tr><tr><td>RoBERTa-NLI</td><td>34.67</td><td>23.00</td><td>42.33</td></tr></table>

Table 16: Results for the information masking scenario in noisy-context setting.   

<table><tr><td>Model</td><td>Checker</td><td>Entail</td><td>Contr</td><td>Neut</td></tr><tr><td rowspan="3">GPT-3.5</td><td>GPT-4</td><td>58.52</td><td>6.67</td><td>34.82</td></tr><tr><td>Claude 2</td><td>60.25</td><td>20.00</td><td>19.75</td></tr><tr><td>RoBERTa-NLI</td><td>9.38</td><td>10.62</td><td>80.00</td></tr><tr><td rowspan="3">GPT-4</td><td>GPT-4</td><td>65.71</td><td>6.29</td><td>28.00</td></tr><tr><td>Claude 2</td><td>58.57</td><td>32.29</td><td>9.14</td></tr><tr><td>RoBERTa-NLI</td><td>8.57</td><td>11.14</td><td>80.28</td></tr></table>

GPT-4-Turbo and Claude 2 when the context is extremely long, or the RoBERTa-NLI model has less associative ability to the memorized knowledge.

![](images/be5de58efdad5688b2a66f8f811736f51faf442c48fc555bc984282243f71c17.jpg)

![](images/4ede092faabbc1f90c97069853dea1fca7e8d1c0a975e04676ccbca123d22695.jpg)

![](images/a09e2a5f7d2d3624311c1c0196afa9bac28b762c80c67c669f5d7ea792e2deb6.jpg)

![](images/c991d062d16eeb21bb4ff499ea81579730e8e99d0123f5d56288090138b5540c.jpg)

![](images/8e6c55f51dcc439dea25259f28ec9b466b47d3283a4a43274439755903037252.jpg)

![](images/da41dc1119b64b8c003093576083e087ead1b5c9df2d62aa4d57f18bd962fe78.jpg)

![](images/1b14a027d7946109ed2d4d9db09013a613da8d108e6ad51e9ddc77ac9f7ce57f.jpg)  
Figure 21: The results of knowledge checking for evaluated models in the accurate-context setting. The labels Yes, No and Unsure are the responses to the interrogative sentences generated from knowledge triplets. Each value refers to the percentage of each checking pairs in the total number of triplets.

![](images/0e758c30e5983e8b65fcf9b5088f48163834c41d356d34ad53ea55b7c8d4d11a.jpg)

![](images/ef1a251e3ecbbabafe2a2be9c94d0c8e4dd5343a534669649c3008206167e694.jpg)

![](images/ff854da1682d3a8522e61c8a85372915c65b403aa0bb85c8517f93d6e2fc4e1f.jpg)

![](images/4cc8610a2985e9ed21adf191bab4a844529c654059b87ef153e0122e336190ef.jpg)

![](images/d3fb920cc42eaf9eda3d1172dd7091d4e846928d8862718be4bdfc0483c4c372.jpg)

![](images/e38530e227b49fe6225ac9310b8ad9cb8fba99c33213838f02d8da45db715695.jpg)

![](images/2c5fe3dc876d60740bcdd015343b32f4c74640fc10f77ddc58551a75c491c3c7.jpg)  
Figure 22: The results of knowledge checking for evaluated models in the noisy-context setting. The label Yes, No and Unsure are the respones to the interrogative sentences generated from knowledge triplets. Each value refers to the percentage of each checking pairs in the total number of triplets.