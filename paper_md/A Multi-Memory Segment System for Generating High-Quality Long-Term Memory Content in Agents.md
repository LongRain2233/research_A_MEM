# A Multi-Memory Segment System for Generating High-Quality Long-Term Memory Content in Agents

Gaoke Zhang1, Bo Wang1*, Yunlong $\mathbf { M } \mathbf { a } ^ { 1 }$ , Dongming Zhao2, Zifei Yu3

1College of Intelligence and Computing, Tianjin University AI Lab, China Mobile Communication Group Tianjin Co., L 3Huizhi Xingyuan Information Technology Co., Ltd {zhanggaoke, bo_wang}@tju.edu.cn

# Abstract

In the current field of agent memory, extensive explorations have been conducted in the area of memory retrieval, yet few studies have focused on exploring the memory content. Most research simply stores summarized versions of historical dialogues, as exemplified by methods like A-MEM and MemoryBank. However, when humans form long-term memories, the process involves multi-dimensional and multicomponent generation, rather than merely creating simple summaries. The low-quality memory content generated by existing methods can adversely affect recall performance and response quality. In order to better construct high-quality long-term memory content, we have designed a multi-memory segment system (MMS) inspired by cognitive psychology theory. The system processes short-term memory into multiple long-term memory segments, and constructs retrieval memory units and contextual memory units based on these segments, with a one-to-one correspondence between the two. During the retrieval phase, MMS will match the most relevant retrieval memory units based on the user’s query. Then, the corresponding contextual memory units is obtained as the context for the response stage to enhance knowledge, thereby effectively utilizing historical data. We conducted experiments on the Lo-CoMo dataset and further performed ablation experiments, experiments on the robustness regarding the number of input memories, and overhead experiments, which demonstrated the effectiveness and practical value of our method.

# 1 Introduction

Agents driven by large language models (LLMs) have emerged as an effective approach to addressing dialogue system challenges (Xi et al., 2025). The memory capabilities of agents are gradually becoming a pivotal factor in propelling them towards higher-level cognition and autonomous behavior

(Packer et al., 2023; Zhang et al., 2024; Zhou et al., 2025). Traditional short-term memory mechanisms are inadequate for meeting the demands of complex tasks, which require contextual understanding, long-term reasoning, and personalized responses (Wang et al., 2024). Consequently, constructing a memory system with durability, structure, and adaptability has become one of the core challenges in realizing truly "intelligent" agents.

To address this issue, researchers have explored both parametric and non-parametric approaches to optimize the performance of LLMs in longform tasks. Parametrically, by augmenting LLMs with additional parameters, knowledge can be retained for future use (Wang et al., 2023). The Elastic Weight Consolidation algorithm (Huszár, 2018) , developed through collaboration between DeepMind and Imperial College London, enables neural networks to retain knowledge from previous tasks while learning new ones, mitigating the "catastrophic forgetting" problem and marking a significant step towards continuous learning in AI. However, parametric memory is prone to generating distorted and non-factual outputs and lacks interpretability (Ji et al., 2023). Nonparametrically, external components are employed to enhance the long-term memory capabilities of LLMs. The external memory module of an agent can store historical dialogue content, often using the Retrieval-Augmented Generation (RAG) approach to store long-term memories as vectors in a database. When a new query arises, relevant vectors are matched from the vector database to serve as memory content (Gao et al., 2023; Alonso et al., 2024). However, simply storing historical dialogues in a database does not adequately simulate the human memory formation process, nor does it effectively match information from historical dialogues.

Despite the proposal of various memory architectures, such as A-MEM (Xu et al., 2025), which

extracts keywords, constructs summaries, and creates tags from dialogues as memory units, employing the Zettelkasten method to dynamically index and link knowledge networks, and MemoryBank (Zhong et al., 2024), which summarizes dialogue content and analyzes user personality and mood to serve as memory units, these approaches often fall short in practical scenarios. Users pose questions from diverse perspectives, yet the aforementioned methods simply extract keywords and summaries, saving them as memory content. This leads to a gap between users’ queries and the content in memory units, with the memory content often lacking in quality, thereby affecting retrieval recall effectiveness. To enhance retrieval performance, we integrate Tulving’s memory system classification (Tulving, 1985) and the encoding specificity principle (Tulving and Thomson, 1973), extracting episodic memory, semantic memory, cognitive perspectives, and keywords as our memory units content. This approach provides higher-quality memory content, thereby improving our retrieval recall effectiveness.

Currently, AI agents’ memory systems encompass various types, including Short-Term Memory (STM), Long-Term Memory (LTM), Episodic Memory, Semantic Memory, and Procedural Memory (Sumers et al., 2023). STM is utilized for processing immediate contexts, such as the context window in dialogue systems. LTM, on the other hand, enables cross-session information storage and retrieval through databases, knowledge graphs, or vector embeddings (Gutiérrez et al., 2024; Xi et al., 2025; Hu et al., 2023). Episodic Memory allows agents to recall specific events, supporting case-based reasoning (Tulving et al., 1972). Semantic Memory stores structured factual knowledge, facilitating logical reasoning and knowledge retrieval (Tulving, 1986). We have designed a multi- memory segment system based on cognitive psychology theory to generate high-quality long-term memory.

Our contributions are as follows:

(1) Current research rarely considers memory content quality, leading to its poor state. To address the issue of low-quality memory content in memory systems, we are inspired by multiple memory systems theory, encoding specificity principle, and levels of processing theory to propose a multimemory segment system to enhance the long-term memory capabilities of agents. The memory system extract keywords, multiple cognitive perspectives, episodic memory, and semantic memory as memory segments through the analysis and process-

ing of short-term memory to construct high-quality long-term memory content.

(2) In order to meet the requirements of different tasks in the retrieval stage and generation stage, we have built retrieval memory units for relevance matching with queries and contextual memory units as context for knowledge enhancement during generation.   
(3) We conducted experiments on the LoCoMo dataset to evaluate the retrieval and generation capabilities, and also performed ablation study. These experimental results demonstrate the effectiveness of our method.

# 2 Related Work

# 2.1 Cognitive Psychology Acting on Memory

The theory of multiple memory systems (Tulving, 1985) posits that memory is not processed by a single system, but rather consists of multiple subsystems that are functionally independent and have different structural foundations. These systems are responsible for different types of information processing and storage, with specific neural bases and behavioral characteristics. Endel Tulving (Tulving et al., 1972) proposed a three-category classification model in 1985, including: Procedural Memory: involving the learning of skills and habits, such as riding a bicycle. Semantic Memory: Stores factual and conceptual knowledge, such as the name of a capital city. Episodic memory: Recording personal experiences and events, such as the last birthday party. The Levels of Processing Theory (Craik and Lockhart, 1972) suggests that the formation of memory does not depend on independent storage systems such as short-term memory and long-term memory, but rather on the depth and manner of information processing. The Encoding Specificity Principle (Tulving and Thomson, 1973) states that the memory effect of information depends on its processing method and context during encoding. Specifically, when information is encoded, the environment, emotional state, sensory stimulation, and so on will all become part of the memory trace. Therefore, only when the conditions during retrieval match those during encoding, the retrieval of memory is most effective.

Inspired by the levels of processing theory, we process short-term memory content via a multilevel approach to form long-term memory representations. Considering the multiple memory theory, we categorize semantic and episodic memories

![](images/3beb532413f7807319599733039854554aa5b1e59982a352c93c666aac22700c.jpg)  
Figure 1: Schematic of multi-memory system process: After acquiring short-term memory, MMS processes it into memory segments and constructs retrieval units and contextual memory units. During retrieval, the k most relevant retrieval units are matched to the query, and their corresponding contextual units are then used as context input for the agent’s response.

as long-term memory components. Recognizing users’ diverse questions, which reflect varied perspectives on the initial memory content they have, we are considering the encoding specificity prin ciple to enhance recall by aligning coding content closely with the question’s context. We integrate keywords from short-term memory and diverse cognitive angles derived from short-term into longterm memory representations to boost matching efficacy.

# 2.2 Memory system for diverse content

In recent years, research on the long-term memory mechanisms of agents has exhibited a diversifying trend. MemoryBank (Zhong et al., 2024) achieves long-term storage and retrieval of knowledge in multi-turn dialogues by introducing an external memory bank, integrating three modules—a writer, a retriever, and a reader—and leveraging the Ebbinghaus memory curve theory. It boasts strong scalability and modularity advantages, yet it suffers from issues such as coarse-grained memory content and suboptimal memory selection effects. MemoChat (Lu et al., 2023), on the other hand, injects user-related information into the model in the form of static summaries using concise, manually constructed "memos," significantly enhancing con-

sistency and efficiency in open-domain dialogues. However, it relies on high-quality memo construction and lacks the capability for dynamic learning and memory adjustment. Think-in-Memory (Liu et al., 2023) proposes a "pre-recollection $^ +$ postreflection" framework that mimics human-like cognition, explicitly separating retrieval and integrated reasoning into distinct stages and introducing a selfreflection mechanism to enhance reasoning capabilities, making it suitable for complex tasks. Nevertheless, it incurs high reasoning costs, is sensitive to retrieval, and demands significant prompt engineering efforts. Integrating Dynamic Human-like Memory Recall and Consolidation in LLM-Based Agents (Hou et al., 2024) divides memory into short-term and long-term categories, dynamically updating and managing memories based on factors such as usage frequency and temporal proximity. ChatDB (Hu et al., 2023) employs a database as the symbolic memory for LLMs. Its approach involves structuring textual data and storing it in a database, enabling LLMs to swiftly retrieve accurate knowledge through database queries when relevant information is needed. Human memory is characterized by features such as forgetting, consolidation, and association. Mem0 (Chhikara et al., 2025) forms memories by summarizing the content of conver-

sations and dynamically organizes these memories and retrieves information from them. Memory-R1 (Yan et al., 2025) has constructed two agents, enabling them to learn how to manage and utilize memory through reinforcement learning.

These methods lack detailed design for the most critical aspect of the memory module—the quality of the memory content. Instead, they only obtain basic information, such as simple keywords and summaries, as the memory content. We believe that high-quality memory content is the key to improving recall and response abilities. We will combine cognitive psychology theories and focus on designing high-quality long-term memory segments to improve these two abilities.

# 3 Method

# 3.1 Overview

The multi-memory segment system constructs short-term memory content into different memory segments and stores them as long-term memory. It utilizes the content of a round of dialogue $C$ as short-term memory $M _ { s h o r t }$ , extracts key information $M _ { k e y }$ from the dialogue, and further processes and analyzes the original dialogue to construct different cognitive perspectives $M _ { c o g }$ , episodic memory $M _ { e p i }$ , and semantic memory $M _ { s e m }$ . Then, these memory segments are used to construct retrieval memory units $M U _ { r e t }$ and contextual memory units $M U _ { c o n t }$ , which are used for memory retrieval and memory use, respectively.

Our multi-memory segment system involves three processes: the construction of long-term memory units, the retrieval of long-term memory units, and the use of contextual memory units. The process of the system processing short-term memory into long-term memory is shown in Figure 1. We employ prompt engineering techniques to extract individual memory segments, with the specific prompt words detailed in Table 5 of Appendix A.

# 3.2 Construction of Long-term Memory Units

The process of converting short-term memory into long-term memory involves two parts. First, the information in short-term memory is processed to generate multiple types of memory segments. Then, these memory segments are used to construct retrieval and matching memory units for retrieval and matching, and contextual memory units for the context of LLM.

Short-term Memory Processing The memory

system processes information into $M _ { l o n g t e r m }$ , analyzes $M _ { s h o r t }$ through LLM, and constructing $M _ { k e y }$ , $M _ { c o g }$ , $M _ { e p i }$ , and $M _ { s e m }$ . First, keywords are extracted from $M _ { s h o r t }$ as important textual identification information in short-term memory. Cognitively analyze short-term memory from different perspectives, and construct a multi-dimensional cognition of the short-term memory to enhance the matching effect. Event information such as plot events is used as $M _ { e p i }$ , and MMS are used to analyze factual information in short-term memory, constructing it as plot memory content. Taking knowledge points as factual knowledge, using cognitive modules to analyze the content of knowledge points in short-term memory, and constructing them as $M _ { c o g }$ . The disclosure of fragmented memory and long-term memory is as follows:

$$
M _ {k e y}, M _ {c o g}, M _ {e p i}, M _ {s e m} = L L M (M _ {s h o r t}) \tag {1}
$$

$$
M _ {\text {l o n g t e r m}} = \left(M _ {\text {k e y}}, M _ {\text {s h o r t}}, M _ {\text {c o g}}, M _ {\text {e p i}}, M _ {\text {s e m}}\right) \tag {2}
$$

Long-term Memory Segment Storing After completing information processing, we have five parts that constitute the long-term memory $M _ { l o n g t e r m }$ : $M _ { k e y }$ , $M _ { s h o r t }$ , $M _ { c o g }$ , $M _ { e p i }$ and $M _ { s e m }$ . We will store long-term memory into two types, one for retrieval and matching, and the other for contextbased knowledge enhancement. The keywords in long-term memory, the original short-term memory content, the various cognitive perspectives of shortterm memory, and the sentence characteristics of episodic memory in short-term memory are more relevant to the semantic form of the user’s query. However, the semantic memory of short-term memory is listed in the form of knowledge points, which is a higher-level knowledge extraction of short-term memory content. It may have some differences from the user’s query language form and is suitable for user knowledge enhancement, but not for retrieval matching. Therefore, considering the principle of encoding specificity, We use $M _ { k e y }$ , $M _ { s h o r t }$ $M _ { c o g }$ and $M _ { e p i }$ as retrieval memory units $M U _ { r e t }$ and construct them into vectors $V _ { m e m o r y }$ for use in the retrieval matching stage.

Episodic memory describes event information, which is similar to the semantics of short-term memory information, and LLM have the ability to understand its content through short-term memory alone. Therefore, we believe that the content of episodic memory does not need to be input as contextual content into large models, only

keywords, short-term memory, multiple cognitive perspectives, and semantic memory are required. Therefore, the composition of the retrieval memory units and the context memory units is as follows:

$$
M U _ {r e t} = \left(M _ {k e y}, M _ {s h o r t}, M _ {c o g}, M _ {e p i}\right) (3)
$$

$$
M U _ {\text {c o n t}} = \left(M _ {\text {k e y}}, M _ {\text {s h o r t}}, M _ {\text {c o g}}, M _ {\text {s e m}}\right) \tag {4}
$$

# 3.3 Retrieval of Long-term Memory Units

The retrieval part will convert the user query $Q$ into a vector form, and then use the cosine similarity formula to calculate the similarity value between the user query vector $V _ { q u e r y }$ and the constructed vector $V _ { m e m o r y }$ , selecting the top-k vectors $V _ { k } = \{ V _ { 1 } , V _ { 2 } , . . . , V _ { K } \}$ as the selected vectors. The cosine similarity used is disclosed as follows:

$$
\cos_ {\text {s i m}} (\mathbf {q}, \mathbf {v}) = \frac {\mathbf {q} \cdot \mathbf {v}}{\| \mathbf {q} \| \| \mathbf {v} \|} \tag {5}
$$

where q and v are respectively the query vector and the vector stored in the memory system.

# 3.4 Utilization of Long-term Memory Units

Based on the selection of top-k vectors $V _ { k } ~ =$ $\{ V _ { 1 } , V _ { 2 } , . . . , V _ { K } \}$ , these retrieval memory units $M U _ { r e t }$ are mapped to corresponding contextual memory units $M U _ { c o n t }$ . $M _ { k e y }$ , $M _ { s h o r t }$ , $M _ { c o g }$ , $M _ { e p i }$ , and $M _ { s e m }$ are used as contextu $C _ { m }$ inputs to LLM to response the user’s query $Q$ and obtain a response $R$ .

$$
R = L L M \left(M U _ {\text {l o n g t e r m}}, Q\right) \tag {6}
$$

# 4 Experimental Setup

# 4.1 Datasets

The LoCoMo dataset (Maharana et al., 2024) is designed to evaluate the long-term dialogue memory capability of large language models. Its primary task is to determine whether these models can accurately recall information from earlier dialogue after multiple rounds. The dataset contains 10 extended sessions, with each session averaging around 600 conversations and 26,000 tokens. Each conversation includes an average of 200 questions along with their corresponding correct answers. This dataset supports multiple evaluation scenarios and is currently a widely adopted authoritative dataset.

There are five problem types: (1) Single-hop questions: Test the memory system’s basic retention by requiring accurate extraction of a single

fact from long dialogue history. (2) Multi-hop questions: Assess the system’s ability to integrate information across multiple dialogue rounds for reasoning. (3) Temporal reasoning questions: Evaluate if the system can understand event sequences and time evolution to construct a clear timeline. (4) Open-domain knowledge questions: Examine the system’s recall and retrieval ability by combining dialogue context with external knowledge. (5) Adversarial questions: Challenge the system’s resistance to forgetting and misleading by inserting interference information.

# 4.2 Evaluation Metrics

We evaluate the performance of memory system using several standard metrics, including Recall@N $( \mathsf { R } @ 1 , \mathsf { R } @ 3 , \mathsf { R } @ 5 )$ , F1 score, and BLEU-1.

Recall $\mathcal { @ } \mathbf { N } ( \mathbf { R } \mathcal { \ @ } \mathbf { N } )$ measures the average proportion of ground-truth answers retrieved within the top-N results. When the number of ground-truth items is less than N, the denominator is adjusted using $\operatorname* { m i n } ( N , | \mathrm { G o l d } _ { i } | )$ to ensure fair evaluation. Formally:

$$
\operatorname {R e c a l l} @ N = \frac {1}{| Q |} \sum_ {i = 1} ^ {| Q |} \frac {\left| \operatorname {T o p} - N _ {i} \cap \operatorname {G o l d} _ {i} \right|}{\min  (N , \left| \operatorname {G o l d} _ {i} \right|)} \tag {7}
$$

where $Q$ is the set of all evaluation queries, $i$ indexes a specific query, ${ \mathrm { G o l d } } _ { i }$ denotes the set of ground-truth answers for the $i$ -th query, $| \mathrm { G o l d } _ { i } |$ is the number of ground-truth answers for that query, and Top- $N _ { i }$ is the set of top-N results retrieved by the system for that query.

F1 Score is the harmonic mean of precision and recall. Let T P , F P , and $F N$ denote the number of true positives, false positives, and false negatives, respectively:

$$
\text {P r e c i s i o n} = \frac {T P}{T P + F P} \tag {8}
$$

$$
\text {R e c a l l} = \frac {T P}{T P + F N} \tag {9}
$$

$$
F 1 = \frac {2 \cdot \text {P r e c i s i o n} \cdot \text {R e c a l l}}{\text {P r e c i s i o n} + \text {R e c a l l}} \tag {10}
$$

F1 is widely used in classification and span-level extraction tasks, such as named entity recognition or QA.

BLEU-1 evaluates unigram precision between the generated output and the reference. BLEU-1 is computed as:

$$
\text {B L E U - 1} = \frac {\# \text {m a t c h i n g u n i g r a m s}}{\# \text {u n i g r a m s i n h y p o t h e s i s}} \tag {11}
$$

A brevity penalty is typically applied to discourage overly short outputs. BLEU-1 is appropriate for tasks like machine translation and text generation where word-level overlap is informative.

# 4.3 Baselines

Our goal is to explore high-quality memory content, and we adopt the representative methods for studying memory content currently employed in research.

Naive RAG Only the character’s dialogue content is vectorized as memory. For a question, it’s vectorized and compared with all memory vectors for similarity. The top-k vectors of original dialogue are chosen as context for answering.

MemoryBank (Zhong et al., 2024) The memory content involved in this method includes: daily chat records, time summaries, user personality traits, and emotional assessments.

A-MEM (Xu et al., 2025) The memory content involved in this method includes: dialogue content, timestamps, keywords, tags, context, and the creation of memory notes via links.

We primarily focus on the impact of memory content on memory effectiveness, and the aforementioned three approaches encompass the distinctive characteristics of different cutting-edge memory contents at present.

# 4.4 Implementation Details

We employed GPT-4o (Hurst et al., 2024), Qwen2.5-14B (Yang et al., 2024), and Gemini-2.5-pro-preview (Comanici et al., 2025) as base models, setting temperature to 0.5 for memory generation and 0.7 for question answering. For the latter, we input the top 5 most relevant contextual memory units (based on retrieval memory units) as context for the agent. We employ an embeddingbased retrieval approach, utilizing all-MiniLM-L6- v2 (Reimers and Gurevych, 2021) as the embedding model.

# 5 Results and Discussion

We conducted an experimental comparative analysis on memory retrieval and memory use ability. In terms of memory retrieval, the recall rates $( \mathbb { R } \ @ 1$ , $\mathbf { R } @ 2$ , and $\mathbf { R } @ 5$ ) were compared and analyzed. In terms of memory usage, F1, which measures the accuracy of responses, and BLEU-1, which measures the quality of responses, were compared and analyzed.

# 5.1 Recall Results

The experimental results show in Table 1 that MMS is superior to other methods in most cases, with a significant overall improvement effect. The improvement effect is most obvious in the Multi Hop scenario, achieving the maximum improvement across models and tasks in the three metrics of R@1, R@3, and $\mathrm { R @ 5 }$ . GPT-4o and Gemini, in particular, improved recall by 8-11 points on the Multi Hop task compared to A-MEM. Multi Hop tasks rely on deep-level associative reasoning among multiple documents. The multi-memory segment strategy of the MMS is more suitable for this complex retrieval scenario, enhancing the effectiveness of cross-document integration and reasoning. Significant improvements have also been observed in Open Domain and Temporal scenarios, indicating that MMS exhibits robustness in contexts with ambiguous and uncertain information structures. In the Single Hop scenario, most cases outperform other methods, but a few cases slightly underperform. This may introduce some redundancy in a simple MMS like Single Hop.

Overall, we think that incorporating this part of content from multiple cognitive perspectives as long-term memory fragments can indeed enhance high recall rates. This aligns with our initial intuitive insight: different users may pose questions from various angles, and by preserving different cognitive perspectives, we can improve the matching degree when questions are asked from different angles.

# 5.2 Answer Results

MMS has achieved leading performance on multiple mainstream large language models, significantly outperforming existing methods such as NaiveRAG, MemoryBank, and A-MEM. Whether it is F1 or BLEU-1, MMS achieves comprehensive improvement in five types of tasks: single-hop, multi-hop, temporal, open-domain, and adversarial question answering. In particular, it has particularly outstanding advantages in multi-hop reasoning and open-domain tasks, demonstrating excellent information integration and reasoning depth. The strong performance of MMS may be due to its use of a multi-level memory structure, which has obvious advantages in improving question-answering accuracy, handling complex logical relationships, and resisting input interference. The experimental results are shown in Table 2.

<table><tr><td rowspan="3">Model</td><td rowspan="2">Method</td><td colspan="3">Single Hop</td><td colspan="3">Multi Hop</td><td colspan="3">Temporal</td><td colspan="3">Open Domain</td><td colspan="3">Adversarial</td><td colspan="3">Average</td></tr><tr><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td></tr><tr><td>NaiveRAG</td><td>14.89</td><td>15.07</td><td>20.90</td><td>26.79</td><td>37.23</td><td>44.11</td><td>7.61</td><td>14.49</td><td>19.38</td><td>20.45</td><td>34.52</td><td>42.15</td><td>9.64</td><td>20.18</td><td>28.81</td><td>15.88</td><td>24.30</td><td>31.07</td></tr><tr><td rowspan="3">GPT-4o</td><td>MemoryBank</td><td>15.60</td><td>12.59</td><td>14.99</td><td>23.05</td><td>30.52</td><td>36.47</td><td>9.78</td><td>13.41</td><td>15.80</td><td>13.67</td><td>20.87</td><td>26.38</td><td>8.52</td><td>17.15</td><td>23.99</td><td>14.12</td><td>18.93</td><td>23.53</td></tr><tr><td>A-MEM</td><td>24.82</td><td>23.76</td><td>29.73</td><td>33.02</td><td>49.79</td><td>58.96</td><td>16.30</td><td>22.28</td><td>30.02</td><td>29.01</td><td>44.81</td><td>53.90</td><td>10.54</td><td>20.96</td><td>25.11</td><td>22.74</td><td>32.32</td><td>39.54</td></tr><tr><td>MMS</td><td>28.53</td><td>30.18</td><td>34.06</td><td>44.18</td><td>59.87</td><td>67.05</td><td>23.73</td><td>26.63</td><td>32.23</td><td>34.98</td><td>53.01</td><td>62.04</td><td>15.31</td><td>31.46</td><td>37.65</td><td>29.35</td><td>40.23</td><td>46.61</td></tr><tr><td rowspan="3">Qwen2.5-14B</td><td>MemoryBank</td><td>21.63</td><td>22.75</td><td>26.51</td><td>32.71</td><td>47.35</td><td>55.78</td><td>13.04</td><td>20.83</td><td>25.09</td><td>26.04</td><td>39.36</td><td>47.23</td><td>9.64</td><td>18.27</td><td>24.89</td><td>20.62</td><td>29.71</td><td>35.90</td></tr><tr><td>A-MEM</td><td>25.21</td><td>25.71</td><td>27.45</td><td>39.19</td><td>52.49</td><td>59.01</td><td>21.73</td><td>23.19</td><td>32.46</td><td>31.27</td><td>46.63</td><td>54.82</td><td>14.57</td><td>27.58</td><td>32.85</td><td>26.39</td><td>35.12</td><td>41.32</td></tr><tr><td>MMS</td><td>23.76</td><td>27.93</td><td>27.83</td><td>39.25</td><td>53.58</td><td>59.13</td><td>23.39</td><td>25.72</td><td>32.59</td><td>30.67</td><td>48.61</td><td>57.57</td><td>16.14</td><td>27.46</td><td>34.64</td><td>26.64</td><td>36.66</td><td>42.35</td></tr><tr><td rowspan="3">Gemini-2.5-pro-preview</td><td>MemoryBank</td><td>14.89</td><td>13.23</td><td>14.10</td><td>28.03</td><td>35.10</td><td>40.58</td><td>8.70</td><td>11.05</td><td>12.63</td><td>14.98</td><td>25.02</td><td>30.42</td><td>11.88</td><td>20.52</td><td>28.04</td><td>15.70</td><td>21.07</td><td>25.15</td></tr><tr><td>A-MEM</td><td>17.73</td><td>17.43</td><td>22.42</td><td>25.55</td><td>38.32</td><td>46.73</td><td>10.87</td><td>15.04</td><td>19.93</td><td>22.47</td><td>37.06</td><td>43.32</td><td>13.46</td><td>23.99</td><td>29.93</td><td>18.02</td><td>26.37</td><td>32.47</td></tr><tr><td>MMS</td><td>26.60</td><td>23.46</td><td>31.27</td><td>34.58</td><td>49.84</td><td>55.56</td><td>16.30</td><td>23.55</td><td>25.85</td><td>31.15</td><td>48.19</td><td>54.30</td><td>12.78</td><td>27.01</td><td>35.20</td><td>23.68</td><td>34.41</td><td>40.44</td></tr></table>

Table 1: The recall performance comparison of each method in five task scenarios. The gold truth corresponding to a single query may consist of multiple documents. For ${ \bf R } @ { \bf N }$ , we use the minimum value between N and the number of documents in the gold truth as the denominator. Therefore, the denominators for $\mathbb { R } \ @ 1$ , ${ \mathrm { R } } @ 3$ , and $\mathbf { R } @ 5$ may differ, and it is reasonable to observe scenarios where, for example, ${ \mathrm { R @ 3 } }$ has a lower value than $\mathbf { R } \ @ 1$ .   
Table 2: Comparison of Generation Performance of Four Methods on the LoCoMo Dataset   

<table><tr><td rowspan="2">Model</td><td rowspan="2">Method</td><td colspan="2">Single Hop</td><td colspan="2">Multi Hop</td><td colspan="2">Temporal</td><td colspan="2">Open Domain</td><td colspan="2">Adversarial</td><td colspan="2">Average</td></tr><tr><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td></tr><tr><td rowspan="4">GPT-4o</td><td>NaiveRAG</td><td>18.04</td><td>12.24</td><td>33.03</td><td>28.28</td><td>14.03</td><td>12.15</td><td>31.74</td><td>26.78</td><td>7.60</td><td>6.90</td><td>20.89</td><td>17.27</td></tr><tr><td>MemoryBank</td><td>15.39</td><td>11.18</td><td>28.44</td><td>24.18</td><td>11.91</td><td>11.23</td><td>22.04</td><td>18.86</td><td>8.65</td><td>7.93</td><td>17.29</td><td>14.68</td></tr><tr><td>A-MEM</td><td>22.98</td><td>15.98</td><td>34.35</td><td>29.57</td><td>14.66</td><td>13.21</td><td>35.64</td><td>30.19</td><td>9.24</td><td>8.58</td><td>23.37</td><td>19.65</td></tr><tr><td>MMS</td><td>28.67</td><td>20.96</td><td>47.37</td><td>39.98</td><td>20.81</td><td>19.07</td><td>42.98</td><td>36.89</td><td>12.87</td><td>11.67</td><td>30.54</td><td>25.74</td></tr><tr><td rowspan="4">Qwen2.5-14B</td><td>NaiveRAG</td><td>18.56</td><td>12.31</td><td>29.26</td><td>25.44</td><td>13.90</td><td>12.28</td><td>31.60</td><td>27.40</td><td>10.77</td><td>9.46</td><td>20.82</td><td>17.38</td></tr><tr><td>MemoryBank</td><td>21.37</td><td>14.79</td><td>28.41</td><td>24.26</td><td>13.97</td><td>12.13</td><td>32.99</td><td>28.28</td><td>11.03</td><td>10.04</td><td>21.55</td><td>17.90</td></tr><tr><td>A-MEM</td><td>30.71</td><td>23.44</td><td>32.46</td><td>28.55</td><td>14.08</td><td>13.97</td><td>43.02</td><td>37.69</td><td>11.38</td><td>10.11</td><td>26.33</td><td>22.75</td></tr><tr><td>MMS</td><td>28.91</td><td>22.40</td><td>34.00</td><td>29.40</td><td>14.52</td><td>14.01</td><td>44.69</td><td>38.76</td><td>13.28</td><td>11.98</td><td>27.08</td><td>23.31</td></tr><tr><td rowspan="4">Gemini-2.5-pro-preview</td><td>NaiveRAG</td><td>23.80</td><td>17.27</td><td>34.96</td><td>26.92</td><td>14.24</td><td>12.50</td><td>30.64</td><td>26.43</td><td>5.76</td><td>5.15</td><td>21.88</td><td>17.65</td></tr><tr><td>MemoryBank</td><td>14.12</td><td>9.30</td><td>34.15</td><td>29.36</td><td>8.55</td><td>6.48</td><td>25.82</td><td>22.17</td><td>4.62</td><td>3.92</td><td>17.45</td><td>14.25</td></tr><tr><td>A-MEM</td><td>22.69</td><td>15.83</td><td>40.97</td><td>35.63</td><td>12.42</td><td>11.11</td><td>36.23</td><td>31.42</td><td>7.20</td><td>7.02</td><td>23.90</td><td>20.20</td></tr><tr><td>MMS</td><td>25.96</td><td>18.96</td><td>41.49</td><td>36.99</td><td>12.91</td><td>11.32</td><td>42.00</td><td>36.59</td><td>7.09</td><td>6.81</td><td>25.89</td><td>22.13</td></tr></table>

# 5.3 Ablation Study

We conducted ablation experiments on each module of the MMS. When the keyword part is not included, the $\mathbf { R } @ 1$ metric under the Single Hop task and the $\mathbf { R } @ 5$ metric under the Open Domain task perform the best. We believe that when the keyword part is not included. When multiple cognitive perspectives are not included, the $\mathbf { R } @ 3$ metric under the Multi Hop task, the $\mathtt { R @ 5 }$ metric under the Temporal task, and the $\mathbf { R } @ 1$ metric under the Adversarial task perform the best. We believe that different memory segments can be used to construct retrieval units based on different character scenarios. In a broader general scenario, the retrieval units composed of keywords, short-term memory, multiple cognitive perspectives, and episodic memory can achieve the best performance in most tasks. The experimental results are shown in Table 3.

Similarly, we can also observe from Table 4 that keywords yield greater benefits for inferential questions (single-hop, multi-hop, and adversarial), whereas for temporal questions, multiple cognitive perspectives and semantic memory bring more benefits. In terms of a broader range of domain-specific questions, the memory content encompassed by our MMS better enhances the quality of generated content.

# 5.4 Analyze Long-term Memory Units

We used GPT-4o as the base to compare the effects of adding the remaining memory segments in the MMS retrieval units and contextual memory units. For retrieval, retrieval memory units includes keywords, short-term memory, multiple cognitive perspectives, and episodic memory. We add semantic memory for comparison, and find recall rate decreases. This is because generated semantic memory is a high-level overview of factual knowledge in short-term memory, creating a gap with the semantic form of user’s question. For generation, contextual memory units of MMS consists of keywords, short-term memory, multiple cognitive perspectives, and semantic memory. We add contextual memory for comparison and find generation quality decreases after adding episodic memory. As episodic memory describes event-related content already present in original short-term memory, adding more can lead to redundancy and lower generation quality. Thus, the composition modules of retrieval and contextual memory units we use are reasonable. The experimental results are shown in Figure 2. In general, during the retrieval process, emphasis should be placed on keywords, multi-perspective, and event-oriented information to ensure that the "net" of the search is sufficiently

<table><tr><td rowspan="2">Method</td><td colspan="3">Single Hop</td><td colspan="3">Multi Hop</td><td colspan="3">Temporal</td><td colspan="3">Open Domain</td><td colspan="3">Adversarial</td><td colspan="3">Avg</td></tr><tr><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td><td>R@1</td><td>R@3</td><td>R@5</td></tr><tr><td>w/o Key</td><td>30.85</td><td>29.72</td><td>34.02</td><td>42.68</td><td>59.29</td><td>66.17</td><td>23.21</td><td>23.55</td><td>31.86</td><td>34.60</td><td>52.55</td><td>62.18</td><td>11.88</td><td>27.36</td><td>33.40</td><td>28.64</td><td>38.49</td><td>45.53</td></tr><tr><td>w/o Cog</td><td>26.24</td><td>26.06</td><td>32.73</td><td>43.61</td><td>59.96</td><td>66.33</td><td>20.65</td><td>27.53</td><td>31.83</td><td>31.98</td><td>49.36</td><td>59.53</td><td>15.47</td><td>30.49</td><td>37.57</td><td>27.59</td><td>38.68</td><td>45.60</td></tr><tr><td>w/o Epi</td><td>27.66</td><td>27.30</td><td>33.17</td><td>42.37</td><td>59.03</td><td>68.38</td><td>18.47</td><td>24.99</td><td>31.68</td><td>31.51</td><td>47.98</td><td>57.51</td><td>15.02</td><td>29.48</td><td>37.00</td><td>27.01</td><td>37.76</td><td>45.55</td></tr><tr><td>w/o Cog &amp; Epi</td><td>21.99</td><td>20.57</td><td>24.21</td><td>37.69</td><td>55.40</td><td>62.12</td><td>13.04</td><td>21.20</td><td>23.82</td><td>26.75</td><td>41.52</td><td>48.79</td><td>13.00</td><td>26.01</td><td>35.99</td><td>22.49</td><td>32.94</td><td>38.99</td></tr><tr><td>MMS</td><td>28.53</td><td>30.18</td><td>34.06</td><td>44.18</td><td>59.87</td><td>67.05</td><td>23.73</td><td>26.63</td><td>32.23</td><td>34.98</td><td>53.01</td><td>62.04</td><td>15.31</td><td>31.46</td><td>37.65</td><td>29.35</td><td>40.23</td><td>46.61</td></tr></table>

Table 3: The ablation experiment using GPT-4o as the base model of MMS method. The symbol "w/o" indicates an experiment in which a specific module was removed. Key represents the key words of short-term memory, Cog represents multiple cognitive perspectives on short-term memory, and Epi represents episodic memory of short-term memory.   
Table 4: The ablation experiment using GPT-4o as the base model of MMS method. The symbol "w/o" indicates an experiment in which a specific module was removed. Key represents the key words of short-term memory, Cog represents multiple cognitive perspectives on short-term memory, and Sem represents semantic memory of short-term memory.   

<table><tr><td rowspan="2">Method</td><td colspan="2">Single Hop</td><td colspan="2">Multi Hop</td><td colspan="2">Temporal</td><td colspan="2">Open Domain</td><td colspan="2">Adversarial</td><td colspan="2">Average</td></tr><tr><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td><td>F1</td><td>BLEU-1</td></tr><tr><td>w/o Key</td><td>27.51</td><td>18.53</td><td>44.24</td><td>36.52</td><td>19.28</td><td>16.12</td><td>41.27</td><td>35.53</td><td>10.88</td><td>10.12</td><td>28.64</td><td>23.36</td></tr><tr><td>w/o Cog</td><td>27.77</td><td>19.77</td><td>45.68</td><td>38.46</td><td>17.36</td><td>15.47</td><td>43.01</td><td>36.87</td><td>12.27</td><td>11.65</td><td>29.22</td><td>24.44</td></tr><tr><td>w/o Sem</td><td>27.79</td><td>19.16</td><td>45.31</td><td>37.94</td><td>19.51</td><td>18.50</td><td>42.69</td><td>36.60</td><td>12.45</td><td>11.44</td><td>29.55</td><td>24.73</td></tr><tr><td>w/o Cog&amp;Sem</td><td>28.24</td><td>20.24</td><td>45.05</td><td>38.34</td><td>17.92</td><td>15.44</td><td>42.71</td><td>37.42</td><td>12.83</td><td>11.53</td><td>29.35</td><td>24.59</td></tr><tr><td>MMS</td><td>28.67</td><td>20.96</td><td>47.37</td><td>39.98</td><td>20.81</td><td>19.07</td><td>42.98</td><td>36.89</td><td>12.87</td><td>11.67</td><td>30.54</td><td>25.74</td></tr></table>

broad and precise. During the generation process, focus should be on facts, keywords, and high-level semantics to ensure that the model, while understanding the general meaning, is not confused by repetitive event descriptions. Therefore, our MMS will achieve high-quality results.

# 5.5 Robustness of the Number of Memories

We manipulated the quantities of memory segments to evaluate the impact on performance. As the value of n increased, performance exhibited enhancement notwithstanding the introduction of noise, owing to the high-caliber memory contents facilitating the differentiation between pivotal and noisy data. This demonstrates the resilience of our method with respect to the quantities of memory segments. The results can be viewed in Table 6 of Appendix C.

# 5.6 Analysis of Token and Latency Overhead

We analyzed token and latency overhead for different methods experimentally, specifically calculating the average overhead needed to generate the memory content for each query. Compared to A-MEM, our approach proves to be faster and more resource-efficient. Although there’s a slight rise in latency and overhead compared to the memory repository, its long-term memory is overly simplistic and of low quality, resulting in poor performance. Conversely, our method generates a larger volume of high-quality memory content despite the

increased overhead, with only a minimal latency increase that barely affects user experience. Our method holds practical value. The results can be viewed in Table 7 of Appendix C.

# 6 Conclusion

This paper creates a multi-memory system by integrating with cognitive psychology to build effective long-term memory, boosting recall and generation quality. Multiple memory theory suggests human memory comes in many forms. Current methods don’t consider the variety of human memory segments and just use summarization, leading to low-quality content. Since people understand problems from different angles, we view short-term memory’s various cognitive aspects as long-term memory segments. Levels-of-processing theory states that deeper encoding leads to better memory retention. MMS turns short-term memory into long-term segments like keywords, different cognitive views, episodic and semantic memories. It constructs retrieval units from these for recall and builds contextual units to enhance knowledge. Experiments indicate MMS enhances recall and generation at lower cost, proving practical value. Our analysis under varying memory segment counts reveals high-quality content ensures sustained gains and noise resilience. Our approach effectively integrates cognitive psychology into the research on agent memory within artificial intelligence for future studies.

# 7 Limitations

Our current experiments have demonstrated the effectiveness of our method on both open-source and closed-source models. We are considering constructing a dedicated memory model in the future specifically for generating high-quality memories, employing Supervised Fine-Tuning and Reinforcement Learning techniques to better enhance the model’s capability in generating high-quality longterm memories.

# 8 Ethics Statement

We guarantee that the dataset we use are public dataset and do not involve data leakage or other issues. Our method is a universal way of processing knowledge, without involving issues such as racial discrimination and moral hazard.

# References

Nick Alonso, Tomás Figliolia, Anthony Ndirango, and Beren Millidge. 2024. Toward conversational agents with context and time sensitive long-term memory. arXiv preprint arXiv:2406.00057.   
Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413.   
Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, and 1 others. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261.   
Fergus IM Craik and Robert S Lockhart. 1972. Levels of processing: A framework for memory research. Journal of verbal learning and verbal behavior, 11(6):671–684.   
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yixin Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. 2023. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2:1.   
Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. 2024. Hipporag: Neurobiologically inspired long-term memory for large language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems.   
Yuki Hou, Haruki Tamoto, and Homei Miyashita. 2024. my agent understands me better": Integrating dynamic human-like memory recall and consolidation in llm-based agents. In Extended Abstracts of the

CHI Conference on Human Factors in Computing Systems, pages 1–7.   
Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901.   
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276.   
Ferenc Huszár. 2018. Note on the quadratic penalties in elastic weight consolidation. Proceedings of the National Academy of Sciences, 115(11):E2496–E2497.   
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM computing surveys, 55(12):1–38.   
Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023. Thinkin-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719.   
Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. 2023. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. arXiv preprint arXiv:2308.08239.   
Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753.   
Charles Packer, Vivian Fang, Shishir_G Patil, Kevin Lin, Sarah Wooders, and Joseph_E Gonzalez. 2023. Memgpt: Towards llms as operating systems.   
Nils Reimers and Iryna Gurevych. 2021. all-minilml6-v2 sentence transformer. https://huggingface. co/sentence-transformers/all-MiniLM-L6-v2. Accessed: 2025-4-25.   
Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. 2023. Cognitive architectures for language agents. Transactions on Machine Learning Research.   
Endel Tulving. 1985. How many memory systems are there? American psychologist, 40(4):385.   
Endel Tulving. 1986. Episodic and semantic memory: Where should we go from here? Behavioral and Brain Sciences, 9(3):573–577.   
Endel Tulving and Donald M Thomson. 1973. Encoding specificity and retrieval processes in episodic memory. Psychological review, 80(5):352.

Endel Tulving and 1 others. 1972. Episodic and semantic memory. Organization of memory, 1(381-403):1.   
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, and 1 others. 2024. A survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345.   
Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023. Augmenting language models with long-term memory. Advances in Neural Information Processing Systems, 36:74530–74543.   
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, and 1 others. 2025. The rise and potential of large language model based agents: A survey. Science China Information Sciences, 68(2):121101.   
Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. 2025. A-mem: Agentic memory for llm agents. In Advances in Neural Information Processing Systems.   
Sikuan Yan, Xiufeng Yang, Zuchao Huang, Ercong Nie, Zifeng Ding, Zonggen Li, Xiaowen Ma, Kristian Kersting, Jeff Z Pan, Hinrich Schütze, and 1 others. 2025. Memory-r1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828.   
An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115.   
Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024. A survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501.   
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2024. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19724–19731.   
Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, and 1 others. 2025. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153.

# A Prompts

We construct keywords, multiple cognitive perspectives, episodic memory, and semantic memory based on the content in short-term memory. Here, episodic memory mainly describes the event information that occurred at a specific time and place, so we label it as situations in the prompt. Semantic memory mainly contains factual world knowledge, so we label it as knowledge points. The specific content is shown in Table 5.

# Prompt

You are a linguistics expert. Please generate the following required content based on the context and dialogue below.

Dialogue time: {time}

Context: {context}

Dialogue: {content}

1. Analyze the key words in the above dialogue.   
2. Analyze the dialogue content from different cognitive perspectives based on the context and the content of the dialogue.   
3. Analyze the situations that can serve as episodic memory from the dialogue.   
3. Analyze the knowledge points in the dialogue.   
4. You must output in the required format.

Format the response as a JSON object:

```jsonl
{"keywords":[ // The key information should be able to represent the dialogue, such as: characters, intentions, topics, time, place, events and other important information. // Only analyze the key words in the dialogue, not those in the context. ], "cognitive_perspectives":[ // Context information is provided to help you understand the entire chat process and assist you in analysis. // Only analyze the content in the dialogue, not the content in the context. ], "situations":[ // An event that occurs at a specific time and place. // Extract the situations that can be part of episodic memory from the dialogue. // Only analyze the key words in the dialogue, not those in the context. ], "knowledge_points":[ // Only extract the knowledge points from the dialog that can be part of semantic memory. The context merely helps you understand the background. // Only analyze the key words in the dialogue, not those in the context. ] }} 
```

Table 5: Prompt for generating long-term memory content. time is the dialogue time, context is the context summary information provided in the dialogue dataset, and content is the content of this round of dialogue.

# B Ablation Experiment

![](images/1eb7546e2bb2ec075461adf7f21ca29bda40a2d32061d5afb3fe0f7d65d915a6.jpg)  
Figure 2: Compare the impact on performance after adding other segments. In terms of recall metrics, MMS and MMS+Sem were compared. In terms of generation, MMS and MMS+Epi were compared. Sem refers to semantic memory, and Epi refers to episodic memory.

# C Peformance

Table 6: Performance comparison for different values of n   

<table><tr><td>Metrics</td><td>n=1</td><td>n=3</td><td>n=5</td><td>n=7</td><td>n=9</td></tr><tr><td>Avg Fl</td><td>20.74</td><td>25.07</td><td>30.54</td><td>34.81</td><td>36.13</td></tr><tr><td>Avg BLUE-1</td><td>17.44</td><td>21.39</td><td>25.74</td><td>28.95</td><td>31.28</td></tr></table>

Table 7: Comparison of Latency Overhead and Token Overhead   

<table><tr><td>Metrics</td><td>MMS</td><td>A-MEM</td><td>Memory Bank</td></tr><tr><td>Avg Latency</td><td>1.309</td><td>3.931</td><td>0.949</td></tr><tr><td>Avg Tokens</td><td>744</td><td>1429</td><td>238</td></tr></table>