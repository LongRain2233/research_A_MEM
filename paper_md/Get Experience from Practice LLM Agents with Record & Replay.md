# Get Experience from Practice: LLM Agents with Record & Replay

Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhichao Hua, Yubin Xia $^ *$ , Haibo Chen Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University

May 26, 2025

# Abstract

AI agents, empowered by Large Language Models (LLMs) and communication protocols such as MCP and A2A, have rapidly evolved from simple chatbots to autonomous entities capable of executing complex, multi-step tasks, demonstrating great potential. However, the LLMs’ inherent uncertainty and heavy computational resource requirements pose four significant challenges to the development of safe and efficient agents: reliability, privacy, cost and performance. Existing approaches, like model alignment, workflow constraints and on-device model deployment, can partially alleviate some issues but often with limitations, failing to fundamentally resolve these challenges.

This paper proposes a new paradigm called AgentRR (Agent Record & Replay), which introduces the classical record-and-replay mechanism into AI agent frameworks. The core idea is to: $\textcircled{1}$ record an agent’s interaction trace with its environment and internal decision process during task execution, $\textcircled{2}$ summarize this trace into a structured “experience” encapsulating the workflow and constraints, and $\textcircled{3}$ replay these experiences in subsequent similar tasks to guide the agent’s behavior. We detail a multilevel experience abstraction method and a check function mechanism in AgentRR: the former balances experience specificity and generality, while the latter serves as a trust anchor to ensure completeness and safety during replay. In addition, we explore multiple application modes of AgentRR, including user-recorded task demonstration, large-small model collaboration and privacy-aware agent execution, and envision an experience repository for sharing and reusing knowledge to further reduce deployment cost.

While AgentRR exhibits significant potential to address current agent problems, it also introduces new challenges regarding experience completeness, replay robustness, generalization, and scope of applicability. Therefore, this paper thoroughly analyzes the feasibility and advantages of AgentRR’s mechanisms, and demonstrates its practical applications across common agent scenarios. Furthermore, we establish theoretical foundations and propose novel solutions for future research in agent systems.

# 1 Introduction

AI agents have seen rapid advancement[48], evolving from basic conversational bots to intelligent entities capable of understanding complex instructions, autonomously planning[10, 46], and executing multi-step tasks[11, 24]. Modern agents[3, 19, 59, 92] demonstrate strong task-handling abilities across diverse domains, e.g., a PC-based agent[86] autonomously conducting research and generating web pages, or a mobile assistant[81] automatically finding coupon codes while shopping. They can invoke various software tools

via agent-to-tools communication protocols like MCP[2, 31] and can coordinate with other agents through agent-to-agent protocols like A2A[28]. For example, the Manus AI system[65] can execute complex tasks like compiling to-do lists or searching real estate listings, and MobileAgent can perform intricate operations on smartphones. This growing autonomy endows agents with unprecedented capabilities, allowing them to independently plan and carry out complex actions.

However, such autonomy also means an agent’s decision-making process can become opaque and unpredictable. If an agent deviates from expected behavior, it may lead to serious consequences, heightening the need for reliability and controllability. In other words, improving an agent’s intelligence must go hand-inhand with mechanisms to constrain and guide its behavior to ensure it stays on track. Despite significant progress, widespread adoption of AI agents is hindered by four core challenges:

1. Challenge-1: Reliability. LLM-based AI agents often exhibit the hallucination phenomenon, which is inherent in the probabilistic nature of LLMs[5, 33, 36, 84]. An agent may function well most of the time, but without guaranteed correctness it can make unstable or incorrect decisions. This unpredictability erodes trust: even powerful LLMs cannot ensure absolutely reliable agent behavior.   
2. Challenge-2: Privacy. Many agents, especially those backed by cloud-hosted LLMs, require users to send large amounts of personal or sensitive data[77, 83] (e.g. screenshots, documents, interaction logs) to remote servers for processing. This raises serious privacy concerns: any cloud-based analysis of user data risks potential data leakage or misuse[8, 37, 58].   
3. Challenge-3: Operational Cost. Sophisticated tasks typically demand multiple turns of interaction between the agent and the LLM[24, 45]. Multi-modal tasks (e.g. analyzing GUI screenshots) tend to be even more expensive[25, 56], since vision-language model calls are costlier than text. As an illustrative datapoint, the Manus agent reportedly costs on the order of $\$ 2$ USD in API usage to complete a single complex task on average. This cost may be unacceptable at scale or for consumer use-cases.   
4. Challenge-4: Execution Performance. In many interactive scenarios, today’s agents are significantly slower than human operators. Complex tasks that require an agent to reason or plan over multiple steps are often bottlenecked by the LLM’s inference speed and the overhead of iterative prompt exchanges[49, 70].

Existing model-based solutions still exhibit significant limitations. Regarding model safety, methods such as RLHF[12, 73] are commonly employed to align the model securely. However, recent studies[82] have demonstrated that these safety mechanisms can be circumvented merely by using specific prompts. Other approaches attempt to enable the model to self-correct[80, 89], but this process lacks any formal safety guarantees. On the other hand, to enhance the execution efficiency of agents as well as minimize the model size, current state-of-the-art methods often rely on model distillation[16, 27, 74, 87] or pruning[30, 41, 47, 71] to design specialized lightweight models for different agents. Nevertheless, such approaches constrain the applicability of agents to specific scenarios; when tasks or requirements change, the distilled or pruned models are unable to adapt. Therefore, relying solely on model itself makes it challenging to simultaneously address the requirements for high reliability, privacy protection, performance efficiency, and cost-effectiveness in agent scenarios.

In this paper, we propose AgentRR (Agent Record & Replay), a new paradigm inspired by the record-and-replay (R&R) techniques long used in software systems for debugging and reliability.

![](images/55294f77b19e50c1e35b7530760a28676b1dde50f41f11a4d963171847deb2f0.jpg)

![](images/260319545af82fe11f9836c31ba058e087a8ffd98a0b8eaaf7f033af45706d9e.jpg)

![](images/8c9f7343ac5d54d77851a4db67ebc94b8121edca891ab61fea69c287b48a9bc2.jpg)

![](images/e61702fb56ab6c5acc350a8397450f7a3834a0f09929b62d29a646f2921cf5e6.jpg)  
Figure 1: Conceptual comparison of human, R&R tools, LLM agents, and AgentRR in task execution.

Figure 1 illustrates our envisioned system concept for AgentRR, contrasting it with human, traditional record-and-replay (R&R) tools, and LLM agent approaches in handling tasks. In scenarios where humans can solve a task, they typically generate valid solutions and avoid incorrect results. R&R tools, by searching a record database for human-generated traces and replaying them, can reliably reproduce these correct solutions. LLM agents, thanks to their strong generalization ability, can not only accomplish tasks previously demonstrated by humans, but also derive novel solutions when tasks and environments change. However, this generalization introduces two major risks: (1) LLM outputs are inherently probabilistic, so there is always a chance of producing incorrect results; (2) even with some self-correction capability, LLM agents may still enter unrecoverable error states

AgentRR is guided by the design philosophy of bounded intelligence: constraining agent intelligence within safe, successful experiences. By combining LLM capabilities with record-and-replay mechanisms, AgentRR leverages model generalization while grounding actions in proven experiences. This approach prevents unrecoverable errors and reduces incorrect outputs, while maintaining flexibility for valid new solutions.

After exploring the design philosophy above, we identify that the core to realizing this framework is the notion of experience. The experience abstraction is intended to decouple an agent’s intelligence from its execution: rather than requiring the agent to reason from scratch for every task, the agent can leverage accumulated knowledge from past successful behaviors. However, if an experience is too abstract, it offers strong generalization but may cause the replaying agent—especially if not sufficiently intelligent—to instantiate incorrect execution details or results, leading to replay failures. Conversely, if an experience is too concrete, it restricts the agent’s generalization ability, making the experience difficult to reuse when the application scenario changes even slightly, which can also result in replay failure. Overall, naively using a single plan,

trace, or prior LLM/human reasoning process as the experience is insufficient to achieve both reliability and generalization in practical agent systems.

To address these limitations, AgentRR introduces the concept of multi-level experience, which abstracts and summarizes past behaviors at different levels of granularity. Low-level experiences capture precise, concrete action sequences—such as specific UI operations or API calls—enabling rapid and reliable replay in environments highly similar to the original context. In contrast, high-level experiences provide more generalized procedural knowledge, allowing the agent to adapt to new environments or task variations by leveraging the LLM’s reasoning ability to instantiate concrete actions from abstract plans. During execution, AgentRR dynamically selects the most appropriate level of experience based on the current environment and task requirements: when the context closely matches the original, low-level experiences are preferred for efficiency and reliability; when the context changes, high-level experiences offer greater flexibility and generalization. This multi-level experience framework allows AgentRR to balance the trade-off between reliability and generalization, ensuring that agents can both robustly reproduce known solutions and safely extend to new scenarios, all while maintaining strong safety guarantees through check functions at each level.

With the notion of multi-level experience, AgentRR adopts a workflow consisting of three main phases: record, summary, and replay. In the record phase, the system captures detailed traces of agent or human interactions while completing a task. The summary phase abstracts and organizes these traces into multi-level experiences, distilling both concrete action sequences and higher-level procedural knowledge. Importantly, during the summary phase, AgentRR also generates corresponding check functions for each experience. These check functions serve as safety boundaries, verifying execution flow integrity, state preconditions, parameter constraints, and other invariants to ensure that the agent’s actions during replay remain reliable and secure. Finally, in the replay phase, the agent leverages the most suitable experience to guide its actions in new tasks, dynamically adapting to the current environment while ensuring safety and reliability through experience boundaries and check functions.

By structuring agent execution around multi-level experience-based R&R, AgentRR fundamentally transforms the agent’s operating mode. This paradigm shift brings significant benefits in reliability, privacy, cost, and performance, as detailed below:

• Reduced reliance on LLM computation: Since most task steps can be executed by replaying a pre-derived plan (experience), the number of calls or the capability required by the LLM is drastically decreased. This not only lowers the runtime cost of LLM invocation, but also reduces the power consumption of the agent which is especially beneficial for battery-powered devices.   
• Improved reliability via guardrails: The structured experience with check functions contains the fixed workflow (sequence of actions) and constraints that were validated in a prior successful run. Replaying this experience provides the agent with clear guardrails, making its execution more deterministic and bounded. Errors stemming from LLM hallucinations can be prevented or caught early, because the agent is no longer free to arbitrarily diverge from the recorded template.   
• Higher execution efficiency: Executing a predefined sequence from experience is typically much faster than dynamic reasoning. Instead of spending time formulating each step via prompting and waiting for large-scale LLM responses, the agent can perform the next recorded action immediately or only needs to rely on a small-scale LLM for task replay. The agent’s performance becomes closer to that of a scripted program or human who already knows the procedure, rather than a model thinking from scratch.

• Enhanced privacy: When experiences are replayed mostly on the local device, and only minimal data need to be sent to the local LLM for interpretation, the exposure of user data is vastly reduced. In AgentRR, one can imagine the heavy planning phase (record) being done in a secure environment or with a trusted model, and the replay phase running locally without transmitting raw user data. Thus, AgentRR shifts the agent’s operating mode in a way that can inherently strengthen privacy.

AgentRR is not a monolithic technique but rather a framework that can be realized in different ways to suit various scenarios. The paradigm opens up a design space of flexible application modes, characterized by who (or what) performs the recording and who performs the replay. By mixing and matching the entities responsible for record vs. replay, we can decouple “intelligence” from “execution” along different dimensions and optimize the use of resources and expertise:

• User Record, Model Replay: In this mode, a human user demonstrates the task to be done (when the system is put in a ‘recording” state). The system captures the user’s sequence of actions and outcomes in detail, then summarizes it into an experience representation that an agent can interpret and execute. It is akin to programming by demonstration: the user provides one successful example, and the agent can repeat that process later on new instances of the task.   
• Large Model Record, Small Model Replay: Here, a powerful and expensive model is used to perform one or more representative runs of a task in a controlled setting, leveraging the large model’s superior planning, understanding, and complex interaction capabilities to produce a correct solution trace. Then, those experiences are deployed to resource-constrained edge devices (like smartphones or IoT devices) where a lightweight local agent can replay. This enables low-cost, fast, and offline-capable execution, assuming the new task instance is sufficiently similar to the recorded experience.   
• Untrusted Model Record, Trusted Model Replay: In this mode, one might use an untrusted model (e.g., on an untrusted cloud) to explore and generate a candidate solution for a task in a sandbox environment. That exploratory run is recorded as an experience, potentially along with checkpoints to verify correctness. Then, a small agent in a trusted environment (e.g., in TEE) replays the experience in the real environment, ensuring that only the vetted actions occur. This effectively separates the model’s “exploration and discovery” ability from the “trusted execution” process, allowing careful use of advanced model capabilities without compromising on critical safety or privacy policies in deployment.

Table 1 shows more possible scenarios of applying the Record & Replay paradigm to LLM agents by outlining different combinations for who or what performs the recording and replaying actions, along with their primary use cases. When one LLM records for the same LLM to replay, it can lead to deterministic execution and skill consolidation. If one LLM records for another LLM, this facilitates knowledge transfer and multi-agent coordination. A small LLM recording for a large LLM can be used for trace verification and experience refinement. Finally, a trusted LLM can record for an untrusted LLM to enable the safe exploration of new solutions.

Beyond these modes, AgentRR also paves the way for an ecosystem of experience sharing. Once experiences are recorded and summarized, they could be standardized and packaged into modules that can be exchanged among users, devices, or applications. We envision an Experience Store or skill marketplace where users can download or subscribe to experiences created by others, or contribute their own. Shared experiences, especially if accompanied by metadata about their conditions of validity, could significantly reduce the cost and expertise required for individuals to deploy effective agents.

Table 1: Comparison of Different R&R Paradigms for LLM Agents   

<table><tr><td>Recorder</td><td>Replayer</td><td>Primary Use Cases</td></tr><tr><td>Human</td><td>LLM</td><td>Task automation, knowledge capture from human experience</td></tr><tr><td>LLM Agent</td><td>Human</td><td>Understanding agent behavior, teaching</td></tr><tr><td>One LLM</td><td>Same LLM</td><td>Deterministic execution, skill consolidation via experience replay</td></tr><tr><td>One LLM</td><td>Another LLM</td><td>Knowledge transfer via shared experience, multi-agent coordination</td></tr><tr><td>Large LLM</td><td>Small LLM</td><td>Bridging capability gaps, execution efficiency</td></tr><tr><td>Small LLM</td><td>Large LLM</td><td>Trace verification, experience refinement</td></tr><tr><td>Untrusted LLM</td><td>Trusted LLM</td><td>Privacy preserved execution</td></tr><tr><td>Trusted LLM</td><td>Untrusted LLM</td><td>Safe exploration of new solutions</td></tr></table>

While AgentRR offers a promising direction to address many challenges of current agents, it is not a panacea. In tackling the old problems, it introduces new research questions that reflect the transformation of dynamic AI behavior into reusable knowledge artifacts. Key among these are: How to ensure the completeness of recorded information so that the experience captures everything needed to reliably replay later? How to maintain robustness of the replay in the face of environment changes or unexpected divergences? How to improve the generalizability of experiences so that they can apply to a broader range of scenarios beyond the exact recorded case? And how to delineate the scope of tasks for which R&R is most suitable versus where a more flexible approach is needed? The remainder of this paper delves into the AgentRR paradigm in detail, describes its implementation mechanisms, compares it to related paradigms, and discusses these open challenges and future directions.

# 2 Related Work

# 2.1 Agent Reliability and Privacy

The rapid advancement of AI agents has brought significant challenges in reliability and privacy. In terms of reliability, LLM agents often struggle with hallucinations — generating plausible yet factually incorrect or nonsensical outputs. This tendency erodes user trust and can lead to substantial errors, particularly in scenarios involving chained actions where inaccuracies can accumulate and compound over time. Moreover, the open-ended nature of LLMs makes their hallucination challenges distinct from earlier task-specific models, with factual errors being the predominant manifestation[24, 45].

To address reliability issues, researchers have developed various approaches. Approaches such as LLM guardrails[15, 17, 29, 35] aim to control an agent’s inputs and outputs through multiple methods. RLHF[12, 73], for example, uses human judgments to shape a reward function for fine-tuning, which can reduce toxic or obviously incorrect outputs, but remains limited in preventing hallucinations or subtle errors. Retrieval-Augmented Generation (RAG) approaches[22, 43, 68] integrate real-time knowledge retrieval to ground responses in verified information, though their effectiveness depends heavily on the quality of retrieved content. Advanced prompting techniques like Chain-of-Thought (CoT)[10, 70] and ReAct prompting[85] help structure the reasoning process and improve accuracy. Fact-checking mechanisms[21] help verify the accuracy of generated content. Additionally, in multi-agent systems[6, 93], employing multiple agents to check each other’s work or vote on outcomes can improve reliability. However, this approach often incurs greater computational cost and system complexity.

Privacy concerns[14, 37, 58] arise particularly in cloud-hosted LLM agents, where uploading user data introduces risks of unauthorized access and potential misuse for personal secrets. The centralized nature of data handling in cloud environments, coupled with users’ reduced control over their own information, underscores the urgency of addressing these privacy vulnerabilities. Research[8, 9] has demonstrated that LLMs can inadvertently memorize and subsequently reproduce sensitive information present in their training data, posing an additional privacy threat.

To address these privacy challenges, researchers have developed various privacy-preserving techniques. Differential privacy[7, 44, 83] provides protection by adding statistical noise to individual data points. Federated learning[50, 55] enables collaborative training without centralizing sensitive data, while on-device processing[87] eliminates the need for cloud transmission entirely. In addition, secure gateways[4, 42] serve as intermediaries to detect and filter sensitive information, and machine unlearning techniques[23] facilitate the removal of specific data points from trained models. Beyond the aforementioned algorithmic and framework-based approaches to privacy protection, a more direct solution involves running models locally, thereby preventing potential data breaches from malicious cloud service providers who might attempt to access users’ private information.

AgentRR is compatible with all the aforementioned mechanisms. Moreover, through its experience-based record and replay approach, it significantly reduces the computational overhead of large language models, enabling the replay phase for specific tasks to be executed entirely on resource-constrained edge devices under user control.

# 2.2 Agent Performance and Cost

The performance and operational cost of AI agents have emerged as significant challenges in their widespread adoption. Current agent systems inherit several limitations from their foundational LLMs, including limited context windows, difficulties in long-term planning and adaptation, inconsistent outputs, and susceptibility to hallucinations[5, 33, 36, 84]. These limitations often lead to substantial computational overhead and financial costs, particularly in complex tasks requiring multiple rounds of LLM interactions. Multi-modal tasks, such as GUI understanding and manipulation, are especially expensive due to the higher cost of vision-language model calls[25, 56].

Recent work has attempted to address these challenges through various approaches. Model distillation[16, 27, 74, 87] and pruning methods[30, 41, 47, 71] aim to reduce model size while maintaining performance. Parameter-efficient fine-tuning (PEFT) methods[32, 38, 66, 78] enable task-specific adaptation with minimal parameter updates. Model quantization techniques[18, 76, 91] reduce precision and memory footprint, potentially lowering computational costs. However, these approaches often trade off model capabilities for efficiency, potentially limiting the agent’s ability to handle complex or novel tasks.

Other solutions focus on optimizing the interaction patterns between agents and LLMs. Task decomposition strategies[24, 60, 72] break down complex tasks into smaller, more manageable components, allowing the use of less expensive models for simpler subtasks. Caching mechanisms[11, 46, 61] store and reuse previous LLM outputs to avoid redundant computations. Multi-agent architectures[64, 79] employ cost-efficient designs where cheaper LLMs handle initial processing while more expensive models are reserved for critical reasoning steps. However, these solutions still rely heavily on expensive model calls for each step of operation, and the fundamental challenge remains: how to maintain high performance while reducing the dependency on costly LLM computations.

In contrast, AgentRR offers a unique approach to address both performance and cost challenges through

its record-and-replay mechanism. By capturing and reusing high-quality experiences from the most capable agents or human demonstrations, AgentRR enables all agents to achieve performance comparable to the best performers without requiring expensive model calls for each execution.

# 2.3 Record & Replay (R&R) in Systems

Record & Replay techniques capture the execution of a program and allow its faithful re-execution later. The core insight is that most program execution is deterministic; only non-deterministic events (e.g., external inputs, thread scheduling, system call results, signals) need to be logged during recording. During replay, the program is re-executed, and the logged non-deterministic inputs are injected at the appropriate points to reproduce the original execution state precisely.

R&R is widely used for debugging hard-to-reproduce bugs[40, 69] (heisenbugs), security forensics[63] (e.g., Backtracker), fault tolerance through replication, and even enhancing hardware security mechanisms (e.g., RnR-Safe[75]). Tools like the rr debugger[62] achieve this for user-space applications using techniques like ptrace for capturing syscalls/signals, seccomp-bpf to reduce interception overhead, hardware performance counters for timing, and single-threading execution to avoid data races. Other approaches involve VM-level recording[13], kernel modules[57], or application-integrated recording for distributed systems[54] (e.g., aiRR) which may selectively record events to reduce overhead.

In the field of Robotic Process Automation (RPA), Record & Replay (R&R) is also crucial for user interaction and browser automation. An RPA bot essentially records a user’s sequence of actions—like data entry or form filling—and faithfully replays them to complete the task automatically. For example, tools like Playwright[52] excel at recording and replaying user actions (e.g., clicks, text input, navigation), proving invaluable for automated testing to create reliable test suites mimicking real user journeys. This landscape is further evolving with the integration of AI agents into RPA. The browser-use system[53] aims to make websites accessible for AI agents by enabling workflows to be recorded and replayed, even with changes to web pages. WeReplay[20] employs deep learning models to infer rendering completion and optimally schedule the next replay event, demonstrating significant improvements in both replay accuracy and efficiency across devices. MobileGPT[88] also explores record and replay for LLM agents, using a hierarchical memory to store modular sub-tasks and replaying them when similar instructions are encountered, with adaptation to new contexts via pattern matching and few-shot learning. Systems like UFO2[86] further advance this paradigm by introducing a comprehensive AgentOS architecture that integrates record and replay capabilities with deep OS-level integration. Recently, Workflow Use[1] also applied record & replay to make Browser Use more reliable and deterministic. While this aligns with the direction of AgentRR, a notable distinction is that Workflow Use focuses on generating scripts to replay identical workflows, rather than summarizing general, multi-level experience.

AgentRR leverages the mechanism of recording user actions and replaying them through the agent, but its goal differs significantly from traditional R&R. Traditional R&R prioritizes fidelity—exact reproduction of a specific past execution instance. The record phase in AgentRR captures user actions, but the aim is not bitperfect replay. Instead, the summary phase introduces generalization, creating an abstract “Experienc” that represents a class of safe executions. The replay phase enforces conformance to this generalized experience, not replication of a single trace. The non-determinism experience-based R&R must handle during replay is primarily the agent’s creative output within designated nodes, which is intentionally not constrained by the replay mechanism itself.

Table 2 summarizes the key differences between pure LLM agents, traditional R&R tools, and Agen-

Table 2: Comparison of different approaches in terms of execution efficiency, accuracy, and generalization capabilities.   

<table><tr><td>Approach</td><td>Execution Efficiency</td><td>Accuracy</td><td>Generalization</td></tr><tr><td>Pure LLM Agent</td><td>Low</td><td>Low</td><td>High</td></tr><tr><td>Traditional R&amp;R</td><td>High</td><td>High</td><td>Low</td></tr><tr><td>AgentRR</td><td>High (Exceeds human speed)</td><td>High</td><td>High (Generalized for repetitive tasks)</td></tr></table>

tRR in terms of execution efficiency, accuracy, and generalization capabilities. Pure LLM agents excel in generalization but often suffer from low execution efficiency and inconsistent accuracy due to their reliance on expensive model calls and potential hallucinations. Traditional R&R tools like Playwright achieve high accuracy and efficiency through deterministic replay, but their generalization capabilities are limited by their strict adherence to recorded actions. AgentRR combines the strengths of both approaches: it maintains high accuracy through experience-based replay, achieves efficient execution by reducing expensive model calls, and enables generalization through its structured experience management and summary mechanisms.

# 3 AgentRR: Multi-level Experience-driven Agent with Record & Replay

Most of contemporary agent systems widely adopt powerful cloud-based LLMs, such as GPT-4o, Claude 3.7, and Gemini 2.5 Pro. This reliance is primarily to facilitate complex task planning and GUI understanding. However, these LLM-based agent systems usually encounter inherent limitations, including high execution costs, unreliable execution outcomes, and potential privacy leakage concerns.

To address the aforementioned challenges in current agent systems, this paper proposes AgentRR, a novel Record & Replay mechanism specifically designed for agent applications. The design philosophy of AgentRR is inspired by human learning patterns, where individuals typically improve their accuracy and efficiency in performing similar tasks after observing demonstrations by others or through personal experience. Following this principle, AI agents should adopt a similar approach by recording historical operations and summarizing them into experiences to facilitate subsequent task execution. Compared to AI agents that solely rely on LLMs, AgentRR leverages system-level Record & Replay mechanisms, achieving significant improvements in performance, reliability, and security.

# 3.1 Challenges in the Existing Record and Replay System

However, unlike the traditional record-and-replay process, in agent-based scenarios, the environment and task requirements often exhibit the uncertainty and variability. Consequently, the environment and tasks encountered during the replay phase typically differ from those in the recording phase. Applying precise record-and-replay methods tends to lead to a lower success rate and generalization capability in agent execution.

To address this issue, a feasible approach is to abstract and summarize past behaviors into more generalized experiences. In some prior work[11, 34, 67, 90], such experiences are often represented as concrete plans that the agent system can reference during execution. Although incorporating such experiences enhances the agent’s generalization capabilities, it also introduces challenges related to the reliability of agent behavior due to the inherent uncertainty of generalized policies. Therefore, balancing the trade-off between

generalization and reliability in the agent’s record-and-replay process is a critical problem that needs careful consideration.

# 3.2 Experience-centric Agent Execution

To balance the reliability and generalization capabilities during the agent’s execution, we propose an experience-centric agent execution framework that encompasses two core components: multi-level experiences and check functions.

# 3.2.1 Multi-level Experiences

While several prior agent systems [39, 86] have proposed experience abstraction to enhance task completion, these approaches primarily focus on improving task success rates without thoroughly investigating how experiences can enhance agent execution efficiency and ensure operational reliability. In our work, we introduce the novel concept of multi-level experiences. The multi-level experience provides summarized knowledge with varying degrees of generalization, as shown in Figure 2. Low-level experiences offer more precise behavioral descriptions, enabling the agent system to replay these experiences more rapidly. However, these low-level experiences demonstrate limited generalization capability, as they are only effective in environments and tasks with high similarity to the original context. For instance, they require the UI layout of applications to remain unchanged. In contrast, high-level experiences represent more generalized summaries of historical records, typically without constraints on specific environments or exact procedural steps. During the replay phase, the high-level experience always requires a local model, and integrate the current environment and task context to generate concrete execution actions. Compared to executing from scratch, the high-level experience provides more prior knowledge, thereby reducing the reliance on the model’s capacity in the replay stage. AgentRR selects the appropriate level of experience for replay based on the current execution environment and task requirements. For instance, when the agent’s execution platform and applications remain identical to those in which the experience was generated, AgentRR tends to utilize low-level experiences for replay operations. However, when the platform or applications undergo changes, to ensure sufficient generalization capability, AgentRR opts for high-level experiences to facilitate better planning during the agent’s execution process.

Hotel Booking Steps: Select Hotel, Check-in Date, Check-out Date, Number of Guests

![](images/6daae873e265f5114f3b5ea6803ede4ce13eccfeb429763d71cb50a081f038ea.jpg)  
High-level Experience

![](images/70cbc3c080e18d55616cbd81a436b130e138c503783e8cc416a718a71350d982.jpg)

![](images/454a21cbbfa78ee8399e7600eaa9e89fc97a50845492bce2710b071e88049305.jpg)

![](images/dd2e320be4ac70dccb5c8a28f6e83206d4e9afe3fb874153ec10534150021330.jpg)

![](images/818c9d04015d0f6dd1191f3907e0e88c0a657ceed21c43d92939bc3926672377.jpg)  
Low-level Experience   
Figure 2: Multi-level experience: High-level experience describes the task planning process without being bound to specific platforms or UI layouts. Low-level experience contains more detailed action decomposition and may be coupled with specific platforms and UI layouts.

# 3.2.2 Check Functions

On the other hand, since experiences are not concrete action trajectories, the system relies on local LLMs to translate experiences into specific action sequences for agent execution. However, due to the inherent hallucination problems and insufficient training of LLMs, the reliability of the agent system may significantly deteriorate and potentially violate user safety requirements. To ensure reliability and security during the experience-centric replay phase, we define distinct check functions for experiences at different levels. These check functions delineate the boundaries of the generalization capability of experiences and serve as a TCB (Trusted Computing Base) during the agent’s execution. Check functions can be generated in various ways; for instance, they may be explicitly defined by the user as specific code implementations. Or, they may be generated by the user-provided description and ML-based summary. Once the user audits and places trust in the given check functions, it guarantees that the agent will adhere to the user’s safety requirements throughout the replay process.

More specifically, the check function typically verifies:

• Execution Flow Integrity: The integrity of the agent’s execution process is formally defined to verify that the agent does not transition into undefined states. For instance, in an order processing agent, making payments without user confirmation would be considered an illegal operation. For such cases, the check function can verify LLM’s execution actions by obtaining application interface information and leveraging OCR capabilities. Or, at the system level, it can prevent unauthorized payment requests by blocking related network transmissions.   
• State Preconditions: Preconditions associated with the action should meet the requirement. For example, in a form-filling agent system, there are often dependencies between form fields. Therefore, the check function needs to verify that all prerequisite fields meet the required conditions before proceeding with subsequent field entries.   
• Data/Parameter Constraints: If the action involves parameters, these parameters should consistent with constraints specified in the Experience. Since user tasks often involve different inputs, specific outputs are not predefined in the experience. Therefore, the check function needs to verify whether the values generated during agent execution align with those defined in the user task.   
• Safety Invariants: Check functions can enforce safety invariants identified during Summary or specified manually. This addresses the requirement that variable parts of an Experience should not compromise safety. For example, loop operations are common task patterns in agent scenarios. While the number of iterations may not compromise the integrity of the Execution Flow, it can significantly impact the correctness of agent task execution. Therefore, check functions must implement dedicated verification mechanisms for loop operations.

# 3.3 Experience Store

The Experience Store acts as a central repository for managing different levels of Experiences. It allows users to upload Experiences generated from their traces, download Experiences created by others, search for Experiences relevant to specific tasks, rate the quality and reliability of Experiences, and potentially mark Experiences as formally audited or verified. The store is likely implemented as a database storing the Experience organized as JSON or a graph database format, along with associated metadata (task description,

creator, version, ratings, audit status, usage statistics). It provides an interface for users or agents to query and select the most appropriate Experience for a given task based on criteria like task similarity, user ratings, success rate, etc.

# 3.4 State Transition Diagram

To better describe the process of agent record and replay, we represent the agent’s operations as a trajectory within a state transition diagram. In this diagram, nodes correspond to environment states, and edges correspond to actions executed by the agent. When an agent completes a task, it is equivalent to obtaining a complete trajectory within the state transition diagram. More specifically:

States (S): The state represents a snapshot of the relevant aspects of the system environment at a given point in time. The definition of “state” is flexible and task-dependent. It could include: the currently focused application and window, the status or content of specific UI elements, relevant file system information, or even abstract states derived from these (e.g., "Logged In," "File Open"). Furthermore, states can be defined at varying levels of granularity. In the context of multi-level experiences, multiple low-level states can be aggregated into a single high-level state. A well-defined state is fundamental in record and replay systems. It enables accurate comparison of the states of different nodes, and can determine the feasibility of subsequent actions.

Actions (A): Actions correspond to the procedural operations represented by the edges in the State Transition Diagram, such as click(button_id), type(text_field_id, ’text’), call_api(endpoint, params). Furthermore, the agent system can effectively minimize the complexity of state transitions within the state transition diagram by specifying a limited set of meta-operations.

Transitions: A transition $S \xrightarrow { A } S ^ { \prime }$ denotes that executing action $A$ in state $S$ leads to state $S ^ { \prime }$ . The state transition diagram defines all allowed transitions.

Trajectories: A complete execution of a task during replay/record phase corresponds to a trajectory in the state transition diagram, which is a sequence of states and actions: $S _ { 0 } \ { \xrightarrow { A _ { 1 } } } \ S _ { 1 } \ { \xrightarrow { A _ { 2 } } } \ S _ { 2 } \cdots \ { \xrightarrow { A _ { n } } } \ S _ { n }$ . The goal of AgentRR is to ensure that this trajectory is valid according to the chosen experience.

The model’s role in this process is to select the most appropriate path from among all possible trajectories for execution. Since each environment state offers a large number of latent executable actions, this leads to high demands on the model’s capability and results in longer inference latency. Experience is defined as a summarized collection of trajectories from similar tasks, effectively representing a small set of trajectories. By leveraging experience, the search space for the model’s action selection is significantly reduced, thereby improving the agent’s efficiency. However, experience also constrains the agent’s generalization ability, with its behavior boundaries regulated by a check function.

# 4 Record&Replay Modules in the AgentRR System

Figure 3 illustrates the overall architecture of AgentRR. The record phase is responsible for capturing user interactions and generating detailed action traces. The summary phase is responsible for summarizing the action traces into reusable experiences. The replay phase is responsible for guiding the agent’s execution according to a selected experience.

![](images/158b77f9176df4925b43c81e1daf660dfc081737ea6034deecf3593715828045.jpg)  
Screenshot

![](images/3082156ec38422f51b2075b19bc1f37c62dcdebe2a8ee9ce6e2f96ff02eff8aa.jpg)

![](images/0faa5419e9b156dc78e2e5f75aa2faedb8f1dc47952526a74d2d4691765336c9.jpg)

![](images/f5386cab71894097a8af39b95f63d364c88b7b65dea5081831bee718fd53c36b.jpg)  
API Calls

![](images/a8d4cb9ad035d4e3d999815ba2117822e1aa56b6b3e0eea898e04ba28bfcc29e.jpg)

![](images/646ae8b32a7fe40b934a5817ebdd309a0f45595fc69556893197c6d79a0c3f6d.jpg)

![](images/1db2eafcc5d065949ee10e3d355caa6986a77cde735547dff15993235cfe14ec.jpg)  
Figure 3: The overall architecture of AgentRR: The AgentRR system consists of three core components: the Record module, Summary module, and Replay module. Additionally, to facilitate experience sharing across different users, AgentRR incorporates an experience store.

# 4.1 Record Phase: Capturing User Traces

The Record phase requires capturing a complete sequence of actions, which can be accomplished by logging either GUI interactions or API calls. To ensure a high success rate during the subsequent replay phase, the information recorded during the record phase is critically important and primarily consists of two aspects:

First, the state of the environment at each step needs to be recorded. Since each state may contain a large amount of information, there is a necessary trade-off between the amount of data recorded and the successful rate during the replay phase. A practical recording strategy involves detailed recording of key elements that influence the current behavior, while recording non-critical elements more coarsely. For example, in GUI operations, the layout of the page can be recorded in a simplified manner, whereas the elements involved in the interactions should be recorded with as much detail as possible. Second, the Record phase must capture the operations that cause state transitions. To accurately record these operations, a set of predefined meta-operations can be used, such as clicking a specific object, entering certain input, sliding by a certain proportion, or directly invoking an API call. Through careful design, a combination of these meta-operations can be used to accomplish a given task.

In summary, the record phase generates a detailed action trace by recording the environment state and user actions at every step, resulting in a path within the state transition diagram.

# 4.2 Summary Phase: Generalizing Traces to Experience

This phase transforms one or more raw execution traces into a generalized, reusable experience graph. This is arguably the most challenging and critical phase for ensuring both utility and safety. Traces for the same task may exhibit variations due to user choices, different starting states, or minor environmental changes. Therefore, during the summary process, whether performed by a ML model or manually, it is essential to identify the commonalities into the experience, but delegate the differences to be handled during the replay phase. Moreover, to address the abstraction of multi-level experiences, summarization can be conducted at varying granularities. For instance, experiences can be categorized either at the level of precise operations or as high-level action plans.

In the context of a state transition diagram, the summary process can be viewed as templated actions from trajectories of similar tasks. Starting from a given node, these templated actions enable transitions to

nodes which have similar states. For multi-level experiences, a high-level experience is created by merging states from multiple low-level experiences, enabling the high-level experience to adapt to more generalized application scenarios.

In addition to summarizing the commonalities among similar operations, the summary phase also needs to generate the corresponding check function. Similarly, the check function itself can be precisely defined by the user or generated by the model combined with the user’s behavior. The check function constitutes the trusted computing base (TCB). We can ensure the trustworthiness of the check function through manual audits, bug bounty programs, and other verification methods. The form of the check function is flexible; it can be a piece of precise verification code (e.g., disallowing clicks on security-sensitive elements), or a natural language description coupled with a validation model. Regardless which type of check function is used, it should be significantly smaller than the agent model size. This design helps reduce the size of the TCB within the overall agent system.

# 4.3 Replay Phase: Agent Execution and Self Optimization

During the replay phase, the agent system replays existing experience based on the current task and environment. Since the replay phase does not utilize exact action trajectory but rather experience generated through summarization, the agent must possess two core capabilities during execution. First, the local model employed by the agent should be able to generate a series of concrete actions from the summarized experience, tailored to the current environment and task, and successfully execute it within the given environment. Second, the agent must be capable of performing a check function and, based on its results, promptly prevent any unsafe operations.

Beyond simply replaying previously accumulated experiences, an agent system should be capable of selecting the most suitable experience for a given task. As multi-level experience may exist for the same task, and different users may exhibit distinct operational behaviors, the agent system can achieve improvements in two key aspects. First, low-level experiences tend to yield better performance during the replay phase; however, changes in the environment or task context can lead to failures when relying on these low-level experiences. Consequently, the agent system aims to select the lowest-level experience that still maintains the highest success rate during replay phase. Furthermore, for frequently executed tasks, the system incrementally refines its experience repository by continuously abstracting more fundamental or lower-level experiences. This process enables the agent to perform common tasks with increasing efficiency over time. Second, when considering the varied experience from different users, the agent employs a ranking mechanism to identify and select the optimal experience for replay. The system can also incorporate high-quality experiences sourced from other users, merging them into its local experience repository to facilitate continuous improvement.

# 5 Case Study

To illustrate the practical effectiveness of AgentRR, we evaluate its performance on a real-world form-filling task (depicted in Figure 4), where existing agent-based solutions face significant challenges in terms of both accuracy and efficiency. The SOTA LLM-based agent like OpenAI’s CUA requires approximately three minutes to process this form, while still falling short of achieving the complete task. Traditional record-replay tools (such as Chrome Recorder) lack generalization capabilities, requiring manual parameter input for each

![](images/eced94eb8dc56f5de6dfb29ef5de4216fe7a25aa255acce79d378fa5d117809f.jpg)

![](images/dbbccc1c998f53c83c5b55521f4ee64aece7db3f1811f3bd65cc71049625fdb6.jpg)  
trace A

# Interactive HTML Elements

[1]<select>

<option value='01>徐汇</option> <option value='02'>闵行</option> <option value='03'>七宝</option>

</select>

[2]<input>具体到房间</input>   
[3]<inputX/input>   
[4]..

Element Detection

# Record Phase

# State

Url: www.xxx.com

Task: Fill out the form on this page

[1]<select('#gate','01'),snapshot, html,timestmp>   
[2]<type('#location','xxxx)..>   
[3]<type('#date','2025-06-01,'),.>   
[4]..

[1]<type('#date','2025-06-10),>   
[2]<select('#gate',>   
[3]<type('#location',xxxxxx)>   
[4]..

# Summary Phase

High-level Experience

Task: Fill in the form on the page (www.xxx.com)

Experience

- State: gate, location and date (..) need to filled out. Action: Fill in the gate or Fillin date.   
- State: location and date (..) need to filled out. Action: Fill in location (Check Function:Fillincorrectlyaccording tothe task).   
-State:date(...)need tofilled out.Action:Fill indate.   
- State:gate,locatin(...)needtofilledout.Action:Fillin the gat   
- State: location(..) need to filled out. Action: Fillin the location.

Low-level Experience

Task: Fillin the form on the page (www.xxx.com)

Experience

f1(gate: str): """Select gate, Options: {'01': 'xuhui', '02': 'minhang','O3':'qibao'}"".Check Function:The parametersare consistent with task. f2(location: str):""Fill out location"" f3(date: str): "Fill out date""*. Check Function: Format must be yyyy-MM-DD.

![](images/63aeee549556830fe8575bf39ea8d09740136fdc1b60c4b531bd149229af18a4.jpg)  
trace B

# Experience Store

![](images/bd615f88fe325055217c5204258c8fe29c82cafd45f008444f5258f40432ae6a.jpg)  
Figure 4: The online form filling example: During the record phase, users capture multiple trace behaviors. In the summary phase, these traces are synthesized into multi-level experiences based on both scripts and natural language descriptions. During replay, the system selects the optimal experience and, in conjunction with the specific task requirements, controls web interactions to complete the form.

Download

# Replay Phase

Task: Fill out the form on the current page (www.xxx.com).My colleague Lihua wants to visit the SAIF on Xuhui the day after tomorrow. His ID number is...

Task Interation:   
- LLM Response: f1(gate="01"), Check  
Function: Passed.   
- LLM Response: f3(date="2025-05-21"), Check Function: Failed.   
- Retry LLM Response: f2(location="SAIF"), Check Function: Passed.

operation sequence. In contrast, AgentRR enables users to simply describe the task in natural language, and LLM autonomously and quickly fills out the form based on the previously recorded experience.

Record Phase As illustrated in the figure 4, we present three form options in the table here for demonstration. During recording, users employ trace tools such as Chrome Recorder [26] and Playwright Codegen [51] to capture their interaction sequence, and then attach information such as the page URL, task descriptions and screenshot. In this example, the user records two traces, trace A fills the form fields in sequential order; trace B fills out of order.

Summary Phase In the summary stage, the system first replays the recorded traces to verify their reproducibility. If errors occur — often due to dynamically generated HTML elements causing locator failures the system collects diagnostic data. Vaild traces and dignostic information are then processed by the LLM to summarize experience:

High-Level Experience is usually generated by LLMs, it describes the task’s current state, next-step actions, and conditions for the check function to validate. Low-Level Experience typically consists of a series of API calls or executable scripts that usually can be directly derived from trace logs. To achieve generalization capabilities, low-Level experience identifies variables within traces, along with their descriptions and associated constraints (e.g., check function).

Replay Phase When users submit new tasks, AgentRR leverages the summarized experiences for execution. For models with poor capabilities, Low-Level Experience fills partial content via parameterized APIs. When Low-Level Experience fails to meet task requirements, the system falls back to High-Level Experience for task execution, though this approach necessitates more powerful model capabilities.

The CheckFunction blocks executions that violate preconditions or deviate from recorded behaviors. For example, in the dependency checking of the check function, the call f3(date=xxx) in figure 4 is technically feasible, but is rejected since none of the valid previous traces demonstrated filling the “date” field after the “gate” field. The system enforces strict ordering constraints in scenarios where arbitrary input sequences could potentially compromise the integrity of the task execution.

# 6 Discussion

Our design and evaluation demonstrate that AgentRR effectively addresses the challenges of agent reliability, privacy, and performance through its innovative record-and-replay mechanism. The multi-level experience design enables efficient task decomposition and long-term planning, while the check functions provide robust safety guarantees. However, several limitations and areas for future work warrant discussion.

Summary Complexity: Summarizing recorded traces into multi-level experiences remains challenging for several reasons. First, there is still a lack of clear definitions for different levels of experiences. In our implementation, we define high-level experiences as knowledge representations, while low-level experiences correspond to UI operations or API calls. For high-level experiences, specialized LLMs can be employed to generate meaningful summaries. Low-level experiences can be directly transformed from recorded traces through dedicated conversion scripts. However, these approaches still face limitations such as incomplete summarization and potential failures in generating executable scripts.

State Space: The definition of states in the state transition diagram also varies across different tasks. For UI-based agents, the states can incorporate UI elements and layouts of applications as integral components. In contrast, for command-line and API-based agents, states can leverage existing operating system and filesystem states. Additionally, adjacent states can be merged synchronously to encompass a broader range

of scenarios.

Generalization vs. Safety: This remains a fundamental tension. The Summary phase must generalize to create reusable Experiences, but over-generalization can compromise the safety guarantees derived from the specific human traces. Our current prototype relies on conservative generalization, focusing primarily on repetitive tasks such as filling out web forms, making hotel reservations and conducting online purchases, etc. These tasks are characterized by well-defined boundaries and exhibit similar execution flows across different requests. In handling such tasks, AgentRR effectively balances agent safety with the ability to generalize across varying users’ requests.

Recording Fidelity and Robustness: Accurately recording user behaviors presents significant challenges. In web-based scenarios, the presence of dynamic HTML elements and inconsistent HTML element usage by developers poses difficulties. Even with tools like Browser-use and Playwright for web behavior recording, achieving 100% reliable replay remains elusive. In UI-based scenarios, challenges persist in accurately capturing user interface interactions due to dynamic pop-ups, varying screen resolutions, and other interface-related complexities.

User Burden: Recording demonstrations requires initial user effort. Furthermore, building trust in shared Experiences within the Experience Store likely requires mechanisms for auditing, rating, or verification, adding an additional layer of human oversight or computational cost.

# 7 Conclusion

This paper introduces AgentRR, a novel paradigm that addresses the fundamental challenges of modern AI agents through an experience-based record-and-replay (R&R) approach. By decoupling an agent’s intelligence from its execution, AgentRR offers practical solutions to the core challenges of reliability, privacy, operational cost, and execution performance. To achieve this, AgentRR proposes multi-level experience design, where lower-level experiences provide precise behavioral operations for rapid replay, while high-level experiences offer more generalized summaries for better adaptation to varying environments. To strengthen the reliability and safety of AI agents, AgentRR proposes check functions as a trusted computing base (TCB) that verifies execution flow integrity, state preconditions, and safety invariants, ensuring reliable and secure agent operations. Compared with other pure LLM-based agents, AgentRR opens up new research directions by integrating system-level record and replay capabilities, while leveraging experiences to achieve increasingly efficient and accurate agent execution.

# References

[1] 2025. Workflow Use. https://github.com/browser-use/workflow-use. Referenced May 2025.   
[2] Anthropic. 2024. Introducing the Model Context Protocol. https://www.anthropic.com/news/ model-context-protocol. Accessed: 2025-05-19.   
[3] Anthropic. 2025. Computer use (beta). https://docs.anthropic.com/en/docs/agents-and-tools/ computer-use. Accessed: 2025-05-19.   
[4] Eugene Bagdasarian, Ren Yi, Sahra Ghalebikesabi, Peter Kairouz, Marco Gruteser, Sewoong Oh, Borja Balle, and Daniel Ramage. 2024. AirGapAgent: Protecting Privacy-Conscious Conversational Agents. arXiv:2405.05175 [cs.CR] https://arxiv.org/abs/2405.05175

[5] Sourav Banerjee, Ayushi Agarwal, and Saloni Singla. 2024. LLMs Will Always Hallucinate, and We Need to Live With This. arXiv:2409.05746 [stat.ML] https://arxiv.org/abs/2409.05746   
[6] Edward Y. Chang and Longling Geng. 2025. SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning. arXiv:2503.11951 [cs.AI] https://arxiv.org/abs/ 2503.11951   
[7] Zachary Charles, Arun Ganesh, Ryan McKenna, H. Brendan McMahan, Nicole Mitchell, Krishna Pillutla, and Keith Rush. 2024. Fine-Tuning Large Language Models with User-Level Differential Privacy. arXiv:2407.07737 [cs.LG] https://arxiv.org/abs/2407.07737   
[8] Chaoran Chen, Zhiping Zhang, Bingcan Guo, Shang Ma, Ibrahim Khalilov, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, and Toby Jia-Jun Li. 2025. The Obvious Invisible Threat: LLM-Powered GUI Agents’ Vulnerability to Fine-Print Injections. arXiv:2504.11281 [cs.HC] https://arxiv.org/abs/2504.11281   
[9] Chaoran Chen, Daodao Zhou, Yanfang Ye, Toby Jia jun Li, and Yaxing Yao. 2025. CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation for Large Language Model Applications. arXiv:2410.13387 [cs.HC] https://arxiv.org/abs/2410.13387   
[10] Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z. Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. 2025. ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning. arXiv:2503.19470 [cs.AI] https://arxiv.org/ abs/2503.19470   
[11] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. 2025. Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory. arXiv:2504.19413 [cs.CL] https: //arxiv.org/abs/2504.19413   
[12] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. 2024. Safe RLHF: Safe Reinforcement Learning from Human Feedback. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=TyFrPOKYXw   
[13] Daniela A. S. de Oliveira, Jedidiah R. Crandall, Gary Wassermann, S. Felix Wu, Zhendong Su, and Frederic T. Chong. 2006. ExecRecorder: VM-based full-system replay for attack analysis and system recovery. In Proceedings of the 1st Workshop on Architectural and System Support for Improving Software Dependability (San Jose, California) (ASID ’06). Association for Computing Machinery, New York, NY, USA, 66–71. https://doi.org/10.1145/1181309.1181320   
[14] Christian Schroeder de Witt. 2025. Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents. arXiv:2505.02077 [cs.CR] https://arxiv.org/abs/2505.02077   
[15] Yihe Deng, Yu Yang, Junkai Zhang, Wei Wang, and Bo Li. 2025. DuoGuard: A Two-Player RL-Driven Framework for Multilingual LLM Guardrails. arXiv:2502.05163 [cs.CL] https://arxiv.org/abs/ 2502.05163   
[16] Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. arXiv:2304.03208 [cs.LG] https://arxiv.org/abs/2304.03208

[17] Yi Dong, Ronghui Mu, Gaojie Jin, Yi Qi, Jinwei Hu, Xingyu Zhao, Jie Meng, Wenjie Ruan, and Xiaowei Huang. 2024. Building Guardrails for Large Language Models. arXiv:2402.01822 [cs.CL] https://arxiv.org/abs/2402.01822   
[18] Haojie Duanmu, Xiuhong Li, Zhihang Yuan, Size Zheng, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. 2025. MxMoE: Mixed-precision Quantization for MoE with Accuracy and Performance Co-Design. arXiv:2505.05799 [cs.LG] https://arxiv.org/abs/2505.05799   
[19] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. 2025. ReTool: Reinforcement Learning for Strategic Tool Use in LLMs. arXiv:2504.11536 [cs.CL] https://arxiv.org/abs/2504.11536   
[20] Sidong Feng, Haochuan Lu, Ting Xiong, Yuetang Deng, and Chunyang Chen. 2023. Towards Efficient Record and Replay: A Case Study in WeChat. arXiv:2308.06657 [cs.SE] https://arxiv.org/abs/ 2308.06657   
[21] Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, and Ivan Oseledets. 2025. I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders. arXiv:2503.18878 [cs.CL] https: //arxiv.org/abs/2503.18878   
[22] Aoran Gan, Hao Yu, Kai Zhang, Qi Liu, Wenyu Yan, Zhenya Huang, Shiwei Tong, and Guoping Hu. 2025. Retrieval Augmented Generation Evaluation in the Era of Large Language Models: A Comprehensive Survey. arXiv:2504.14891 [cs.CL] https://arxiv.org/abs/2504.14891   
[23] Jiahui Geng, Qing Li, Herbert Woisetschlaeger, Zongxiong Chen, Yuxia Wang, Preslav Nakov, Hans-Arno Jacobsen, and Fakhri Karray. 2025. A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models. arXiv:2503.01854 [cs.CL] https://arxiv.org/abs/2503.01854   
[24] Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, and Christopher D. Manning. 2025. Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use. arXiv:2504.04736 [cs.AI] https:// arxiv.org/abs/2504.04736   
[25] Google. 2025. Gemini Developer API Pricing. https://ai.google.dev/gemini-api/docs/pricing. Accessed: 2025-05-19.   
[26] Google Chrome. 2024. Chrome DevTools Recorder. https://developer.chrome.com/docs/devtools/ recorder?hl=zh-cn. Accessed: 2024-03-19.   
[27] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge Distillation of Large Language Models. arXiv:2306.08543 [cs.CL] https://arxiv.org/abs/2306.08543   
[28] Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni. 2025. Building A Secure Agentic AI Application Leveraging A2A Protocol. arXiv:2504.16902 [cs.CR] https://arxiv.org/abs/2504. 16902   
[29] Shanshan Han, Salman Avestimehr, and Chaoyang He. 2025. Bridging the Safety Gap: A Guardrail Pipeline for Trustworthy LLM Inferences. arXiv:2502.08142 [cs.AI] https://arxiv.org/abs/2502. 08142

[30] Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, and Tao Lei. 2025. Instruction-Following Pruning for Large Language Models. arXiv:2501.02086 [cs.CL] https://arxiv.org/abs/2501.02086   
[31] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. 2025. Model Context Protocol (MCP): Landscape, Security Threats, and Future Research Directions. arXiv:2503.23278 [cs.CR] https: //arxiv.org/abs/2503.23278   
[32] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685   
[33] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Transactions on Information Systems 43, 2 (Jan. 2025), 1–55. https://doi.org/10.1145/3703155   
[34] Tenghao Huang, Kinjal Basu, Ibrahim Abdelaziz, Pavan Kapanipathi, Jonathan May, and Muhao Chen. 2025. R2D2: Remembering, Reflecting and Dynamic Decision Making for Web Agents. arXiv:2501.12485 [cs.AI] https://arxiv.org/abs/2501.12485   
[35] Mintong Kang and Bo Li. 2024. $R ^ { 2 }$ -Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning. arXiv:2407.05557 [cs.AI] https://arxiv.org/abs/2407.05557   
[36] Kelsey Kinzer. 2025. LLM Hallucination Detection in App Development. https://www.comet.com/ site/blog/llm-hallucination/. Accessed: 2025-05-19.   
[37] Hanna Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin, and Kimin Lee. 2025. When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs. arXiv:2410.14569 [cs.CR] https://arxiv.org/ abs/2410.14569   
[38] Rui Kong, Qiyang Li, Xinyu Fang, Qingtian Feng, Qingfeng He, Yazhu Dong, Weijun Wang, Yuanchun Li, Linghe Kong, and Yunxin Liu. 2024. LoRA-Switch: Boosting the Efficiency of Dynamic LLM Adapters via System-Algorithm Co-design. arXiv:2405.17741 [cs.AI] https://arxiv.org/abs/2405. 17741   
[39] Owen Kwon, Abraham George, Alison Bartsch, and Amir Barati Farimani. 2025. RT-cache: Efficient Robot Trajectory Retrieval System. arXiv:2505.09040 [cs.RO] https://arxiv.org/abs/2505.09040   
[40] Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo, Peng Yan, Yuetang Deng, and Tao Xie. 2017. Record and replay for Android: are we there yet in industrial cases?. In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering (Paderborn, Germany) (ESEC/FSE 2017). Association for Computing Machinery, New York, NY, USA, 854–859. https://doi.org/10.1145/3106237.3117769   
[41] Jaewoo Lee, Keyang Xuan, Chanakya Ekbote, Sandeep Polisetty, Yi R. Fung, and Paul Pu Liang. 2025. TAMP: Token-Adaptive Layerwise Pruning in Multimodal Large Language Models. arXiv:2504.09897 [cs.CV] https://arxiv.org/abs/2504.09897

[42] Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, and Cristina Nita-Rotaru. 2025. ACE: A Security Architecture for LLM-Integrated App Systems. arXiv:2504.20984 [cs.CR] https: //arxiv.org/abs/2504.20984   
[43] Siran Li, Linus Stenzel, Carsten Eickhoff, and Seyed Ali Bahrainian. 2025. Enhancing Retrieval-Augmented Generation: A Study of Best Practices. arXiv:2501.07391 [cs.CL] https://arxiv.org/ abs/2501.07391   
[44] Xuechen Li, Florian Tramèr, Percy Liang, and Tatsunori Hashimoto. 2022. Large Language Models Can Be Strong Differentially Private Learners. arXiv:2110.05679 [cs.LG] https://arxiv.org/abs/ 2110.05679   
[45] Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, and Rema Padman. 2025. Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models. arXiv:2504.04717 [cs.CL] https://arxiv.org/abs/2504.04717   
[46] Kevin Lin, Charlie Snell, Yu Wang, Charles Packer, Sarah Wooders, Ion Stoica, and Joseph E. Gonzalez. 2025. Sleep-time Compute: Beyond Inference Scaling at Test-time. arXiv:2504.13171 [cs.AI] https: //arxiv.org/abs/2504.13171   
[47] Gui Ling, Ziyang Wang, Yuliang Yan, and Qingwen Liu. 2024. SlimGPT: Layer-wise Structured Pruning for Large Language Models. arXiv:2412.18110 [cs.AI] https://arxiv.org/abs/2412.18110   
[48] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, and Chenglin Wu. 2025. Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems. arXiv:2504.01990 [cs.AI] https://arxiv.org/abs/ 2504.01990   
[49] Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, and Bryan Hooi. 2025. Efficient Inference for Large Reasoning Models: A Survey. arXiv:2503.23077 [cs.CL] https://arxiv.org/abs/2503.23077   
[50] Qianren Mao, Qili Zhang, Hanwen Hao, Zhentao Han, Runhua Xu, Weifeng Jiang, Qi Hu, Zhijun Chen, Tyler Zhou, Bo Li, Yangqiu Song, Jin Dong, Jianxin Li, and Philip S. Yu. 2025. Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation. arXiv:2504.19101 [cs.CL] https://arxiv.org/abs/2504.19101   
[51] Microsoft. 2024. Playwright Codegen. https://playwright.dev/docs/codegen. Accessed: 2024-03- 19.   
[52] Microsoft. 2025. Playwright. https://playwright.dev/ Accessed: 2025-05-20.   
[53] Magnus Müller and Gregor Žunič. 2024. Browser Use: Enable AI to control your browser. https: //github.com/browser-use/browser-use

[54] Narek Galstyan. 2024. Application-Integrated Record-Replay of Distributed Systems. Technical Report. University of California, Berkeley. https://www2.eecs.berkeley.edu/Pubs/TechRpts/2024/ EECS-2024-4.pdf Accessed: 2025-05-19.   
[55] Wanli Ni, Haofeng Sun, Huiqing Ao, and Hui Tian. 2025. Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning and Collaborative Reasoning at the Network Edge. arXiv:2503.21412 [cs.AI] https://arxiv.org/abs/2503.21412   
[56] OpenAI. 2025. OpenAI Platform Pricing. https://platform.openai.com/docs/pricing. Accessed: 2025-05-19.   
[57] Konstantinos Parasyris, Giorgis Georgakoudis, Esteban Rangel, Ignacio Laguna, and Johannes Doerfert. 2023. Scalable Tuning of (OpenMP) GPU Applications via Kernel Record and Replay. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (Denver, CO, USA) (SC ’23). Association for Computing Machinery, New York, NY, USA, Article 28, 14 pages. https://doi.org/10.1145/3581784.3607098   
[58] Pedro Pinacho-Davidson, Fernando Gutierrez, Pablo Zapata, Rodolfo Vergara, and Pablo Aqueveque. 2025. A Proposal for Evaluating the Operational Risk for ChatBots based on Large Language Models. arXiv:2505.04784 [cs.CR] https://arxiv.org/abs/2505.04784   
[59] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. 2022. A Generalist Agent. arXiv:2205.06175 [cs.AI] https://arxiv. org/abs/2205.06175   
[60] Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, and Chong Ruan. 2025. DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition. arXiv:2504.21801 [cs.CL] https: //arxiv.org/abs/2504.21801   
[61] Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, and Dan Alistarh. 2025. Hogwild! Inference: Parallel LLM Generation via Concurrent Attention. arXiv:2504.06261 [cs.LG] https://arxiv.org/abs/2504.06261   
[62] rr-debugger. 2025. Record and Replay Framework. https://github.com/rr-debugger/rr. Accessed: 2025-05-19.   
[63] Yasser Shalabi, Mengjia Yan, Nima Honarmand, Ruby B. Lee, and Josep Torrellas. 2018. Record-Replay Architecture as a General Security Framework. In 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA). 180–193. https://doi.org/10.1109/HPCA.2018.00025   
[64] Chenyang Shao, Xinyuan Hu, Yutang Lin, and Fengli Xu. 2025. Division-of-Thoughts: Harnessing Hybrid Language Model Synergy for Efficient On-Device Agents. arXiv:2502.04392 [cs.CL] https: //arxiv.org/abs/2502.04392

[65] Minjie Shen and Qikai Yang. 2025. From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent. arXiv:2505.02024 [cs.AI] https://arxiv.org/abs/2505.02024   
[66] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, and Ion Stoica. 2024. S-LoRA: Serving Thousands of Concurrent LoRA Adapters. arXiv:2311.03285 [cs.LG] https://arxiv.org/ abs/2311.03285   
[67] Yuchen Shi, Siqi Cai, Zihan Xu, Yuei Qin, Gang Li, Hang Shao, Jiawei Chen, Deqing Yang, Ke Li, and Xing Sun. 2025. FlowAgent: Achieving Compliance and Flexibility for Workflow Agents. arXiv:2502.14345 [cs.AI] https://arxiv.org/abs/2502.14345   
[68] Aditi Singh, Abul Ehtesham, Saket Kumar, and Tala Talaei Khoei. 2025. Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG. arXiv:2501.09136 [cs.AI] https://arxiv.org/abs/2501. 09136   
[69] Zihe Song, S M Hasan Mansur, Ravishka Rathnasuriya, Yumna Fatima, Wei Yang, Kevin Moran, and Wing Lam. 2025. Can You Mimic Me? Exploring the Use of Android Record & Replay Tools in Debugging. arXiv:2504.20237 [cs.SE] https://arxiv.org/abs/2504.20237   
[70] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. 2025. Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models. arXiv:2503.16419 [cs.CL] https://arxiv.org/abs/ 2503.16419   
[71] Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. 2024. A Simple and Effective Pruning Approach for Large Language Models. arXiv:2306.11695 [cs.CL] https://arxiv.org/abs/2306.11695   
[72] Yiliu Sun, Yanfang Zhang, Zicheng Zhao, Sheng Wan, Dacheng Tao, and Chen Gong. 2025. Fast-Slow-Thinking: Complex Task Solving with Large Language Models. arXiv:2504.08690 [cs.CL] https: //arxiv.org/abs/2504.08690   
[73] Yingshui Tan, Yilei Jiang, Yanshi Li, Jiaheng Liu, Xingyuan Bu, Wenbo Su, Xiangyu Yue, Xiaoyong Zhu, and Bo Zheng. 2025. Equilibrate RLHF: Towards Balancing Helpfulness-Safety Trade-off in Large Language Models. arXiv:2502.11555 [cs.AI] https://arxiv.org/abs/2502.11555   
[74] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ

Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. 2025. Gemma 3 Technical Report. arXiv:2503.19786 [cs.CL] https://arxiv.org/abs/2503.19786

[75] Pierre-Ugo Tournoux, Jeremie Leguay, Farid Benbadis, John Whitbeck, Vania Conan, and Marcelo Dias de Amorim. 2011. Density-Aware Routing in Highly Dynamic DTNs: The RollerNet Case. IEEE Transactions on Mobile Computing 10, 12 (2011), 1755–1768. https://doi.org/10.1109/TMC.2010. 247   
[76] Hongyu Wang, Shuming Ma, and Furu Wei. 2025. BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs. arXiv:2504.18415 [cs.CL] https://arxiv.org/abs/2504.18415   
[77] Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu Fu, Yibo Yan, Hanjun Luo, Liang Lin, Zhihao Xu, Haolang Lu, Xinye Cao, Xinyun Zhou, Weifei Jin, Fanci Meng, Junyuan Mao, Hao Wu, Minghe Wang, Fan Zhang, Junfeng Fang, Chengwei Liu, Yifan Zhang, Qiankun Li, Chongye Guo, Yalan Qin, Yi Ding, Donghai Hong, Jiaming Ji, Xinfeng Li, Yifan Jiang, Dongxia Wang, Yihao Huang, Yufei Guo, Jen tse Huang, Yanwei Yue, Wenke Huang, Guancheng Wan, Tianlin Li, Lei Bai, Jie Zhang, Qing Guo, Jingyi Wang, Tianlong Chen, Joey Tianyi Zhou, Xiaojun Jia, Weisong Sun, Cong Wu, Jing Chen, Xuming Hu, Yiming Li, Xiao Wang, Ningyu Zhang, Luu Anh Tuan, Guowen Xu, Tianwei Zhang, Xingjun Ma, Xiang Wang, Bo An, Jun Sun, Mohit Bansal, Shirui Pan, Yuval Elovici, Bhavya Kailkhura, Bo Li, Yaodong Yang, Hongwei Li, Wenyuan Xu, Yizhou Sun, Wei Wang, Qing Li, Ke Tang, Yu-Gang Jiang, Felix Juefei-Xu, Hui Xiong, Xiaofeng Wang, Shuicheng Yan, Dacheng Tao, Philip S. Yu, Qingsong Wen, and Yang Liu. 2025. A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment. arXiv:2504.15585 [cs.CR] https://arxiv.org/abs/2504.15585

[78] Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, and Willie Neiswanger. 2025. Tina: Tiny Reasoning Models via LoRA. arXiv:2504.15777 [cs.CL] https: //arxiv.org/abs/2504.15777   
[79] Yongdong Wang, Runze Xiao, Jun Younes Louhi Kasahara, Ryosuke Yajima, Keiji Nagatani, Atsushi Yamashita, and Hajime Asama. 2025. DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution using Large Language Models. arXiv:2411.09022 [cs.RO] https: //arxiv.org/abs/2411.09022   
[80] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. 2025. Reinforcement Learning for Reasoning in Large Language Models with One Training Example. arXiv:2504.20571 [cs.LG] https://arxiv.org/abs/2504.20571   
[81] Zhenhailong Wang, Haiyang Xu, Junyang Wang, Xi Zhang, Ming Yan, Ji Zhang, Fei Huang, and Heng Ji. 2025. Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks. arXiv:2501.11733 [cs.CL] https://arxiv.org/abs/2501.11733   
[82] Peter West and Christopher Potts. 2025. Base Models Beat Aligned Models at Randomness and Creativity. arXiv:2505.00047 [cs.CL] https://arxiv.org/abs/2505.00047   
[83] Haoqi Wu, Wei Dai, Li Wang, and Qiang Yan. 2025. Cape: Context-Aware Prompt Perturbation Mechanism with Differential Privacy. arXiv:2505.05922 [cs.CR] https://arxiv.org/abs/2505.05922   
[84] Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2025. Hallucination is Inevitable: An Innate Limitation of Large Language Models. arXiv:2401.11817 [cs.CL] https://arxiv.org/abs/2401.11817   
[85] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629 [cs.CL] https: //arxiv.org/abs/2210.03629   
[86] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, Liqun Li, Yu Kang, Zhao Jiang, Suzhen Zheng, Rujia Wang, Jiaxu Qian, Minghua Ma, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. 2025. UFO2: The Desktop AgentOS. arXiv:2504.14603 [cs.AI] https://arxiv.org/abs/2504.14603   
[87] Tianyi Zhang, Yang Sui, Shaochen Zhong, Vipin Chaudhary, Xia Hu, and Anshumali Shrivastava. 2025. 70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float. arXiv:2504.11651 [cs.LG] https://arxiv.org/abs/2504.11651   
[88] Yuxuan Zhang et al. 2023. MobileGPT: Augmenting LLM with Human-like App Memory for Mobile Task Automation. arXiv preprint arXiv:2312.03003 (2023). https://arxiv.org/pdf/2312.03003   
[89] Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, and Gao Huang. 2025. Absolute Zero: Reinforced Self-play Reasoning with Zero Data. arXiv:2505.03335 [cs.LG] https://arxiv.org/abs/2505.03335   
[90] Xinran Zhao, Hanie Sedghi, Bernd Bohnet, Dale Schuurmans, and Azade Nova. 2025. Improving Large Language Model Planning with Action Sequence Similarity. arXiv:2505.01009 [cs.AI] https: //arxiv.org/abs/2505.01009

[91] Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong Qin, Michele Magno, and Xianglong Liu. 2025. An Empirical Study of Qwen3 Quantization. arXiv:2505.02214 [cs.LG] https://arxiv.org/abs/2505.02214   
[92] Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. 2023. Agents: An Open-source Framework for Autonomous Language Agents. arXiv:2309.07870 [cs.CL] https://arxiv.org/abs/2309. 07870   
[93] Yifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and Xian Li. 2025. SWEET-RL: Training Multi-Turn LLM Agents on Collaborative Reasoning Tasks. arXiv:2503.15478 [cs.LG] https://arxiv.org/abs/2503.15478