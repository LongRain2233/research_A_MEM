# Large Multimodal Agents: A Survey

Junlin Xie♣♡∗ Zhihong Chen♣♡∗ Ruifei Zhang♣♡ Xiang Wan♣ Guanbin Li♠†

♡The Chinese University of Hong Kong, Shenzhen

♣Shenzhen Research Institute of Big Data, ♠Sun Yat-sen University

{junlinxie,zhihongchen,ruifeizhang}@link.cuhk.edu.cn

wanxiang@sribd.cn, liguanbin@mail.sysu.edu.cn

# Abstract

Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents (LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs, enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs. Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/ jun0wanan/awesome-large-multimodal-agents.

# 1 Introduction

An agent is a system capable of perceiving its environment and making decisions based on these perceptions to achieve specific goals [56]. While proficient in narrow domains, early agents[35, 50] often lack adaptability and generalization, highlighting a significant disparity with human intelligence. Recent advancements in large language models (LLMs) have begun to bridge this gap, where LLMs enhance their capabilities in command interpretation, knowledge assimilation [36, 78], and mimicry of human reasoning and learning [21, 66]. These agents use LLMs as their primary decision-making tool and are further enhanced with critical human-like features, such as memory. This enhancement allows them to handle a variety of natural language processing tasks and interact with the environment using language [40, 38].

However, real-world scenarios often involve information that spans beyond text, encompassing multiple modalities, with a significant emphasis on the visual aspect. Consequently, the next evolutionary step for LLM-powered intelligent agents is to acquire the capability to process and

![](images/02bca84070fb0778ac22ef43806bd06e7b5116e52bd773dbeb8dc25b2999b26a.jpg)  
Figure 1: Representative research papers from top AI conferences on LLM-powered multimodal agents, published between November 2022 and February 2024, are categorized by model names, with earlier publication dates corresponding to names listed earlier.

generate multimodal information, particularly visual data. This ability is essential for these agents to evolve into more robust AI entities, mirroring human-level intelligence. Agents equipped with this capability are referred to as large multimodal agents (LMAs) in our paper.3 Typically, they face more sophisticated challenges than language-only agents. Take web searching for example, an LMA first requires the input of user requirements to look up relevant information through a search bar. Subsequently, it navigates to web pages through mouse clicks and scrolls to browse real-time web page content. Lastly, the LMA needs to process multimodal data (e.g., text, videos, and images) and perform multi-step reasoning, including extracting key information from web articles, video reports, and social media updates, and integrating this information to respond to the user’s query. We note that existing studies in LMAs were conducted in isolation therefore it is necessary to further advance the field by summarizing and comparing existing frameworks. There exist several surveys related to LLM-powered agents [60, 42, 49] while few of them focused on the multimodal aspects.

In this paper, we aim to fill the gap by summarizing the main developments of LMAs. First, we give an introduction about the core components (§2) and propose a new taxonomy for existing studies (§3) with further discussion on existing collaborative frameworks (§4). Regarding the evaluation, we outline the existing methodologies for assessing the performance of LMAs, followed by a comprehensive summary (§5). Then, the application section provides an exhaustive overview of the broad real-world applications of multimodal agents and their related tasks (§6). We conclude this work by discussing and suggesting possible future directions for LMAs to provide useful research guidance.

# 2 The Core Components of LMAs

In this section, we detail four core elements of LMAs including perception, planning, action, and memory.

Perception. Perception is a complex cognitive process that enables humans to collect and interpret environmental information. In LMAs, the perception component primarily focuses on processing multimodal information from diverse environments. As illustrated in Table 1, LMAs in different tasks involve various modalities. They require extracting key information from these different modalities that is most beneficial for task completion, thereby facilitating more effective planning and execution of the tasks.

Early research [57, 43, 70, 9] on processing multimodal information often rely on simple correlation models or tools to convert images or audio into text descriptions. However, this conversion approach tends to generate a large amount of irrelevant and redundant information, particularly for complex modalities (e.g., video). Along with the input length constraint, LLMs frequently face challenges in effectively extracting pertinent information for planning. To address this issue, recent studies [71, 47] have introduced the concept of sub-task tools, which are designed to handle sophisticated data types. In an environment resembling the real world (i.e., open-world games), [51] proposed a novel method for processing non-textual modal information. This approach begins by extracting

Table 1: This presentation delineates the component details of all LMAs, encompassing their taskspecific modalities, the models outlined by planners, the methodologies and formats employed in planning, the variety of actions involved, the extent of multi-agent collaboration, and the incorporation of long-term memory. Within this table, ‘V’ represents the virtual action, ‘T’ indicates the use of a tool, and ‘E’ embodies the physical action.   

<table><tr><td rowspan="2">Type</td><td rowspan="2">Model</td><td colspan="4">Task Focus</td><td colspan="4">Planner</td><td colspan="2">Action</td><td rowspan="2">Multi-Agent</td><td rowspan="2">LongMemory</td></tr><tr><td>Text</td><td>Image</td><td>Video</td><td>Audio</td><td>Model</td><td>Format</td><td>Inspect</td><td>Planning Method</td><td>Action Type</td><td>Action Learning</td></tr><tr><td rowspan="22">Type I</td><td>VisProg [11]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT3.5</td><td>Program</td><td>X</td><td>Fix</td><td>T (VFMs &amp; Python)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>ControlLLM [11]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT4</td><td>Language</td><td>✓</td><td>Fix</td><td>T (VFMs &amp; Python)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>Visual ChatGPT [57]</td><td>X</td><td>✓</td><td>X</td><td>X</td><td>GPT3.5</td><td>Language</td><td>X</td><td>Fix</td><td>T (VFMs)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>ViperGPT [43]</td><td>✓</td><td>✓</td><td>✓</td><td>X</td><td>GPT3.5</td><td>Program</td><td>X</td><td>Fix</td><td>T (VFMs &amp; API &amp; Python)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>MM-ReAct [70]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>ChatGPT &amp; GPT3.5</td><td>Language</td><td>✓</td><td>Fix</td><td>T (Web &amp; API)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>Chameleon [30]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT3.5</td><td>Language</td><td>X</td><td>Fix</td><td>T (VFMs &amp; API &amp; Python &amp; Web)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>HuggingGPT [41]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>GPT3.5</td><td>Language</td><td>X</td><td>Fix</td><td>T (VFMs)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>CLOVA [10]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs &amp; API)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>CRAFT [74]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT4</td><td>Program</td><td>X</td><td>Fix</td><td>T (Custom tools)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>Cola [3]</td><td>✓</td><td>✓</td><td>✓</td><td>X</td><td>ChatGPT</td><td>Language</td><td>X</td><td>Fix</td><td>T (VFMs)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>M3 [25]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>GPT3.5</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs &amp; API)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>DEPS [52]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>E</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>GRID [45]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs &amp; API)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>DroidBot-GPT [54]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>ChatGPT</td><td>Language</td><td>X</td><td>Dynamic</td><td>V</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>ASSISTGUI [8]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>V(GUI parser)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>GPT-Driver [32]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-3.5</td><td>Language</td><td>X</td><td>Dynamic</td><td>E</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>LLAva-Interactive [4]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>X</td><td>Dynamic</td><td>T (VFMs)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>MusicAgent [73]</td><td>✓</td><td>X</td><td>X</td><td>✓</td><td>ChatGPT</td><td>Language</td><td>X</td><td>Dynamic</td><td>T (Music-Models)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>AudioGPT [13]</td><td>✓</td><td>X</td><td>X</td><td>✓</td><td>GPT-4(V)</td><td>Language</td><td>X</td><td>Fix</td><td>T (API)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>AssistGPT [9]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>GPT3.5</td><td>Lang. &amp; Prog.</td><td>✓</td><td>Dynamic</td><td>T (VFMs &amp; API)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>Mulan[20]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT3.5</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs &amp; Python)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td>Mobile-Agent[48]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>V &amp; T(VFMs)</td><td>Prompt</td><td>X</td><td>X</td></tr><tr><td rowspan="8">Type II</td><td>GPT-Driver [32]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>GPT4</td><td>Language</td><td>X</td><td>Fix</td><td>E</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>LLAVA-PLUS [23]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>Llama</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs)</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>GPT4tools [67]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>Llama</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs)</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>Tool-LMM [46]</td><td>✓</td><td>✓</td><td>✓</td><td>✓</td><td>Vicuna</td><td>Language</td><td>X</td><td>Dynamic</td><td>T (VFMs &amp; API)</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>STEVE [79]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>STEVE-13B</td><td>Program</td><td>X</td><td>Fix</td><td>E</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>EMMA [68]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>LMDecoder</td><td>Language</td><td>X</td><td>Fix</td><td>E</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>Auto-UI [75]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>LMDecoder</td><td>Language</td><td>X</td><td>Dynamic</td><td>E</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td>WebWISE [44]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>LMDecoder</td><td>Language</td><td>✓</td><td>Dynamic</td><td>E</td><td>Learning</td><td>X</td><td>X</td></tr><tr><td rowspan="3">Type III</td><td>DORAEMONGPT [71]</td><td>✓</td><td>X</td><td>✓</td><td>X</td><td>GPT4</td><td>Language</td><td>X</td><td>Dynamic</td><td>T (VFMs)</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>ChatVideo [47]</td><td>✓</td><td>✓</td><td>✓</td><td>X</td><td>ChatGPT</td><td>Language</td><td>X</td><td>Fix</td><td>T (VFMs)</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>OS-Copilot [59]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>V</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td rowspan="8">Type IV</td><td>Openagents [62]</td><td>✓</td><td>✓</td><td>✓</td><td>X</td><td>GPT3.5 &amp; GPT4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>V &amp; T</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>MEIA [27]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT3.5 &amp; GPT4</td><td>Language</td><td>X</td><td>Fix</td><td>E</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>JARVIS-1 [51]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>E</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>AppAgent [69]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4(V)</td><td>Language</td><td>✓</td><td>Dynamic</td><td>E</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>MM-Navigator [64]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4(V)</td><td>Language</td><td>X</td><td>Dynamic</td><td>T (API)</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>DLAH [7]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-3.5</td><td>Language</td><td>X</td><td>Dynamic</td><td>V(Simulator-interfaces)</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>Copilot [77]</td><td>✓</td><td>X</td><td>X</td><td>✓</td><td>GPT-3.5</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (Music-Models)</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td>Wavjourney [26]</td><td>✓</td><td>X</td><td>X</td><td>✓</td><td>GPT-4</td><td>Program</td><td>X</td><td>Dynamic</td><td>T (Music-Models)</td><td>Prompt</td><td>X</td><td>✓</td></tr><tr><td rowspan="4">Multi-agent</td><td>AVIS [12]</td><td>X</td><td>✓</td><td>✓</td><td>X</td><td>GPT4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>T (VFMs)</td><td>Prompt</td><td>✓</td><td>X</td></tr><tr><td>MP5 [37]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>E</td><td>Prompt</td><td>✓</td><td>✓</td></tr><tr><td>MemoDroid [17]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4</td><td>Language</td><td>✓</td><td>Dynamic</td><td>V(VFMs &amp; API)</td><td>Prompt</td><td>✓</td><td>✓</td></tr><tr><td>DiscussNav [29]</td><td>✓</td><td>✓</td><td>X</td><td>X</td><td>GPT-4 &amp; ChatGPT</td><td>Language</td><td>X</td><td>Dynamic</td><td>E</td><td>Prompt</td><td>✓</td><td>X</td></tr></table>

key visual vocabulary from the environment and then employs the GPT model to further refine this vocabulary into a series of descriptive sentences. As LLMs perceive visual modalities within the environment, they use them to retrieve the most relevant descriptive sentences, which effectively enhances their understanding of the surroundings.

Planning. Planners play a central role in LMAs, akin to the function of the human brain. They are responsible for deep reasoning about the current task and formulating corresponding plans. Compared to language-only agents, LMAs operate in a more complicated environment, making it more challenging to devise reasonable plans. We detail planners from four perspectives (models, format, inspection & reflection, and planning methods):

• Models: As shown in Table 1, existing studies employ different models as planners. Among them, the most popular ones are GPT-3.5 or GPT-4 [43, 41, 9, 30, 57, 51]. Yet, these models are not publicly available and therefore some studies have begun shifting towards using open-source models, such as LLaMA [67] and LLaVA [23], where the latter can directly process information of multiple modalities, enhancing their ability to make more optimal plans.   
• Format: It represents how to formulate the plans made by planners. As shown in Table 1, there are two formatting ways. The first one is natural language. For example, in [41], the planning content obtained is “The first thing I did was use OpenCV’s openpose control model to analyze the pose of the boy in the image....”, where the plan made is to use “OpenCV’s openpose control model”. The second one is in the form of programs, like “ image_patch $=$ ImagePatch(image)” as described in [43], which invokes the ImagePatch function to execute the planning. There are also hybrid forms, such as [9].   
• Inspection & Reflection: It is challenging for an LMAs to consistently make meaningful and task-completing plans in a complex multimodal environment. This component aims at enhancing robustness and adaptability. Some research methods [51, 52] store successful experiences in long-term memory, including the multimodal states, to guide planning.

![](images/5d6843951af693b761bb92c1a177eb1a50ed7df2b2a19eb0cc3ddb483f1596dd.jpg)  
(a)

![](images/c9a22b6767d019e6c96769b9e9b0b8cc9e7430ae1b549177af544bf58e578549.jpg)  
(b)

![](images/e65e149afd81e6055427eb9b4d53115ac47cbc331921e8f3fab1e759d07ddde7.jpg)  
(c)

![](images/5e057d35a32d0ec78a8c69d248afa5c34e0bcb35201e727251008aaf903dbca0.jpg)  
(d)   
Figure 2: Illustrations on four types of LMAs: (a) Type I: Closed-source LLMs as Planners w/o Longterm Memory. They mainly use prompt techniques to guide closed-source LLMs in decision-making and planning to complete tasks without long memory. (b) Type II:Finetuned LLMs as Planners w/o Long-term Memory. They use action-related data to finetune existing open-source large models, enabling them to achieve decision-making, planning, and tool invocation capabilities comparable to closed-source LLMs. Unlike (a) and (b), (c) and (d) introduce long-term memory functions, further enhancing their generalization and adaptation abilities in environments closer to the real world. However, because their planners use different methods to retrieve memories, they can be further divided into: (c) Type III: Planners with Indirect Long-term Memory; (d) Type IV: Planners with Native Long-term Memory.

During the planning process, they first retrieve relevant experiences, aiding planners in thoughtful deliberation to reduce uncertainty. Additionally, [12] utilizes plans made by humans in different states while performing the same tasks. When encountering similar states, planners can refer to these “standard answers" for contemplation, leading to more rational plans. Moreover, [71] employs more complex planning methods, like Monte Carlo, to expand the scope of planning search to find the optimal planning strategy.

• Planning Methods: Existing planning strategies can be categorized into two types: dynamic and static planning as shown in Table 1. The former [57, 43, 70, 30, 41] refers to decomposing the goal into a series of sub-plans based on the initial input, similar to Chain of Thought (CoT) [80], where plans are not reformulated even if errors occur during the process; The latter [9, 25, 51, 71] implies that each plan is formulated based on the current environmental information or feedback. If errors are detected in the plan, it will revert to the original state for re-planning [12].

Action. The action component in multimodal agent systems is responsible for executing the plans and decisions formulated by the planner. It translates these plans into specific actions, such as the use of tools, physical movements, or interactions with interfaces, thereby ensuring that the agent can achieve its goals and interact with the environment accurately and efficiently. Our discussion focuses on two aspects: types and approaches.

![](images/92737582d9196d47d0fadc938c944c5769fa4c17739969ec480fbdefca719b38.jpg)

![](images/ddcbf07e50e9479f04a265b1fc8d74e704466ee39b8f17ce40236aca88cfec6b.jpg)  
(b)   
Figure 3: Illustrations on two types of multi-agent frameworks: in these two frameworks, facing tasks or instructions from the environment, the completion relies on the cooperation of multiple agents. Each agent is responsible for a specific duty, which may involve processing environmental information or handling decision-making and planning, thus distributing the pressure that would otherwise be borne by a single- agent to complete the task. The unique aspect of framework (b) is its long-term memory capability.

Actions in Table 1 are classified into three categories: tool use (T), embodied actions (E), and virtual actions (V), where tools includes visual foundation models (VFMs), APIs, Python, etc (as listed in Table 2); Embodied actions are performed by physical entities like robots [32, 7] or virtual characters [51, 52, 45, 68]; Virtual actions [8, 76, 44, 54] include web tasks (e.g., clicking links, scrolling, and keyboard use). In terms of approaches, as shown in Table 1, there are primarily two types. The first type involves using prompts to provide agents with information about executable actions, such as the tools available at the moment and their functions; The second type involves collecting data on actions and leveraging this information to self-instruct the fine-tuning process of open-source large models, such as LLaVA [23]. This data is typically generated by advanced models, such as GPT-4. Compared to language-only agents, the complexity of information and data related to actions requires more sophisticated methods to optimize the learning strategy.

Table 2: A summary of different tools, including their corresponding modalities, skills, and available sources.   

<table><tr><td>Modality</td><td>Skill</td><td>Tools</td><td>Source</td></tr><tr><td rowspan="7">Image</td><td>VQA</td><td>BLIP2 [18]</td><td>Github, HuggingFace</td></tr><tr><td>Grounding/Detection</td><td>G-DINO [24]</td><td>Github, HuggingFace</td></tr><tr><td>Image Caption</td><td>BLIP [19],BLIP2 [18],InstructBLIP [5]</td><td>Github, HuggingFace, API</td></tr><tr><td>OCR</td><td>EasyOCR,Umi-OCR</td><td>Github, API</td></tr><tr><td>Image Editing</td><td>Instruct P2P [63]</td><td>Github, HuggingFace, API</td></tr><tr><td>Image Generation</td><td>Stable Diffusion [39], DALLE-3 [1]</td><td>Github, HuggingFace, API</td></tr><tr><td>Image Segmentation</td><td>SAM [15], PaddleSeg [28]</td><td>Github, HuggingFace, API</td></tr><tr><td rowspan="2">Text</td><td>Knowledge Retrieval</td><td>Bing Search</td><td>Website, API</td></tr><tr><td>Programming Related Skill</td><td>PyLint, PyChecker</td><td>Python, API</td></tr><tr><td rowspan="2">Video</td><td>Video Editing</td><td>Editly</td><td>Github, API</td></tr><tr><td>Object-tracking</td><td>OSTrack [72]</td><td>Github, HuggingFace, API</td></tr><tr><td rowspan="2">Audio</td><td>Speech to Text</td><td>Whisper [2]</td><td>Github, HuggingFace, API</td></tr><tr><td>Text to Speech</td><td>StyleTTS 2 [22]</td><td>Github, API</td></tr></table>

Memory. Early studies show that memory mechanisms play a vital role in the operation of generalpurpose agents. Similar to humans, memory in agents can be categorized into long and short memory. In a simple environment, short memory suffices for an agent to handle tasks at hand. However, in more complex and realistic settings, long memory becomes essential. In Table 1, we can see that only a minority of LMAs incorporate long memory. Unlike language-only agents, these multimodal agents require long memory capable of storing information across various modalities. In some studies [71, 47, 69, 7], all modalities are converted into textual formats for storage. However, in [51], a multimodal long memory system is proposed, designed specifically to archive previous successful experiences. Specifically, these memories are stored as key-value pairs, where the key is

the multimodal state and the value is the successful plan. Upon encountering a new multimodal state, the most analogous examples are retrieved based on their encoded similarity:

$$
p (t | x) \propto \operatorname {C L I P} _ {v} \left(k _ {t}\right) ^ {\top} \operatorname {C L I P} _ {v} \left(k _ {x}\right), \tag {1}
$$

where $k _ { t }$ represents the key’s visual information encoded via the CLIP model, compared for similarity with the current visual state $k _ { x }$ , also encoded by CLIP.

# 3 The Taxonomy of LMAs

In this section, we present a taxonomy of existing studies by classifying them into four types.

Type I: Closed-source LLMs as Planners w/o Long-term Memory. Early studies [11, 43, 57, 41, 9, 25] employ prompts to utilize closed-source large language models (e.g., GPT-3.5) as the planner for inference and planning as illustrated in Figure 2(a). Depending on the specific environment or task requirements, the execution of these plans may be carried out by downstream toolkits or through direct interaction with the environment using physical devices like mice or robotic arms. LMAs of this type typically operate in simpler settings, undertaking conventional tasks such as image editing, visual grounding, and visual question answering (VQA).

Type II: Finetuned LLMs as Planners w/o Long-term Memory. LMAs of this type involve collecting multimodal instruction-following data or employing self-instruction to fine-tune opensource large language models (such as LLaMA) [67] or multimodal models (like LLaVA) [23, 46], as illustrated in Figure 2(b). This enhancement not only allows the models to serve as the central “brain" for reasoning and planning but also to execute these plans. The environments and tasks faced by Type II LMAs are similar to those in Type I, typically involving traditional visual or multimodal tasks. Compared to canonical scenarios characterized by relatively simple dynamics, closed environments, and basic tasks, LMAs in open-world games like Minecraft are required to execute precise planning in dynamic contexts, handle tasks of high complexity, and engage in lifelong learning to adapt to new challenges. Therefore, building upon the foundation of Type I and Type II, Type III and Type IV LMAs integrate a memory component, showing great promise in developing towards a generalist agent in the field of artificial intelligence.

Type III: Planners with Indirect Long-term Memory. For Type III LMAs [71, 47], as illustrated in Figure 2(c), LLMs function as the central planner and are equipped with long memory. These planners access and retrieve long memories by invoking relevant tools, leveraging these memories for enhanced reasoning and planning. For example, the multimodal agent framework developed in [71] is tailored for dynamic tasks such as video processing. This framework consists of a planner, a toolkit, and a task-relevant memory bank that catalogues spatial and temporal attributes. The planner employs specialized sub-task tools to query the memory bank for spatiotemporal attributes related to the video content, enabling inference on task-relevant temporal and spatial data. Stored within the toolkit, each tool is designed for specific types of spatiotemporal reasoning and acts as an executor within the framework.

Type IV: Planners with Native Long-term Memory. Different from Type III, Type IV LMAs [51, 37, 7, 76] feature LLMs directly interacting with long memory, bypassing the need for tools to access long memories, as illustrated in Figure 2(d). For example, the multimodal agent proposed in [51] demonstrates proficiency in completing over 200 distinct tasks within the open-world context of Minecraft. In their multimodal agent design, the interactive planner, merging a multimodal foundation model with an LLM, first translates environmental multimodal inputs into text. The planner further employs a self-check mechanism to anticipate and assess each step in execution, proactively spotting potential flaws and, combined with environmental feedback and self-explanation, swiftly corrects and refines plans without extra information. Moreover, this multimodal agent framework includes a novel multimodal memory. Successful task plans and their initial multimodal states are stored, and the planner retrieves similar states from this database for new tasks, using accumulated experiences for faster, more efficient task completion.

# 4 Multi-agent Collaboration

We further introduce the collaborative framework for LMAs beyond the discussion within isolated agents in this section.

As shown in Figure 3(a)(b), these frameworks employ multiple LMAs working collaboratively. The key distinction between the two frameworks lies in the presence or absence of a memory component, but their underlying principle is consistent: multiple LMAs have different roles and responsibilities, enabling them to coordinate actions to collectively achieve a common goal. This structure alleviates the burden on a single agent, thereby enhancing task performance [12, 37, 17, 29].

For example, in Table 1, in the multimodal agent framework by [37], a perceiver agent is introduced to sense the multimodal environment, comprised of large multimodal models. An agent, designated as Patroller, is responsible for engaging in multiple interactions with the perceiver agent, conducting real-time checks and feedback on the perceived environmental data to ensure the accuracy of current plans and actions. When execution failures are detected or reevaluation is necessitated, Patroller provides pertinent information to the planner, prompting a reorganization or update of the action sequences under the sub-goals. The MemoDroid framework [17] comprises several key agents that collaboratively work to automate mobile tasks. The Exploration Agent is responsible for offline analysis of the target application interface, generating a list of potential sub-tasks based on UI elements, which are then stored in the application memory. During the online execution phase, the Selection Agent determines specific sub-tasks to execute from the explored set, based on user commands and the current screen state. The Deduction Agent further identifies and completes the underlying action sequences required for the selected sub-tasks by prompting an LLM. Concurrently, the Recall Agent, upon encountering tasks similar to those previously learned, can directly invoke and execute the corresponding sub-tasks and action sequences from memory.

# 5 Evaluation

The predominant focus of research is on enhancing the capabilities of current LMAs. However, limited efforts are devoted to developing methodologies for the assessment and evaluation of these agents. The majority of research continues to depend on conventional metrics for evaluating performance, clearly illustrating the challenges inherent in assessing LMAs. This also underscores the necessity of developing pragmatic assessment criteria and establishing benchmark datasets in this domain. This section summarizes existing evaluations of LMAs and offers perspectives on future developments.

# 5.1 Subjective Evaluation

Subjective Evaluation mainly refers to using humans to assess the capabilities of these LMAs. Our ultimate goal is to create a LMA that can comprehend the world like humans and autonomously execute a variety of tasks. Therefore, it is crucial to adopt subjective evaluations of human users on the capabilities of LMAs. The main evaluation metrics include versatility, user-friendliness, scalability, and value and safety.

Versatility. Versatility denotes the capacity of an LMA to adeptly utilize diverse tools, execute both physical and virtual actions, and manage assorted tasks. [30] propose comparing the scale and types of tools utilized in existing LMAs, as well as assessing the diversity of their capabilities.

User-Friendliness. User-friendliness involves user satisfaction with the outcomes of tasks completed by LMAs, including efficiency, accuracy, and the richness of the results. This type of assessment is relatively subjective. In [64], human evaluation of the LMA is essential to precisely assess its effectiveness in interpreting and executing user instructions.

Scalability. Scalability fundamentally evaluates the capability of LMAs to assimilate new competencies and address emerging challenges. Given the dynamic nature of human requirements, it is imperative to rigorously assess the adaptability and lifelong learning potential of LMAs. For example, the evaluation in [23] focuses on the proficiency of agents in using previously unseen tools to complete tasks.

Value and Safety. In addition to the metrics previously mentioned, the “Value and Safety” metric plays a critical role in determining the practical significance and safety of agents for human users. While many current evaluations overlook this metric, it is essential to consider the “Value and Safety” of LMAs. Compared to language agents, LMAs can handle a wider range of task categories, making it even more important for them to follow ethical and moral principles consistent with human societal values.

# 5.2 Objective Evaluation

Objective evaluation, distinct from subjective assessment, relies on quantitative metrics to comprehensively, systematically, and standardizedly assess the capabilities of LMAs. It is currently the most widely adopted evaluation method in multimodal agent research.

Metrics. Metrics play a crucial role in objective assessment. In current multimodal agent research [43, 70, 9, 71, 12, 57, 30], specific task-related metrics are employed, such as the accuracy of answers generated by the agent in tasks like visual question answering (VQA) [10, 43]. However, the traditional task metrics established prior to the emergence of LLMs are not sufficiently effective in evaluating llm-powered LMAs. As a result, an increasing number of research efforts are directed towards identifying more appropriate metrics for assessment. For instance, in VisualWebArena [16], a specialized assessment metric is designed to evaluate the performance of LMAs in handling visually guided tasks. This includes measuring the accuracy of the agent’s visual understanding of webpage content, such as the ability to recognize and utilize interactable elements marked by Set-of-Marks for operations and achieving state transitions based on task objectives, as defined by a manually designed reward function. Besides, it encompasses the accuracy of responses to specific visual scene questions and the alignment of actions executed based on visual information.

Benchmarks. Benchmark represents a testing environment that encompasses a suite of evaluation standards, datasets, and tasks. It is utilized to assess and compare the performance of different algorithms or systems. Compared to benchmarks for conventional tasks [30, 12, 57, 23], SmartPlay [58] utilizes a carefully designed set of games to comprehensively measure the various abilities of LMAs, establishing detailed evaluation metrics and challenge levels for each capability. Contrasting with the approach of using games to evaluate, GAIA [34] has developed a test set comprising 466 questions and their answers. These questions require AI systems to possess a range of fundamental abilities, such as reasoning, processing multimodal information, web navigation, and proficient tool use. Diverging from the current trend of creating increasingly difficult tasks for humans, it focuses on conceptually simple yet challenging questions for existing advanced AI systems. These questions involve real-world scenarios that necessitate the precise execution of complex operational sequences, with outputs that are easy to verify. Similarly, VisualWebArena [16] is a benchmark test suite designed to assess and advance the capabilities of LMAs in processing visual and textual understanding tasks on real webpages. There are also other benchmarks[31, 61] that have effectively tested the capabilities of agents.

# 6 Application

LMAs, proficient in processing diverse data modalities, surpass language-only agents in decisionmaking and response generation across varied scenarios. Their adaptability makes them exceptionally useful in real-world, multisensory environments, as illustrated in Figure 4.

GUI Automation. In this application, the objective of LMAs is to understand and simulate human actions within user interfaces, enabling the execution of repetitive tasks, navigation across multiple applications, and the simplification of complex workflows. This automation holds the potential to save users’ time and energy, allowing them to focus on the more critical and creative aspects of their work [44, 6, 53, 64, 75, 69, 54, 17, 8]. For example, GPT-4V-Act [6], is an advanced AI that combines GPT-4V’s capabilities with web browsing to improve human-computer interactions. Its main goal is to make user interfaces more accessible, simplify workflow automation, and enhance automated UI testing. This AI is especially beneficial for people with disabilities or limited tech skills, helping them navigate complex interfaces more easily.

![](images/6b6225c7a08e825168bf3bc0033adde0de082e8e4b17038ab2670919a7f76ca3.jpg)  
Figure 4: A variety of applications of LMAs.

Robotics and Embodied AI. This application [37, 51, 68, 52, 45, 65, 79] focuses on integrating the perceptual, reasoning, and action capabilities of robots with physical interactions in their environments. Employing a multimodal agent, robots are enabled to utilize diverse sensory channels, such as vision, audition, and touch, to acquire comprehensive environmental data. For example, the MP5 system [37], is a cutting-edge, multimodal entity system used in Minecraft that utilizes active perception to smartly break down and carry out extensive, indefinite tasks with large language models.

Game Developement. Game AI [58, 16] endeavors to design and implement these agents to exhibit intelligence and realism, thereby providing engaging and challenging player experiences. The successful integration of agent technology in games has led to the creation of more sophisticated and interactive virtual environments.

Autonomous Driving. Traditional approaches to autonomous vehicles [33] face obstacles in effectively perceiving and interpreting complex scenarios. Recent progress in multimodal agentbased technologies, notably driven by LLMs, marks a substantial advancement in overcoming these challenges and bridging the perception gap [32, 7, 81, 55]. [32] present GPT-Driver, a pioneering approach that employs the OpenAI GPT-3.5 model as a reliable motion planner for autonomous vehicles, with a specific focus on generating safe and comfortable driving trajectories. Harnessing the inherent reasoning capabilities of LLMs, their method provides a promising solution to the issue of limited generalization in novel driving scenarios.

Video Understanding. The video understanding agents [9, 71] are artificial intelligence systems specifically designed for analyzing and comprehending video content. It utilizes deep learning techniques to extract essential information from videos, identifying objects, actions, and scenes to enhance understanding of the video content.

Visual Generation & Editing. Applications of this kind [4, 70, 47] are designed for the creation and manipulation of visual content. Using advanced technologies, this tool effortlessly creates and modifies images, offering users a flexible option for creative projects. For instance, LLaVA-Interactive [4] is an open-source multimodal interactive system that amalgamates the capabilities of pre-trained AI models to facilitate multi-turn dialogues with visual cues and generate edited images, thereby realizing a cost-effective, flexible, and intuitive AI-assisted visual content creation experience.

Complex Visual Reasoning Tasks. This area is a key focus in multimodal agent research, mainly emphasizing the analysis of multimodal content. This prevalence is attributed to the superior cognitive capabilities of LLMs in comprehending and reasoning through knowledge-based queries, surpassing the capabilities of previous models [14, 25, 80]. Within these applications, the primary focus is on QA tasks [41, 57, 70, 30]. This entails leveraging visual modalities (images or videos) and textual modalities (questions or questions with accompanying documents) for reasoned responses.

Audio Editing & Generation. The LMAs in this application integrate foundational expert models in the audio domain, making the editing and creation of music efficient[77, 73, 13, 26].

# 7 Conclusion and Future Research

In this survey, we provide a thorough overview of the latest research on multimodal agents driven by LLMs (LMAs). We start by introducing the core components of LMAs (i.e., perception, planning, action, and memory) and classify existing studies into four categories. Subsequently, we compile existing methodologies for evaluating LMAs and devise a comprehensive evaluation framework. Finally, we spotlight a range of current and significant application scenarios within the realm of LMAs. Despite the notable progress, this field still faces many unresolved challenges, and there is considerable room for improvement. We finally highlight several promising directions based on the reviewed progress:

• On frameworks: The future frameworks of LMAs may evolve from two distinct perspectives. From the viewpoint of a single agent, development could progress towards the creation of a more unified system. This entails planners directly interacting with multimodal environments [71], utilizing a comprehensive set of tools [30], and manipulating memory directly [51]; From the perspective of multiple agents, advancing the effective coordination among multiple multimodal agents for the execution of collective tasks emerges as a critical research trajectory. This encompasses essential aspects such as collaborative mechanisms, communication protocols, and strategic task distribution.   
• On evaluation: Systematic and standard evaluation frameworks are highly desired for this field. An ideal evaluation framework should encompass a spectrum of assessment tasks [58, 16], varying from straightforward to intricate, each bearing significant relevance and utility for humans. It ought to incorporate lucid and judicious evaluation metrics, meticulously designed to evaluate the diverse capabilities of an LMA in a comprehensive, yet non-repetitive manner. Moreover, the dataset used for evaluation should be meticulously curated to reflect a closer resemblance to real-world scenarios.   
• On application: The potential applications of LMAs in the real world are substantial, offering solutions to problems that were previously challenging for conventional models, such as web browsing. Furthermore, the intersection of LMAs with the field of human-computer interaction [54, 44] represents one of the significant directions for future applications. Their ability to process and understand information from various modalities enables them to perform more complex and nuanced tasks, thereby enhancing their utility in real-world scenarios and improving the interaction between humans and machines.

# References

[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2:3, 2023. 5   
[2] Nan Cao, Yu-Ru Lin, Xiaohua Sun, David Lazer, Shixia Liu, and Huamin Qu. Whisper: Tracing the spatiotemporal process of information diffusion in real time. IEEE transactions on visualization and computer graphics, 18(12):2649–2658, 2012. 5   
[3] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell, and Ziwei Liu. Large language models are visual reasoning coordinators. arXiv preprint arXiv:2310.15166, 2023. 3   
[4] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, and Chunyuan Li. Llava-interactive: An all-in-one demo for image chat, segmentation, generation and editing. arXiv preprint arXiv:2311.00571, 2023. 3, 9   
[5] W Dai, J Li, D Li, AMH Tiong, J Zhao, W Wang, B Li, P Fung, and S Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning. arxiv 2023. arXiv preprint arXiv:2305.06500, 2023. 5   
[6] ddupont808. Gpt-4v-act. https://github.com/ddupont808/GPT-4V-Act, 2023. Accessed on February 26, 2024. 8   
[7] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive like a human: Rethinking autonomous driving with large language models. arXiv preprint arXiv:2307.07162, 2023. 3, 5, 6, 9   
[8] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, et al. Assistgui: Task-oriented desktop graphical user interface automation. arXiv preprint arXiv:2312.13108, 2023. 3, 5, 8   
[9] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023. 2, 3, 4, 6, 8, 9   
[10] Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. Clova: A closed-loop visual assistant with tool usage and update. arXiv preprint arXiv:2312.10908, 2023. 3, 8   
[11] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14953–14962, 2023. 3, 6   
[12] Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David A Ross, Cordelia Schmid, and Alireza Fathi. Avis: Autonomous visual information seeking with large language model agent. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 3, 4, 7, 8   
[13] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and generating speech, music, sound, and talking head. arXiv preprint arXiv:2304.12995, 2023. 3, 10   
[14] Shyamgopal Karthik, Karsten Roth, Massimiliano Mancini, and Zeynep Akata. Vision-by-language for training-free compositional image retrieval. arXiv preprint arXiv:2310.09291, 2023. 10   
[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 5   
[16] Yujing Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Mingchong Lim, Po-yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. 8, 9, 10   
[17] Sunjae Lee, Junyoung Choi, Jungjae Lee, Hojun Choi, Steven Y Ko, Sangeun Oh, and Insik Shin. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation. arXiv preprint arXiv:2312.03003, 2023. 3, 7, 8   
[18] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 5

[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pages 12888–12900. PMLR, 2022. 5   
[20] Sen Li, Ruochen Wang, Cho-Jui Hsieh, et al. Mulan: Multimodal-llm agent for progressive multi-object diffusion. arXiv preprint arXiv:2402.12741, 2024. 3   
[21] Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson d’Autume, Phil Blunsom, and Aida Nematzadeh. A systematic investigation of commonsense knowledge in large language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11838–11855, 2022. 1   
[22] Yinghao Aaron Li, Cong Han, Vinay Raghavan, Gavin Mischler, and Nima Mesgarani. Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models. Advances in Neural Information Processing Systems, 36, 2024. 5   
[23] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023. 3, 5, 6, 7, 8   
[24] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 5   
[25] Xiangyan Liu, Rongxue Li, Wei Ji, and Tao Lin. Towards robust multi-modal reasoning via model selection. arXiv preprint arXiv:2310.08446, 2023. 3, 4, 6, 10   
[26] Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D Plumbley, et al. Wavjourney: Compositional audio creation with large language models. arXiv preprint arXiv:2307.14335, 2023. 3, 10   
[27] Yang Liu, Xinshuai Song, Kaixuan Jiang, Weixing Chen, Jingzhou Luo, Guanbin Li, and Liang Lin. Multimodal embodied interactive agent for cafe scene. arXiv preprint arXiv:2402.00290, 2024. 3   
[28] Yi Liu, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Baohua Lai, and Yuying Hao. Paddleseg: A high-efficient development toolkit for image segmentation. arXiv preprint arXiv:2101.06175, 2021. 5   
[29] Yuxing Long, Xiaoqi Li, Wenzhe Cai, and Hao Dong. Discuss before moving: Visual language navigation via multi-expert discussions. arXiv preprint arXiv:2309.11382, 2023. 3, 7   
[30] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023. 3, 4, 7, 8, 10   
[31] Xing Han Lù, Zdenek Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multi-turn ˇ dialogue. arXiv preprint arXiv:2402.05930, 2024. 8   
[32] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt. arXiv preprint arXiv:2310.01415, 2023. 3, 5, 9   
[33] Markus Maurer, J Christian Gerdes, Barbara Lenz, and Hermann Winner. Autonomous driving: technical, legal and social aspects. Springer Nature, 2016. 9   
[34] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants. arXiv preprint arXiv:2311.12983, 2023. 8   
[35] Osonde A Osoba, Raffaele Vardavas, Justin Grana, Rushil Zutshi, and Amber Jaycocks. Policy-focused agent-based modeling using rl behavioral models. arXiv preprint arXiv:2006.05048, 2020. 1   
[36] Jeff Z Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, et al. Large language models and knowledge graphs: Opportunities and challenges. arXiv preprint arXiv:2308.06374, 2023. 1   
[37] Yiran Qin, Enshen Zhou, Qichang Liu, Zhenfei Yin, Lu Sheng, Ruimao Zhang, Yu Qiao, and Jing Shao. Mp5: A multi-modal open-ended embodied system in minecraft via active perception. arXiv preprint arXiv:2312.07472, 2023. 3, 6, 7, 9   
[38] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master $1 6 0 0 0 +$ real-world apis. arXiv preprint arXiv:2307.16789, 2023. 1

[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 5   
[40] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. 1   
[41] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023. 3, 4, 6, 10   
[42] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L Griffiths. Cognitive architectures for language agents. arXiv preprint arXiv:2309.02427, 2023. 2   
[43] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023. 2, 3, 4, 6, 8   
[44] Heyi Tao, Sethuraman TV, Michal Shlapentokh-Rothman, Derek Hoiem, and Heng Ji. Webwise: Web interface control and sequential exploration with large language models. arXiv preprint arXiv:2310.16042, 2023. 3, 5, 8, 10   
[45] Sai Vemprala, Shuhang Chen, Abhinav Shukla, Dinesh Narayanan, and Ashish Kapoor. Grid: A platform for general robot intelligence development. arXiv preprint arXiv:2310.00887, 2023. 3, 5, 9   
[46] Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, jindi Guo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao. Mllm-tool: A multimodal large language model for tool agent learning. arXiv preprint arXiv:2401.10727, 2024. 3, 6   
[47] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu, and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video understanding system. arXiv preprint arXiv:2304.14407, 2023. 2, 3, 5, 6, 9   
[48] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024. 3   
[49] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents. arXiv preprint arXiv:2308.11432, 2023. 2   
[50] Xiaoling Wang and Housheng Su. Completely model-free rl-based consensus of continuous-time multiagent systems. Applied Mathematics and Computation, 382:125312, 2020. 1   
[51] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et al. Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. arXiv preprint arXiv:2311.05997, 2023. 2, 3, 4, 5, 6, 9, 10   
[52] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023. 3, 5, 9   
[53] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272, 2023. 8   
[54] Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. Droidbot-gpt: Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061, 2023. 3, 5, 8, 10   
[55] Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran Xu, Dengke Shang, et al. On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving. arXiv preprint arXiv:2311.05332, 2023. 9   
[56] Michael Wooldridge and Nicholas R Jennings. Intelligent agents: Theory and practice. The knowledge engineering review, 10(2):115–152, 1995. 1   
[57] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 2, 3, 4, 6, 8, 10

[58] Yue Wu, Xuan Tang, Tom M Mitchell, and Yuanzhi Li. Smartplay: A benchmark for llms as intelligent agents. arXiv preprint arXiv:2310.01557, 2023. 8, 9, 10   
[59] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456, 2024. 3   
[60] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023. 2   
[61] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024. 8   
[62] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023. 3   
[63] Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Shan, and Shenghua Gao. Instructp2p: Learning to edit 3d point clouds with text instructions. arXiv preprint arXiv:2306.07154, 2023. 5   
[64] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. 3, 7, 8   
[65] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. arXiv preprint arXiv:2310.08588, 2023. 9   
[66] Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, et al. Supervised knowledge makes large language models better in-context learners. arXiv preprint arXiv:2312.15918, 2023. 1   
[67] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. arXiv preprint arXiv:2305.18752, 2023. 3, 6   
[68] Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li, Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. Embodied multi-modal agent trained by an llm from a parallel textworld. arXiv preprint arXiv:2311.16714, 2023. 3, 5, 9   
[69] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023. 3, 5, 8   
[70] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 2, 3, 4, 8, 9, 10   
[71] Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, and Yi Yang. Doraemongpt: Toward understanding dynamic scenes with large language models. arXiv preprint arXiv:2401.08392, 2024. 2, 3, 4, 5, 6, 8, 9, 10   
[72] Botao Ye, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Joint feature learning and relation modeling for tracking: A one-stream framework. In European Conference on Computer Vision, pages 341–357. Springer, 2022. 5   
[73] Dingyao Yu, Kaitao Song, Peiling Lu, Tianyu He, Xu Tan, Wei Ye, Shikun Zhang, and Jiang Bian. Musicagent: An ai agent for music understanding and generation with large language models. arXiv preprint arXiv:2310.11954, 2023. 3, 10   
[74] Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, and Heng Ji. Craft: Customizing llms by creating and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428, 2023. 3   
[75] Zhuosheng Zhan and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. arXiv preprint arXiv:2309.11436, 2023. 3, 8   
[76] Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk Chang, Shao-Hua Sun, and Joseph J Lim. Bootstrap your own skills: Learning to solve new tasks with large language model guidance. arXiv preprint arXiv:2310.10021, 2023. 5, 6

[77] Yixiao Zhang, Akira Maezawa, Gus Xia, Kazuhiko Yamamoto, and Simon Dixon. Loop copilot: Conducting ai ensembles for music generation and iterative editing. arXiv preprint arXiv:2310.12404, 2023. 3, 10   
[78] Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, and Jun Wang. How do large language models capture the ever-changing world knowledge? a review of recent advances. arXiv preprint arXiv:2310.07343, 2023. 1   
[79] Zhonghan Zhao, Wenhao Chai, Xuan Wang, Li Boyi, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, and Gaoang Wang. See and think: Embodied agent in virtual environment. arXiv preprint arXiv:2311.15209, 2023. 3, 9   
[80] Ge Zheng, Bin Yang, Jiajin Tang, Hong-Yu Zhou, and Sibei Yang. Ddcot: Duty-distinct chain-of-thought prompting for multimodal reasoning in language models. arXiv preprint arXiv:2310.16436, 2023. 4, 10   
[81] Xingcheng Zhou, Mingyu Liu, Bare Luka Zagar, Ekim Yurtsever, and Alois C Knoll. Vision language models in autonomous driving and intelligent transportation systems. arXiv preprint arXiv:2310.14414, 2023. 9