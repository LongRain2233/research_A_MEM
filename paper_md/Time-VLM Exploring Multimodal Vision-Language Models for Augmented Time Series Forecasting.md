# Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting

Siru Zhong 1 Weilin Ruan 1 Ming Jin 2 Huan Li 3 Qingsong Wen 4 Yuxuan Liang 1

# Abstract

Recent advancements in time series forecasting have explored augmenting models with text or vision modalities to improve accuracy. While text provides contextual understanding, it often lacks fine-grained temporal details. Conversely, vision captures intricate temporal patterns but lacks semantic context, limiting the complementary potential of these modalities. To address this, we propose Time-VLM, a novel multimodal framework that leverages pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced forecasting. Our framework comprises three key components: (1) a Retrieval-Augmented Learner, which extracts enriched temporal features through memory bank interactions; (2) a Vision-Augmented Learner, which encodes time series as informative images; and (3) a Text-Augmented Learner, which generates contextual textual descriptions. These components collaborate with frozen pretrained VLMs to produce multimodal embeddings, which are then fused with temporal features for final prediction. Extensive experiments demonstrate that Time-VLM achieves superior performance, particularly in few-shot and zeroshot scenarios, thereby establishing a new direction for multimodal time series forecasting. Code is available at https://github.com/ CityMind-Lab/ICML25-TimeVLM.

# 1. Introduction

Time series data captures temporal signal evolution and underpins forecasting in diverse domains such as finance

1The Hong Kong University of Science and Technology (Guangzhou), China 2Griffith University, Australia 3Zhejiang University, China 4Squirrel Ai Learning, USA. Correspondence to: Yuxuan Liang <yuxliang@outlook.com>.

Proceedings of the $4 2 ^ { n d }$ International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s).

![](images/8c8edceb5cbc6d07890ed677e67a411766a2f9c7bd18b6081eeafe0c1f445029.jpg)  
Figure 1: Our Time-VLM combines text (Right) and vision (Left) modalities to augment time series forecasting.

(Idrees et al., 2019), climate (Karevan & Suykens, 2020), energy (Deb et al., 2017), and transportation (Zheng & Huang, 2020). Accurate forecasting supports proactive risk mitigation, efficient resource allocation, and data-driven decisionmaking. Traditional models like ARIMA, while historically dominant, struggle to capture complex nonlinear patterns. In contrast, deep learning methods—from recurrent neural networks (RNNs) (Medsker et al., 2001) to Transformerbased architectures (Li et al., 2019; Wu et al., 2021; Zhou et al., 2021; Liu et al., 2022a; Zhou et al., 2022; Nie et al., 2023)—leverage innovations such as patch-based feature extraction, auto-correlation mechanisms, and frequency decomposition to model complex temporal dynamics. Despite their success, these models often fail to generalize across domains or adapt to data-limited scenarios, particularly fewshot and zero-shot settings (Liang et al., 2024).

To overcome these issues, researchers have turned to augmenting time series forecasting with additional modalities, such as text and images, which provide complementary information that can enhance predictive accuracy:

• Text-Augmented Models: Textual data offers valuable semantics crucial for accurate forecasting. For instance, contextual descriptions or dataset statistics can significantly enrich the understanding of time series patterns (Figure 1, right). Methods like Time-LLM (Jin et al., 2024) and UniTime (Liu et al., 2024b) leverage the superior pre-trained inference capabilities of Large Language

Models (LLMs) by mapping time series into textual representations. However, these approaches encounter two key challenges: (1) the modality gap between continuous time series and discrete text leads to information loss during representation alignment, and (2) pre-trained language knowledge is rare for capturing fine-grained temporal patterns, limiting their ability to learn nuanced dynamics.

• Vision-Augmented Models: Visual representations of time series—such as line graphs, recurrence plots, or grayscale images—enable models to exploit spatial patterns embedded within temporal data. By transforming sequences into images, methods such as CNNs, ViTs, and MAEs can extract hierarchical features that reveal latent temporal relationships (Figure 1, left). Recent studies (Wu et al., 2023b; Wang et al., 2024; Chen et al., 2024) have demonstrated the natural alignment between time series and vision, as both are continuous and share structural similarities, allowing pre-trained vision models to effectively encode temporal hierarchies. However, these methods lack semantic interpretability, restricting their capacity to incorporate domain-specific knowledge.

Despite advancements in text- and vision-augmented models, integrating both modalities with time series remains underexplored. Current approaches often focus on single modalities, failing to harness their combined strengths. To address this gap, we propose Time-VLM, a novel framework that leverages pre-trained VLMs to enhance time series forecasting by unifying temporal, visual, and textual information. VLMs provide a promising foundation, as they excel at aligning visual and textual modalities, making them well-suited for incorporating temporal information to unify the three modalities. By projecting time series into a unified vision-language semantic space, Time-VLM enables rich cross-modal interactions, combining the strengths of both modalities while mitigating their individual limitations. In this paradigm, each modality contributes uniquely: text provides semantic context, vision captures spatial-temporal patterns, and time series encodes sequential dynamics.

Specifically, Time-VLM introduces three key components: (1) a Retrieval-Augmented Learner that processes raw time series data through patch-based feature extraction and memory bank interactions to generate enriched temporal representations, capturing both local and global dependencies; (2) a Vision-Augmented Learner that adaptively transforms time series into images using multi-scale convolution, frequency encoding, and periodic encoding, preserving both fine-grained details and high-level structures; and (3) a Text-Augmented Learner that generates rich textual context (e.g., statistics and dataset descriptions) to complement the visual representations. These modules collaborate with VLMs to integrate temporal, visual, and textual modalities, producing accurate forecasts through a fine-tuned predictor.

Our key contributions can be summarized as follows:

• We propose the first framework that unifies temporal, visual, and textual modalities by leveraging their complementary strengths for enhanced time series forecasting.   
• We introduce a retrieval-augmented learner for hierarchical temporal feature enhancement, a vision-augmented learner for adaptive time-series-to-image transformation, and a text-augmented learner for contextual prompt generation, enabling seamless integration with VLMs.   
• Extensive evaluations show Time-VLM’s strong performance, especially under data-scarce conditions, offering a brand new paradigm for multimodal time series research.

# 2. Related Work

# Text-Augmented Models for Time Series Forecasting.

The success of LLMs has inspired their application to time series forecasting. Methods like LLMTime (Gruver et al., 2023) and LLM4TS (Chang et al., 2023) tokenize time series for autoregressive prediction but inherit limitations such as poor arithmetic and recursive reasoning. Approaches including GPT4TS (Zhou et al., 2023) and TimeLLM (Jin et al., 2024) project time series into textual representations to leverage LLMs’ reasoning capabilities, yet face challenges like the modality gap and limited temporal adaptability of word embeddings. UniTime (Liu et al., 2024b) and TimeFFM (Liu et al., 2024a) incorporate domain knowledge and federated learning, respectively, but remain constrained by their exclusive dependence on textual modeling.

# Vision-Augmented Models for Time Series Forecasting.

Vision provides a natural way to preserve temporal patterns in time series data. Early approaches apply CNNs to matrix-formed time series (Li et al., 2020; Sood et al., 2021), while TimesNet (Wu et al., 2023b) introduces multiperiodic decomposition for unified 2D modeling. VisionTS (Chen et al., 2024) pioneers the use of pre-trained visual encoders with grayscale time series images, and TimeMixer++ (Wang et al., 2024) advances the field through multi-scale frequency-based time-image transformations. Despite their effectiveness in temporal modeling, these methods often lack semantic context, limiting their ability to utilize high-level contextual information for prediction.

# Vision-Language Models.

VLMs like ViLT (Kim et al., 2021), CLIP (Radford et al., 2021), and ALIGN (Jia et al., 2021), have transformed multimodal understanding by aligning image and text representations. Recent progress, like BLIP-2 (Li et al., 2023) and LLaVA (Liu et al., 2023), further enhance multimodal reasoning. However, VLMs remain underexplored for time series analysis. Our work bridges this gap by leveraging VLMs to integrate temporal, visual, and textual modalities, addressing the limitations of unimodal approaches.

![](images/8aabc7d035252cc8d9508059187168401096191a32c1e0408e965e4542971c12.jpg)  
Figure 2: Overview of the Time-VLM framework.

# 3. Methodology

To address the limitations of single-modality approaches and leverage the complementary strengths of visual, textual, and temporal modalities, we propose Time-VLM, a unified framework that integrates these modalities for enhanced time series forecasting. As illustrated in Figure 2, the framework comprises three core components:

• Retrieval-Augmented Learner (RAL): Extracts temporal features from raw time series patches and maintains a memory bank to refine patch embeddings through multihead self-attention and pooling mechanisms, thereby preserving rich temporal representations and enhancing longterm dependency modeling for robust forecasting.   
• Vision-Augmented Learner (VAL): Transforms time series into informative three-channel images through multiscale convolutions, frequency and periodic encoding. The images are processed by a frozen VLM vision encoder to extract hierarchical visual features, capturing both finegrained details and high-level temporal patterns.   
• Text-Augmented Learner (TAL): Generates contextual textual prompts for input time series, including statistical features (e.g., mean, variance, trends), domain-specific context (e.g., electricity consumption patterns), and image descriptions. These prompts are encoded by a frozen VLM text encoder to produce textual embeddings.

The image and text embeddings, extracted by the VLM, are integrated with temporal memory features via a gated fusion mechanism, effectively capturing complementary information to improve forecasting accuracy. These enriched multimodal features are then processed by a fine-tuned predictor to generate precise and reliable forecasts.

# 3.1. Retrieval-Augmented Learner (RAL)

The RAL module extracts high-level temporal features via patch-based processing and retrieval-augmented memory mechanisms. It dynamically retrieves historical patterns and integrates them with current observations, adapting to complex time series structures. It operates in two key stages.

Patch Embedding: The input time series $\boldsymbol { x } _ { \mathrm { e n c } } \in \mathbb { R } ^ { B \times L \times D }$ is divided into overlapping patches of length $p l$ with stride $^ { s t }$ , where $B , L$ , and $D$ denote the batch size, sequence length, and number of variables, respectively. Each patch is linearly projected into a $d _ { \mathrm { m o d e l } }$ -dimensional latent space, and positional embeddings are added to preserve temporal order. This yields patch embeddings $E _ { \mathrm { p } } \in \mathbb { R } ^ { B \times N _ { p } \times d _ { \mathrm { m o d e l } } ^ { - } }$ , where $\begin{array} { r } { N _ { p } = \frac { L - p l } { s t } + 1 } \end{array}$ 一 represents the total number of patches.

Retrieval-Augmented Memory: A memory bank with maximum capacity $M$ stores historical patch representations $\mathcal { M } \in \mathbb { R } ^ { M \times d _ { \mathrm { m o d e l } } }$ . During each forward pass, the current patch embeddings are averaged across the temporal dimension and added to the memory bank using a circular buffer update strategy, ensuring that the most recent patterns are retained. To better capture temporal dynamics, we introduce a hierarchical memory structure:

• Local Memory: Given current patch embeddings $P \in$ $\mathbb { R } ^ { B \times N _ { p } \times d _ { \mathrm { m o d e l } } }$ , we retrieve top- $k$ similar patches from the memory bank $\mathcal { M }$ based on cosine similarity:

$$
\operatorname {s i m} (P, \mathcal {M}) = P \cdot \mathcal {M} ^ {\top}, \tag {1}
$$

where $\mathcal { M } \in \mathbb { R } ^ { M \times d _ { \mathrm { m o d e l } } }$ stores historical patch representations. The retrieved patches are processed through a

two-layer MLP to extract local memory features:

$$
M _ {\text {l o c a l}} ^ {(i)} = \operatorname {M L P} \left(\operatorname {t o p k} \left(E _ {p} ^ {(i)}\right)\right), \quad i = 1, \dots , B. \tag {2}
$$

These features are averaged across patch dimension and combined with the original $P$ via a residual connection.

• Global Memory: To capture long-range dependencies, we apply multi-head self-attention over the current patch embeddings $P$ , yielding contextualized representations:

$$
\operatorname {A t t n} (P) = \operatorname {M u l t i H e a d} (Q, K, V), \tag {3}
$$

where $Q , K , V$ are linear projections of $P$ . The global memory is obtained by temporal averaging:

$$
M _ {\text {g l o b a l}} = \frac {1}{N _ {p}} \sum_ {i = 1} ^ {N _ {p}} \operatorname {A t t n} (P) _ {i}. \tag {4}
$$

The two memories are fused via element-wise addition:

$$
M _ {\text {f u s e d}} = M _ {\text {l o c a l}} + M _ {\text {g l o b a l}}, \tag {5}
$$

where $M _ { \mathrm { f u s e d } } \in \mathbb { R } ^ { B \times N _ { p } \times d _ { \mathrm { m o d e l } } }$ captures high-level temporal patterns. This fused representation supports dynamic retrieval and integration with other modalities (e.g., vision or text), enabling adaptive context-aware forecasting.

# 3.2. Vision-Augmented Learner (VAL)

The VAL module adaptively transforms the input time series $\boldsymbol { x } _ { \mathrm { e n c } } \in \mathbb { R } ^ { B \times L \times D }$ into image representations, enabling finegrained and high-level temporal pattern extraction via the VLM vision encoder. The process is in three steps:

Frequency and Periodicity Encoding: To capture spectral and temporal dependencies, the VAL module applies two complementary encoding techniques to the input time series, explicitly adding frequency and time-domain information.

1. Frequency Encoding: A Fast Fourier Transform (FFT) extracts frequency components from raw input $x _ { \mathrm { e n c } }$ as:

$$
\operatorname {F F T} \left(x _ {\text {e n c}}\right) = \sum_ {t = 0} ^ {L - 1} x _ {\text {e n c}} (t) \cdot e ^ {- 2 \pi i k t / L}, \tag {6}
$$

where $k$ is the frequency index. The resulting frequency features are concatenated with the input time series, resulting in a tensor of shape $\mathbb { R } ^ { B \times L \times D \times 2 }$ .

2. Periodicity Encoding: Temporal dependencies are encoded using sine and cosine functions for each time step:

$$
\operatorname {e n c o d i n g} (t) = \left[ \sin \left(\frac {2 \pi t}{P}\right), \cos \left(\frac {2 \pi t}{P}\right) \right], \tag {7}
$$

where $P$ is the periodicity hyperparameter. These encodings are concatenated with the input time series, resulting in a tensor of shape $\mathbb { R } ^ { B \times L \times D \times \bar { 3 } }$ . Complete periodic parameter settings can be found in Appendix A.

Multi-scale Convolution: The concat tensor is processed through multiple convolutional layers to extract hierarchical temporal patterns. A 1D convolutional layer captures local dependencies, transforming the input into RB×D×Hhidden×L, $\mathbb { R } ^ { B \times D \times H _ { \mathrm { h i d d e n } } \times L }$ where $H _ { \mathrm { h i d d e n } }$ is the hidden dimension. Averaging along $D$ yields $\mathbb { R } ^ { B \times H _ { \mathrm { h i d d e n } } \times L }$ . Two 2D convolutional layers follow: the first halves the channel dimension, and the second maps features to $C$ output channels, producing the final output capturing both local and global temporal structures.

Image Interpolation & Normalization: The output tensor is resized to the desired image dimensions $( H , W )$ using bilinear interpolation. For a target pixel $( x , y )$ , the interpolated value $\scriptstyle \mathbf { I } ( x , y )$ is computed as follows:

$$
\mathbf {I} (x, y) = \sum_ {i = 1} ^ {2} \sum_ {j = 1} ^ {2} \mathbf {I} \left(x _ {i}, y _ {j}\right) \cdot w _ {i j}, \tag {8}
$$

$$
\operatorname {I n o r m} = 2 5 5 \cdot \frac {\mathbf {I} _ {\text {r a w}} - \operatorname {M i n} \left(\mathbf {I} _ {\text {r a w}}\right)}{\operatorname {M a x} \left(\mathbf {I} _ {\text {r a w}}\right) - \operatorname {M i n} \left(\mathbf {I} _ {\text {r a w}}\right) + \epsilon}, \tag {9}
$$

where $( x _ { i } , y _ { j } )$ are the coordinates of the four nearest neighbors, $w _ { i j }$ are weights based on relative distances, and $\epsilon = 1 0 ^ { - 5 }$ prevents division by zero. Pixel values are scaled to [0, 255] via min-max normalization, producing the normalized image $\mathbf { I } _ { \mathrm { n o r m } } \in \mathbb { R } ^ { B \times C \times H \times W }$ $C$ is the number of channels). This ensures alignment with the VLM vision encoder’s input distribution for effective feature extraction. Example images and descriptions can see Appendix C.

# 3.3. Text-Augmented Learner (TAL)

The TAL module provides contextual textual representations, either pre-defined (e.g., expert annotations) or dynamically generated, offering flexibility across diverse scenarios.

For dynamically generated prompts, TAL extracts key statistical properties from the input time series, including:

• Statistical Properties: Value range (min/max), central tendency (median), and overall trend direction.   
• Contextual Information: Periodic description, taskspecific parameters (input window length and forecasting horizon), and domain-specific dataset characteristics.

These features are formatted into structured textual prompts, such as the example shown in Figure 2(c). When domainspecific knowledge is available (e.g., in medical diagnostics or financial analysis), TAL incorporates pre-defined textual descriptions, which are combined with the dynamically generated prompts to enhance contextual understanding.

The final textual inputs are processed by the VLM text encoder, producing contextual embeddings that complement both visual and temporal features. This dual-path design supporting both static and dynamic textual ensures strong generalization across a wide range of applications, from generic forecasting tasks to specialized domain scenarios.

# 3.4. Multimodal Fusion with VLMs

The multimodal fusion pipeline integrates visual (VAL), textual (TAL), and temporal (RAL) information, leveraging their complementary strengths to enhance time series forecasting. It consists of three key steps:

Multimodal Embeddings Extraction: The generated images and text are processed by a frozen VLM (e.g., ViLT or CLIP), producing multimodal embeddings of shape $\mathbb { R } ^ { B \times L _ { f } \times d _ { h } }$ , where $B$ is the batch size, $L _ { f }$ is the sequence length, and $d _ { h }$ is the VLM’s hidden dimension. These embeddings capture visual and textual context, leveraging the VLM’s pre-trained multimodal understanding capabilities.

Temporal Feature Fusion: To address the distribution shift between temporal and multimodal features, both modalities are projected into a shared $d _ { \mathrm { m o d e l } }$ -dimensional space. Temporal memory embeddings $\mathbf { F } _ { \mathrm { t e m } }$ from RAL encode highlevel temporal patterns and serve as queries in a cross-modal multi-head attention (CM-MHA) mechanism, while the multimodal embeddings $\mathbf { F } _ { \mathrm { m m } }$ from the VLM serve as keys and values. The CM-MHA is defined as:

$$
\operatorname {C M} - \operatorname {M H A} (Q, K, V) = \operatorname {C a t} \left(\text {h e a d} _ {1}, \dots , \text {h e a d} _ {h}\right) W ^ {O}, \tag {10}
$$

$$
\operatorname {h e a d} _ {i} = \operatorname {s o f t m a x} \left(\frac {Q W _ {i} ^ {Q} \left(K W _ {i} ^ {K}\right) ^ {\top}}{\sqrt {d _ {k}}}\right) V W _ {i} ^ {V}. \tag {11}
$$

where $Q = \mathbf { F } _ { \mathrm { t e m } } W ^ { Q }$ , $K = \mathbf { F } _ { \mathrm { m m } } W ^ { K }$ , and $V = \mathbf { F } _ { \mathrm { m m } } W ^ { V }$ . Here, $\it { W _ { i } ^ { Q } }$ , $W _ { i } ^ { K }$ , $W _ { i } ^ { V }$ , and $W ^ { O }$ are learnable projection matrices. $d _ { k } = d _ { \mathrm { m o d e l } } / h$ is the head dimension, and $h$ is the number of attention heads. This mechanism aligns and integrates temporal and multimodal features, capturing both fine-grained patterns and high-level context. A residual connection and layer normalization stabilize training:

$$
\mathbf {F} _ {\text {a t t n}} = \operatorname {L a y e r N o r m} \left(\mathbf {F} _ {\text {t e m}} + \operatorname {C M - M H A} (Q, K, V)\right). \tag {12}
$$

A gated fusion mechanism further enhances the output by dynamically weighting each modality:

$$
\mathbf {G} = \sigma \left(\mathbf {W} _ {g} \left[ \mathbf {F} _ {\text {t e m}}; \mathbf {F} _ {\text {m m}} \right] + \mathbf {b} _ {g}\right), \tag {13}
$$

$$
\mathbf {F} _ {\text {f u s e d}} = \mathbf {G} \odot \mathbf {F} _ {\text {a t t n}} + (1 - \mathbf {G}) \odot \mathbf {F} _ {\mathrm {m m}}, \tag {14}
$$

where $\mathbf { W } _ { g }$ and ${ \bf b } _ { g }$ are learnable parameters, and $\sigma ( \cdot )$ is the sigmoid function. This gated mechanism adaptively balances temporal and multimodal features for robust fusion.

Forecasting: The fused embedding is processed by a finetuned predictor, consisting of fully connected layers, to generate forecasts $\hat { y } \in \mathbb { R } ^ { B \times T _ { \mathrm { p r e d } } \times D }$ . By combining visual, textual, and temporal modalities, the pipeline captures both detailed patterns and high-level context, leveraging pre-trained VLMs for enhanced forecasting across diverse scenes.

# 3.5. Optimization

The model is trained end-to-end using mean squared error (MSE). Given historical observations $\mathbf { X } \in \mathbb { R } ^ { N \times T }$ (with $N$ variables and $T$ time steps), the objective is to predict future values $\hat { \mathbf { Y } } \in \mathbb { R } ^ { N \times H }$ over $H$ steps:

$$
\mathcal {L} = \frac {1}{H} \sum_ {h = 1} ^ {H} \| \hat {\mathbf {Y}} _ {h} - \mathbf {Y} _ {h} \| ^ {2}, \tag {15}
$$

where $\hat { \mathbf Y } _ { h }$ and ${ \bf Y } _ { h }$ denote predicted and ground-truth values at step $h$ . The pre-trained VLM is kept frozen, and only lightweight components are optimized during fine-tuning:

• RAL: Patch embedding, memory retrieval, and attention modules for temporal pattern learning;   
• VAL: Frequency/periodicity encoding and multi-scale CNNs for visual representation generation;   
• Prediction Head: A gate network and linear projection fuse multimodal features to generate final forecasts.

This strategy adapts the VLM to time series forecasting, achieving robust performance with minimal overhead.

# 4. Experiments

Datasets and Metrics. We evaluate Time-VLM on seven widely used time series datasets across diverse domains: energy (ETTh1, ETTh2, ETTm1, ETTm2), weather, electricity (ECL), and traffic (Zhou et al., 2021; Lai et al., 2018). These datasets are commonly used for benchmarking long-term forecasting models (Wu et al., 2023a), and vary in frequency, dimensionality, and temporal characteristics. For short-term forecasting, we use the M4 benchmark (Makridakis et al., 2018), which includes marketing data across multiple frequencies. Performance is measured using Mean Absolute Error (MAE) and Mean Squared Error (MSE), following standard evaluation practices in this field. Additional details are provided in Appendices A.1 and A.3.

Baselines. We compare Time-VLM with state-of-the-art time series models, including text-augmented methods like TimeLLM (2024), GPT4TS (2023), and LLMTime (2023); vision-augmented methods like TimesNet (2023b); traditional deep models like PatchTST (2023), ESTformer (2022), Non-Stationary Transformer (2022b), FEDformer (2022), Autoformer (2021), Informer (2021), and Reformer (2020); and recent competitive models like DLinear (2023), LightTS (2022), N-HiTS (2023), and N-BEATS (2020). Notably, Time-VLM is the first framework combining three modalities for time series forecasting. Performance results for some baselines are cited from (2024a) where applicable.

Implementation Details. We compare Time-VLM against superior baselines using a unified evaluation pipeline under the same configurations as (Wu et al., 2023a) to ensure a fair comparison. ViLT (Kim et al., 2021)

Table 1: Few-shot learning on $5 \%$ training data. Results are averaged over forecasting horizons $H \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ . Lower values indicate better performance. Full results see Section B.1. Red: best, Blue: second best.   

<table><tr><td>Methods</td><td colspan="2">Time-VLM143M(Ours)</td><td colspan="2">Time-LLM3405M(2024)</td><td colspan="2">GPT4TS(2023)</td><td colspan="2">DLinear(2023)</td><td colspan="2">PatchTST(2023)</td><td colspan="2">TimesNet(2023a)</td><td colspan="2">FEDformer(2022)</td><td colspan="2">Autoformer(2021)</td><td colspan="2">Stationary(2022b)</td><td colspan="2">ETSformer(2022)</td><td colspan="2">LightTS(2022)</td><td colspan="2">Informer(2021)</td><td colspan="2">Reformer(2020)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>ETTh1</td><td>0.442</td><td>0.453</td><td>0.627</td><td>0.543</td><td>0.681</td><td>0.560</td><td>0.750</td><td>0.611</td><td>0.694</td><td>0.569</td><td>0.925</td><td>0.647</td><td>0.658</td><td>0.562</td><td>0.722</td><td>0.598</td><td>0.943</td><td>0.646</td><td>1.189</td><td>0.839</td><td>1.451</td><td>0.903</td><td>1.225</td><td>0.817</td><td>1.241</td><td>0.835</td></tr><tr><td>ETTh2</td><td>0.354</td><td>0.402</td><td>0.382</td><td>0.418</td><td>0.400</td><td>0.433</td><td>0.694</td><td>0.577</td><td>0.827</td><td>0.615</td><td>0.439</td><td>0.448</td><td>0.463</td><td>0.454</td><td>0.441</td><td>0.457</td><td>0.470</td><td>0.489</td><td>0.809</td><td>0.681</td><td>3.206</td><td>1.268</td><td>3.922</td><td>1.653</td><td>3.527</td><td>1.472</td></tr><tr><td>ETTm1</td><td>0.364</td><td>0.385</td><td>0.425</td><td>0.434</td><td>0.472</td><td>0.450</td><td>0.400</td><td>0.417</td><td>0.526</td><td>0.476</td><td>0.717</td><td>0.561</td><td>0.730</td><td>0.592</td><td>0.796</td><td>0.620</td><td>0.857</td><td>0.598</td><td>1.125</td><td>0.782</td><td>1.123</td><td>0.765</td><td>1.163</td><td>0.791</td><td>1.264</td><td>0.826</td></tr><tr><td>ETTm2</td><td>0.262</td><td>0.323</td><td>0.274</td><td>0.323</td><td>0.308</td><td>0.346</td><td>0.399</td><td>0.426</td><td>0.314</td><td>0.352</td><td>0.344</td><td>0.372</td><td>0.381</td><td>0.404</td><td>0.388</td><td>0.433</td><td>0.341</td><td>0.372</td><td>0.534</td><td>0.547</td><td>1.415</td><td>0.871</td><td>3.658</td><td>1.489</td><td>3.581</td><td>1.487</td></tr><tr><td>Weather</td><td>0.240</td><td>0.280</td><td>0.260</td><td>0.309</td><td>0.263</td><td>0.301</td><td>0.263</td><td>0.308</td><td>0.269</td><td>0.303</td><td>0.298</td><td>0.318</td><td>0.309</td><td>0.353</td><td>0.310</td><td>0.353</td><td>0.327</td><td>0.328</td><td>0.333</td><td>0.371</td><td>0.305</td><td>0.345</td><td>0.584</td><td>0.527</td><td>0.447</td><td>0.453</td></tr><tr><td>ECL</td><td>0.218</td><td>0.315</td><td>0.179</td><td>0.268</td><td>0.178</td><td>0.273</td><td>0.176</td><td>0.275</td><td>0.181</td><td>0.277</td><td>0.402</td><td>0.453</td><td>0.266</td><td>0.353</td><td>0.346</td><td>0.404</td><td>0.627</td><td>0.603</td><td>0.800</td><td>0.685</td><td>0.878</td><td>0.725</td><td>1.281</td><td>0.929</td><td>1.289</td><td>0.904</td></tr><tr><td>Traffic</td><td>0.558</td><td>0.410</td><td>0.423</td><td>0.298</td><td>0.434</td><td>0.305</td><td>0.450</td><td>0.317</td><td>0.418</td><td>0.296</td><td>0.867</td><td>0.493</td><td>0.676</td><td>0.423</td><td>0.833</td><td>0.502</td><td>1.526</td><td>0.839</td><td>1.859</td><td>0.927</td><td>1.557</td><td>0.795</td><td>1.591</td><td>0.832</td><td>1.618</td><td>0.851</td></tr></table>

Table 2: Few-shot learning on $10 \%$ training data. We use the same protocol in Table 1. Full results see Section B.1.   

<table><tr><td>Methods</td><td colspan="2">Time-VLM143M(Ours)</td><td colspan="2">Time-LLM3405M(2024)</td><td colspan="2">GPT4TS(2023)</td><td colspan="2">DLinear(2023)</td><td colspan="2">PatchTST(2023)</td><td colspan="2">TimesNet(2023a)</td><td colspan="2">FEDformer(2022)</td><td colspan="2">Autoformer(2021)</td><td colspan="2">Stationary(2022b)</td><td colspan="2">ETSformer(2022)</td><td colspan="2">LightTS(2022)</td><td colspan="2">Informer(2021)</td><td colspan="2">Reformer(2020)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td></td><td></td></tr><tr><td>ETTh1</td><td>0.431</td><td>0.442</td><td>0.556</td><td>0.522</td><td>0.590</td><td>0.525</td><td>0.691</td><td>0.600</td><td>0.633</td><td>0.542</td><td>0.869</td><td>0.628</td><td>0.639</td><td>0.561</td><td>0.702</td><td>0.596</td><td>0.915</td><td>0.639</td><td>1.180</td><td>0.834</td><td>1.375</td><td>0.877</td><td>1.199</td><td>0.809</td><td>1.249</td><td>0.833</td></tr><tr><td>ETTh2</td><td>0.361</td><td>0.405</td><td>0.370</td><td>0.394</td><td>0.397</td><td>0.421</td><td>0.605</td><td>0.538</td><td>0.415</td><td>0.431</td><td>0.479</td><td>0.465</td><td>0.466</td><td>0.475</td><td>0.488</td><td>0.499</td><td>0.462</td><td>0.455</td><td>0.894</td><td>0.713</td><td>2.655</td><td>1.160</td><td>3.872</td><td>1.513</td><td>3.485</td><td>1.486</td></tr><tr><td>ETTm1</td><td>0.360</td><td>0.382</td><td>0.404</td><td>0.427</td><td>0.464</td><td>0.441</td><td>0.411</td><td>0.429</td><td>0.501</td><td>0.466</td><td>0.677</td><td>0.537</td><td>0.722</td><td>0.605</td><td>0.802</td><td>0.628</td><td>0.797</td><td>0.578</td><td>0.980</td><td>0.714</td><td>0.971</td><td>0.705</td><td>1.192</td><td>0.821</td><td>1.426</td><td>0.856</td></tr><tr><td>ETTm2</td><td>0.263</td><td>0.323</td><td>0.277</td><td>0.323</td><td>0.293</td><td>0.335</td><td>0.316</td><td>0.368</td><td>0.296</td><td>0.343</td><td>0.320</td><td>0.353</td><td>0.463</td><td>0.488</td><td>1.342</td><td>0.930</td><td>0.332</td><td>0.366</td><td>0.447</td><td>0.487</td><td>0.987</td><td>0.756</td><td>3.370</td><td>1.440</td><td>3.978</td><td>1.587</td></tr><tr><td>Weather</td><td>0.233</td><td>0.274</td><td>0.234</td><td>0.273</td><td>0.238</td><td>0.275</td><td>0.241</td><td>0.283</td><td>0.242</td><td>0.279</td><td>0.279</td><td>0.301</td><td>0.284</td><td>0.324</td><td>0.300</td><td>0.342</td><td>0.318</td><td>0.323</td><td>0.318</td><td>0.360</td><td>0.289</td><td>0.322</td><td>0.597</td><td>0.495</td><td>0.546</td><td>0.469</td></tr><tr><td>ECL</td><td>0.198</td><td>0.291</td><td>0.175</td><td>0.270</td><td>0.176</td><td>0.269</td><td>0.180</td><td>0.280</td><td>0.180</td><td>0.273</td><td>0.323</td><td>0.392</td><td>0.346</td><td>0.427</td><td>0.431</td><td>0.478</td><td>0.444</td><td>0.480</td><td>0.660</td><td>0.617</td><td>0.441</td><td>0.489</td><td>1.195</td><td>0.891</td><td>0.965</td><td>0.768</td></tr><tr><td>Traffic</td><td>0.484</td><td>0.357</td><td>0.429</td><td>0.306</td><td>0.440</td><td>0.310</td><td>0.447</td><td>0.313</td><td>0.430</td><td>0.305</td><td>0.951</td><td>0.535</td><td>0.663</td><td>0.425</td><td>0.749</td><td>0.446</td><td>1.453</td><td>0.815</td><td>1.914</td><td>0.936</td><td>1.248</td><td>0.684</td><td>1.534</td><td>0.811</td><td>1.551</td><td>0.821</td></tr></table>

Table 3: Zero-shot learning results. Full results see Section B.2.   

<table><tr><td>Methods</td><td colspan="2">Time-VLM 143M (Ours)</td><td colspan="2">Time-LLM3405M1 (2024)</td><td colspan="2">LLMTime (2023)</td><td colspan="2">GPT4TS (2023)</td><td colspan="2">DLinear (2023)</td><td colspan="2">PatchTST (2023)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td>ETTh1 → ETTh2</td><td>0.338</td><td>0.385</td><td>0.353</td><td>0.387</td><td>0.992</td><td>0.708</td><td>0.406</td><td>0.422</td><td>0.493</td><td>0.488</td><td>0.380</td><td>0.405</td></tr><tr><td>ETTh1 → ETTm2</td><td>0.293</td><td>0.350</td><td>0.273</td><td>0.340</td><td>1.867</td><td>0.869</td><td>0.325</td><td>0.363</td><td>0.415</td><td>0.452</td><td>0.314</td><td>0.360</td></tr><tr><td>ETTh2 → ETTh1</td><td>0.496</td><td>0.480</td><td>0.479</td><td>0.474</td><td>1.961</td><td>0.981</td><td>0.757</td><td>0.578</td><td>0.703</td><td>0.574</td><td>0.565</td><td>0.513</td></tr><tr><td>ETTh2 → ETTm2</td><td>0.297</td><td>0.353</td><td>0.272</td><td>0.341</td><td>1.867</td><td>0.869</td><td>0.335</td><td>0.370</td><td>0.328</td><td>0.386</td><td>0.325</td><td>0.365</td></tr><tr><td>ETTm1 → ETTh2</td><td>0.354</td><td>0.397</td><td>0.381</td><td>0.412</td><td>0.992</td><td>0.708</td><td>0.433</td><td>0.439</td><td>0.464</td><td>0.475</td><td>0.439</td><td>0.438</td></tr><tr><td>ETTm1 → ETTm2</td><td>0.264</td><td>0.319</td><td>0.268</td><td>0.320</td><td>1.867</td><td>0.869</td><td>0.313</td><td>0.348</td><td>0.335</td><td>0.389</td><td>0.296</td><td>0.334</td></tr><tr><td>ETTm2 → ETTh2</td><td>0.359</td><td>0.399</td><td>0.354</td><td>0.400</td><td>0.992</td><td>0.708</td><td>0.435</td><td>0.443</td><td>0.455</td><td>0.471</td><td>0.409</td><td>0.425</td></tr><tr><td>ETTm2 → ETTm1</td><td>0.432</td><td>0.426</td><td>0.414</td><td>0.438</td><td>1.933</td><td>0.984</td><td>0.769</td><td>0.567</td><td>0.649</td><td>0.537</td><td>0.568</td><td>0.492</td></tr></table>

("vilt-b32-finetuned-coco") serves as the default vision-language backbone; CLIP and BLIP-2 are also supported. All models are trained with Adam $1 0 ^ { - 3 }$ initial learning rate, halved per epoch), batch size 32, for up to 10 epochs with early stopping. Experiments run on Nvidia RTX A6000 GPU (48GB). More details are in Appendix A.2.

# 4.1. Few-shot Forecasting

Setting. We evaluate the few-shot long-term forecasting capabilities of Time-VLM by testing its performance using only $5 \%$ or $10 \%$ of the training data. This setting assesses how effectively Time-VLM integrates pre-trained multimodal knowledge from the VLM with time seriesspecific features under minimal task-specific supervision.

Results. As shown in Table 1 and Table 2, Time-VLM consistently outperforms most baselines across datasets. For example, on ETTh1 with $5 \%$ training data, Time-VLM reduces MSE by $2 9 . 5 \%$ and MAE by $1 6 . 6 \%$ compared to the second-best model, TimeLLM. On ETTm1 with $10 \%$ data, it surpasses TimeLLM by $1 1 . 1 \%$ in MSE and $1 0 . 5 \%$ in

MAE. On Weather with $5 \%$ data, Time-VLM outperforms TimeLLM by $7 . 7 \%$ in MSE and $9 . 4 \%$ in MAE. The performance gap between Time-VLM and traditional models (e.g., PatchTST, FEDformer) is particularly pronounced in fewshot settings, demonstrating the effectiveness of multimodal integration when data is scarce. This performance gain stems from the model’s ability to leverage rich multimodal priors from pre-trained VLMs, while effectively capturing temporal patterns through memory-enhanced attention.

# 4.2. Zero-shot Forecasting

Setting. We evaluate the zero-shot forecasting capability of Time-VLM in cross-domain settings, where the model predicts on unseen datasets by effectively transferring knowledge from unrelated domains. To ensure a rigorous comparison, we conduct experiments using the ETT datasets as source and target domains, following previous setup (Jin et al., 2024). Results are summarized in Table 3.

Results. Time-VLM demonstrates strong generalizability, consistently outperforming or matching state-of-the-art baselines while using fewer parameters. For example, in the ETTh1->ETTh2 transfer setting, Time-VLM achieves a $4 . 2 \%$ lower MSE and $0 . 5 \%$ lower MAE than TimeLLM. In ETTm1->ETTh2, it outperforms TimeLLM by $7 . 1 \%$ in MSE and $3 . 6 \%$ in MAE. In ETTm2->ETTh2, Time-VLM performs competitively, closely matching TimeLLM with only a $1 . 4 \%$ difference in MSE and $0 . 3 \%$ in MAE. These results highlight Time-VLM’s ability to generalize across domains without fine-tuning, leveraging pre-trained visionlanguage priors for effective knowledge transfer.

Table 4: Short-term time series forecasting results on M4. The forecasting horizons are in [6, 48] and the three rows provided are weighted averaged from all datasets under different sampling intervals. Full results see Section B.3.   

<table><tr><td>Methods</td><td>Time-VLM143M(Ours)</td><td>Time-LLM3405M(2024)</td><td>GPT4TS(2023)</td><td>TimesNet(2023a)</td><td>PatchTST(2023)</td><td>N-HiTS(2023)</td><td>N-BEATS(2020)</td><td>ETSformer(2022)</td><td>LightTS(2022)</td><td>DLinear(2023)</td><td>FEDformer(2022)</td><td>Stationary(2022b)</td><td>Autoformer(2021)</td><td>Informer(2021)</td><td>Reformer(2020)</td></tr><tr><td>SMAPE</td><td>11.894</td><td>11.983</td><td>12.690</td><td>12.880</td><td>12.059</td><td>12.035</td><td>12.250</td><td>14.718</td><td>13.525</td><td>13.639</td><td>13.160</td><td>12.780</td><td>12.909</td><td>14.086</td><td>18.200</td></tr><tr><td>MASE</td><td>1.592</td><td>1.595</td><td>1.808</td><td>1.836</td><td>1.623</td><td>1.625</td><td>1.698</td><td>2.408</td><td>2.111</td><td>2.095</td><td>1.775</td><td>1.756</td><td>1.771</td><td>2.718</td><td>4.223</td></tr><tr><td>OWA</td><td>0.855</td><td>0.859</td><td>0.940</td><td>0.955</td><td>0.869</td><td>0.869</td><td>0.896</td><td>1.172</td><td>1.051</td><td>1.051</td><td>0.949</td><td>0.930</td><td>0.939</td><td>1.230</td><td>1.775</td></tr></table>

Table 5: Long-term forecasting results. We use the same protocol in Table 1. Full results see in Section B.4.   

<table><tr><td>Methods</td><td colspan="2">Time-VLM143M(Ours)</td><td colspan="2">Time-LLM3405M(2024)</td><td colspan="2">GPT4TS(2023)</td><td colspan="2">DLinear(2023)</td><td colspan="2">PatchTST(2023)</td><td colspan="2">TimesNet(2023a)</td><td colspan="2">FEDformer(2022)</td><td colspan="2">Autoformer(2021)</td><td colspan="2">Stationary(2022b)</td><td colspan="2">ETSformer(2022)</td><td colspan="2">LightTS(2022)</td><td colspan="2">Informer(2021)</td><td colspan="2">Reformer(2020)</td></tr><tr><td>Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td></td><td></td></tr><tr><td>ETTh1</td><td>0.405</td><td>0.420</td><td>0.408</td><td>0.423</td><td>0.465</td><td>0.455</td><td>0.422</td><td>0.437</td><td>0.413</td><td>0.430</td><td>0.458</td><td>0.450</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td><td>0.570</td><td>0.537</td><td>0.542</td><td>0.510</td><td>0.491</td><td>0.479</td><td>1.040</td><td>0.795</td><td>1.029</td><td>0.805</td></tr><tr><td>ETTh2</td><td>0.341</td><td>0.391</td><td>0.334</td><td>0.383</td><td>0.381</td><td>0.412</td><td>0.431</td><td>0.446</td><td>0.330</td><td>0.379</td><td>0.414</td><td>0.427</td><td>0.437</td><td>0.449</td><td>0.450</td><td>0.459</td><td>0.526</td><td>0.516</td><td>0.439</td><td>0.452</td><td>0.602</td><td>0.543</td><td>4.431</td><td>1.729</td><td>6.736</td><td>2.191</td></tr><tr><td>ETTm1</td><td>0.347</td><td>0.377</td><td>0.329</td><td>0.372</td><td>0.388</td><td>0.403</td><td>0.357</td><td>0.378</td><td>0.351</td><td>0.380</td><td>0.400</td><td>0.406</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td><td>0.481</td><td>0.456</td><td>0.429</td><td>0.425</td><td>0.435</td><td>0.437</td><td>0.961</td><td>0.734</td><td>0.799</td><td>0.671</td></tr><tr><td>ETTm2</td><td>0.248</td><td>0.311</td><td>0.251</td><td>0.313</td><td>0.284</td><td>0.339</td><td>0.267</td><td>0.333</td><td>0.255</td><td>0.315</td><td>0.291</td><td>0.333</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td><td>0.306</td><td>0.347</td><td>0.293</td><td>0.342</td><td>0.409</td><td>0.436</td><td>1.410</td><td>0.810</td><td>1.479</td><td>0.915</td></tr><tr><td>Weather</td><td>0.224</td><td>0.263</td><td>0.225</td><td>0.257</td><td>0.237</td><td>0.270</td><td>0.248</td><td>0.300</td><td>0.225</td><td>0.264</td><td>0.259</td><td>0.287</td><td>0.309</td><td>0.360</td><td>0.338</td><td>0.382</td><td>0.288</td><td>0.314</td><td>0.271</td><td>0.334</td><td>0.261</td><td>0.312</td><td>0.634</td><td>0.548</td><td>0.803</td><td>0.656</td></tr><tr><td>Electricity</td><td>0.172</td><td>0.273</td><td>0.158</td><td>0.252</td><td>0.167</td><td>0.263</td><td>0.166</td><td>0.263</td><td>0.161</td><td>0.252</td><td>0.192</td><td>0.295</td><td>0.214</td><td>0.327</td><td>0.227</td><td>0.338</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.323</td><td>0.229</td><td>0.329</td><td>0.311</td><td>0.397</td><td>0.338</td><td>0.422</td></tr><tr><td>Traffic</td><td>0.419</td><td>0.303</td><td>0.388</td><td>0.264</td><td>0.414</td><td>0.294</td><td>0.433</td><td>0.295</td><td>0.390</td><td>0.263</td><td>0.620</td><td>0.336</td><td>0.610</td><td>0.376</td><td>0.628</td><td>0.379</td><td>0.624</td><td>0.340</td><td>0.621</td><td>0.396</td><td>0.622</td><td>0.392</td><td>0.764</td><td>0.416</td><td>0.741</td><td>0.422</td></tr></table>

# 4.3. Short-term Forecasting

Setting. For short-term forecasting, we evaluate Time-VLM on the M4 benchmark, which includes marketing data at various sampling frequencies. Performance is measured using SMAPE, MASE, and OWA metrics, averaged across datasets and sampling intervals (see Table 4).

Results. Time-VLM demonstrates strong performance, consistently outperforming state-of-the-art baselines across all metrics. For instance, it surpasses the second-best model, Time-LLM, with improvements of $0 . 7 \%$ in SMAPE, $0 . 2 \%$ in MASE, and $0 . 5 \%$ in OWA, all while utilizing significantly fewer parameters and computational resources. Compared to traditional models like PatchTST and N-HiTS, the performance gains more, highlighting the benefit of multimodal knowledge in short-term forecasting. These gains stem from Time-VLM’s integration of temporal, visual, and textual data, capturing richer features for improved accuracy.

# 4.4. Long-term Forecasting

Setting. We evaluate the long-term forecasting capabilities of Time-VLM across multiple horizons and datasets.

Results. As shown in Table 5, Time-VLM achieves competitive performance compared to state-of-the-art baselines. On ETTh1, Time-VLM improves upon TimeLLM by $0 . 7 \%$ in both MSE and MAE. On ETTm2, it outperforms TimeLLM by $1 . 2 \%$ in MSE and $0 . 6 \%$ in MAE. However, on the Weather dataset, Time-VLM slightly underperforms TimeLLM, with a $0 . 4 \%$ higher MSE and $2 . 3 \%$ higher MAE.

Overall, Time-VLM demonstrates robust performance across diverse tasks and datasets, highlighting its generalization and efficiency. By leveraging multimodal knowledge, it consistently outperforms state-of-the-art baselines with significantly fewer parameters (143M vs. TimeLLM’s 3405M), making it a practical solution for real-world applications.

# 4.5. Model Analysis

Ablation Studies. Table 6 evaluates the contributions of key components of Time-VLM, including the RAL (with Local and Global Memory), VAL, and TAL. The study highlights the importance of each module. Removing the RAL causes a significant performance drop $3 5 . 6 \%$ in MSE), with its local (RAL L) and global (RAL G) branches contributing $1 7 . 2 \%$ and $4 . 3 \%$ , respectively, validating our hierarchical memory design. The VAL is also essential—removing it increases MSE by $9 . 0 \%$ , demonstrating its ability to preserve fine-grained temporal patterns via the VLM vision encoder. In contrast, removing the TAL results in only minor degradation ( $2 . 1 \%$ in MSE), likely due to the sparsity of textual tokens in the VLM output; for example, ViLT produces just 11 textual tokens out of 156 total, the rest being visual embeddings. While the TAL provides useful semantic context, its impact is limited by the scarcity of textual signals. Future work may explore VLMs with stronger language capabilities for better temporal-semantic alignment.

Table 6: Ablation study on multimodal components over forecasting horizons $H \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ on Weather dataset, with MSE performance degradation (%Deg) measured for each variant.   

<table><tr><td>Horizon</td><td>Full</td><td>w/o RAL</td><td>w/o RAL.L</td><td>w/o RAL.G</td><td>w/o VAL</td><td>w/o TAL</td></tr><tr><td>96</td><td>0.160</td><td>0.273</td><td>0.185</td><td>0.165</td><td>0.213</td><td>0.165</td></tr><tr><td>192</td><td>0.203</td><td>0.297</td><td>0.235</td><td>0.210</td><td>0.237</td><td>0.208</td></tr><tr><td>336</td><td>0.253</td><td>0.325</td><td>0.295</td><td>0.265</td><td>0.255</td><td>0.258</td></tr><tr><td>720</td><td>0.317</td><td>0.369</td><td>0.375</td><td>0.330</td><td>0.309</td><td>0.322</td></tr><tr><td>Avg</td><td>0.233</td><td>0.316</td><td>0.273</td><td>0.243</td><td>0.254</td><td>0.238</td></tr><tr><td>%Deg</td><td>-</td><td>35.6%↑</td><td>17.2%↑</td><td>4.3%↑</td><td>9.0%↑</td><td>2.1%↑</td></tr></table>

Multimodal and Few-/Zero-shot Analysis. To understand the source of Time-VLM’s strong performance in data-scarce scenarios, we examine the relationship between RAL (temporal) and TAL/VAL (multimodal) embeddings. As shown in Figure 3, their complementary nature is evident. The left panel illustrates balanced gate weight distributions, indicating effective fusion of temporal and multimodal sig-

nals. The right panel presents a UMAP visualization revealing distinct yet partially overlapping clusters, confirming successful integration of multimodal information while preserving modality-specific characteristics. The robust fewshot and zero-shot capabilities of Time-VLM stem from its integration of temporal, visual, and textual modalities. Specifically, the RAL models temporal dependencies via memory bank interactions, enabling robust feature extraction even with limited data. The VAL captures interpretable visual patterns—such as trends, seasonality, and periodicity—in domain-agnostic representations, while the TAL generates semantic descriptions that enhance generalization. Together, these components allow Time-VLM to leverage pre-trained multimodal knowledge, making it highly adaptable to new tasks and domains with minimal fine-tuning.

![](images/19647eed3d515ff7d4062b6f600b6a2f4c932cdc2c336611dc62fe672ec05a3f.jpg)

![](images/2738b29efaf7e99d785a232ab0a7a4879e97c49f6ea6b1c26a172cb563b2f815.jpg)  
Figure 3: UMAP visualization (left) and gate weight distributions (right) of of multimodal and temporal embeddings.

Interpretability Analysis. We investigate how pre-trained VLMs can be leveraged for time series forecasting by analyzing the alignment between visual, textual, and temporal representations. To this end, we sample 400 image-text pairs from MSCOCO, the primary pre-training dataset for VLMs, and 200 samples each from time series datasets: ETT, Traffic, Weather, and ECL. Using UMAP, we visualize four types of embeddings in a shared 2D space:

• multimodal embeddings derived from COCO-Pair samples using VLM, reflecting the model’s general pretrained cross-modal knowledge, which is Time-VLM’s main motivation of the time series project to.   
• multimodal embeddings from time series-generated image-text pairs processed through the same VLM, representing Time-VLM’s task-specific augmentation.   
• Visual-only embeddings from COCO-Image samples extracted via ViT, reflecting pure visual knowledge.   
• Text-only embeddings from COCO-Text samples encoded with BERT, capturing linguistic knowledge.

As shown in Figure 4, both COCO-Image and COCO-Text form distinct, separate clusters, remaining isolated from time-series features. This suggests that while low-level visual patterns in COCO-Image resemble temporal dynamics,

![](images/f3604cef078b11467d1addbc13d07b0f07e0377d9aadad3daf63a3fe1964b5c7.jpg)  
Figure 4: Interpretability visualization of Time-VLM: multimodal feature alignment via UMAP.

pure image representations retain modality-specific characteristics. Similarly, COCO-Text forms a completely separate cluster, highlighting significant modality gaps. In contrast, COCO-Pair exhibits maximal overlap with time-series data, demonstrating strong cross-modal complementarity. The textual semantics in COCO-Pair bridge visual and temporal modalities, enhancing their alignment. Notably, COCO-Pair is positioned near the center of time-series clusters, suggesting its role as a key mediator between modalities.

These observations motivate Time-VLM’s design: instead of relying on single-modality projections, we embed time series into multimodal space for richer semantic understanding. The model’s strong performance in data-scarce settings stems from pre-trained VLM knowledge. With more time series data, better alignment of multimodal embeddings is expected, further enhancing performance.

Computation Studies. Time-VLM demonstrates strong computational efficiency, as shown in Table 7. With only 143.6M parameters (1/20 of Time-LLM’s 3404.6M), memory usage scales from 1968 MiB (Weather) to 24916 MiB (Traffic), adapting to dataset complexity. Inference speed ranges from 0.2057s/iter (ECL) to 0.4809s/iter (ETTh1), efficiently handling varying loads. In contrast, Time-LLM requires over 37GB of memory even for smaller datasets like ETTh1 and ETTh2, making it infeasible for larger datasets such as Weather, ECL, and Traffic. This highlights Time-VLM’s lightweight design and practical scalability.

Table 7: Computational efficiency comparison between Time-VLM and Time-LLM across datasets. “-” denotes memory exceeds 49GB, infeasible on a single GPU. Results are averaged over multiple prediction steps under consistent conditions.   

<table><tr><td>Method</td><td>Metric</td><td>ETTh1</td><td>ETTh2</td><td>ETTm1</td><td>ETTm2</td><td>Weather</td><td>ECL</td><td>Traffic</td></tr><tr><td rowspan="3">Time-VLM</td><td>Param. (M)</td><td>143.6</td><td>143.6</td><td>143.6</td><td>143.6</td><td>143.6</td><td>143.6</td><td>143.6</td></tr><tr><td>Mem. (MiB)</td><td>2630</td><td>2630</td><td>2640</td><td>2640</td><td>1968</td><td>10818</td><td>24916</td></tr><tr><td>Speed (s/iter)</td><td>0.481</td><td>0.438</td><td>0.277</td><td>0.210</td><td>0.296</td><td>0.206</td><td>0.323</td></tr><tr><td rowspan="3">Time-LLM</td><td>Param. (M)</td><td>3404.6</td><td>3404.6</td><td>3404.6</td><td>3404.6</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Mem. (MiB)</td><td>37723</td><td>37723</td><td>37849</td><td>37849</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Speed (s/iter)</td><td>0.607</td><td>0.553</td><td>0.349</td><td>0.265</td><td>-</td><td>-</td><td>-</td></tr></table>

Hyperparameter Studies. We analyze the impact of key hyperparameters on performance, as shown in Figure 5. Sequence length performs best between 96 and 1024 timesteps, with 512 being optimal for most datasets. Longer input introduce noise without significant gains, indicating that local temporal patterns are sufficient for accurate forecasting. The normalization constant peaks at 0.4, reflecting a balance between feature scaling and training stability. Model dimension shows dataset-dependent behavior: values of 128–256 suffice for short-term datasets like ETTh1 and ETTh2, while longer horizons and higher-dimensional data (e.g., Traffic, Weather) benefit from larger dimensions (up to 512), suggesting greater capacity is needed to model complex dynamics and variable interactions. Similarly, the gate network dimension—responsible for multimodal fusion—achieves optimal performance at 256 for medium-range forecasts. For more challenging settings like long-horizon or highvariable inputs, increasing it to 336 or 512 further improves results, highlighting the importance of adaptive fusion in capturing complex cross-modal relationships.

![](images/600881762f6194e83e2ad29263e81dee6aac87b5846142342aa7074b8b44ff03.jpg)  
Figure 5: Hyperparameters sensitivity analysis on input length, normalization constant, dimension of model and dimension of gate network, reflected by MAE.

VLM Variants Analysis. We conduct ablation studies on different VLM backbones and custom combinations to assess their impact on forecasting performance and computational efficiency. We evaluate three widely-used VLMs—ViLT, CLIP, and BLIP-2—with varying size, and find that increased model size does not improve accuracy. Notably, although BLIP-2 is the largest (3.7B parameters, ${ 2 5 } \mathrm { G B + }$ memory), it underperforms ViLT and CLIP in terms of MSE (0.342 vs. 0.337) and exhibits slow training speed (0.98 s/iter), limiting its practical use. In contrast, lightweight models like ViLT (128.9M parameters, 1346 MiB memory) and CLIP (168.4M, 1174 MiB) achieve comparable or better accuracy at a fraction of the cost. To

evaluate the benefit of VLMs’ pre-trained cross-modal alignment, we construct a modular baseline using separately trained vision and language encoders: ViT-B/16 and BERT-Base. As shown in Table 8, this custom combination underperforms all pretrained VLMs across metrics, achieving an average MSE of 0.348 versus 0.336 for ViLT, with no compensating gain in speed (0.17 s/iter). These findings highlight that the cross-modal alignment in VLMs offers a key inductive bias for time series modeling. Modular approaches, lacking such a unified space, fail to match this performance—demonstrating the value of pre-aligned multimodal representations for efficient fusion and forecasting.

Table 8: Comparison of different VLM variants on ETTh2 in terms of performance and computational efficiency.   

<table><tr><td>VLM Type</td><td>Params (M)</td><td>Mem. (MiB)</td><td>Speed (s/iter)</td><td>MSE (avg)</td><td>MAE (avg)</td></tr><tr><td>ViLT</td><td>128.9</td><td>1346</td><td>0.36</td><td>0.336</td><td>0.388</td></tr><tr><td>CLIP</td><td>168.4</td><td>1174</td><td>0.12</td><td>0.339</td><td>0.391</td></tr><tr><td>BLIP-2</td><td>3763.1</td><td>25200</td><td>0.98</td><td>0.342</td><td>0.393</td></tr><tr><td>Custom</td><td>213.2</td><td>1474</td><td>0.17</td><td>0.348</td><td>0.397</td></tr></table>

# 5. Conclusion

We presented Time-VLM, a novel framework that leverages pretrained VLMs to unify temporal, visual, and textual modalities for time series forecasting. By integrating the RAL, VAL, and TAL modules, Time-VLM bridges modality gaps and enables rich cross-modal interactions. Notably, it operates solely on raw time series data without requiring external information, enabling fair comparisons and demonstrating the ability to generate textual and visual representations internally for self-augmentation. This design not only improves accuracy but also highlights the framework’s robustness—particularly in domains where auxiliary data is scarce. Extensive experiments show that Time-VLM achieves superior performance across diverse datasets, especially in few-shot and zero-shot settings, outperforming existing methods while maintaining computational efficiency. Our work establishes a new direction in multimodal time series forecasting by highlighting the potential of VLMs in capturing both temporal dynamics and semantic context.

Limitations. Despite its strengths, Time-VLM has several limitations. First, the TAL module provides semantic context but has limited impact due to current VLMs’ constrained understanding of time series semantics. Second, while excelling in low-data regimes, full-shot performance slightly lags behind specialized unimodal models on certain tasks (e.g., ECL, Traffic), suggesting room for domainspecific adaptation. Third, although computationally efficient compared to LLM-based methods, deployment on resource-constrained devices remains challenging. These limitations suggest promising directions for future work, including temporally aware VLMs, improved time series imaging, visual distillation, and enhanced text encoders with stronger temporal reasoning. For details, see Appendix D.

# Acknowledgements

This work is mainly supported by the Guangdong Basic and Applied Basic Research Foundation (No. 2025A1515011994). This work is also supported by the National Natural Science Foundation of China (No. 62402414, No. 62402420), Guangzhou Municipal Science and Technology Project (No. 2023A03J0011), the Guangzhou Industrial Information and Intelligent Key Laboratory Project (No. 2024A03J0628), and a grant from State Key Laboratory of Resources and Environmental Information System, and Guangdong Provincial Key Lab of Integrated Communication, Sensing and Computation for Ubiquitous Internet of Things (No. 2023B1212010007).

# Impact Statement

This paper presents work whose goal is to advance the field of Machine Learning by integrating temporal, visual, and textual modalities for time series forecasting. While our approach improves accuracy and cross-domain generalization, we acknowledge potential risks such as data privacy concerns, algorithmic bias, and increased computational costs. We encourage further research into mitigating these risks to ensure responsible deployment in high-stakes applications.

# References

Challu, C., Olivares, K. G., Oreshkin, B. N., Ramirez, F. G., Canseco, M. M., and Dubrawski, A. Nhits: neural hierarchical interpolation for time series forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 6989–6997, 2023.   
Chang, C., Peng, W.-C., and Chen, T.-F. Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.   
Chen, M., Shen, L., Li, Z., Wang, X. J., Sun, J., and Liu, C. Visionts: Visual masked autoencoders are free-lunch zero-shot time series forecasters, 2024. URL https: //arxiv.org/abs/2408.17253.   
Deb, C., Zhang, F., Yang, J., Lee, S. E., and Shah, K. W. A review on time series forecasting techniques for building energy consumption. Renewable and Sustainable Energy Reviews, 74:902–924, 2017.   
Gruver, N., Finzi, M., Qiu, S., and Wilson, A. G. Large language models are zero-shot time series forecasters. In Advances in Neural Information Processing Systems, 2023.   
Idrees, S. M., Alam, M. A., and Agarwal, P. A prediction approach for stock market volatility based on time series data. IEEE Access, 7:17287–17298, 2019.

Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung, Y.-H., Li, Z., and Duerig, T. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pp. 4904–4916. PMLR, 2021.   
Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S., and Wen, Q. Time-LLM: Time series forecasting by reprogramming large language models. In International Conference on Learning Representations (ICLR), 2024.   
Karevan, Z. and Suykens, J. A. Transductive lstm for timeseries prediction: An application to weather forecasting. Neural Networks, 125:1–9, 2020.   
Kim, W., Son, B., and Kim, I. Vilt: Vision-and-language transformer without convolution or region supervision. In International conference on machine learning, pp. 5583– 5594. PMLR, 2021.   
Kitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020.   
Lai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95–104, 2018.   
Li, J., Li, D., Savarese, S., et al. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pp. 19730–19742. PMLR, 2023.   
Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. In Advances in neural information processing systems, 2019.   
Li, X., Kang, Y., and Li, F. Forecasting with time series imaging. Expert Systems with Applications, 160:113680, 2020.   
Liang, Y., Wen, H., Nie, Y., Jiang, Y., Jin, M., Song, D., Pan, S., and Wen, Q. Foundation models for time series analysis: A tutorial and survey. In Proceedings of the 30th ACM SIGKDD conference on knowledge discovery and data mining, pp. 6555–6565, 2024.   
Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.   
Liu, Q., Liu, X., Liu, C., Wen, Q., and Liang, Y. Time-ffm: Towards lm-empowered federated foundation model for time series forecasting. arXiv preprint arXiv:2405.14252, 2024a.

Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A. X., and Dustdar, S. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022a.   
Liu, X., Hu, J., Li, Y., Diao, S., Liang, Y., Hooi, B., and Zimmermann, R. Unitime: A language-empowered unified model for cross-domain time series forecasting. In Proceedings of the ACM on Web Conference 2024, pp. 4095–4106, 2024b.   
Liu, Y., Wu, H., Wang, J., and Long, M. Non-stationary transformers: Exploring the stationarity in time series forecasting. Advances in Neural Information Processing Systems, 35:9881–9893, 2022b.   
Makridakis, S., Spiliotis, E., and Assimakopoulos, V. The m4 competition: Results, findings, conclusion and way forward. International Journal of Forecasting, 34(4): 802–808, 2018.   
Medsker, L. R., Jain, L., et al. Recurrent neural networks. Design and Applications, 5(64-67):2, 2001.   
Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. A time series is worth 64 words: Long-term forecasting with transformers. In International Conference on Learning Representations, 2023.   
Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-beats: Neural basis expansion analysis for interpretable time series forecasting. In International Conference on Learning Representations, 2020.   
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021.   
Sood, S., Zeng, Z., Cohen, N., Balch, T., and Veloso, M. Visual time series forecasting: an image-driven approach. In Proceedings of the Second ACM International Conference on AI in Finance, pp. 1–9, 2021.   
Wang, S., Li, J., Shi, X., Ye, Z., Mo, B., Lin, W., Ju, S., Chu, Z., and Jin, M. Timemixer++: A general time series pattern machine for universal predictive analysis. arXiv preprint arXiv:2410.16032, 2024.   
Woo, G., Liu, C., Sahoo, D., Kumar, A., and Hoi, S. Etsformer: Exponential smoothing transformers for timeseries forecasting. In arXiv preprint arXiv:2202.01381, 2022.   
Wu, H., Xu, J., Wang, J., and Long, M. Autoformer: Decomposition transformers with auto-correlation for long-term

series forecasting. Advances in Neural Information Processing Systems, 34:22419–22430, 2021.   
Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023a.   
Wu, H., Hu, T., Liu, Y., Zhou, H., Wang, J., and Long, M. Timesnet: Temporal 2d-variation modeling for general time series analysis. In International Conference on Learning Representations, 2023b.   
Zeng, A., Chen, M., Zhang, L., and Xu, Q. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 11121–11128, 2023.   
Zhang, T., Zhang, Y., Cao, W., Bian, J., Yi, X., Zheng, S., and Li, J. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprint arXiv:2207.01186, 2022.   
Zheng, J. and Huang, M. Traffic flow forecast through time series analysis based on deep learning. IEEE Access, 8: 82562–82570, 2020.   
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on Artificial Intelligence, pp. 11106– 11115, 2021.   
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International Conference on Machine Learning, pp. 27268–27286, 2022.   
Zhou, T., Niu, P., Wang, X., Sun, L., and Jin, R. One fits all: Power general time series analysis by pretrained lm. In Advances in Neural Information Processing Systems, 2023.

# A. Experimental Details

# A.1. Dataset Details

Table 9: Summary of benchmark datasets. Each dataset includes multiple time series (Dim.) with varying sequence lengths, split into training, validation, and testing sets. Data are collected at different frequencies across various domains.   

<table><tr><td>Tasks</td><td>Dataset</td><td>Dim.</td><td>Series Length</td><td>Dataset Size</td><td>Frequency</td><td>Domain</td><td>Periodicity</td></tr><tr><td rowspan="7">Long-term Forecasting</td><td>ETTm1</td><td>7</td><td>{96, 192, 336, 720}</td><td>(34465, 11521, 11521)</td><td>15 min</td><td>Temperature</td><td>96</td></tr><tr><td>ETTm2</td><td>7</td><td>{96, 192, 336, 720}</td><td>(34465, 11521, 11521)</td><td>15 min</td><td>Temperature</td><td>96</td></tr><tr><td>ETTh1</td><td>7</td><td>{96, 192, 336, 720}</td><td>(8545, 2881, 2881)</td><td>1 hour</td><td>Temperature</td><td>24</td></tr><tr><td>ETTh2</td><td>7</td><td>{96, 192, 336, 720}</td><td>(8545, 2881, 2881)</td><td>1 hour</td><td>Temperature</td><td>24</td></tr><tr><td>Electricity</td><td>321</td><td>{96, 192, 336, 720}</td><td>(18317, 2633, 5261)</td><td>1 hour</td><td>Electricity</td><td>24</td></tr><tr><td>Traffic</td><td>862</td><td>{96, 192, 336, 720}</td><td>(12185, 1757, 3509)</td><td>1 hour</td><td>Transportation</td><td>24</td></tr><tr><td>Weather</td><td>21</td><td>{96, 192, 336, 720}</td><td>(36792, 5271, 10540)</td><td>10 min</td><td>Weather</td><td>144</td></tr><tr><td rowspan="6">Short-term Forecasting</td><td>M4 - Yearly</td><td>1</td><td>6</td><td>(23000, 0, 23000)</td><td>Yearly</td><td>Demographic</td><td>1</td></tr><tr><td>M4 - Quarterly</td><td>1</td><td>8</td><td>(24000, 0, 24000)</td><td>Quarterly</td><td>Finance</td><td>4</td></tr><tr><td>M4 - Monthly</td><td>1</td><td>18</td><td>(48000, 0, 48000)</td><td>Monthly</td><td>Industry</td><td>3</td></tr><tr><td>M4 - Weekly</td><td>1</td><td>13</td><td>(359, 0, 359)</td><td>Weekly</td><td>Macro</td><td>4</td></tr><tr><td>M4 - Daily</td><td>1</td><td>14</td><td>(4227, 0, 4227)</td><td>Daily</td><td>Micro</td><td>1</td></tr><tr><td>M4 - Hourly</td><td>1</td><td>48</td><td>(414, 0, 414)</td><td>Hourly</td><td>Other</td><td>24</td></tr></table>

The benchmark datasets used in our experiments are summarized in Table 9. These datasets span diverse domains, including temperature monitoring (ETTm1, ETTm2, ETTh1, ETTh2), electricity consumption (Electricity), transportation (Traffic), and weather forecasting (Weather). Each dataset contains multiple time series with varying sequence lengths, split into training, validation, and testing sets. The datasets are collected at different frequencies, ranging from 15 minutes to yearly intervals, and exhibit distinct periodic patterns. For short-term forecasting, we utilize the M4 benchmark, which includes datasets with yearly, quarterly, monthly, weekly, daily, and hourly frequencies, covering domains such as finance, industry, and demographics. This diverse collection of datasets ensures a comprehensive evaluation of our method.

# A.1.1. DATASET DESCRIPTION

The datasets used in our experiments are described below:

• ECL: Measurements of electric power consumption in one household with a one-minute sampling rate over 4 years. It includes various electrical quantities and sub-metering values, totaling 2,075,259 measurements from a house in Sceaux, France (December 2006 to November 2010).   
• ETT: The Electricity Transformer Temperature (ETT) dataset, crucial for electric power deployment, contains 2 years of data from two counties in China. Subsets ETTh1 and ETTh2 provide 1-hour-level data, while ETTm1 offers 15-minute-level data. Each point includes the target ”oil temperature” and 6 power load features, with a 12/4/4 month train/val/test split.   
• Traffic: Hourly data from the California Department of Transportation, describing road occupancy rates measured by sensors on San Francisco Bay area freeways.   
• Weather: Recorded every 10 minutes throughout 2020, this dataset includes 21 meteorological indicators, such as air temperature and humidity.   
• M4: A collection of 100,000 time series from the Makridakis Forecasting Competition, including yearly, quarterly, monthly, weekly, daily, and hourly data. The training sets have minimum observations of 13 (yearly), 16 (quarterly), 42 (monthly), 80 (weekly), 93 (daily), and 700 (hourly). Forecasts required are 6 (yearly), 8 (quarterly), 18 (monthly), 13 (weekly), 14 (daily), and 48 (hourly).

# A.1.2. PERIODICITY PARAMETER

The Periodicity column in Table 9 specifies the periodicity hyperparameter $P$ used in the periodicity encoding process. This parameter is derived from the inherent characteristics of each dataset and reflects the dominant temporal patterns, such as

daily, weekly, or seasonal cycles. For example, in the ETTm1 and ETTm2 datasets, which are sampled every 15 minutes, the periodicity $P = 9 6$ corresponds to a daily cycle (24 hours $\times 4$ samples per hour). Similarly, for the ETTh1 and ETTh2 datasets, sampled hourly, $P = 2 4$ represents a daily cycle. The Weather dataset, sampled every 10 minutes, has $P = 1 4 4$ , reflecting a daily cycle (24 hours $\times 6$ samples per hour). For the M4 benchmark datasets, the periodicity values are set based on their sampling frequencies: $P = 1$ for yearly data, $P = 4$ for quarterly and weekly data, $P = 3$ for monthly data, and $P = 2 4$ for hourly data. These values are used in the periodicity encoding formula:

$$
\operatorname {e n c o d i n g} (t) = \left[ \sin \left(\frac {2 \pi t}{P}\right), \cos \left(\frac {2 \pi t}{P}\right) \right], \tag {16}
$$

where $t$ is the time step and $P$ is the periodicity hyperparameter. The resulting encodings are concatenated with the input time series, enriching the model’s ability to capture temporal dependencies and periodic patterns.

# A.2. Optimization Settings

# A.2.1. MODEL ARCHITECTURE PARAMETERS

Time-VLM consists of several key components, each with specific parameter configurations. Image representations are set to a size of $6 4 \times 6 4$ , balancing computational efficiency and temporal information preservation. The model backbone utilizes a hidden dimension of $d _ { - } m o d e l = 1 2 8$ , while the encoder-decoder structure comprises $e \_ l a y e r s = 2$ encoder layers and $d \_ l a y e r s = 1$ decoder layer. A dropout rate of 0.1 is applied to mitigate overfitting during training. For efficient data loading, the model employs num worker $s = 3 2$ to parallelize data preprocessing tasks.

The gated fusion module is designed with a dimension of $d _ { - } f u s i o n = 2 5 6$ , facilitating the effective integration of multimodal features. The VLM component generates multimodal embeddings with a token length of vlm fused len = 156 and a hidden dimension of vlm hidden dim $= 7 6 8$ , ensuring seamless compatibility with the pre-trained VLM’s architecture.

Table 10: Default Model Architecture Parameters   

<table><tr><td>Parameter</td><td>Default Value</td><td>Description</td></tr><tr><td>image_size</td><td>64</td><td>Size of generated image representation</td></tr><tr><td>d_model</td><td>128</td><td>Dimension of hidden embeddings</td></tr><tr><td>d_fusion</td><td>256</td><td>Dimension of gated fusion module</td></tr><tr><td>num_workers</td><td>32</td><td>Number of data loader workers</td></tr><tr><td>e_layers</td><td>2</td><td>Number of encoder layers</td></tr><tr><td>d_layers</td><td>1</td><td>Number of decoder layers</td></tr><tr><td>dropout</td><td>0.1</td><td>Dropout rate</td></tr><tr><td>vlm_fused_len</td><td>156</td><td>Token length of VLM multimodal embedding</td></tr><tr><td>vlm-hidden_dim</td><td>768</td><td>Hidden dimension of VLM</td></tr></table>

# A.2.2. TRAINING PARAMETERS

We adopt a comprehensive training strategy with both general and task-specific parameters. The model is trained with a batch size of 32 and an initial learning rate of 0.001, using the AdamW optimizer. Early stopping with a patience of 3 epochs is implemented to prevent overfitting. The training process employs Mean Squared Error (MSE) as the primary loss function and runs for a maximum of 10 epochs. For time series processing, we use an input sequence length of 512 and prediction lengths of 96, 192, 336, or 720, depending on the task. The output dimension (c out) varies by dataset: 7 for $\mathrm { E T T h 1 / h } 2 / \mathrm { m } 1 / \mathrm { m } 2$ , 21 for Weather, 321 for Electricity, and 862 for Traffic. The periodicity parameter is set to 24 for ETTh1/h2, Electricity, and Traffic; 96 for $\mathrm { E T T m } 1 / \mathrm { m } 2 $ ; and 144 for Weather, ensuring alignment with dataset-specific temporal patterns. A normalization coefficient of 0.4 is applied to stabilize training dynamics. The patch embedding module uses a patch length of 16, a stride of 8, and padding of 8 to process the input sequences. The temporal memory mechanism employs 8 learnable queries and 4 attention heads to capture high-level dependencies. Additionally, the training process leverages automatic mixed precision (AMP) to accelerate training while maintaining numerical stability.

Table 11: Default Training Parameters   

<table><tr><td>Parameter</td><td>Default Value</td><td>Description</td></tr><tr><td>batch_size</td><td>32</td><td>Training batch size</td></tr><tr><td>learning_rate</td><td>0.001</td><td>Initial learning rate</td></tr><tr><td>training_epochs</td><td>10</td><td>Number of training epochs</td></tr><tr><td>patience</td><td>3</td><td>Early stopping patience</td></tr><tr><td>loss</td><td>MSE</td><td>Mean square error</td></tr><tr><td>seq_len</td><td>512</td><td>Input sequence length</td></tr><tr><td>c_out</td><td>7 (ETTh1/h2/m1/m2)21 (Weather)321 (Electricity)862 (Traffic)</td><td>Output dimension (dataset-specific)</td></tr><tr><td>pred_len</td><td>96/192/336/720</td><td>Prediction length</td></tr><tr><td>periodicity</td><td>24 (ETTh1/h2/Electricity/Traffic)96 (ETTm1/m2)144 (Weather)</td><td>Dataset periodicity (dataset-specific)</td></tr><tr><td>norm_const</td><td>0.4</td><td>Normalization coefficient</td></tr><tr><td>patch_len</td><td>16</td><td>Patch length</td></tr><tr><td>padding</td><td>8</td><td>Padding length</td></tr><tr><td>stride</td><td>8</td><td>Stride length</td></tr><tr><td>num_queries</td><td>8</td><td>Number of learnable queries for temporal memory</td></tr><tr><td>n_heads</td><td>4</td><td>Number of attention heads</td></tr></table>

# A.3. Evaluation Metrics

For evaluation, we utilize mean squared error (MSE) and mean absolute error (MAE) for long-term forecasting. For short-term forecasting on the M4 benchmark, we adopt symmetric mean absolute percentage error (SMAPE), mean absolute scaled error (MASE), and overall weighted average (OWA), following the evaluation protocol of N-BEATS (Oreshkin et al., 2020). OWA is a specific metric used in the M4 competition. The metrics are calculated as follows:

$$
\mathrm {M S E} = \frac {1}{H} \sum_ {h = 1} ^ {T} \left(\mathbf {Y} _ {h} - \hat {\mathbf {Y}} _ {h}\right) ^ {2},
$$

$$
\mathrm {M A E} = \frac {1}{H} \sum_ {h = 1} ^ {H} | \mathbf {Y} _ {h} - \hat {\mathbf {Y}} _ {h} |,
$$

$$
\mathrm {S M A P E} = \frac {2 0 0}{H} \sum_ {h = 1} ^ {H} \frac {\left| \mathbf {Y} _ {h} - \hat {\mathbf {Y}} _ {h} \right|}{\left| \mathbf {Y} _ {h} \right| + \left| \hat {\mathbf {Y}} _ {h} \right|},
$$

$$
\mathrm {M A P E} = \frac {1 0 0}{H} \sum_ {h = 1} ^ {H} \frac {\left| \mathbf {Y} _ {h} - \hat {\mathbf {Y}} _ {h} \right|}{\left| \mathbf {Y} _ {h} \right|},
$$

$$
\mathbf {M A S E} = \frac {1}{H} \sum_ {h = 1} ^ {H} \frac {\left| \mathbf {Y} _ {h} - \hat {\mathbf {Y}} _ {h} \right|}{\frac {1}{H - s} \sum_ {j = s + 1} ^ {H} \left| \mathbf {Y} _ {j} - \mathbf {Y} _ {j - s} \right|},
$$

$$
\mathrm {O W A} = \frac {1}{2} \left[ \frac {\mathrm {S M A P E}}{\mathrm {S M A P E} _ {\mathrm {N a i v e} 2}} + \frac {\mathrm {M A S E}}{\mathrm {M A S E} _ {\mathrm {N a i v e} 2}} \right],
$$

where $s$ is the periodicity of the time series, $H$ is the prediction horizon, and ${ \bf Y } _ { h }$ and $\hat { \mathbf Y } _ { h }$ are the ground truth and prediction at time step $h$ , respectively.

# B. Complete results

# B.1. Few-shot Forecasting

Table 12: Full few-shot learning results on $5 \%$ training data with forecasting horizons $H \in \{ 9 6 , 1 9 2 , 3 3 6 , 7 2 0 \}$ . A lower value indicates better performance. ’-’ means that $5 \%$ time series is not sufficient to constitute a training set. Red: the best, Blue: the second best   

<table><tr><td colspan="2">Methods</td><td colspan="2">Time-VLM</td><td colspan="2">Time-LLM</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td><td colspan="2">Stationary</td><td colspan="2">ETSformer</td><td colspan="2">LightTS</td><td colspan="2">Informer</td><td colspan="2">Reformer</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">ETTh1</td><td>96</td><td>0.417</td><td>0.435</td><td>0.483</td><td>0.464</td><td>0.543</td><td>0.506</td><td>0.547</td><td>0.503</td><td>0.557</td><td>0.519</td><td>0.892</td><td>0.625</td><td>0.593</td><td>0.529</td><td>0.681</td><td>0.570</td><td>0.952</td><td>0.650</td><td>1.169</td><td>0.832</td><td>1.483</td><td>0.910</td><td>1.225</td><td>0.812</td><td>1.198</td><td>0.795</td></tr><tr><td>192</td><td>0.450</td><td>0.458</td><td>0.629</td><td>0.540</td><td>0.748</td><td>0.580</td><td>0.720</td><td>0.604</td><td>0.711</td><td>0.570</td><td>0.940</td><td>0.665</td><td>0.652</td><td>0.563</td><td>0.725</td><td>0.602</td><td>0.943</td><td>0.645</td><td>1.221</td><td>0.853</td><td>1.525</td><td>0.930</td><td>1.249</td><td>0.828</td><td>1.273</td><td>0.853</td></tr><tr><td>336</td><td>0.460</td><td>0.465</td><td>0.768</td><td>0.626</td><td>0.754</td><td>0.595</td><td>0.984</td><td>0.727</td><td>0.816</td><td>0.619</td><td>0.945</td><td>0.653</td><td>0.731</td><td>0.594</td><td>0.761</td><td>0.624</td><td>0.935</td><td>0.644</td><td>1.179</td><td>0.832</td><td>1.347</td><td>0.870</td><td>1.202</td><td>0.811</td><td>1.254</td><td>0.857</td></tr><tr><td>720</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>Avg</td><td>0.442</td><td>0.453</td><td>0.627</td><td>0.543</td><td>0.681</td><td>0.560</td><td>0.750</td><td>0.611</td><td>0.694</td><td>0.569</td><td>0.925</td><td>0.647</td><td>0.658</td><td>0.562</td><td>0.722</td><td>0.598</td><td>0.943</td><td>0.646</td><td>1.189</td><td>0.839</td><td>1.451</td><td>0.903</td><td>1.225</td><td>0.817</td><td>1.241</td><td>0.835</td></tr><tr><td rowspan="5">ETTh2</td><td>96</td><td>0.302</td><td>0.365</td><td>0.336</td><td>0.397</td><td>0.376</td><td>0.421</td><td>0.442</td><td>0.456</td><td>0.401</td><td>0.421</td><td>0.409</td><td>0.420</td><td>0.390</td><td>0.424</td><td>0.428</td><td>0.468</td><td>0.408</td><td>0.423</td><td>0.678</td><td>0.619</td><td>2.022</td><td>1.006</td><td>3.837</td><td>1.508</td><td>3.753</td><td>1.518</td></tr><tr><td>192</td><td>0.361</td><td>0.406</td><td>0.406</td><td>0.425</td><td>0.418</td><td>0.441</td><td>0.617</td><td>0.542</td><td>0.452</td><td>0.455</td><td>0.483</td><td>0.464</td><td>0.457</td><td>0.465</td><td>0.496</td><td>0.504</td><td>0.497</td><td>0.468</td><td>0.845</td><td>0.697</td><td>3.534</td><td>1.348</td><td>3.975</td><td>3.935</td><td>1.473</td><td></td></tr><tr><td>336</td><td>0.398</td><td>0.434</td><td>0.405</td><td>0.432</td><td>0.408</td><td>0.439</td><td>1.424</td><td>0.849</td><td>0.464</td><td>0.469</td><td>0.499</td><td>0.479</td><td>0.477</td><td>0.483</td><td>0.486</td><td>0.496</td><td>0.507</td><td>0.481</td><td>0.905</td><td>0.727</td><td>4.063</td><td>1.451</td><td>3.956</td><td>1.520</td><td>3.312</td><td>1.427</td></tr><tr><td>720</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>Avg</td><td>0.354</td><td>0.402</td><td>0.382</td><td>0.418</td><td>0.400</td><td>0.433</td><td>0.694</td><td>0.577</td><td>0.827</td><td>0.615</td><td>0.439</td><td>0.448</td><td>0.463</td><td>0.454</td><td>0.441</td><td>0.457</td><td>0.470</td><td>0.489</td><td>0.809</td><td>0.681</td><td>3.206</td><td>1.268</td><td>3.922</td><td>1.653</td><td>3.527</td><td>1.472</td></tr><tr><td rowspan="5">ETThm1</td><td>96</td><td>0.314</td><td>0.357</td><td>0.316</td><td>0.377</td><td>0.386</td><td>0.405</td><td>0.332</td><td>0.374</td><td>0.399</td><td>0.414</td><td>0.606</td><td>0.518</td><td>0.628</td><td>0.544</td><td>0.726</td><td>0.578</td><td>0.823</td><td>0.587</td><td>1.031</td><td>0.747</td><td>1.048</td><td>0.733</td><td>1.130</td><td>0.775</td><td>1.234</td><td>0.798</td></tr><tr><td>192</td><td>0.343</td><td>0.373</td><td>0.450</td><td>0.464</td><td>0.440</td><td>0.438</td><td>0.358</td><td>0.390</td><td>0.441</td><td>0.436</td><td>0.681</td><td>0.539</td><td>0.666</td><td>0.566</td><td>0.750</td><td>0.591</td><td>0.844</td><td>0.591</td><td>1.087</td><td>0.766</td><td>1.097</td><td>0.756</td><td>1.150</td><td>0.788</td><td>1.287</td><td>0.839</td></tr><tr><td>336</td><td>0.373</td><td>0.391</td><td>0.450</td><td>0.424</td><td>0.485</td><td>0.459</td><td>0.402</td><td>0.416</td><td>0.499</td><td>0.467</td><td>0.786</td><td>0.597</td><td>0.807</td><td>0.628</td><td>0.851</td><td>0.659</td><td>0.870</td><td>0.603</td><td>1.138</td><td>0.787</td><td>1.147</td><td>0.775</td><td>1.198</td><td>0.809</td><td>1.288</td><td>0.842</td></tr><tr><td>720</td><td>0.425</td><td>0.420</td><td>0.483</td><td>0.471</td><td>0.577</td><td>0.499</td><td>0.511</td><td>0.489</td><td>0.767</td><td>0.587</td><td>0.796</td><td>0.593</td><td>0.822</td><td>0.633</td><td>0.857</td><td>0.655</td><td>0.893</td><td>0.611</td><td>1.245</td><td>0.831</td><td>1.200</td><td>0.799</td><td>1.175</td><td>0.794</td><td>1.247</td><td>0.828</td></tr><tr><td>Avg</td><td>0.364</td><td>0.385</td><td>0.425</td><td>0.434</td><td>0.472</td><td>0.450</td><td>0.400</td><td>0.417</td><td>0.526</td><td>0.476</td><td>0.717</td><td>0.561</td><td>0.730</td><td>0.592</td><td>0.796</td><td>0.620</td><td>0.857</td><td>0.598</td><td>1.125</td><td>0.782</td><td>1.123</td><td>0.765</td><td>1.163</td><td>0.791</td><td>1.264</td><td>0.826</td></tr><tr><td rowspan="5">ETThm2</td><td>96</td><td>0.169</td><td>0.260</td><td>0.174</td><td>0.261</td><td>0.199</td><td>0.280</td><td>0.236</td><td>0.326</td><td>0.206</td><td>0.288</td><td>0.220</td><td>0.299</td><td>0.229</td><td>0.320</td><td>0.232</td><td>0.322</td><td>0.238</td><td>0.316</td><td>0.404</td><td>0.485</td><td>1.108</td><td>0.772</td><td>3.599</td><td>1.478</td><td>3.883</td><td>1.545</td></tr><tr><td>192</td><td>0.224</td><td>0.298</td><td>0.215</td><td>0.287</td><td>0.256</td><td>0.316</td><td>0.306</td><td>0.373</td><td>0.264</td><td>0.324</td><td>0.311</td><td>0.361</td><td>0.394</td><td>0.361</td><td>0.291</td><td>0.357</td><td>0.298</td><td>0.349</td><td>0.479</td><td>0.521</td><td>1.317</td><td>0.850</td><td>3.578</td><td>1.475</td><td>3.553</td><td>1.484</td></tr><tr><td>336</td><td>0.282</td><td>0.338</td><td>0.273</td><td>0.330</td><td>0.318</td><td>0.353</td><td>0.380</td><td>0.423</td><td>0.334</td><td>0.367</td><td>0.338</td><td>0.366</td><td>0.378</td><td>0.427</td><td>0.478</td><td>0.517</td><td>0.353</td><td>0.380</td><td>0.552</td><td>0.555</td><td>1.415</td><td>0.879</td><td>3.561</td><td>1.473</td><td>3.446</td><td>1.460</td></tr><tr><td>720</td><td>0.375</td><td>0.397</td><td>0.433</td><td>0.412</td><td>0.460</td><td>0.436</td><td>0.674</td><td>0.583</td><td>0.454</td><td>0.432</td><td>0.509</td><td>0.465</td><td>0.523</td><td>0.510</td><td>0.553</td><td>0.538</td><td>0.475</td><td>0.445</td><td>0.701</td><td>0.627</td><td>1.822</td><td>0.984</td><td>3.896</td><td>1.533</td><td>3.345</td><td>1.460</td></tr><tr><td>Avg</td><td>0.262</td><td>0.323</td><td>0.274</td><td>0.323</td><td>0.308</td><td>0.346</td><td>0.399</td><td>0.426</td><td>0.314</td><td>0.352</td><td>0.344</td><td>0.372</td><td>0.381</td><td>0.404</td><td>0.388</td><td>0.433</td><td>0.341</td><td>0.372</td><td>0.534</td><td>0.547</td><td>1.415</td><td>0.871</td><td>3.658</td><td>1.489</td><td>3.581</td><td>1.487</td></tr><tr><td rowspan="5">Weather</td><td>96</td><td>0.176</td><td>0.231</td><td>0.172</td><td>0.263</td><td>0.175</td><td>0.230</td><td>0.184</td><td>0.242</td><td>0.171</td><td>0.224</td><td>0.207</td><td>0.253</td><td>0.229</td><td>0.309</td><td>0.227</td><td>0.299</td><td>0.215</td><td>0.252</td><td>0.218</td><td>0.295</td><td>0.230</td><td>0.285</td><td>0.497</td><td>0.497</td><td>0.406</td><td>0.435</td></tr><tr><td>192</td><td>0.216</td><td>0.263</td><td>0.224</td><td>0.271</td><td>0.227</td><td>0.276</td><td>0.228</td><td>0.283</td><td>0.230</td><td>0.277</td><td>0.272</td><td>0.307</td><td>0.265</td><td>0.317</td><td>0.278</td><td>0.333</td><td>0.290</td><td>0.307</td><td>0.294</td><td>0.331</td><td>0.274</td><td>0.323</td><td>0.620</td><td>0.545</td><td>0.446</td><td>0.450</td></tr><tr><td>336</td><td>0.264</td><td>0.298</td><td>0.282</td><td>0.321</td><td>0.286</td><td>0.322</td><td>0.279</td><td>0.322</td><td>0.294</td><td>0.326</td><td>0.313</td><td>0.328</td><td>0.353</td><td>0.392</td><td>0.351</td><td>0.393</td><td>0.353</td><td>0.348</td><td>0.359</td><td>0.398</td><td>0.318</td><td>0.355</td><td>0.649</td><td>0.547</td><td>0.465</td><td>0.459</td></tr><tr><td>720</td><td>0.327</td><td>0.342</td><td>0.366</td><td>0.381</td><td>0.366</td><td>0.379</td><td>0.364</td><td>0.388</td><td>0.384</td><td>0.400</td><td>0.385</td><td>0.391</td><td>0.394</td><td>0.387</td><td>0.452</td><td>0.389</td><td>0.452</td><td>0.407</td><td>0.461</td><td>0.461</td><td>0.418</td><td>0.401</td><td>0.418</td><td>0.570</td><td>0.522</td><td>0.471</td></tr><tr><td>Avg</td><td>0.246</td><td>0.284</td><td>0.260</td><td>0.309</td><td>0.263</td><td>0.301</td><td>0.263</td><td>0.308</td><td>0.269</td><td>0.303</td><td>0.298</td><td>0.318</td><td>0.309</td><td>0.353</td><td>0.310</td><td>0.353</td><td>0.327</td><td>0.328</td><td>0.333</td><td>0.371</td><td>0.305</td><td>0.345</td><td>0.584</td><td>0.527</td><td>0.447</td><td>0.453</td></tr><tr><td rowspan="5">Electricity</td><td>96</td><td>0.185</td><td>0.296</td><td>0.147</td><td>0.242</td><td>0.143</td><td>0.241</td><td>0.150</td><td>0.251</td><td>0.145</td><td>0.244</td><td>0.315</td><td>0.389</td><td>0.235</td><td>0.322</td><td>0.297</td><td>0.367</td><td>0.484</td><td>0.518</td><td>0.697</td><td>0.638</td><td>0.639</td><td>0.609</td><td>1.265</td><td>0.919</td><td>1.414</td><td>0.855</td></tr><tr><td>192</td><td>0.194</td><td>0.302</td><td>0.158</td><td>0.241</td><td>0.159</td><td>0.255</td><td>0.163</td><td>0.263</td><td>0.163</td><td>0.260</td><td>0.318</td><td>0.396</td><td>0.247</td><td>0.341</td><td>0.308</td><td>0.375</td><td>0.501</td><td>0.531</td><td>0.718</td><td>0.648</td><td>0.772</td><td>0.678</td><td>1.298</td><td>0.939</td><td>1.240</td><td>0.919</td></tr><tr><td>336</td><td>0.210</td><td>0.315</td><td>0.178</td><td>0.277</td><td>0.179</td><td>0.274</td><td>0.175</td><td>0.278</td><td>0.183</td><td>0.281</td><td>0.340</td><td>0.415</td><td>0.267</td><td>0.356</td><td>0.354</td><td>0.411</td><td>0.574</td><td>0.578</td><td>0.758</td><td>0.667</td><td>0.901</td><td>0.745</td><td>1.302</td><td>0.942</td><td>1.253</td><td>0.921</td></tr><tr><td>720</td><td>0.251</td><td>0.346</td><td>0.224</td><td>0.312</td><td>0.233</td><td>0.323</td><td>0.219</td><td>0.311</td><td>0.233</td><td>0.323</td><td>0.635</td><td>0.613</td><td>0.318</td><td>0.394</td><td>0.426</td><td>0.466</td><td>0.952</td><td>0.786</td><td>1.028</td><td>0.788</td><td>1.200</td><td>0.871</td><td>1.259</td><td>0.919</td><td>1.249</td><td>0.921</td></tr><tr><td>Avg</td><td>0.218</td><td>0.315</td><td>0.179</td><td>0.268</td><td>0.178</td><td>0.273</td><td>0.176</td><td>0.275</td><td>0.181</td><td>0.277</td><td>0.402</td><td>0.453</td><td>0.266</td><td>0.353</td><td>0.346</td><td>0.404</td><td>0.627</td><td>0.603</td><td>0.800</td><td>0.685</td><td>0.878</td><td>0.725</td><td>1.281</td><td>0.929</td><td>1.289</td><td>0.904</td></tr></table>

Table 13: Full few-shot learning results on $10 \%$ training data.   

<table><tr><td colspan="2">Methods</td><td colspan="2">Time-VLM</td><td colspan="2">Time-LLM</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td><td colspan="2">Stationary</td><td colspan="2">ETSformer</td><td colspan="2">LightTS</td><td colspan="2">Informer</td><td colspan="2">Reformer</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">ETTh1</td><td>96</td><td>0.391</td><td>0.404</td><td>0.448</td><td>0.460</td><td>0.458</td><td>0.456</td><td>0.492</td><td>0.495</td><td>0.516</td><td>0.485</td><td>0.861</td><td>0.628</td><td>0.512</td><td>0.499</td><td>0.613</td><td>0.552</td><td>0.918</td><td>0.639</td><td>1.112</td><td>0.806</td><td>1.298</td><td>0.838</td><td>1.179</td><td>0.792</td><td>1.184</td><td>0.790</td><td></td><td></td></tr><tr><td>192</td><td>0.420</td><td>0.431</td><td>0.484</td><td>0.483</td><td>0.570</td><td>0.516</td><td>0.565</td><td>0.538</td><td>0.598</td><td>0.524</td><td>0.797</td><td>0.593</td><td>0.624</td><td>0.555</td><td>0.722</td><td>0.598</td><td>0.915</td><td>0.629</td><td>1.155</td><td>0.823</td><td>1.322</td><td>0.854</td><td>1.199</td><td>0.806</td><td>1.295</td><td>0.850</td><td></td><td></td></tr><tr><td>336</td><td>0.439</td><td>0.448</td><td>0.589</td><td>0.540</td><td>0.608</td><td>0.535</td><td>0.721</td><td>0.622</td><td>0.657</td><td>0.550</td><td>0.941</td><td>0.648</td><td>0.691</td><td>0.574</td><td>0.750</td><td>0.619</td><td>0.939</td><td>0.644</td><td>1.179</td><td>0.832</td><td>1.347</td><td>0.870</td><td>1.202</td><td>0.811</td><td>1.294</td><td>0.854</td><td></td><td></td></tr><tr><td>720</td><td>0.476</td><td>0.484</td><td>0.700</td><td>0.604</td><td>0.725</td><td>0.591</td><td>0.986</td><td>0.743</td><td>0.762</td><td>0.610</td><td>0.877</td><td>0.641</td><td>0.728</td><td>0.614</td><td>0.721</td><td>0.616</td><td>0.887</td><td>0.645</td><td>1.273</td><td>0.874</td><td>1.534</td><td>0.947</td><td>1.217</td><td>0.825</td><td>1.223</td><td>0.838</td><td></td><td></td></tr><tr><td>Avg</td><td>0.431</td><td>0.442</td><td>0.556</td><td>0.522</td><td>0.590</td><td>0.525</td><td>0.691</td><td>0.600</td><td>0.633</td><td>0.542</td><td>0.869</td><td>0.628</td><td>0.639</td><td>0.561</td><td>0.702</td><td>0.596</td><td>0.915</td><td>0.639</td><td>1.180</td><td>0.834</td><td>1.375</td><td>0.877</td><td>1.199</td><td>0.809</td><td>1.249</td><td>0.833</td><td></td><td></td></tr><tr><td rowspan="5">ETTh2</td><td>96</td><td>0.284</td><td>0.347</td><td>0.275</td><td>0.326</td><td>0.331</td><td>0.374</td><td>0.357</td><td>0.411</td><td>0.353</td><td>0.389</td><td>0.378</td><td>0.409</td><td>0.382</td><td>0.416</td><td>0.413</td><td>0.451</td><td>0.389</td><td>0.411</td><td>0.678</td><td>0.619</td><td>2.022</td><td>1.006</td><td>3.837</td><td>1.508</td><td>3.788</td><td>1.533</td><td></td><td></td></tr><tr><td>192</td><td>0.349</td><td>0.398</td><td>0.374</td><td>0.373</td><td>0.402</td><td>0.411</td><td>0.569</td><td>0.519</td><td>0.403</td><td>0.414</td><td>0.490</td><td>0.467</td><td>0.478</td><td>0.474</td><td>0.477</td><td>0.474</td><td>0.475</td><td>0.455</td><td>0.785</td><td>0.666</td><td>2.329</td><td>1.104</td><td>3.856</td><td>1.513</td><td>3.552</td><td>1.483</td><td></td><td></td></tr><tr><td>336</td><td>0.370</td><td>0.412</td><td>0.406</td><td>0.429</td><td>0.406</td><td>0.433</td><td>0.671</td><td>0.572</td><td>0.426</td><td>0.441</td><td>0.537</td><td>0.494</td><td>0.504</td><td>0.501</td><td>0.547</td><td>0.543</td><td>0.507</td><td>0.480</td><td>0.839</td><td>0.694</td><td>2.453</td><td>1.122</td><td>3.952</td><td>1.526</td><td>3.395</td><td>1.526</td><td></td><td></td></tr><tr><td>720</td><td>0.441</td><td>0.466</td><td>0.427</td><td>0.449</td><td>0.449</td><td>0.464</td><td>0.824</td><td>0.648</td><td>0.477</td><td>0.480</td><td>0.510</td><td>0.491</td><td>0.499</td><td>0.509</td><td>0.516</td><td>0.523</td><td>0.477</td><td>0.472</td><td>1.273</td><td>0.874</td><td>3.816</td><td>1.407</td><td>3.842</td><td>1.503</td><td>3.205</td><td>1.401</td><td></td><td></td></tr><tr><td>Avg</td><td>0.361</td><td>0.405</td><td>0.370</td><td>0.394</td><td>0.397</td><td>0.421</td><td>0.605</td><td>0.538</td><td>0.415</td><td>0.431</td><td>0.479</td><td>0.465</td><td>0.466</td><td>0.475</td><td>0.488</td><td>0.499</td><td>0.462</td><td>0.455</td><td>0.894</td><td>0.713</td><td>2.655</td><td>1.160</td><td>3.872</td><td>1.513</td><td>3.485</td><td>1.486</td><td></td><td></td></tr><tr><td rowspan="5">ETThm1</td><td>96</td><td>0.310</td><td>0.354</td><td>0.346</td><td>0.388</td><td>0.390</td><td>0.404</td><td>0.352</td><td>0.392</td><td>0.410</td><td>0.419</td><td>0.583</td><td>0.501</td><td>0.578</td><td>0.518</td><td>0.774</td><td>0.614</td><td>0.761</td><td>0.568</td><td>0.911</td><td>0.688</td><td>0.921</td><td>0.682</td><td>1.162</td><td>0.785</td><td>1.442</td><td>0.847</td><td></td><td></td></tr><tr><td>192</td><td>0.340</td><td>0.370</td><td>0.373</td><td>0.416</td><td>0.429</td><td>0.423</td><td>0.382</td><td>0.412</td><td>0.437</td><td>0.434</td><td>0.630</td><td>0.528</td><td>0.617</td><td>0.546</td><td>0.754</td><td>0.592</td><td>0.781</td><td>0.574</td><td>0.955</td><td>0.703</td><td>0.957</td><td>0.701</td><td>1.172</td><td>0.793</td><td>1.444</td><td>0.862</td><td></td><td></td></tr><tr><td>336</td><td>0.369</td><td>0.387</td><td>0.413</td><td>0.426</td><td>0.469</td><td>0.439</td><td>0.419</td><td>0.434</td><td>0.476</td><td>0.454</td><td>0.725</td><td>0.568</td><td>0.998</td><td>0.775</td><td>0.869</td><td>0.677</td><td>0.803</td><td>0.587</td><td>0.991</td><td>0.719</td><td>0.998</td><td>0.716</td><td>1.227</td><td>0.908</td><td>1.450</td><td>0.866</td><td></td><td></td></tr><tr><td>720</td><td>0.423</td><td>0.417</td><td>0.485</td><td>0.476</td><td>0.569</td><td>0.498</td><td>0.490</td><td>0.477</td><td>0.681</td><td>0.556</td><td>0.769</td><td>0.549</td><td>0.693</td><td>0.579</td><td>0.810</td><td>0.630</td><td>0.844</td><td>0.581</td><td>1.062</td><td>0.747</td><td>1.007</td><td>0.719</td><td>1.207</td><td>0.797</td><td>1.366</td><td>0.850</td><td></td><td></td></tr><tr><td>Avg</td><td>0.360</td><td>0.382</td><td>0.404</td><td>0.427</td><td>0.464</td><td>0.441</td><td>0.411</td><td>0.429</td><td>0.501</td><td>0.466</td><td>0.677</td><td>0.537</td><td>0.722</td><td>0.605</td><td>0.802</td><td>0.628</td><td>0.797</td><td>0.578</td><td>0.980</td><td>0.714</td><td>0.971</td><td>0.705</td><td>1.192</td><td>0.821</td><td>1.426</td><td>0.856</td><td></td><td></td></tr><tr><td rowspan="5">ETThm2</td><td>96</td><td>0.169</td><td>0.260</td><td>0.177</td><td>0.261</td><td>0.188</td><td>0.269</td><td>0.213</td><td>0.303</td><td>0.191</td><td>0.274</td><td>0.212</td><td>0.285</td><td>0.291</td><td>0.399</td><td>0.352</td><td>0.454</td><td>0.229</td><td>0.308</td><td>0.331</td><td>0.430</td><td>0.813</td><td>0.688</td><td>3.203</td><td>1.407</td><td>4.195</td><td>1.628</td><td></td><td></td></tr><tr><td>192</td><td>0.222</td><td>0.296</td><td>0.241</td><td>0.314</td><td>0.251</td><td>0.309</td><td>0.278</td><td>0.345</td><td>0.252</td><td>0.317</td><td>0.270</td><td>0.323</td><td>0.307</td><td>0.379</td><td>0.694</td><td>0.691</td><td>0.291</td><td>0.343</td><td>0.400</td><td>0.464</td><td>1.008</td><td>0.768</td><td>3.112</td><td>1.387</td><td>4.042</td><td>1.601</td><td></td><td></td></tr><tr><td>336</td><td>0.278</td><td>0.335</td><td>0.274</td><td>0.327</td><td>0.307</td><td>0.346</td><td>0.338</td><td>0.385</td><td>0.306</td><td>0.353</td><td>0.323</td><td>0.353</td><td>0.543</td><td>0.559</td><td>2.408</td><td>1.407</td><td>0.348</td><td>0.376</td><td>0.469</td><td>0.498</td><td>1.031</td><td>0.775</td><td>3.255</td><td>1.421</td><td>3.963</td><td>1.585</td><td></td><td></td></tr><tr><td>720</td><td>0.381</td><td>0.401</td><td>0.417</td><td>0.390</td><td>0.426</td><td>0.417</td><td>0.436</td><td>0.440</td><td>0.433</td><td>0.427</td><td>0.474</td><td>0.449</td><td>0.712</td><td>0.614</td><td>1.913</td><td>1.166</td><td>0.461</td><td>0.438</td><td>0.589</td><td>0.557</td><td>1.096</td><td>0.791</td><td>3.909</td><td>1.543</td><td>3.711</td><td>1.532</td><td></td><td></td></tr><tr><td>Avg</td><td>0.263</td><td>0.323</td><td>0.277</td><td>0.323</td><td>0.293</td><td>0.335</td><td>0.316</td><td>0.368</td><td>0.296</td><td>0.343</td><td>0.320</td><td>0.353</td><td>0.463</td><td>0.488</td><td>1.342</td><td>0.930</td><td>0.332</td><td>0.366</td><td>0.447</td><td>0.487</td><td>0.987</td><td>0.756</td><td>3.370</td><td>1.440</td><td>3.978</td><td>1.587</td><td></td><td></td></tr><tr><td rowspan="5" colspan="2">Weather</td><td>96</td><td>0.174</td><td>0.228</td><td>0.161</td><td>0.210</td><td>0.163</td><td>0.215</td><td>0.171</td><td>0.224</td><td>0.165</td><td>0.215</td><td>0.184</td><td>0.230</td><td>0.188</td><td>0.253</td><td>0.221</td><td>0.297</td><td>0.192</td><td>0.234</td><td>0.199</td><td>0.272</td><td>0.217</td><td>0.269</td><td>0.374</td><td>0.401</td><td>0.335</td><td>0.380</td><td></td></tr><tr><td>192</td><td>0.217</td><td>0.262</td><td>0.204</td><td>0.248</td><td>0.210</td><td>0.254</td><td>0.215</td><td>0.263</td><td>0.210</td><td>0.257</td><td>0.245</td><td>0.283</td><td>0.250</td><td>0.304</td><td>0.270</td><td>0.322</td><td>0.269</td><td>0.295</td><td>0.279</td><td>0.332</td><td>0.259</td><td>0.304</td><td>0.552</td><td>0.478</td><td>0.522</td><td>0.462</td><td></td></tr><tr><td>336</td><td>0.263</td><td>0.296</td><td>0.261</td><td>0.302</td><td>0.256</td><td>0.292</td><td>0.258</td><td>0.299</td><td>0.259</td><td>0.305</td><td>0.321</td><td>0.312</td><td>0.346</td><td>0.320</td><td>0.351</td><td>0.370</td><td>0.357</td><td>0.356</td><td>0.386</td><td>0.303</td><td>0.334</td><td>0.724</td><td>0.541</td><td>0.715</td><td>0.535</td><td></td><td></td></tr><tr><td>720</td><td>0.326</td><td>0.340</td><td>0.309</td><td>0.332</td><td>0.331</td><td>0.329</td><td>0.326</td><td>0.346</td><td>0.332</td><td>0.346</td><td>0.381</td><td>0.371</td><td>0.387</td><td>0.393</td><td>0.390</td><td>0.396</td><td>0.441</td><td>0.405</td><td>0.437</td><td>0.448</td><td>0.377</td><td>0.382</td><td>0.739</td><td>0.558</td><td>0.611</td><td>0.500</td><td></td></tr><tr><td>Avg</td><td>0.245</td><td>0.282</td><td>0.234</td><td>0.273</td><td>0.238</td><td>0.275</td><td>0.241</td><td>0.283</td><td>0.242</td><td>0.279</td><td>0.279</td><td>0.301</td><td>0.284</td><td>0.324</td><td>0.300</td><td>0.342</td><td>0.318</td><td>0.323</td><td>0.318</td><td>0.360</td><td>0.289</td><td>0.322</td><td>0.597</td><td>0.495</td><td></td><td></td><td></td></tr><tr><td rowspan="5" colspan="2">Electricity</td><td>96</td><td>0.160</td><td>0.269</td><td>0.139</td><td>0.241</td><td>0.139</td><td>0.237</td><td>0.150</td><td>0.253</td><td>0.140</td><td>0.238</td><td>0.299</td><td>0.373</td><td>0.231</td><td>0.323</td><td>0.261</td><td>0.348</td><td>0.420</td><td>0.466</td><td>0.599</td><td>0.587</td><td>0.350</td><td>0.425</td><td>1.259</td><td>0.919</td><td>0.993</td><td>0.784</td><td></td></tr><tr><td>192</td><td>0.174</td><td>0.279</td><td>0.151</td><td>0.248</td><td>0.156</td><td>0.252</td><td>0.164</td><td>0.264</td><td>0.160</td><td>0.255</td><td>0.305</td><td>0.379</td><td>0.261</td><td>0.356</td><td>0.338</td><td>0.406</td><td>0.411</td><td>0.459</td><td>0.620</td><td>0.598</td><td>0.376</td><td>0.448</td><td>1.160</td><td>0.873</td><td>0.753</td><td></td><td></td></tr><tr><td>336</td><td>0.190</td><td>0.294</td><td>0.169</td><td>0.270</td><td>0.175</td><td>0.270</td><td>0.181</td><td>0.282</td><td>0.180</td><td>0.276</td><td>0.319</td><td>0.391</td><td>0.465</td><td>0.340</td><td>0.410</td><td>0.474</td><td>0.434</td><td>0.672</td><td>0.469</td><td>0.662</td><td>0.419</td><td>0.428</td><td>1.157</td><td>0.872</td><td>0.925</td><td>0.745</td><td></td></tr><tr><td>720</td><td>0.229</td><td>0.323</td><td>0.240</td><td>0.322</td><td>0.233</td><td>0.317</td><td>0.223</td><td>0.321</td><td>0.241</td><td>0.323</td><td>0.369</td><td>0.426</td><td>0.530</td><td>0.585</td><td>0.715</td><td>0.685</td><td>0.510</td><td>0.521</td><td>0.757</td><td>0.664</td><td>0.611</td><td>0.597</td><td>1.203</td><td>0.898</td><td>1.004</td><td>0.790</td><td></td></tr><tr><td>Avg</td><td>0.198</td><td>0.291</td><td>0.175</td><td>0.270</td><td>0.176</td><td>0.269</td><td>0.180</td><td>0.280</td><td>0.180</td><td>0.273</td><td>0.323</td><td>0.392</td><td>0.346</td><td>0.427</td><td>0.431</td><td>0.478</td><td>0.448</td><td>0.660</td><td>0.617</td><td>0.441</td><td>0.489</td><td>1.195</td><td>0.891</td><td>0.965</td><td>0.768</td><td></td><td></td></tr></table>

# B.2. Zero-shot Forecasting

Table 14: Full zero-shot learning results on ETT datasets. A lower value indicates better performance.   

<table><tr><td colspan="2">Methods</td><td colspan="2">Time-VLM</td><td colspan="2">Time-LLM</td><td colspan="2">LLMTime</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">Autoformer</td><td></td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td></td></tr><tr><td rowspan="5" colspan="2">\(ETTh1\rightarrow ETTh2\)</td><td>96</td><td>0.277</td><td>0.338</td><td>0.279</td><td>0.337</td><td>0.510</td><td>0.576</td><td>0.335</td><td>0.374</td><td>0.347</td><td>0.400</td><td>0.304</td><td>0.350</td><td>0.358</td><td>0.387</td><td>0.469</td><td>0.486</td></tr><tr><td>192</td><td>0.333</td><td>0.378</td><td>0.351</td><td>0.374</td><td>0.523</td><td>0.586</td><td>0.412</td><td>0.417</td><td>0.447</td><td>0.460</td><td>0.386</td><td>0.400</td><td>0.427</td><td>0.429</td><td>0.634</td><td>0.567</td></tr><tr><td>336</td><td>0.360</td><td>0.399</td><td>0.388</td><td>0.415</td><td>0.640</td><td>0.637</td><td>0.441</td><td>0.444</td><td>0.515</td><td>0.505</td><td>0.414</td><td>0.428</td><td>0.449</td><td>0.451</td><td>0.655</td><td>0.588</td></tr><tr><td>720</td><td>0.383</td><td>0.425</td><td>0.391</td><td>0.420</td><td>2.296</td><td>1.034</td><td>0.438</td><td>0.452</td><td>0.665</td><td>0.589</td><td>0.419</td><td>0.443</td><td>0.448</td><td>0.458</td><td>0.570</td><td>0.549</td></tr><tr><td>Avg</td><td>0.338</td><td>0.385</td><td>0.353</td><td>0.387</td><td>0.992</td><td>0.708</td><td>0.406</td><td>0.422</td><td>0.493</td><td>0.488</td><td>0.380</td><td>0.405</td><td>0.421</td><td>0.431</td><td>0.582</td><td>0.548</td></tr><tr><td rowspan="5" colspan="2">\(ETTh1\rightarrow ETTm2\)</td><td>96</td><td>0.207</td><td>0.297</td><td>0.189</td><td>0.293</td><td>0.646</td><td>0.563</td><td>0.236</td><td>0.315</td><td>0.255</td><td>0.357</td><td>0.215</td><td>0.304</td><td>0.239</td><td>0.313</td><td>0.352</td><td>0.432</td></tr><tr><td>192</td><td>0.258</td><td>0.329</td><td>0.237</td><td>0.312</td><td>0.934</td><td>0.654</td><td>0.287</td><td>0.342</td><td>0.338</td><td>0.413</td><td>0.275</td><td>0.339</td><td>0.291</td><td>0.342</td><td>0.413</td><td>0.460</td></tr><tr><td>336</td><td>0.310</td><td>0.360</td><td>0.291</td><td>0.365</td><td>1.157</td><td>0.728</td><td>0.341</td><td>0.374</td><td>0.425</td><td>0.465</td><td>0.334</td><td>0.373</td><td>0.342</td><td>0.371</td><td>0.465</td><td>0.489</td></tr><tr><td>720</td><td>0.398</td><td>0.412</td><td>0.372</td><td>0.390</td><td>4.730</td><td>1.531</td><td>0.435</td><td>0.422</td><td>0.640</td><td>0.573</td><td>0.431</td><td>0.424</td><td>0.434</td><td>0.419</td><td>0.599</td><td>0.551</td></tr><tr><td>Avg</td><td>0.293</td><td>0.350</td><td>0.273</td><td>0.340</td><td>1.867</td><td>0.869</td><td>0.325</td><td>0.363</td><td>0.415</td><td>0.452</td><td>0.314</td><td>0.360</td><td>0.327</td><td>0.361</td><td>0.457</td><td>0.483</td></tr><tr><td rowspan="5" colspan="2">\(ETTh2\rightarrow ETTh1\)</td><td>96</td><td>0.434</td><td>0.441</td><td>0.450</td><td>0.452</td><td>1.130</td><td>0.777</td><td>0.732</td><td>0.577</td><td>0.689</td><td>0.555</td><td>0.485</td><td>0.465</td><td>0.848</td><td>0.601</td><td>0.693</td><td>0.569</td></tr><tr><td>192</td><td>0.464</td><td>0.454</td><td>0.465</td><td>0.461</td><td>1.242</td><td>0.820</td><td>0.758</td><td>0.559</td><td>0.707</td><td>0.568</td><td>0.565</td><td>0.509</td><td>0.860</td><td>0.610</td><td>0.760</td><td>0.601</td></tr><tr><td>336</td><td>0.489</td><td>0.481</td><td>0.501</td><td>0.482</td><td>1.328</td><td>0.864</td><td>0.759</td><td>0.578</td><td>0.710</td><td>0.577</td><td>0.581</td><td>0.515</td><td>0.867</td><td>0.626</td><td>0.781</td><td>0.619</td></tr><tr><td>720</td><td>0.595</td><td>0.543</td><td>0.501</td><td>0.502</td><td>4.145</td><td>1.461</td><td>0.781</td><td>0.597</td><td>0.704</td><td>0.596</td><td>0.628</td><td>0.561</td><td>0.887</td><td>0.648</td><td>0.796</td><td>0.644</td></tr><tr><td>Avg</td><td>0.496</td><td>0.480</td><td>0.479</td><td>0.474</td><td>1.961</td><td>0.981</td><td>0.757</td><td>0.578</td><td>0.703</td><td>0.574</td><td>0.565</td><td>0.513</td><td>0.865</td><td>0.621</td><td>0.757</td><td>0.608</td></tr><tr><td rowspan="5" colspan="2">\(ETTh2\rightarrow ETTm2\)</td><td>96</td><td>0.204</td><td>0.297</td><td>0.174</td><td>0.276</td><td>0.646</td><td>0.563</td><td>0.253</td><td>0.329</td><td>0.240</td><td>0.336</td><td>0.226</td><td>0.309</td><td>0.248</td><td>0.324</td><td>0.263</td><td>0.352</td></tr><tr><td>192</td><td>0.255</td><td>0.328</td><td>0.233</td><td>0.315</td><td>0.934</td><td>0.654</td><td>0.293</td><td>0.346</td><td>0.295</td><td>0.369</td><td>0.289</td><td>0.345</td><td>0.296</td><td>0.352</td><td>0.326</td><td>0.389</td></tr><tr><td>336</td><td>0.311</td><td>0.362</td><td>0.291</td><td>0.337</td><td>1.157</td><td>0.728</td><td>0.347</td><td>0.376</td><td>0.345</td><td>0.397</td><td>0.348</td><td>0.379</td><td>0.353</td><td>0.383</td><td>0.387</td><td>0.426</td></tr><tr><td>720</td><td>0.420</td><td>0.425</td><td>0.392</td><td>0.417</td><td>4.730</td><td>1.531</td><td>0.446</td><td>0.429</td><td>0.432</td><td>0.442</td><td>0.439</td><td>0.427</td><td>0.471</td><td>0.446</td><td>0.487</td><td>0.478</td></tr><tr><td>Avg</td><td>0.297</td><td>0.353</td><td>0.272</td><td>0.341</td><td>1.867</td><td>0.869</td><td>0.335</td><td>0.370</td><td>0.328</td><td>0.386</td><td>0.325</td><td>0.365</td><td>0.342</td><td>0.376</td><td>0.366</td><td>0.411</td></tr><tr><td rowspan="5" colspan="2">\(ETTm1\rightarrow ETTh2\)</td><td>96</td><td>0.297</td><td>0.356</td><td>0.321</td><td>0.369</td><td>0.510</td><td>0.576</td><td>0.353</td><td>0.392</td><td>0.365</td><td>0.415</td><td>0.354</td><td>0.385</td><td>0.377</td><td>0.407</td><td>0.435</td><td>0.470</td></tr><tr><td>192</td><td>0.349</td><td>0.388</td><td>0.389</td><td>0.410</td><td>0.523</td><td>0.586</td><td>0.443</td><td>0.437</td><td>0.454</td><td>0.462</td><td>0.447</td><td>0.434</td><td>0.471</td><td>0.453</td><td>0.495</td><td>0.489</td></tr><tr><td>336</td><td>0.374</td><td>0.409</td><td>0.408</td><td>0.433</td><td>0.640</td><td>0.637</td><td>0.469</td><td>0.461</td><td>0.496</td><td>0.494</td><td>0.481</td><td>0.463</td><td>0.472</td><td>0.484</td><td>0.470</td><td>0.472</td></tr><tr><td>720</td><td>0.396</td><td>0.433</td><td>0.406</td><td>0.436</td><td>2.296</td><td>1.034</td><td>0.466</td><td>0.468</td><td>0.541</td><td>0.529</td><td>0.474</td><td>0.471</td><td>0.495</td><td>0.482</td><td>0.480</td><td>0.485</td></tr><tr><td>Avg</td><td>0.354</td><td>0.397</td><td>0.381</td><td>0.412</td><td>0.992</td><td>0.708</td><td>0.433</td><td>0.439</td><td>0.464</td><td>0.475</td><td>0.439</td><td>0.438</td><td>0.457</td><td>0.454</td><td>0.470</td><td>0.479</td></tr><tr><td rowspan="5" colspan="2">\(ETTm1\rightarrow ETTm2\)</td><td>96</td><td>0.178</td><td>0.264</td><td>0.169</td><td>0.257</td><td>0.646</td><td>0.563</td><td>0.217</td><td>0.294</td><td>0.221</td><td>0.314</td><td>0.195</td><td>0.271</td><td>0.222</td><td>0.295</td><td>0.385</td><td>0.457</td></tr><tr><td>192</td><td>0.226</td><td>0.298</td><td>0.227</td><td>0.318</td><td>0.934</td><td>0.654</td><td>0.277</td><td>0.327</td><td>0.286</td><td>0.359</td><td>0.258</td><td>0.311</td><td>0.288</td><td>0.337</td><td>0.433</td><td>0.469</td></tr><tr><td>336</td><td>0.279</td><td>0.329</td><td>0.290</td><td>0.338</td><td>1.157</td><td>0.728</td><td>0.331</td><td>0.360</td><td>0.357</td><td>0.406</td><td>0.317</td><td>0.348</td><td>0.341</td><td>0.367</td><td>0.476</td><td>0.477</td></tr><tr><td>720</td><td>0.373</td><td>0.385</td><td>0.375</td><td>0.367</td><td>4.730</td><td>1.531</td><td>0.429</td><td>0.413</td><td>0.476</td><td>0.476</td><td>0.416</td><td>0.404</td><td>0.436</td><td>0.418</td><td>0.582</td><td>0.535</td></tr><tr><td>Avg</td><td>0.264</td><td>0.319</td><td>0.268</td><td>0.320</td><td>1.867</td><td>0.869</td><td>0.313</td><td>0.348</td><td>0.335</td><td>0.389</td><td>0.296</td><td>0.334</td><td>0.322</td><td>0.354</td><td>0.469</td><td>0.484</td></tr><tr><td rowspan="4" colspan="2">\(ETTm2\rightarrow ETTh2\)</td><td>96</td><td>0.285</td><td>0.347</td><td>0.298</td><td>0.356</td><td>0.510</td><td>0.576</td><td>0.360</td><td>0.401</td><td>0.333</td><td>0.391</td><td>0.327</td><td>0.367</td><td>0.360</td><td>0.401</td><td>0.353</td><td>0.393</td></tr><tr><td>336</td><td>0.380</td><td>0.415</td><td>0.367</td><td>0.412</td><td>0.640</td><td>0.637</td><td>0.460</td><td>0.459</td><td>0.505</td><td>0.503</td><td>0.439</td><td>0.447</td><td>0.460</td><td>0.459</td><td>0.452</td><td>0.459</td></tr><tr><td>720</td><td>0.424</td><td>0.451</td><td>0.393</td><td>0.434</td><td>2.296</td><td>1.034</td><td>0.485</td><td>0.477</td><td>0.543</td><td>0.534</td><td>0.459</td><td>0.470</td><td>0.485</td><td>0.477</td><td>0.453</td><td>0.467</td></tr><tr><td>Avg</td><td>0.359</td><td>0.399</td><td>0.354</td><td>0.400</td><td>0.992</td><td>0.708</td><td>0.435</td><td>0.443</td><td>0.455</td><td>0.471</td><td>0.409</td><td>0.425</td><td>0.435</td><td>0.443</td><td>0.423</td><td>0.439</td></tr><tr><td rowspan="5" colspan="2">\(ETTm2\rightarrow ETTm1\)</td><td>96</td><td>0.370</td><td>0.390</td><td>0.359</td><td>0.397</td><td>1.179</td><td>0.781</td><td>0.747</td><td>0.558</td><td>0.570</td><td>0.490</td><td>0.491</td><td>0.437</td><td>0.747</td><td>0.558</td><td>0.735</td><td>0.576</td></tr><tr><td>192</td><td>0.400</td><td>0.409</td><td>0.390</td><td>0.420</td><td>1.327</td><td>0.846</td><td>0.781</td><td>0.560</td><td>0.590</td><td>0.506</td><td>0.530</td><td>0.470</td><td>0.781</td><td>0.560</td><td>0.753</td><td>0.586</td></tr><tr><td>336</td><td>0.426</td><td>0.420</td><td>0.421</td><td>0.445</td><td>1.478</td><td>0.902</td><td>0.778</td><td>0.578</td><td>0.706</td><td>0.567</td><td>0.565</td><td>0.497</td><td>0.778</td><td>0.578</td><td>0.750</td><td>0.593</td></tr><tr><td>720</td><td>0.531</td><td>0.487</td><td>0.487</td><td>0.488</td><td>3.749</td><td>1.408</td><td>0.769</td><td>0.573</td><td>0.731</td><td>0.584</td><td>0.686</td><td>0.565</td><td>0.769</td><td>0.573</td><td>0.782</td><td>0.609</td></tr><tr><td>Avg</td><td>0.432</td><td>0.426</td><td>0.414</td><td>0.438</td><td>1.933</td><td>0.984</td><td>0.769</td><td>0.567</td><td>0.649</td><td>0.537</td><td>0.568</td><td>0.492</td><td>0.769</td><td>0.567</td><td>0.755</td><td>0.591</td></tr></table>

# B.3. Short-term Forecasting

Table 15: Full short-term time series forecasting results. The forecasting horizons are in [6, 48] and the last three rows are weighted averaged from all datasets under different sampling intervals. A lower value indicates better performance.   

<table><tr><td colspan="2">Methods</td><td>Time-VLM</td><td>Time-LLM</td><td>GPT4TS</td><td>TimesNet</td><td>PatchTST</td><td>N-HiTS</td><td>N-BEATS</td><td>ETSformer</td><td>LightTS</td><td>DLinear</td><td>FEDformer</td><td>Stationary</td><td>Autoformer</td><td>Informer</td><td>Reformer</td></tr><tr><td rowspan="3">Yearly</td><td>SMAPE</td><td>13.285</td><td>13.419</td><td>15.110</td><td>15.378</td><td>13.477</td><td>13.422</td><td>13.487</td><td>18.009</td><td>14.247</td><td>16.965</td><td>14.021</td><td>13.717</td><td>13.974</td><td>14.727</td><td>16.169</td></tr><tr><td>MASE</td><td>2.993</td><td>3.005</td><td>3.565</td><td>3.554</td><td>3.019</td><td>3.056</td><td>3.036</td><td>4.487</td><td>3.109</td><td>4.283</td><td>3.036</td><td>3.078</td><td>3.134</td><td>3.418</td><td>3.800</td></tr><tr><td>OWA</td><td>0.783</td><td>0.789</td><td>0.911</td><td>0.918</td><td>0.792</td><td>0.795</td><td>0.795</td><td>1.115</td><td>0.827</td><td>1.058</td><td>0.811</td><td>0.807</td><td>0.822</td><td>0.881</td><td>0.973</td></tr><tr><td rowspan="3">Quarterly</td><td>SMAPE</td><td>10.218</td><td>10.110</td><td>10.597</td><td>10.465</td><td>10.380</td><td>10.185</td><td>10.564</td><td>13.376</td><td>11.364</td><td>12.145</td><td>11.100</td><td>10.958</td><td>11.338</td><td>11.360</td><td>13.313</td></tr><tr><td>MASE</td><td>1.203</td><td>1.178</td><td>1.253</td><td>1.227</td><td>1.233</td><td>1.180</td><td>1.252</td><td>1.906</td><td>1.328</td><td>1.520</td><td>1.350</td><td>1.325</td><td>1.365</td><td>1.401</td><td>1.775</td></tr><tr><td>OWA</td><td>0.903</td><td>0.889</td><td>0.938</td><td>0.923</td><td>0.921</td><td>0.893</td><td>0.936</td><td>1.302</td><td>1.000</td><td>1.106</td><td>0.996</td><td>0.981</td><td>1.012</td><td>1.027</td><td>1.252</td></tr><tr><td rowspan="3">Monthly</td><td>SMAPE</td><td>12.788</td><td>12.980</td><td>13.258</td><td>13.513</td><td>12.959</td><td>13.059</td><td>13.089</td><td>14.588</td><td>14.014</td><td>13.514</td><td>14.403</td><td>13.917</td><td>13.958</td><td>14.062</td><td>20.128</td></tr><tr><td>MASE</td><td>0.942</td><td>0.963</td><td>1.003</td><td>1.039</td><td>0.970</td><td>1.013</td><td>0.996</td><td>1.368</td><td>1.053</td><td>1.037</td><td>1.147</td><td>1.097</td><td>1.103</td><td>1.141</td><td>2.614</td></tr><tr><td>OWA</td><td>0.886</td><td>0.903</td><td>0.931</td><td>0.957</td><td>0.905</td><td>0.929</td><td>0.922</td><td>1.149</td><td>0.981</td><td>0.956</td><td>1.038</td><td>0.998</td><td>1.002</td><td>1.024</td><td>1.927</td></tr><tr><td rowspan="3">Others</td><td>SMAPE</td><td>4.945</td><td>4.795</td><td>6.124</td><td>6.913</td><td>4.952</td><td>4.711</td><td>6.599</td><td>7.267</td><td>15.880</td><td>6.709</td><td>7.148</td><td>6.302</td><td>5.485</td><td>24.460</td><td>32.491</td></tr><tr><td>MASE</td><td>3.257</td><td>3.178</td><td>4.116</td><td>4.507</td><td>3.347</td><td>3.054</td><td>4.430</td><td>5.240</td><td>11.434</td><td>4.953</td><td>4.041</td><td>4.064</td><td>3.865</td><td>20.960</td><td>33.355</td></tr><tr><td>OWA</td><td>1.034</td><td>1.006</td><td>1.259</td><td>1.438</td><td>1.049</td><td>0.977</td><td>1.393</td><td>1.591</td><td>3.474</td><td>1.487</td><td>1.389</td><td>1.304</td><td>1.187</td><td>5.879</td><td>8.679</td></tr><tr><td rowspan="3">Average</td><td>SMAPE</td><td>11.894</td><td>11.983</td><td>12.690</td><td>12.880</td><td>12.059</td><td>12.035</td><td>12.250</td><td>14.718</td><td>13.525</td><td>13.639</td><td>13.160</td><td>12.780</td><td>12.909</td><td>14.086</td><td>18.200</td></tr><tr><td>MASE</td><td>1.592</td><td>1.595</td><td>1.808</td><td>1.836</td><td>1.623</td><td>1.625</td><td>1.698</td><td>2.408</td><td>2.111</td><td>2.095</td><td>1.775</td><td>1.756</td><td>1.771</td><td>2.718</td><td>4.223</td></tr><tr><td>OWA</td><td>0.855</td><td>0.859</td><td>0.940</td><td>0.955</td><td>0.869</td><td>0.869</td><td>0.896</td><td>1.172</td><td>1.051</td><td>1.051</td><td>0.949</td><td>0.930</td><td>0.939</td><td>1.230</td><td>1.775</td></tr></table>

# B.4. Long-term Forecasting

Table 16: Full long-term forecasting results. We use the same protocol as in Table 1.   

<table><tr><td colspan="2">Methods</td><td colspan="2">Time-VLM</td><td colspan="2">Time-LLM</td><td colspan="2">GPT4TS</td><td colspan="2">DLinear</td><td colspan="2">PatchTST</td><td colspan="2">TimesNet</td><td colspan="2">FEDformer</td><td colspan="2">Autoformer</td><td colspan="2">Stationary</td><td colspan="2">ETSformer</td><td colspan="2">LightTS</td><td colspan="2">Informer</td><td colspan="2">Reformer</td></tr><tr><td colspan="2">Metric</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td><td>MSE</td><td>MAE</td></tr><tr><td rowspan="5">ETTh1</td><td>96</td><td>0.361</td><td>0.386</td><td>0.362</td><td>0.392</td><td>0.376</td><td>0.397</td><td>0.375</td><td>0.399</td><td>0.370</td><td>0.399</td><td>0.384</td><td>0.402</td><td>0.376</td><td>0.419</td><td>0.449</td><td>0.459</td><td>0.513</td><td>0.491</td><td>0.494</td><td>0.479</td><td>0.424</td><td>0.432</td><td>0.865</td><td>0.713</td><td>0.837</td><td>0.728</td></tr><tr><td>192</td><td>0.397</td><td>0.415</td><td>0.398</td><td>0.418</td><td>0.416</td><td>0.418</td><td>0.405</td><td>0.416</td><td>0.413</td><td>0.421</td><td>0.436</td><td>0.429</td><td>0.420</td><td>0.448</td><td>0.500</td><td>0.482</td><td>0.534</td><td>0.504</td><td>0.538</td><td>0.504</td><td>0.475</td><td>0.462</td><td>1.008</td><td>0.792</td><td>0.923</td><td>0.766</td></tr><tr><td>336</td><td>0.420</td><td>0.421</td><td>0.430</td><td>0.437</td><td>0.442</td><td>0.433</td><td>0.439</td><td>0.443</td><td>0.422</td><td>0.436</td><td>0.491</td><td>0.469</td><td>0.459</td><td>0.465</td><td>0.521</td><td>0.496</td><td>0.588</td><td>0.535</td><td>0.574</td><td>0.521</td><td>0.518</td><td>0.488</td><td>1.107</td><td>0.809</td><td>1.097</td><td>0.835</td></tr><tr><td>720</td><td>0.441</td><td>0.458</td><td>0.442</td><td>0.457</td><td>0.477</td><td>0.456</td><td>0.472</td><td>0.490</td><td>0.447</td><td>0.466</td><td>0.521</td><td>0.500</td><td>0.506</td><td>0.507</td><td>0.514</td><td>0.512</td><td>0.643</td><td>0.616</td><td>0.562</td><td>0.535</td><td>0.547</td><td>0.533</td><td>1.181</td><td>0.865</td><td>1.257</td><td>0.889</td></tr><tr><td>Avg</td><td>0.405</td><td>0.420</td><td>0.408</td><td>0.423</td><td>0.465</td><td>0.455</td><td>0.422</td><td>0.437</td><td>0.413</td><td>0.430</td><td>0.458</td><td>0.450</td><td>0.440</td><td>0.460</td><td>0.496</td><td>0.487</td><td>0.570</td><td>0.537</td><td>0.542</td><td>0.510</td><td>0.491</td><td>0.479</td><td>1.040</td><td>0.795</td><td>1.029</td><td>0.805</td></tr><tr><td rowspan="5">ETTh2</td><td>96</td><td>0.267</td><td>0.335</td><td>0.268</td><td>0.328</td><td>0.285</td><td>0.342</td><td>0.289</td><td>0.353</td><td>0.274</td><td>0.336</td><td>0.340</td><td>0.374</td><td>0.358</td><td>0.397</td><td>0.346</td><td>0.388</td><td>0.476</td><td>0.458</td><td>0.340</td><td>0.391</td><td>0.397</td><td>0.437</td><td>3.755</td><td>1.525</td><td>2.626</td><td>1.317</td></tr><tr><td>192</td><td>0.326</td><td>0.373</td><td>0.329</td><td>0.375</td><td>0.354</td><td>0.389</td><td>0.383</td><td>0.418</td><td>0.339</td><td>0.379</td><td>0.402</td><td>0.414</td><td>0.429</td><td>0.439</td><td>0.456</td><td>0.452</td><td>0.512</td><td>0.493</td><td>0.430</td><td>0.439</td><td>0.520</td><td>0.504</td><td>5.602</td><td>1.931</td><td>11.120</td><td>2.979</td></tr><tr><td>336</td><td>0.357</td><td>0.406</td><td>0.368</td><td>0.409</td><td>0.373</td><td>0.407</td><td>0.448</td><td>0.465</td><td>0.329</td><td>0.380</td><td>0.452</td><td>0.452</td><td>0.496</td><td>0.487</td><td>0.482</td><td>0.486</td><td>0.552</td><td>0.551</td><td>0.485</td><td>0.479</td><td>0.626</td><td>0.559</td><td>4.721</td><td>1.835</td><td>9.323</td><td>2.769</td></tr><tr><td>720</td><td>0.412</td><td>0.449</td><td>0.372</td><td>0.420</td><td>0.406</td><td>0.441</td><td>0.605</td><td>0.551</td><td>0.379</td><td>0.422</td><td>0.462</td><td>0.468</td><td>0.463</td><td>0.474</td><td>0.515</td><td>0.511</td><td>0.562</td><td>0.560</td><td>0.500</td><td>0.497</td><td>0.863</td><td>0.672</td><td>3.647</td><td>1.625</td><td>3.874</td><td>1.697</td></tr><tr><td>Avg</td><td>0.341</td><td>0.391</td><td>0.334</td><td>0.383</td><td>0.381</td><td>0.412</td><td>0.431</td><td>0.446</td><td>0.330</td><td>0.379</td><td>0.414</td><td>0.427</td><td>0.437</td><td>0.449</td><td>0.450</td><td>0.459</td><td>0.526</td><td>0.516</td><td>0.439</td><td>0.452</td><td>0.602</td><td>0.543</td><td>4.431</td><td>1.729</td><td>6.736</td><td>2.191</td></tr><tr><td rowspan="5">ETThm1</td><td>96</td><td>0.304</td><td>0.346</td><td>0.272</td><td>0.334</td><td>0.292</td><td>0.346</td><td>0.299</td><td>0.343</td><td>0.290</td><td>0.342</td><td>0.338</td><td>0.375</td><td>0.379</td><td>0.419</td><td>0.505</td><td>0.475</td><td>0.386</td><td>0.398</td><td>0.375</td><td>0.398</td><td>0.374</td><td>0.400</td><td>0.672</td><td>0.571</td><td>0.538</td><td>0.528</td></tr><tr><td>192</td><td>0.332</td><td>0.366</td><td>0.310</td><td>0.358</td><td>0.332</td><td>0.372</td><td>0.335</td><td>0.365</td><td>0.332</td><td>0.369</td><td>0.374</td><td>0.387</td><td>0.426</td><td>0.441</td><td>0.553</td><td>0.496</td><td>0.459</td><td>0.444</td><td>0.408</td><td>0.410</td><td>0.400</td><td>0.407</td><td>0.795</td><td>0.669</td><td>0.658</td><td>0.592</td></tr><tr><td>336</td><td>0.364</td><td>0.383</td><td>0.352</td><td>0.384</td><td>0.366</td><td>0.394</td><td>0.369</td><td>0.386</td><td>0.366</td><td>0.392</td><td>0.410</td><td>0.411</td><td>0.445</td><td>0.459</td><td>0.621</td><td>0.537</td><td>0.495</td><td>0.464</td><td>0.435</td><td>0.428</td><td>0.438</td><td>1.212</td><td>0.871</td><td>0.898</td><td>0.721</td><td></td></tr><tr><td>720</td><td>0.402</td><td>0.410</td><td>0.383</td><td>0.411</td><td>0.417</td><td>0.421</td><td>0.425</td><td>0.421</td><td>0.416</td><td>0.420</td><td>0.478</td><td>0.450</td><td>0.543</td><td>0.490</td><td>0.671</td><td>0.561</td><td>0.585</td><td>0.516</td><td>0.459</td><td>0.492</td><td>0.462</td><td>0.527</td><td>0.502</td><td>1.166</td><td>0.823</td><td>1.102</td></tr><tr><td>Avg</td><td>0.350</td><td>0.377</td><td>0.329</td><td>0.372</td><td>0.388</td><td>0.403</td><td>0.357</td><td>0.378</td><td>0.351</td><td>0.380</td><td>0.400</td><td>0.406</td><td>0.448</td><td>0.452</td><td>0.588</td><td>0.517</td><td>0.481</td><td>0.456</td><td>0.429</td><td>0.425</td><td>0.435</td><td>0.437</td><td>0.961</td><td>0.734</td><td>0.799</td><td>0.671</td></tr><tr><td rowspan="5">ETThm2</td><td>96</td><td>0.160</td><td>0.250</td><td>0.161</td><td>0.253</td><td>0.173</td><td>0.262</td><td>0.167</td><td>0.269</td><td>0.165</td><td>0.255</td><td>0.187</td><td>0.267</td><td>0.203</td><td>0.287</td><td>0.255</td><td>0.339</td><td>0.192</td><td>0.274</td><td>0.189</td><td>0.280</td><td>0.209</td><td>0.308</td><td>0.365</td><td>0.453</td><td>0.658</td><td>0.619</td></tr><tr><td>192</td><td>0.215</td><td>0.291</td><td>0.219</td><td>0.293</td><td>0.229</td><td>0.301</td><td>0.224</td><td>0.303</td><td>0.220</td><td>0.292</td><td>0.249</td><td>0.309</td><td>0.269</td><td>0.328</td><td>0.281</td><td>0.340</td><td>0.280</td><td>0.339</td><td>0.253</td><td>0.319</td><td>0.311</td><td>0.382</td><td>0.533</td><td>0.563</td><td>1.078</td><td>0.827</td></tr><tr><td>336</td><td>0.270</td><td>0.325</td><td>0.271</td><td>0.329</td><td>0.286</td><td>0.341</td><td>0.281</td><td>0.342</td><td>0.274</td><td>0.329</td><td>0.321</td><td>0.351</td><td>0.325</td><td>0.366</td><td>0.339</td><td>0.372</td><td>0.334</td><td>0.361</td><td>0.314</td><td>0.357</td><td>0.442</td><td>0.466</td><td>1.363</td><td>0.887</td><td>1.549</td><td></td></tr><tr><td>720</td><td>0.348</td><td>0.378</td><td>0.352</td><td>0.379</td><td>0.378</td><td>0.401</td><td>0.397</td><td>0.421</td><td>0.362</td><td>0.385</td><td>0.408</td><td>0.403</td><td>0.421</td><td>0.415</td><td>0.433</td><td>0.432</td><td>0.417</td><td>0.413</td><td>0.414</td><td>0.413</td><td>0.675</td><td>0.587</td><td>3.379</td><td>1.338</td><td>2.631</td><td></td></tr><tr><td>Avg</td><td>0.248</td><td>0.311</td><td>0.251</td><td>0.313</td><td>0.284</td><td>0.339</td><td>0.267</td><td>0.333</td><td>0.255</td><td>0.315</td><td>0.291</td><td>0.333</td><td>0.305</td><td>0.349</td><td>0.327</td><td>0.371</td><td>0.306</td><td>0.347</td><td>0.293</td><td>0.342</td><td>0.409</td><td>0.436</td><td>1.410</td><td>0.810</td><td>1.479</td><td></td></tr><tr><td rowspan="5">Weather</td><td>96</td><td>0.148</td><td>0.200</td><td>0.147</td><td>0.201</td><td>0.162</td><td>0.212</td><td>0.176</td><td>0.237</td><td>0.149</td><td>0.198</td><td>0.172</td><td>0.220</td><td>0.217</td><td>0.296</td><td>0.266</td><td>0.336</td><td>0.173</td><td>0.223</td><td>0.197</td><td>0.281</td><td>0.182</td><td>0.242</td><td>0.300</td><td>0.384</td><td>0.689</td><td>0.596</td></tr><tr><td>192</td><td>0.193</td><td>0.240</td><td>0.189</td><td>0.234</td><td>0.204</td><td>0.248</td><td>0.220</td><td>0.282</td><td>0.194</td><td>0.241</td><td>0.219</td><td>0.261</td><td>0.276</td><td>0.336</td><td>0.307</td><td>0.367</td><td>0.245</td><td>0.285</td><td>0.237</td><td>0.312</td><td>0.227</td><td>0.287</td><td>0.598</td><td>0.544</td><td>0.752</td><td>0.638</td></tr><tr><td>336</td><td>0.243</td><td>0.281</td><td>0.262</td><td>0.279</td><td>0.254</td><td>0.286</td><td>0.265</td><td>0.319</td><td>0.245</td><td>0.282</td><td>0.280</td><td>0.306</td><td>0.339</td><td>0.380</td><td>0.359</td><td>0.395</td><td>0.321</td><td>0.338</td><td>0.298</td><td>0.353</td><td>0.282</td><td>0.334</td><td>0.578</td><td>0.823</td><td>0.639</td><td></td></tr><tr><td>720</td><td>0.312</td><td>0.332</td><td>0.304</td><td>0.316</td><td>0.326</td><td>0.337</td><td>0.332</td><td>0.362</td><td>0.314</td><td>0.334</td><td>0.365</td><td>0.359</td><td>0.403</td><td>0.428</td><td>0.419</td><td>0.428</td><td>0.414</td><td>0.410</td><td>0.352</td><td>0.288</td><td>0.352</td><td>0.386</td><td>1.059</td><td>0.741</td><td>1.130</td><td></td></tr><tr><td>Avg</td><td>0.224</td><td>0.263</td><td>0.225</td><td>0.257</td><td>0.237</td><td>0.270</td><td>0.248</td><td>0.300</td><td>0.225</td><td>0.264</td><td>0.259</td><td>0.287</td><td>0.309</td><td>0.360</td><td>0.338</td><td>0.382</td><td>0.288</td><td>0.314</td><td>0.271</td><td>0.334</td><td>0.261</td><td>0.312</td><td>0.634</td><td>0.548</td><td>0.803</td><td></td></tr><tr><td rowspan="5">Electricity</td><td>96</td><td>0.142</td><td>0.245</td><td>0.131</td><td>0.224</td><td>0.139</td><td>0.238</td><td>0.140</td><td>0.237</td><td>0.129</td><td>0.222</td><td>0.168</td><td>0.272</td><td>0.193</td><td>0.308</td><td>0.201</td><td>0.317</td><td>0.169</td><td>0.273</td><td>0.187</td><td>0.304</td><td>0.207</td><td>0.307</td><td>0.274</td><td>0.368</td><td>0.312</td><td></td></tr><tr><td>192</td><td>0.157</td><td>0.260</td><td>0.152</td><td>0.241</td><td>0.153</td><td>0.251</td><td>0.153</td><td>0.249</td><td>0.157</td><td>0.240</td><td>0.184</td><td>0.289</td><td>0.201</td><td>0.315</td><td>0.222</td><td>0.334</td><td>0.182</td><td>0.286</td><td>0.199</td><td>0.315</td><td>0.213</td><td>0.316</td><td>0.296</td><td>0.386</td><td>0.348</td><td></td></tr><tr><td>336</td><td>0.174</td><td>0.276</td><td>0.160</td><td>0.248</td><td>0.169</td><td>0.266</td><td>0.169</td><td>0.267</td><td>0.163</td><td>0.259</td><td>0.198</td><td>0.300</td><td>0.214</td><td>0.329</td><td>0.231</td><td>0.338</td><td>0.200</td><td>0.304</td><td>0.212</td><td>0.329</td><td>0.230</td><td>0.333</td><td>0.300</td><td>0.394</td><td></td><td></td></tr><tr><td>720</td><td>0.214</td><td>0.308</td><td>0.192</td><td>0.298</td><td>0.206</td><td>0.297</td><td>0.203</td><td>0.301</td><td>0.197</td><td>0.290</td><td>0.220</td><td>0.320</td><td>0.246</td><td>0.355</td><td>0.254</td><td>0.361</td><td>0.222</td><td>0.321</td><td>0.233</td><td>0.345</td><td>0.265</td><td>0.360</td><td>0.373</td><td>0.439</td><td></td><td></td></tr><tr><td>Avg</td><td>0.172</td><td>0.273</td><td>0.158</td><td>0.252</td><td>0.167</td><td>0.263</td><td>0.166</td><td>0.263</td><td>0.161</td><td>0.252</td><td>0.192</td><td>0.295</td><td>0.214</td><td>0.327</td><td>0.227</td><td>0.338</td><td>0.193</td><td>0.296</td><td>0.208</td><td>0.323</td><td>0.229</td><td>0.329</td><td>0.311</td><td>0.397</td><td></td><td></td></tr></table>

# C. Visualizations

# C.1. Visualization of Generated Time Series Images

The image generation module employs advanced techniques—frequency and periodicity Encoding, multi-scale convolution, interpolation and normalization—to create informative and discriminative image representations of time series data. These representations enhance downstream VLMs for improved forecasting. As shown in Figure 6, the generated images capture key temporal characteristics through the following features:

![](images/88ac6a88c1b9f7571cc0303f48ddc5d06285a5ca1c1aba01fd615aac8b582707.jpg)

![](images/cac35786099b64d2af4a378a74f486ad721cf666507e73f94f5daadf59ef5bdd.jpg)

![](images/b503ae1e2889eb673f07c4595d0505fabe93c2780011549ae30c1c5eb6ea13ee.jpg)

![](images/9d9a845e05795a15d63deeec6cedb8a4346915bcbecb7882a00a2101c821a7e0.jpg)

![](images/4870c4e2e4619f53729531d00ad13ba498cea48f8b4959dea10682c342ee6fd3.jpg)

![](images/ac464b29ac2824428271232e2b03e94770cfa245b2979f3a9d4df1e8ee3ac665.jpg)

![](images/c7dd89f1a704927ebe1a0d1181d8cceac163572da9f5f48efb575436ad22e543.jpg)

![](images/c530b465ae406486474ddaa1fe6859e9a0b807c778b69227312ec9793065e084.jpg)

![](images/bd1c76134e6638186f87ee93c7fa61890253d13f077d2b80c1daa63e08a4093f.jpg)

![](images/7f3099606df5965e181949570b020f2b9f50bebae23118727930d1ed18a83901.jpg)

![](images/abca592efca84df1177a66a08e7930b3d584d68a01a7d4823bb4b12f2ea0c3db.jpg)

![](images/7855385deb1b2835bb9b656fc9560a600bd9fbd0781ddb9d2254db5fdacc971d.jpg)  
Figure 6: Time series transformed images, capturing key temporal characteristics, including trends, stationarity, seasonality, sudden changes, and frequency-domain patterns.

• Frequency-Domain Information: FFT integration captures frequency-domain characteristics, visualized as distinct textures—fine-grained for high-frequency components and broader color regions for low-frequency components.   
• Multi-scale Periodic Encoding: Temporal dependencies at multiple scales (e.g., daily, weekly) are encoded, visible as regular patterns such as repeating vertical bands for daily cycles or broader horizontal patterns for weekly cycles.   
• Image Interpolation: Bilinear interpolation ensures smooth and coherent images, preserving essential time series characteristics through seamless transitions between color intensities.   
• Color Trends: Color intensity corresponds to time series values—darker regions (e.g., deep blue) indicate lower values, while brighter regions (e.g., yellow) represent higher values, enabling easy identification of trends.   
• Abrupt Changes and Anomalies: Sudden shifts in color intensity (e.g., sharp transitions from dark to bright) highlight abrupt changes or anomalies, crucial for identifying irregular events like traffic spikes or weather shifts.

# C.2. Visualization of prediction results

The prediction results in Figures 7, 8, 9, and 10 demonstrate Time-VLM’s ability to accurately forecast time series across diverse datasets and prediction horizons. For datasets with clear periodic structures, such as the daily cycles in ETTh1 and ETTm1, Time-VLM captures both global trends and fine-grained temporal patterns effectively. This is evident in the close alignment between the true values (solid lines) and predicted values (dashed lines) across all horizons. Similarly, for the ECL dataset, which exhibits regular consumption patterns, Time-VLM delivers highly accurate forecasts, showcasing its strength in handling structured environments.

![](images/ed9d38e3107a8097ccf15853b9133143b067c929308a49fa6b8668f06eedec42.jpg)  
a) ETTh1

![](images/6750ea9ddc6e4052d2dd9105e55d1e72dcb9f75e8589d296586036f81a67867c.jpg)  
b) ETTm1

![](images/6bd2a2844e3033eca953d4eb77b3ef7019e1f2983a075f29be67590f9ea10e74.jpg)  
c) ECL

![](images/11743a2123ecf624e0fcb80e9b7966c09649e740a99671ac25744e5b20144595.jpg)  
c) Traffic   
a) ETTh1      Figure 7: Prediction results visualization for ETTh1, ETTm1, ECL, and Traffic datasets at 96 prediction lengths. True values (solid line) and predicted values (dashed line) are shown for each dataset and horizon.

![](images/18c8034ffd0522ca9605e78bc0ad2e275d372b09570770a7d8e9e5e1ea2b2e72.jpg)  
a) ETTh1

![](images/3cac5f8056c76c92c8a4e5601c353e1194446a209b919430a4f47b7070eca43a.jpg)  
b) ETTm1

![](images/4a40670ce529e72cd54280caf4ca9359fb96ac65f94bd28fade205bbc2fcac41.jpg)  
c) ECL

![](images/b1d663ec247516f66a8a9462b163eda0f48902dbed9dc49c6f8027d2d323fbab.jpg)  
c) Traffic   
a) ETTh1      Figure 8: Prediction results visualization for ETTh1, ETTm1, ECL, and Traffic datasets at 192 prediction lengths. True values (solid line) and predicted values (dashed line) are shown for each dataset and horizon.

a) ETTh1 b) ETTm1 c) ECL c) TrafficHowever, performance varies for datasets with irregular or abrupt changes. On the Traffic dataset, which is characterized by non-stationary patterns, Time-VLM shows slight deviations in capturing sudden fluctuations, particularly at longer horizons (e.g., 336 and 720). These deviations highlight the challenges of modeling highly irregular data and suggest opportunities for refining the time series-to-image transformation process to better handle such scenarios.

![](images/f83dac82936cf3b8121ae54f653069d69adab551e92593d5625c256e1551f3b7.jpg)  
a) ETTh1

![](images/792d47fbf862f5ff885733f0cf26de767808da439ae4eb60aa7299a2afca6ee8.jpg)  
b) ETTm1

![](images/db8140b63f9f7b2e73237c4838091a7d370d10d2fde3e4bb61960d7a302c960d.jpg)  
c) ECL

![](images/9ecb91ef96a9e0f85a09904e3d941a8fb11c5bc7a1337d668af413e5ddf6bec8.jpg)  
c) Traffic   
a) ETTh1      Figure 9: Prediction results visualization for ETTh1, ETTm1, ECL, and Traffic datasets at 336 prediction lengths. True values (solid line) and predicted values (dashed line) are shown for each dataset and horizon.

![](images/2ff4c9a363af19e256fa002322b7439b9bf126a2e3a90878f04e4d3386a5871a.jpg)  
a) ETTh1

![](images/2367070a8fb7f0ea73c08e2987da6621445ccac8cfbc292f352a0d9c583405b2.jpg)  
b) ETTm1

![](images/f06691d8c6c495198f59fdc8d1315586ea220e32b06fbd0653085e5c30cd78b6.jpg)  
c) ECL

![](images/6d5e9d728ed5f3b3a848dad3269d4c9353e0aa41e625b526d34c7c1c4c9e6ab1.jpg)  
c) Traffic   
Figure 10: Prediction results visualization for ETTh1, ETTm1, ECL, and Traffic datasets at 720 prediction lengths. True values (solid line) and predicted values (dashed line) are shown for each dataset and horizon.

# D. Future Work

# D.1. Limitations

While Time-VLM demonstrates significant improvements in time series forecasting by integrating temporal, visual, and textual modalities, it has some limitations.

First, the framework performs less robustly on datasets with highly volatile or irregular patterns, such as those with sudden changes or non-stationary trends, compared to datasets with periodic structures. This limitation may arise from the current visual transformation techniques, which may not adequately capture abrupt temporal dynamics or sudden shifts. Future work could refine these transformations to better handle such irregularities.

Second, the current implementation relies on pre-trained VLMs like ViLT and CLIP, which are optimized for natural vision-language tasks rather than time series forecasting. While these models excel in visual understanding, their textual capabilities are limited, often supporting only shorter text inputs and lacking domain-specific knowledge relevant to time series. This restricts their ability to fully utilize textual context for forecasting. Future work could involve developing larger, domain-specific VLMs trained on multimodal time series datasets to address these limitations.

# D.2. Future Work

Building on the current framework, several promising directions for future research emerge:

• Optimizing Visual Transformations: Future work could focus on developing adaptive visual transformation techniques that better preserve temporal dynamics, especially for datasets with irregular or non-stationary patterns, to more effectively highlight sudden changes and complex trends.   
• Scaling Multimodal VLMs for Enhanced Forecasting: While the current framework uses smaller pre-trained Vision-Language Models (VLMs), scaling to larger models could improve forecasting accuracy. Investigating trade-offs between model size, computational efficiency, and performance is a promising direction for future research. Additionally, studying different VLM architectures could identify optimal designs for temporal modeling.   
• Interpretable Multimodal Learning for Time Series Analysis: Understanding the contributions of visual and textual modalities in time series forecasting is crucial for improving model transparency. Future work could explore the

interpretability of multimodal features, analyzing how different types of information contribute to performance gains. This would provide deeper insights into temporal dependencies and enhance trust in multimodal forecasting models.

• Pre-training Multimodal Foundation Models for Time Series Analysis: Existing VLMs are not designed to handle time series data, limiting their ability to capture domain-specific temporal context. Future research could focus on constructing large-scale multimodal datasets that pair time series data with rich textual and visual annotations, enabling the development of models specifically optimized for time series forecasting. Additionally, this multimodal framework could be extended to support multi-task learning, enhancing the model’s versatility for tasks such as anomaly detection, classification, or imputation. This would allow the model to capture a broader range of temporal patterns and dependencies, improving its applicability across various domains.

By addressing these directions, future research can build on the foundation laid by Time-VLM, advancing the field of multimodal time series forecasting while ensuring responsible and ethical deployment in real-world applications.