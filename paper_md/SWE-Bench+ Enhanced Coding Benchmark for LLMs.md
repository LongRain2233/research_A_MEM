# SWE-Bench+: Enhanced Coding Benchmark for LLMs

Reem Aleithan, Haoran Xue, Mohammad Mahdi Mohajer, Elijah Nnorom, Gias Uddin, Song Wang

Lassonde School of Engineering

York University

{reem1100,hrx00,mmm98,ennorom,guddin,wangsong}@yorku.ca

# Abstract

Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where $S W E  – A g e n t + G P T – 4$ successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) $3 2 . 6 7 \%$ of the successful patches involve “cheating” as the solutions were directly provided in the issue report or the comments. We refer to as ‘solution leakage’ problem. 2) $3 1 . 0 8 \%$ of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 drops from $1 2 . 4 7 \%$ to $3 . 9 7 \%$ . We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over $94 \%$ of the issues were created before LLM’s knowledge cutoff dates, posing potential data leakage issues.

The critical problem in the current versions of SWE-bench dataset motivated us to refine it to build a more rigorous evaluation dataset SWE-Bench+. We created SWE-bench+ by collecting GitHub issues that were created after the training cutoff dates of the LLMs to prevent the potential data leakage problem. We also ensure that the issues collected do not contain solutions in their reports or comments. After carefully analyzing the passed instances from the SWE-Agent $+ \ G P T { - } 4$ model with the new dataset, SWE-Bench+, we observed a decline in the pass rate, dropping from $3 . 9 7 \%$ (as seen on the refined SWE-Bench) to a resolution rate of $0 . 5 5 \%$ . We further evaluated SWE-RAG + GPT-4, SWE-RAG + GPT-3.5, and AutoCodeRover $+ \ G P T – 4 o$ models on the new dataset to verify our findings, where the resolution rates of the models drop significantly, which are $0 . 7 3 \%$ , $0 . 5 5 \%$ , and $3 . 8 3 \%$ , respectively.

# 1 Introduction

SWE-bench aka Software Engineering Benchmark dataset is created to systematically evaluate the capabilities of an LLM in resolving software issues. The dataset contains 2,294 complex issues from GitHub Jimenez et al. (2024). Given as input the issue information to an LLM, the task for the LLM is to modify the code base to address the issue (i.e., resolution). Each input for an issue consists of a description and a pull request with a reference to the corresponding buggy code repository. Each issue can be either a bug report or a new feature request. The pull request contains the code changes made by developers to address the issue, along with test cases designed to check if the feature is properly implemented or if the bug is successfully fixed. Two variants of the SWE-bench datasets are recently developed: SWE-bench Lite1 and SWE-bench Verified2. SWE-bench Lite focuses on 300 issues related to bug fixing. SWE-bench Verified contains 500 verified issues with clear issue descriptions and strong test cases.

A significant body of work from both academia and industry has so far utilized SWE-bench and its variants to develop and to test LLM coding capabilities Chen et al. (2024); Zhang et al. (2024a); Xia et al. (2024); Yang et al. (2024b); Zhang et al. (2024c); Rosa et al. (2024); Zan et al. (2024). Given an issue and its associated buggy code repository, these LLM-based approaches can perform a series of complex tasks, such as reasoning about the target bug’s location, analyzing the root cause of the issue, proposing strategies for fixing the bug, and ultimately writing a patch to fix the issue. Within less than one year, the resolution rate on SWE-bench Full increased from $0 . 1 7 \%$ (for RAG+GPT3.5) to around $2 2 . 0 0 \%$ (for Honeycomb). The performance of the LLMs on SWE-bench Lite and Verified went up to $45 \%$ .

# However, are the LLMs actually resolving the issues in SWE-bench?

In this paper, we answer the above question by offering two contributions. First, we present an empirical study of state-of-the-art (SOTA) LLMs on SWE-bench Full that explores 1) the quality of SWE-bench issues with a focus on the testing adequacy of the test cases used for validating patches and 2) the quality of patches generated by the LLMs to fix the issues. Second, we present an enhancement of SWE-bench Full, which we call SWE-bench+.

During our empirical study, SWE-Agent+GPT-4 was at the top of the SWE-bench online leaderboard. Other top approaches (e.g., Honeycomb, Amazon Q Developer Agent, and Factory Code Droid) were either closed-sourced commercial tools or not verified by the SWE-bench team regarding reproducibility. SWE-agent Yang et al. (2024b) allows LLM agents to execute basic file operations via shell commands to achieve interaction between the LLM engine and a software repository. First, we picked issues claimed as resolved by SWE-Agent+GPT-4. We did this by filtering only the instances with evaluation logs showing that all tests passed. Second, we performed a patch validation study by comparing the gold patches (i.e., original) to the model patches (i.e., generated). We did this by comparing the files changed, the lines changed, and the code changes made in the fixes (both original and generated). Third, we determined eight patterns in the generated fixes by reviewing the issue reports, the corresponding tests, and the available discussions of issues (which are treated as hints to LLMs).

Our identified six patterns in the 251 SWE-Agent+GPT-4 patches can be broadly divided into two types: suspicious fixes and correct fixes. Suspicious fixes corresponded to $6 3 . 7 5 \%$ (i.e., 160) of the patches. Two patterns were prevalent in those fixes: 1) Answer Leak. In $3 2 . 6 7 \%$ of the resolved instances, the solutions were outlined directly in the issue reports or comments. 2) Weak Tests. In $3 1 . 0 8 \%$ of the resolved instances, the changes made by the model are either incorrect, incomplete, or applied to different files or functions compared to the gold patch. Despite these discrepancies, the changes pass the tests, indicating that the tests are too weak to catch such errors. In addition, the dataset can also suffer from potential data leak issues. This is because $94 \%$ of the instances in SWE-bench and their pull requests were created prior to the training cut-off dates of the LLMs, meaning that all issue reports in the full SWE-bench dataset may have been exposed to the LLMs during their training phases, raising concerns about potential data leakage.

Based on the above observation, we considered fixes corresponding to issues with answer leakage and weak tests as ‘suspicious fixes’. In Figure 1a, we show the distribution of such fixes in the three

![](images/075a14278bc3ff00ef20a957abb237230cae0678f6062957a6668a21c7c369b6.jpg)

![](images/f72750f52e91cda3af792c8a98805716c03a5842f989f6b24d8579e80f53108a.jpg)  
Figure 1: Comparison of performance metrics and patterns across SWE-bench datasets

![](images/08ce934b83f2ff192a09a7aa1ed4c1a60de4ad26c5c1539b0b0e2c061c26538e.jpg)  
Figure 2: Overview of robustness analysis for SWE-Bench datasets

literature datasets (i.e., SWE-bench full, lite, verified). In Figure 1b, we show that after filtering these suspicious fixes, the correct resolution rate of SWE-Agent+GPT-4 dropped to $3 . 9 7 \%$ from $1 2 . 4 7 \%$ . This drastic drop in resolution rate raises concerns about the robustness of the model-generated patches and the reliability of the SWE-bench dataset itself.

To address the problems in SWE-bnech datasets, we created SWE-bench+ dataset, which ensures that: 1) the data were created after the models’ training cut-off dates, and 2) the issues do not include solutions in the issue description or comments. SWE-bench+ dataset is created by following the same data collection methodology described in the SWE-Bench dataset; except we filtered out issues with answer leakage problems. Considering the training cut-off dates of the LLMs used in our study—GPT-3.5 (turbo-16k-0613) with a cut-off in September 2021, GPT-4 (1106) up to April 2023, and GPT-4o (2024-05-13) up to October 2023, we opted to collect data starting a month after the most recent model’s cut-off date. To the end, we gathered issues from the period of 2023-11-01 to 2024-08-22. As we show in Figure 1a, SWE-bench+ has no issues with solution leakage whereas all the other three datasets suffer from this. SWE-bench $^ +$ also has the lowest proportion of issues with weak test cases among all the SWE-bench variants. As such, we consider SWE-bench+ as the most robust dataset among the available SWE-bench variants. When we ran SWE-Agent+GPT-4 on SWE-bench+ dataset, its resolution rate dropped to $0 . 5 5 \%$ (see Figure 1).

We further evaluated the $S W E  – R A G + G P T – 4$ , SWE-RAG + GPT-3.5, and AutoCodeRover + GPT-4o models on the new dataset to verify our findings. The resolution rates of the models dropped significantly, with the new rates being $0 . 7 3 \%$ , $0 . 5 5 \%$ , and $3 . 8 3 \%$ , respectively. In comparison, the previously reported resolution rates on the SWE-Bench leaderboard were $1 . 3 1 \%$ for $S W E  – R A G +$ GPT-4, $0 . 1 7 \%$ for $S W E  – R A G + G P T – 3 . 5$ , and $1 8 . 8 3 \%$ for AutoCodeRover + GPT-4o.

Artifacts. While working on merging our SWE-bench $^ +$ to SWE-bench project repository, we release the dataset of SWE-bench+ to help other researchers replicate and extend our study3.

Table 1: Patterns found among the 251 successful patches generated by SWE-Agent + GPT-4   

<table><tr><td>Type</td><td>Pattern</td><td>Numbers (percentage)</td><td>Root cause</td></tr><tr><td rowspan="4">Suspicious fixes</td><td>Solution leak</td><td>82 (32.67%)</td><td>solution leakage</td></tr><tr><td>Incorrect fixes</td><td>32 (12.75%)</td><td>weak tests</td></tr><tr><td>Different files/functions changed</td><td>9 (3.59%)</td><td>weak tests</td></tr><tr><td>Incomplete fixes</td><td>37 (14.74%)</td><td>weak tests</td></tr><tr><td rowspan="2">Correct fixes</td><td>Different fixes from gold patches</td><td>76 (30.27%)</td><td>-</td></tr><tr><td>More comprehensive fixes than gold patches</td><td>15 (5.98%)</td><td>-</td></tr></table>

# 2 Robustness Analysis of SWE-Bench

We conducted an empirical study of SWE-Agent+GPT-4 generated patches for issues in the SWEbench Full dataset. The goal of our study was to identify whether the patches exhibit any potential problems. Figure 2 outlines the major steps we followed in our study. The input is the set of all issues in SWE-bench. Each issue contains a description and the patch to address the issue. Each patch is a diff of code changes. We call this a “gold patch”. We picked $S W E  – A g e n t + G P T – 4$ and applied it on each issue to create a fix. We refer to the model output for an issue as a “generated patch”. We then compared the gold and generated patches to an issue by analyzing the corresponding files changed in the pull requests on GitHub with the same instance_id. As we studied the model generated patches, we also examined the logs and trajectories generated by the model. Logs provide the step-by-step execution of the models. The trajectory data provide a detailed record of the models’ decision-making processes while making a resolution as a patch.

To reduce potential biases during the comparison between gold and generated patches, three authors independently performed the patch validation study. Each author carefully examined the files and lines changed, reviewed the issue descriptions, and evaluated the implementation styles and intentions behind both the model-generated and developer-generated patches. The disagreements were resolved through a broader discussion involving all the authors.

For the model generated patches, we focused on instances where the generated patches resolved the issue and passed all associated tests. As a result, we identified 251 instances from the SWE-Bench Full dataset. Note that, to ensure the patches passed all tests, we reviewed the evaluation logs of 286 instances initially marked as resolved by SWE-Agent + GPT-4 in the results.json file from the SWE-Bench Full evaluation repository and selected only those with logs confirming that all tests passed following the application of the generated patches.

# 2.1 Critical issues of SWE-Bench

Among the 251 generated patches that passed all test cases in SWE-bench Full dataset, we found several patches as problematic/suspicious. Table 1 outlines six patterns in the 251 generated patches, four related to the suspicious fixes and two related to the correct fixes. To explain each pattern, we provide definitions, including the number of instances associated with each pattern and the likely root causes. We discuss each pattern below.

# 2.1.1 Patterns extracted from the suspicious fixes

We observed four patterns in the suspicious fixes, one is attributed to solution leakage and the other three are attributed to weak test case problems in the SWE-bench dataset.

1. Solution leak: represents instances where the solution to the issue is clearly outlined in the issue description or comments on GitHub. Since both the issue descriptions and comments (referred to as hints_text in the SWE-Bench study) are provided as input to the models, these LLM models can extract the solutions directly from this information instead of generating it independently. $3 2 . 6 7 \%$ of the successfully resolved issues followed this pattern, making it the most common among resolved patches. This raises significant concerns about a model’s actual performance and the validity of the SWE-Bench instances as benchmarks. If a model is simply copying the solution it already has access

![](images/0d4043e7e4bd044169ebf4e73ebe7073e711f98d060882c6dbb415160e8d6ad8.jpg)  
Figure 3: Solution Leakage in issue report for sympy-16669

![](images/095b3eb9f4a9067854dd5a7d25772106e9f0337b2547dfce1fdde71c0bb5bf3c.jpg)  
Figure 4: Incorrect fix generated by the model for django-32517

to, it isn’t demonstrating true problem-solving capabilities but rather replicating what is provided, thus limiting the assessment of its ability to generate new solutions. The example shown in Figure 3 illustrates issue report $1 6 6 6 9 ^ { 4 }$ from the sympy project, where the issue description provided the exact solution code patch required to resolve the issue, which makes it possible for the model to directly copy the solution from the issue report and generate the same solution as provided.

2. Incorrect fixes: refer to cases where the model-generated patches provide incorrect solutions, yet pass the test cases when they should have failed. This pattern was present $1 2 . 7 5 \%$ of the passed instances. Suggesting a weakness in test cases where the functionality of the issue resolution is not correctly captured. The fact that incorrect patches can pass the test cases raises suspicion about the relevance and accuracy of the test cases in assessing whether the issue has been fully resolved. Figure 4 shows a comparison between the model-generated patch and the gold patch for django- $3 2 \bar { 5 } 1 7 ^ { 5 }$ . According to the issue description, a new functionality is needed to reverse a Python OrderedSet by implementing the __reversed__ function. The gold patch demonstrates the correct behavior, where the entire dictionary is reversed, while the generated patch only reverses the dictionary’s keys. As a result, the two patches produce entirely different outputs, as they apply different methods to the dictionary.   
3. Different files/functions changed: This pattern refers to cases where the model-generated patches modify files or functions unrelated to the issue at hand. These files differ from those altered in the gold patch, yet the model’s patches still pass the test cases despite this discrepancy. This highlights a weakness in the model’s ability to accurately locate and address the source of the issue. The fact that the test cases pass, even though changes were made in irrelevant files, suggests that the test cases are either weak or irrelevant and should have failed in detecting the incorrect modifications. Figure 5 presents an example from issue-26093 of Matplotlib project6, where the model-generated patch modifies the cbook.py file, while the gold patch makes changes to the _axes.py file. This shows that the model’s patch affects a completely different file from the gold patch, highlighting the model’s inability to accurately identify the correct file containing the bug.

![](images/49971e2d3d80d8d73285245cc76b60a3862a1e5643945c63fb952052e880aa7f.jpg)

![](images/6e5ab24f4f46c74efd20902b84c247e10ec5971d1a180e60c414cbb4779d6ccb.jpg)  
Figure 5: Different files changed by model for issue-26093 of Matplotlib

![](images/780c9b8313468f4059f23afb4ed6661b721dd8a0fa897b613d25a353a3a0d08a.jpg)

![](images/2a0306cb1f219812058dff01a6b8980007dc3df09477fcca8482958487a269cb.jpg)  
Figure 6: Incomplete fix generated by the model for django-31056

4. Incomplete fixes: This pattern refers to model-generated patches that offer incomplete implementations compared to the gold patches, often omitting critical details. For instance, some patches include only partial if-else statements, neglecting edge cases that the gold patch addresses. Although the model-generated patches follow the correct implementation approach, they overlook important aspects that could lead to failures in production or when handling edge cases. This underscores a weakness in the test cases, as they fail to capture the finer details necessary for a comprehensive issue resolution.

The example provided in Figure 6 shows the same change being made by the model and the one made by the developers in the gold patch7. The gold patch provides a complete fix while the model patch provides a partial fix. Specifically, the gold patch properly handles the detection of an event loop in the current thread by including a try-except block to catch RuntimeError when an event loop is unavailable and checks if the event loop is running before raising an exception. Additionally, it wraps the entire logic in a condition that checks the environment variable DJANGO_ALLOW_ASYNC_UNSAFE. In contrast, the generated patch is missing critical parts of this logic, such as the try-except block and the check for a running event loop. As a result, the model-generated patch is incomplete, missing key error handling and flow control that are necessary for ensuring safe operation.

# 2.1.2 Patterns extracted from the correct fixes

1. Different fixes from gold patches: This pattern refers to cases where the model-generated patches present an entirely different solution to the issue compared to the gold patch. Although the coding style and implementation differ, the model-generated patches correctly resolve the issue. Instances in this pattern are considered a correct resolution of the issues.

Table 2: Patterns found among SWE-Bench Lite (total passed: 54, $1 8 . 0 \%$ ) and SWE-Bench Verified (total passed: 112, $2 2 . 4 \%$ ) datasets with successful patches generated by SWE-Agent $^ +$ GPT-4.   

<table><tr><td>Pattern</td><td>SWE-Bench Lite</td><td>Percentage (Lite)</td><td>SWE-Bench Verified</td><td>Percentage (Verified)</td></tr><tr><td>Incorrect fixes</td><td>5</td><td>9.26%</td><td>14</td><td>12.50%</td></tr><tr><td>Incomplete fixes</td><td>3</td><td>5.56%</td><td>11</td><td>9.82%</td></tr><tr><td>Different Files/Functions Changed</td><td>0</td><td>0.00%</td><td>0</td><td>0.00%</td></tr><tr><td>Solution Leak</td><td>18</td><td>33.33%</td><td>37</td><td>33.04%</td></tr></table>

2. More comprehensive fixes: In contrast to the previous pattern, this refers to instances where the model-generated patches are more comprehensive than the gold patches. For example, the modelgenerated patches may include additional if-else cases or other logic that the gold patch does not address. This showcases a strength of LLMs, as they can generate more thorough solutions that cover scenarios developers might overlook, resulting in potentially safer and more robust solutions (e.g., by incorporating try-catch statements or handling edge cases more effectively). We consider this pattern as a correct resolution of the issue.

# 2.2 Updated Resolution Rate of SWE-Agent $^ +$ GPT-4 on SWE-Bench Full

After identifying these patterns, we recalculated the resolution rate of $S W E  – A g e n t + G P T – 4$ , focusing exclusively on patches classified under the correct fixes patterns. This new resolution criteria resulted in a significant drop in the resolution percentage, as shown in Figure 1b. The performance of SWE-Agent+GPT-4 decreased from $1 2 . 4 7 \%$ to $5 . 4 9 \%$ when considering only correct fixes—those where the model generated either correct but different implementations from the gold patch $( 3 0 . 2 7 \%$ of the time) or more comprehensive fixes than the gold patch $5 . 9 8 \%$ of the time). We excluded all suspicious fixes, which included instances where the model generated incorrect solutions $( 1 2 . 7 5 \% )$ , where the issue report contained a direct solution $( 3 2 . 6 7 \% )$ , where changes were made in files unrelated to the gold patch $( 3 . 5 9 \% )$ , and cases where the model produced incomplete fixes, missing critical details of the solution $( 1 4 . 7 4 \% )$ .

# 2.3 Updated Resolution Rate of SWE-Agent+GPT-4 on SWE-bench Lite and Verified

Two new variants of SWE-Bench are recently developed, SWE-Bench Lite and SWE-Bench Verified, each designed with different goals. SWE-Bench Lite focuses on instances with lower evaluation costs and increased accessibility. On the other hand, SWE-Bench Verified aims to provide a curated subset of SWE-Bench, where human annotators filter out issues with underspecified descriptions or weak unit tests that might reject valid solutions. However, neither of these new datasets addresses the solution leakage problem, which was the primary motivation for our study. To investigate this, three of the authors reviewed all issue reports for the instances in SWE-Bench Lite and SWE-Bench Verified for cases of solution leakage. In SWE-Bench Lite, we identified 18 instances where the solution was directly provided in the issue description or discussion on GitHub. Similarly, in SWE-Bench Verified, 37 instances contained direct solutions in either the issue description or the discussion on GitHub. Table 2 shows more details.

We also observed additional suspicious fixes in both SWE-Bench Lite and SWE-Bench Verified, as summarized in Table 2. Specifically, $9 . 2 6 \%$ of fixes in SWE-Bench Lite and $1 2 . 5 0 \%$ in SWE-Bench Verified were incorrect fixes generated by SWE-Agent+GPT-4. Additionally, $5 . 5 6 \%$ of fixes in SWE-Bench Lite and $9 . 8 2 \%$ in SWE-Bench Verified were incomplete fixes from the same model. Notably, there were no issues involving changes to different functions or files, as the model correctly identified the buggy file in all cases for both datasets. Despite this, the overall suspicious fix patterns led to $4 8 . 1 4 \%$ suspicious fixes in SWE-Bench Lite and $5 5 . 3 6 \%$ in SWE-Bench Verified, significantly reducing the resolution rates—from $18 \%$ to $9 . 3 3 \%$ in SWE-Bench Lite and from $2 2 . 4 \%$ to $1 0 . 0 \%$ in SWE-Bench Verified.

![](images/02eb1a0d5b3a8249bb5520399f120bc1d8473629be8236e78fdc5cde080d5194.jpg)  
Figure 7: SWE-Bench $^ +$ dataset compared to SWE-Bench

# 3 Building SWE-Bench+

To address the issues of the current SWE-Bench datasets and ensure a more accurate evaluation of the models’ effectiveness in resolving issues, we utilized a new dataset, SWE-Bench+, which focuses on issues with no clear solution provided in the issue report and without potential risk of data leakage. The primary objective of SWE-Bench+ is to assess the models’ ability to generate accurate patches for real-world GitHub issues without the risk of bias or prior exposure to the solution. To maintain consistency and fairness when comparing the resolution rates of the models onSWE-Bench and SWE-Bench+, we followed the same data collection methodology outlined in the SWE-Bench study, using their open-source scripts.

First, we selected the same 12 projects from SWE-Bench, except Django, which was excluded as its issues are now tracked outside of GitHub. Given that the LLMs we used in the models (GPT-4, GPT-3.5, and GPT-4o) were all trained on data up to October 2023, we collected issues that appeared after October 2023. Once the issues were collected, we applied the same filtering process described in the SWE-Bench study. This included using attribute filtering to retain only issues that resolve a problem and contribute tests, followed by an execution filter to keep only issues that install successfully and their PRs pass all tests. We then manually check all the instances to eliminate all the instances with a clear solution details in the issue report. By following this method, we obtained 548 task instances (issues) from the selected projects, with the distribution shown in Figure 7.

# 4 Robustness of SWE-Bench+

After collecting the data and identifying the correct repository versions, the next step was to run the selected models to generate patches that would address the issues. The models chosen for this task were SWE-RAG with GPT-3.5 (turbo-16k-0613), SWE-RAG with GPT-4 (1106), SWE-Agent with GPT-4 (1106), and AutoCodeRover (v20240620) paired with GPT-4o (2024-05-13). These models were selected based on their performance, large context window, cost, and the fact that they are open-source, making them more favorable compared to other options.

At the beginning of the project, SWE-Agent had the highest resolution rate on the SWE-Bench leaderboard among all open-source models. Since we studied this model thoroughly in relation to SWE-Bench, we selected it to ensure a fair comparison. $S W E { \cdot } R A G { + } G P T { \cdot } 4$ and $S W E - R A G + G P T -$ 3.5 were also chosen for the study due to their higher token limits compared to the Claude models. Initial trials with SWE-Agent+Claude 3 Opus and RAG-based Claude models (SWE-RAG+Claude 3 Opus and SWE-RAG+Claude 2) revealed that the token limits were exceeded before completing experiments on all 548 SWE-Bench $^ +$ instances. As the leaderboard frequently updates, with new models surpassing older ones, AutoCodeRover+GPT-4o recently ranked among the top three models, and since it is open-source, we also included it in the study. We following their instructions to run the models on our SWE-Bench+ dataset.

Our evaluation followed a four-step technique, described in Figure 2, to ensure that the resolution rates of the models on SWE-Bench+ accurately reflected their correctness. Specifically, in Step 1, we stored all the model-generated diff files as patches in separate json files, we then utilized the

Table 3: Performance of different models on SWE-bench+   

<table><tr><td>Pattern</td><td>SWE-RAG+GPT-4</td><td>SWE-RAG+GPT-3.5</td><td>SWE-Agent+GPT-4</td><td>AutoCodeRover+GPT-4o</td></tr><tr><td>Correct</td><td>4</td><td>3</td><td>3</td><td>21</td></tr><tr><td>(Suspicious) Different files/functions changed</td><td>16</td><td>11</td><td>14</td><td>7</td></tr><tr><td>(Suspicious) Incorrect fixes</td><td>2</td><td>6</td><td>3</td><td>15</td></tr><tr><td>(Suspicious) Incomplete fixes</td><td>1</td><td>0</td><td>0</td><td>3</td></tr><tr><td>Total</td><td>23</td><td>20</td><td>32</td><td>42</td></tr></table>

Table 4: Average cost of different models on SWE-Bench+   

<table><tr><td>Model</td><td>Avg cost per instance</td><td>cost per issue fixing</td><td>Avg time per instance</td></tr><tr><td>RAG+GPT 4</td><td>$0.24</td><td>$32.5</td><td>30 seconds</td></tr><tr><td>RAG+GPT 3.5</td><td>$0.05</td><td>$10.0</td><td>30 seconds</td></tr><tr><td>SWE(agent+GPT 4</td><td>$3.59</td><td>$655.0</td><td>4 minutes</td></tr><tr><td>AutoCode Rover+GPT 4o</td><td>$0.46</td><td>$12.61</td><td>4.5 minutes</td></tr></table>

scripts provided by the SWE-Bench to evaluate the generated patches. In Step 2, once the evaluation results were generated, we manually reviewed the instances marked as resolved and selected only those where all tests in PASS_TO_PASS and FAIL_TO_PASS had passed. In other words, we filtered out instances where the patches were successfully applied, and all associated tests passed. In Step 3, after filtering the resolved instances where all tests passed, we analyzed the resolution rates of the models. Finally, in Step 4, we performed a similar patch validation study as described in Section 2, comparing the model-generated patches to the gold patches. We found that solution-leak related issues are now resolved. However, a prominent issue with weak test cases persists. As shown in Table 3, on average, around $6 7 . 7 2 \%$ of the resolved instances did not truly resolve the issue, despite passing all the tests. The prominent pattern identified was the models’ inability to accurately locate the buggy files or lines. This raises concerns about the models’ ability to locate the bug. In other cases, the model generated incomplete or incorrect fixes, not resolving the bug. These findings show that the issue with weak tests still persists and needs future investigation.

As a result of the patch validation analysis, the resolution rates were $0 . 7 3 \%$ for SWE-$R A G + G P T { - } 4$ , $0 . 5 5 \%$ for $S W E  – R A G + G P T  – 3 . S$ , $0 . 5 5 \%$ for $S W E  – A g e n t + G P T – 4$ , and $3 . 8 3 \%$ for AutoCodeRover+GPT-4o. These rates are significantly lower than the reported resolution rates on the SWE-Bench leaderboard were $1 . 3 1 \%$ for $S W E { \cdot } R A G + G P T { \cdot } 4 , 0 . 1 7 \% 1$ $0 . 1 7 \%$ for $S W E  – R A G + G P T – 3 . 5 .$ , and $1 8 . 8 3 \%$ for AutoCodeRover $^ +$ GPT-4o. This drop in performance for all models when moving from SWE-Bench Full to SWE-Bench+, highlights the impact of excluding suspicious fixes on the overall resolution rates.

# 5 Effectiveness-aware Evaluation

Despite the notable success in resolving issues, we also observed significant variations in the costs of the approaches tested. While some models excelled in accuracy and efficiency, they required more computational resources, longer processing times, and higher costs. Specifically, SWE-Agent+GPT-4 and AutoCodeRover+GPT-4 had the longest code generation times, with SWE-Agent+GPT-4 averaging around 4 minutes per instance, resulting in a total of approximately 37 hours to generate patches for SWE-Bench $^ +$ issues. The average generation time for AutoCodeRover was 4.5 minutes per instance, resulting in 41 hours in total to generate patches for all SWE-Bench+ instances, making it a top performer in issue resolution for SWE-Bench+ instances. However, this disparity highlights the trade-offs between performance and cost-effectiveness, particularly for models like SWE-Agent+, where balancing time, cost, and resource allocation is critical for real-world applications.

In terms of cost, we identified two metrics, i.e., the average cost per instance (calculated by dividing the total cost by the 548 instances in SWE-Bench+ that the models were tested on) and the effectiveness-aware cost per instance (calculated by dividing the total cost by the number of instances successfully resolved by the model). The detailed cost of each model measured by the two metrics is shown in Table 4. Overall, SWE-Agent+GPT-4 was the most expensive model, with

an average cost of $\$ 0.24$ per instance and an effectiveness-aware cost of $\$ 32.5$ per issue fixed, as shown in Table 4. Despite its high cost, its performance was comparable to $R A G + G P T { - } 4$ , which was much more cost-efficient, with an average cost of $\$ 0.05$ per instance and an effectiveness-aware cost of $\$ 10.0$ . On the other hand, AutoCodeRover+GPT-4 delivered the highest resolution rate of $3 . 8 3 \%$ among all models. Although relatively costly on average, especially for larger datasets with an average cost of $\$ 0.46$ per instance, AutoCodeRover’s effectiveness-aware cost was relatively low at $\$ 12.61$ per issue fixed, given its high-resolution rate compared to the SWE-Agent approach. Meanwhile, $R A G \mathrm { + } G P T \mathrm { - } 3 . 5$ had the lowest average cost, at $\$ 0.05$ per instance, but a relatively high effectiveness-aware cost of $\$ 32$ due to its poorer performance in resolving issues correctly.

We recommend that future research not only assess the accuracy of the proposed models but also take into account the financial cost associated with their implementation and operation, ensuring they are not only performant but also practical for large-scale and long-term use.

# 6 Related Work

LLM for Software Engineering. Large Language Models (LLMs) have emerged as powerful tools and demonstrated impressive capabilities in various software engineering tasks, including code generation Jiang et al. (2024); Li & Döhmen (2024); Chen et al. (2021); Luo et al. (2024); Du et al. (2024), program repair Zhang et al. (2024b); Yang et al. (2024a); de Fitero-Dominguez et al. (2024) and bug detection Alrashedy & Binjahlan (2024); Hossain et al. (2024). The development of code generation benchmarks has been crucial for evaluating LLM performance. Notably, HumanEval Chen et al. (2021) was introduced to assess the functional correctness of code generated by LLMs. Building on this foundation, AlphaCode Li et al. (2022) demonstrated competitive performance in solving complex programming problems. To address limitations in existing benchmarks, EvalPlus Liu et al. (2024) enhanced HumanEval with more comprehensive test cases and revealed a significant overestimation of LLM performance in previous evaluations. LLMs also have shown promising results in program repair and bug detection. For example, AlphaRepair Xia & Zhang (2022) employed a zero-shot learning approach that outperformed traditional automated program repair (APR) tools. Further research demonstrated that LLMs could surpass existing APR techniques, particularly when fine-tuned on domain-specific data Xia et al. (2023). The application of LLMs in bug detection with innovative approaches like FUzzGPT Deng et al. (2023b) and TitanFuzz Deng et al. (2023a) leveraging these models to generate edge-case test inputs and perform mutation-based fuzzing for deep learning libraries. There are several comprehensive studies have explored LLM applications across various software engineering domains Fan et al. (2023); Hou et al. (2024), delved into the natural language to code generation Zan et al. (2023), and analyzed the evolution and performance of Code LLMs across different tasks Zheng et al. (2024).

Benchmark Dataset Quality for Code Generation. To achieve accurate and reliable outcomes in code generation tasks with LLMs, it is essential to use high-quality evaluation benchmark datasets during the training and evaluation steps Shi et al. (2022); Jimenez et al. (2024). With the growing interest and significance of code generation and program repair in software engineering, this highlights the critical need for trustworthy and reliable evaluation benchmark datasets Jimenez et al. (2024); Zan et al. (2024). To tackle this challenge, SWE-bench was developed to assist developers in evaluating LLMs using real-world GitHub issues Jimenez et al. (2024). A critical part of making a reliable dataset is to make it representative of real-world and complex software challenges and issues Chen et al. (2024); Tao et al. (2024). The “Diversity Empowers Intelligence” (DEI) framework Zhang et al. (2024a) further shows how diversity and comprehensiveness of data can enhance the performance of Large Language Models (LLMs) in code generation tasks. Given the critical need for high-quality benchmark datasets in evaluating code generation models, Liu et al. Liu et al. (2024) introduce EvalPlus, a framework specifically designed to improve the evaluation of LLMgenerated code. This framework addresses challenges like insufficient tests and noisy benchmarks by augmenting existing datasets with automated test input generation using LLM and mutationbased strategies. Their findings show the need for more rigorous testing in LLM code generation to improve benchmark quality in the field.

# 7 Conclusion

In this paper, we presented the first empirical study on the robustness of the SWE-Bench dataset. Our study identified significant limitations in the original SWE-Bench dataset, particularly issues with solution leakage and weak test cases, which undermined the reliability of previous model assessments. To address these challenges, we introduced SWE-Bench+, a dataset free from solution leakage and built with issues created after LLM training cut-off dates to ensure more rigorous and accurate evaluations. Through extensive testing, we demonstrated that while SWE-Bench+ resolves the data leakage concerns, weak test cases continue to pose challenges, with model resolution rates dropping further in this refined environment. Despite the reduced pass rates, SWE-Bench+ establishes a more reliable framework for assessing the true capabilities of LLMs in software development, offering insights into how these models can be better developed and evaluated. Future work should focus on improving the test case robustness in SWE-Bench+ and exploring more effective strategies for filtering data to further minimize biases and inaccuracies.

For future work, further investigation into the issue of weak test cases is needed, along with suggestions for improving test quality to create more accurate and relevant test suites. Another potential avenue of research could explore the underlying causes of the high failure rates and propose strategies to mitigate them. Additionally, similar studies could be conducted on other leading evaluation benchmarks, such as Human-Eval, to compare results and identify broader patterns.

# References

Kamel Alrashedy and Ahmed Binjahlan. Language models are better bug detector through code-pair classification, 2024. URL https://arxiv.org/abs/2311.07957.   
Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, Jie Wang, Xiao Cheng, Guangtai Liang, Yuchi Ma, Pan Bian, Tao Xie, and Qianxiang Wang. Coder: Issue resolving with multi-agent and task graphs, 2024. URL https://arxiv.org/abs/2406.01304.   
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374.   
David de Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, and Jose-Javier Martinez-Herraiz. Enhanced automated code vulnerability repair using large language models. Engineering Applications of Artificial Intelligence, 138:109291, 2024. ISSN 0952-1976. doi: https://doi.org/10.1016/j.engappai.2024.109291. URL https://www.sciencedirect.com/science/article/pii/S0952197624014490.   
Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming Zhang. Large language models are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models, 2023a. URL https://arxiv.org/abs/2212.14834.   
Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, and Lingming Zhang. Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt, 2023b. URL https://arxiv.org/abs/2304.02014.   
Xueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng, Chaofeng Sha, Xin Peng, and Yiling Lou. Evaluating large language models in class-level code generation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE ’24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400702174. doi: 10.1145/3597503.3639219. URL https://doi.org/10.1145/3597503.3639219.   
Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M. Zhang. Large language models for software engineering: Survey and open problems, 2023. URL https://arxiv.org/abs/2310.03533.   
Soneya Binta Hossain, Nan Jiang, Qiang Zhou, Xiaopeng Li, Wen-Hao Chiang, Yingjun Lyu, Hoan Nguyen, and Omer Tripp. A deep dive into large language models for automated bug localization and repair. Proc. ACM Softw. Eng., 1(FSE), July 2024. doi: 10.1145/3660773. URL https://doi.org/10.1145/3660773.   
Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. Large language models for software engineering: A systematic literature review, 2024. URL https://arxiv.org/abs/2308.10620.   
Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. A survey on large language models for code generation, 2024. URL https://arxiv.org/abs/2406.00515.   
Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770.

Xue Li and Till Döhmen. Towards efficient data wrangling with llms using code generation. In Proceedings of the Eighth Workshop on Data Management for End-to-End Machine Learning, DEEM ’24, pp. 62–66, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400706110. doi: 10.1145/3650203.3663334. URL https://doi.org/10.1145/3650203.3663334.   
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode. Science, 378(6624):1092–1097, December 2022. ISSN 1095-9203. doi: 10.1126/science.abq1158. URL http://dx.doi.org/10.1126/science.abq1158.   
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024.   
Hanbin Luo, Jianxin Wu, Jiajing Liu, and Maxwell Fordjour Antwi-Afari. Large language model-based code generation for the control of construction assembly robots: A hierarchical generation approach. Developments in the Built Environment, 19:100488, 2024. ISSN 2666-1659. doi: https://doi.org/10.1016/j.dibe.2024.100488. URL https://www.sciencedirect.com/science/article/pii/S2666165924001698.   
Ricardo La Rosa, Corey Hulse, and Bangdi Liu. Can github issues be solved with tree of thoughts?, 2024. URL https://arxiv.org/abs/2405.13057.   
Lin Shi, Fangwen Mu, Xiao Chen, Song Wang, Junjie Wang, Ye Yang, Ge Li, Xin Xia, and Qing Wang. Are we building on the rock? on the importance of data preprocessing for code summarization. pp. 107–119, 2022.   
Wei Tao, Yucheng Zhou, Yanlin Wang, Wenqiang Zhang, Hongyu Zhang, and Yu Cheng. Magis: Llm-based multi-agent framework for github issue resolution, 2024. URL https://arxiv.org/abs/2403.17927.   
Chunqiu Steven Xia and Lingming Zhang. Less training, more repairing please: Revisiting automated program repair via zero-shot learning, 2022. URL https://arxiv.org/abs/2207.08281.   
Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Automated program repair in the era of large pre-trained language models. In Proceedings of the 45th International Conference on Software Engineering, ICSE ’23, pp. 1482–1494. IEEE Press, 2023. ISBN 9781665457019. doi: 10.1109/ICSE48619.2023.00129. URL https://doi.org/10.1109/ICSE48619.2023.00129.   
Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents, 2024. URL https://arxiv.org/abs/2407.01489.   
Boyang Yang, Haoye Tian, Weiguo Pian, Haoran Yu, Haitao Wang, Jacques Klein, Tegawendé F. Bissyandé, and Shunfu Jin. Cref: An llm-based conversational software repair framework for programming tutors. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2024, pp. 882–894, New York, NY, USA, 2024a. Association for Computing Machinery. ISBN 9798400706127. doi: 10.1145/3650212.3680328. URL https://doi.org/10.1145/3650212.3680328.   
John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering, 2024b. URL https://arxiv.org/abs/2405.15793.   
Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and Jian-Guang Lou. Large language models meet nl2code: A survey, 2023. URL https://arxiv.org/abs/2212.09420.

Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, Dezhi Ran, Muhan Zeng, Bo Shen, Pan Bian, Guangtai Liang, Bei Guan, Pengjie Huang, Tao Xie, Yongji Wang, and Qianxiang Wang. Swe-bench-java: A github issue resolving benchmark for java, 2024. URL https://arxiv.org/abs/2408.14354.   
Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, Bo Pang, Yingbo Zhou, Shelby Heinecke, Silvio Savarese, Huan Wang, and Caiming Xiong. Diversity empowers intelligence: Integrating expertise of software engineering agents, 2024a. URL https://arxiv.org/abs/2408.07060.   
Quanjun Zhang, Tongke Zhang, Juan Zhai, Chunrong Fang, Bowen Yu, Weisong Sun, and Zhenyu Chen. A critical review of large language model on software engineering: An example from chatgpt and automated program repair, 2024b. URL https://arxiv.org/abs/2310.08879.   
Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement, 2024c. URL https://arxiv.org/abs/2404.05427.   
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen. A survey of large language models for code: Evolution, benchmarking, and future trends, 2024. URL https://arxiv.org/abs/2311.10372.